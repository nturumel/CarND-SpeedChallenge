{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"finalopflow.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tQX-8ZXTQK8I","colab_type":"text"},"source":["# **Working with exisiting model without any LSTM**"]},{"cell_type":"code","metadata":{"id":"OSrfh3zWXhsm","colab_type":"code","outputId":"90d2078a-8bcc-48e7-a5bb-1e23977acd39","executionInfo":{"status":"ok","timestamp":1590094101292,"user_tz":360,"elapsed":372153,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednetNoAugment.py /content/train.mp4 /content/train.txt --mode=train --split=0.2 --model  finalSolution.h5 --epoch 100 --history 1 --wipe"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-21 20:42:10.072654: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednetNoAugment.py:174: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), input_shape=(100, 100,..., strides=(4, 4), padding=\"same\")`\n","  self.model.add(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4),input_shape=(self.DSIZE[0],self.DSIZE[1],2)))\n","2020-05-21 20:42:11.999636: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-21 20:42:12.013598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:42:12.014318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-21 20:42:12.014356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-21 20:42:12.016758: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-21 20:42:12.018586: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-21 20:42:12.018914: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-21 20:42:12.020866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-21 20:42:12.022068: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-21 20:42:12.026216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-21 20:42:12.026310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:42:12.027210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:42:12.028147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-21 20:42:12.033604: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199995000 Hz\n","2020-05-21 20:42:12.033954: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1725100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-21 20:42:12.033989: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-21 20:42:12.122021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:42:12.123004: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x17252c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-21 20:42:12.123037: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-21 20:42:12.123202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:42:12.124053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-21 20:42:12.124127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-21 20:42:12.124170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-21 20:42:12.124198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-21 20:42:12.124223: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-21 20:42:12.124247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-21 20:42:12.124270: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-21 20:42:12.124294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-21 20:42:12.124354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:42:12.125085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:42:12.125773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-21 20:42:12.125818: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-21 20:42:12.621050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-21 20:42:12.621104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-21 20:42:12.621114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-21 20:42:12.621418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:42:12.622204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:42:12.623058: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-21 20:42:12.623100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednetNoAugment.py:176: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  self.model.add(Convolution2D(64, 8,8 ,border_mode='same',subsample=(4,4)))\n","speednetNoAugment.py:178: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (4, 4), strides=(2, 2), padding=\"same\")`\n","  self.model.add(Convolution2D(128, 4,4,border_mode='same',subsample=(2,2)))\n","speednetNoAugment.py:180: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (2, 2), strides=(1, 1), padding=\"same\")`\n","  self.model.add(Convolution2D(128, 2,2,border_mode='same',subsample=(1,1)))\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400frames\n","Shuffling data\n","Done prepping data\n","Starting training\n","Train on 16319 samples, validate on 4080 samples\n","Epoch 1/100\n","2020-05-21 20:45:21.119913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-21 20:45:21.315269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","16319/16319 [==============================] - 6s 386us/step - loss: 35.3123 - val_loss: 13.9738\n","\n","Epoch 00001: loss improved from inf to 35.31234, saving model to finalSolution.h5\n","Epoch 2/100\n","16319/16319 [==============================] - 5s 291us/step - loss: 20.0764 - val_loss: 11.1889\n","\n","Epoch 00002: loss improved from 35.31234 to 20.07639, saving model to finalSolution.h5\n","Epoch 3/100\n","16319/16319 [==============================] - 5s 294us/step - loss: 15.8100 - val_loss: 10.9991\n","\n","Epoch 00003: loss improved from 20.07639 to 15.80998, saving model to finalSolution.h5\n","Epoch 4/100\n","16319/16319 [==============================] - 5s 288us/step - loss: 13.2516 - val_loss: 9.4080\n","\n","Epoch 00004: loss improved from 15.80998 to 13.25165, saving model to finalSolution.h5\n","Epoch 5/100\n","16319/16319 [==============================] - 5s 292us/step - loss: 12.0661 - val_loss: 10.8570\n","\n","Epoch 00005: loss improved from 13.25165 to 12.06611, saving model to finalSolution.h5\n","Epoch 6/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 11.2129 - val_loss: 9.2854\n","\n","Epoch 00006: loss improved from 12.06611 to 11.21289, saving model to finalSolution.h5\n","Epoch 7/100\n","16319/16319 [==============================] - 5s 301us/step - loss: 10.6308 - val_loss: 9.8059\n","\n","Epoch 00007: loss improved from 11.21289 to 10.63085, saving model to finalSolution.h5\n","Epoch 8/100\n","16319/16319 [==============================] - 5s 295us/step - loss: 10.3690 - val_loss: 9.1494\n","\n","Epoch 00008: loss improved from 10.63085 to 10.36896, saving model to finalSolution.h5\n","Epoch 9/100\n","16319/16319 [==============================] - 5s 291us/step - loss: 10.0586 - val_loss: 8.7603\n","\n","Epoch 00009: loss improved from 10.36896 to 10.05857, saving model to finalSolution.h5\n","Epoch 10/100\n","16319/16319 [==============================] - 5s 298us/step - loss: 9.8940 - val_loss: 9.0326\n","\n","Epoch 00010: loss improved from 10.05857 to 9.89400, saving model to finalSolution.h5\n","Epoch 11/100\n","16319/16319 [==============================] - 5s 292us/step - loss: 9.2269 - val_loss: 8.3831\n","\n","Epoch 00011: loss improved from 9.89400 to 9.22686, saving model to finalSolution.h5\n","Epoch 12/100\n","16319/16319 [==============================] - 5s 298us/step - loss: 9.0954 - val_loss: 8.2074\n","\n","Epoch 00012: loss improved from 9.22686 to 9.09539, saving model to finalSolution.h5\n","Epoch 13/100\n","16319/16319 [==============================] - 5s 297us/step - loss: 8.8025 - val_loss: 7.9041\n","\n","Epoch 00013: loss improved from 9.09539 to 8.80249, saving model to finalSolution.h5\n","Epoch 14/100\n","16319/16319 [==============================] - 5s 315us/step - loss: 8.7386 - val_loss: 8.4740\n","\n","Epoch 00014: loss improved from 8.80249 to 8.73856, saving model to finalSolution.h5\n","Epoch 15/100\n","16319/16319 [==============================] - 5s 293us/step - loss: 8.1915 - val_loss: 9.3601\n","\n","Epoch 00015: loss improved from 8.73856 to 8.19152, saving model to finalSolution.h5\n","Epoch 16/100\n","16319/16319 [==============================] - 5s 288us/step - loss: 8.2419 - val_loss: 8.5330\n","\n","Epoch 00016: loss did not improve from 8.19152\n","Epoch 17/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 7.7690 - val_loss: 9.6096\n","\n","Epoch 00017: loss improved from 8.19152 to 7.76900, saving model to finalSolution.h5\n","Epoch 18/100\n","16319/16319 [==============================] - 5s 290us/step - loss: 7.8535 - val_loss: 8.4191\n","\n","Epoch 00018: loss did not improve from 7.76900\n","Epoch 19/100\n","16319/16319 [==============================] - 5s 288us/step - loss: 7.5427 - val_loss: 8.0178\n","\n","Epoch 00019: loss improved from 7.76900 to 7.54273, saving model to finalSolution.h5\n","Epoch 20/100\n","16319/16319 [==============================] - 5s 297us/step - loss: 7.6699 - val_loss: 10.5806\n","\n","Epoch 00020: loss did not improve from 7.54273\n","Epoch 21/100\n","16319/16319 [==============================] - 5s 289us/step - loss: 7.5239 - val_loss: 8.2802\n","\n","Epoch 00021: loss improved from 7.54273 to 7.52391, saving model to finalSolution.h5\n","Epoch 22/100\n","16319/16319 [==============================] - 5s 292us/step - loss: 7.5273 - val_loss: 8.2945\n","\n","Epoch 00022: loss did not improve from 7.52391\n","Epoch 23/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 7.0504 - val_loss: 8.3407\n","\n","Epoch 00023: loss improved from 7.52391 to 7.05042, saving model to finalSolution.h5\n","Epoch 24/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 7.0110 - val_loss: 7.9684\n","\n","Epoch 00024: loss improved from 7.05042 to 7.01100, saving model to finalSolution.h5\n","Epoch 25/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 7.1561 - val_loss: 7.5191\n","\n","Epoch 00025: loss did not improve from 7.01100\n","Epoch 26/100\n","16319/16319 [==============================] - 5s 297us/step - loss: 6.8763 - val_loss: 9.0604\n","\n","Epoch 00026: loss improved from 7.01100 to 6.87631, saving model to finalSolution.h5\n","Epoch 27/100\n","16319/16319 [==============================] - 5s 308us/step - loss: 6.7215 - val_loss: 7.5596\n","\n","Epoch 00027: loss improved from 6.87631 to 6.72155, saving model to finalSolution.h5\n","Epoch 28/100\n","16319/16319 [==============================] - 5s 288us/step - loss: 6.4681 - val_loss: 8.1584\n","\n","Epoch 00028: loss improved from 6.72155 to 6.46811, saving model to finalSolution.h5\n","Epoch 29/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 6.4230 - val_loss: 8.2301\n","\n","Epoch 00029: loss improved from 6.46811 to 6.42305, saving model to finalSolution.h5\n","Epoch 30/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 6.0841 - val_loss: 8.4814\n","\n","Epoch 00030: loss improved from 6.42305 to 6.08413, saving model to finalSolution.h5\n","Epoch 31/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 6.1896 - val_loss: 8.9032\n","\n","Epoch 00031: loss did not improve from 6.08413\n","Epoch 32/100\n","16319/16319 [==============================] - 5s 290us/step - loss: 6.2170 - val_loss: 7.7413\n","\n","Epoch 00032: loss did not improve from 6.08413\n","Epoch 33/100\n","16319/16319 [==============================] - 5s 288us/step - loss: 6.4220 - val_loss: 8.4177\n","\n","Epoch 00033: loss did not improve from 6.08413\n","Epoch 34/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 6.4728 - val_loss: 8.2301\n","\n","Epoch 00034: loss did not improve from 6.08413\n","Epoch 35/100\n","16319/16319 [==============================] - 5s 292us/step - loss: 5.8247 - val_loss: 8.3455\n","\n","Epoch 00035: loss improved from 6.08413 to 5.82469, saving model to finalSolution.h5\n","Epoch 36/100\n","16319/16319 [==============================] - 5s 289us/step - loss: 5.9271 - val_loss: 7.9417\n","\n","Epoch 00036: loss did not improve from 5.82469\n","Epoch 37/100\n","16192/16319 [============================>.] - ETA: 0s - loss: 5.8536Traceback (most recent call last):\n","  File \"speednetNoAugment.py\", line 314, in <module>\n","    net.main(args)\n","  File \"speednetNoAugment.py\", line 47, in main\n","    self.train(args.video_file,args.speed_file,args.split,args.wipe,self.EPOCHS,self.BATCH_SIZE,args.augment)\n","  File \"speednetNoAugment.py\", line 237, in train\n","    callbacks=[checkpoint])\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1239, in fit\n","    validation_freq=validation_freq)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\", line 196, in fit_loop\n","    outs = fit_function(ins_batch)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\", line 3792, in __call__\n","    outputs = self._graph_fn(*converted_inputs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1605, in __call__\n","    return self._call_impl(args, kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1645, in _call_impl\n","    return self._call_flat(args, self.captured_inputs, cancellation_manager)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\n","    ctx, args, cancellation_manager=cancellation_manager))\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 598, in call\n","    ctx=ctx)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n","    inputs, attrs, num_outputs)\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8ZS7SVOC_WzT","colab_type":"code","outputId":"57ba9e8a-4304-400a-c249-fa293999d456","executionInfo":{"status":"ok","timestamp":1590094818080,"user_tz":360,"elapsed":670606,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python \"speednet_1.py\" /content/train.mp4 /content/train.txt --mode=train --split=0.2 --wipe"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-21 20:49:08.265132: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","Running SpeedNet by Daniel Nugent\n","Compiling Model\n","speednet_1.py:133: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), input_shape=(100, 100,..., strides=(4, 4), padding=\"same\")`\n","  self.model.add(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4),input_shape=(self.DSIZE[0],self.DSIZE[1],2)))\n","2020-05-21 20:49:10.135240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-21 20:49:10.149102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:49:10.149971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-21 20:49:10.150007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-21 20:49:10.151607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-21 20:49:10.158573: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-21 20:49:10.158878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-21 20:49:10.160452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-21 20:49:10.162082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-21 20:49:10.169417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-21 20:49:10.169541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:49:10.170273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:49:10.170975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-21 20:49:10.176030: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199995000 Hz\n","2020-05-21 20:49:10.176331: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1cb92c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-21 20:49:10.176361: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-21 20:49:10.261425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:49:10.262262: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1cb9480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-21 20:49:10.262294: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-21 20:49:10.262497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:49:10.263490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-21 20:49:10.263536: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-21 20:49:10.263586: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-21 20:49:10.263615: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-21 20:49:10.263650: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-21 20:49:10.263676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-21 20:49:10.263716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-21 20:49:10.263740: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-21 20:49:10.263816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:49:10.264555: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:49:10.265316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-21 20:49:10.265418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-21 20:49:10.772857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-21 20:49:10.772924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-21 20:49:10.772935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-21 20:49:10.773172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:49:10.774221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 20:49:10.775036: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-21 20:49:10.775091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet_1.py:135: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  self.model.add(Convolution2D(64, 8,8 ,border_mode='same',subsample=(4,4)))\n","speednet_1.py:137: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (4, 4), strides=(2, 2), padding=\"same\")`\n","  self.model.add(Convolution2D(128, 4,4,border_mode='same',subsample=(2,2)))\n","speednet_1.py:139: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (2, 2), strides=(1, 1), padding=\"same\")`\n","  self.model.add(Convolution2D(128, 2,2,border_mode='same',subsample=(1,1)))\n","Prepping data\n","Decoding json\n","loaded 20399 json entries\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400frames\n","Shuffling data\n","Done prepping data\n","Starting training\n","Train on 16319 samples, validate on 4080 samples\n","Epoch 1/100\n","2020-05-21 20:52:18.586069: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-21 20:52:18.779329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","16319/16319 [==============================] - 6s 374us/step - loss: 37.1727 - val_loss: 22.6138\n","\n","Epoch 00001: loss improved from inf to 37.17270, saving model to best_model.hdf5\n","Epoch 2/100\n","16319/16319 [==============================] - 5s 299us/step - loss: 19.5285 - val_loss: 19.7020\n","\n","Epoch 00002: loss improved from 37.17270 to 19.52853, saving model to best_model.hdf5\n","Epoch 3/100\n","16319/16319 [==============================] - 5s 297us/step - loss: 15.7805 - val_loss: 11.4397\n","\n","Epoch 00003: loss improved from 19.52853 to 15.78050, saving model to best_model.hdf5\n","Epoch 4/100\n","16319/16319 [==============================] - 5s 303us/step - loss: 12.9147 - val_loss: 10.2298\n","\n","Epoch 00004: loss improved from 15.78050 to 12.91471, saving model to best_model.hdf5\n","Epoch 5/100\n","16319/16319 [==============================] - 5s 303us/step - loss: 11.7245 - val_loss: 13.4933\n","\n","Epoch 00005: loss improved from 12.91471 to 11.72450, saving model to best_model.hdf5\n","Epoch 6/100\n","16319/16319 [==============================] - 5s 295us/step - loss: 10.9233 - val_loss: 8.6830\n","\n","Epoch 00006: loss improved from 11.72450 to 10.92334, saving model to best_model.hdf5\n","Epoch 7/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 10.2704 - val_loss: 9.3563\n","\n","Epoch 00007: loss improved from 10.92334 to 10.27044, saving model to best_model.hdf5\n","Epoch 8/100\n","16319/16319 [==============================] - 5s 285us/step - loss: 9.5892 - val_loss: 8.8877\n","\n","Epoch 00008: loss improved from 10.27044 to 9.58921, saving model to best_model.hdf5\n","Epoch 9/100\n","16319/16319 [==============================] - 5s 277us/step - loss: 9.6308 - val_loss: 8.8847\n","\n","Epoch 00009: loss did not improve from 9.58921\n","Epoch 10/100\n","16319/16319 [==============================] - 5s 288us/step - loss: 9.3459 - val_loss: 9.8812\n","\n","Epoch 00010: loss improved from 9.58921 to 9.34587, saving model to best_model.hdf5\n","Epoch 11/100\n","16319/16319 [==============================] - 5s 293us/step - loss: 8.8351 - val_loss: 8.1787\n","\n","Epoch 00011: loss improved from 9.34587 to 8.83505, saving model to best_model.hdf5\n","Epoch 12/100\n","16319/16319 [==============================] - 5s 288us/step - loss: 9.4171 - val_loss: 8.2252\n","\n","Epoch 00012: loss did not improve from 8.83505\n","Epoch 13/100\n","16319/16319 [==============================] - 5s 284us/step - loss: 8.9170 - val_loss: 9.5125\n","\n","Epoch 00013: loss did not improve from 8.83505\n","Epoch 14/100\n","16319/16319 [==============================] - 5s 284us/step - loss: 9.0983 - val_loss: 7.8487\n","\n","Epoch 00014: loss did not improve from 8.83505\n","Epoch 15/100\n","16319/16319 [==============================] - 5s 294us/step - loss: 8.2439 - val_loss: 8.3619\n","\n","Epoch 00015: loss improved from 8.83505 to 8.24387, saving model to best_model.hdf5\n","Epoch 16/100\n","16319/16319 [==============================] - 5s 291us/step - loss: 8.1339 - val_loss: 8.3504\n","\n","Epoch 00016: loss improved from 8.24387 to 8.13386, saving model to best_model.hdf5\n","Epoch 17/100\n","16319/16319 [==============================] - 5s 291us/step - loss: 7.8592 - val_loss: 8.2822\n","\n","Epoch 00017: loss improved from 8.13386 to 7.85915, saving model to best_model.hdf5\n","Epoch 18/100\n","16319/16319 [==============================] - 5s 279us/step - loss: 7.6477 - val_loss: 7.5847\n","\n","Epoch 00018: loss improved from 7.85915 to 7.64766, saving model to best_model.hdf5\n","Epoch 19/100\n","16319/16319 [==============================] - 5s 284us/step - loss: 7.5986 - val_loss: 7.9035\n","\n","Epoch 00019: loss improved from 7.64766 to 7.59855, saving model to best_model.hdf5\n","Epoch 20/100\n","16319/16319 [==============================] - 5s 289us/step - loss: 7.3866 - val_loss: 8.6264\n","\n","Epoch 00020: loss improved from 7.59855 to 7.38656, saving model to best_model.hdf5\n","Epoch 21/100\n","16319/16319 [==============================] - 5s 294us/step - loss: 7.1319 - val_loss: 9.5528\n","\n","Epoch 00021: loss improved from 7.38656 to 7.13193, saving model to best_model.hdf5\n","Epoch 22/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 6.9625 - val_loss: 8.3744\n","\n","Epoch 00022: loss improved from 7.13193 to 6.96253, saving model to best_model.hdf5\n","Epoch 23/100\n","16319/16319 [==============================] - 5s 289us/step - loss: 7.0910 - val_loss: 8.7021\n","\n","Epoch 00023: loss did not improve from 6.96253\n","Epoch 24/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 7.0653 - val_loss: 7.6176\n","\n","Epoch 00024: loss did not improve from 6.96253\n","Epoch 25/100\n","16319/16319 [==============================] - 5s 293us/step - loss: 6.9795 - val_loss: 7.4919\n","\n","Epoch 00025: loss did not improve from 6.96253\n","Epoch 26/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 6.6788 - val_loss: 7.3146\n","\n","Epoch 00026: loss improved from 6.96253 to 6.67878, saving model to best_model.hdf5\n","Epoch 27/100\n","16319/16319 [==============================] - 5s 284us/step - loss: 6.9094 - val_loss: 8.1221\n","\n","Epoch 00027: loss did not improve from 6.67878\n","Epoch 28/100\n","16319/16319 [==============================] - 5s 282us/step - loss: 6.2982 - val_loss: 8.0913\n","\n","Epoch 00028: loss improved from 6.67878 to 6.29824, saving model to best_model.hdf5\n","Epoch 29/100\n","16319/16319 [==============================] - 5s 284us/step - loss: 6.3264 - val_loss: 8.2243\n","\n","Epoch 00029: loss did not improve from 6.29824\n","Epoch 30/100\n","16319/16319 [==============================] - 5s 277us/step - loss: 6.2740 - val_loss: 7.8989\n","\n","Epoch 00030: loss improved from 6.29824 to 6.27397, saving model to best_model.hdf5\n","Epoch 31/100\n","16319/16319 [==============================] - 5s 285us/step - loss: 6.5173 - val_loss: 7.8940\n","\n","Epoch 00031: loss did not improve from 6.27397\n","Epoch 32/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 6.1597 - val_loss: 7.5276\n","\n","Epoch 00032: loss improved from 6.27397 to 6.15974, saving model to best_model.hdf5\n","Epoch 33/100\n","16319/16319 [==============================] - 5s 282us/step - loss: 6.0702 - val_loss: 6.3232\n","\n","Epoch 00033: loss improved from 6.15974 to 6.07019, saving model to best_model.hdf5\n","Epoch 34/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 5.9622 - val_loss: 8.2898\n","\n","Epoch 00034: loss improved from 6.07019 to 5.96217, saving model to best_model.hdf5\n","Epoch 35/100\n","16319/16319 [==============================] - 5s 284us/step - loss: 5.8042 - val_loss: 7.4241\n","\n","Epoch 00035: loss improved from 5.96217 to 5.80415, saving model to best_model.hdf5\n","Epoch 36/100\n","16319/16319 [==============================] - 5s 291us/step - loss: 6.0255 - val_loss: 7.4521\n","\n","Epoch 00036: loss did not improve from 5.80415\n","Epoch 37/100\n","16319/16319 [==============================] - 5s 288us/step - loss: 5.6532 - val_loss: 7.1666\n","\n","Epoch 00037: loss improved from 5.80415 to 5.65318, saving model to best_model.hdf5\n","Epoch 38/100\n","16319/16319 [==============================] - 5s 283us/step - loss: 5.7036 - val_loss: 7.9873\n","\n","Epoch 00038: loss did not improve from 5.65318\n","Epoch 39/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 6.0713 - val_loss: 7.4271\n","\n","Epoch 00039: loss did not improve from 5.65318\n","Epoch 40/100\n","16319/16319 [==============================] - 5s 283us/step - loss: 5.6950 - val_loss: 6.7012\n","\n","Epoch 00040: loss did not improve from 5.65318\n","Epoch 41/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 5.9668 - val_loss: 7.2241\n","\n","Epoch 00041: loss did not improve from 5.65318\n","Epoch 42/100\n","16319/16319 [==============================] - 5s 278us/step - loss: 5.6348 - val_loss: 7.4762\n","\n","Epoch 00042: loss improved from 5.65318 to 5.63481, saving model to best_model.hdf5\n","Epoch 43/100\n","16319/16319 [==============================] - 5s 285us/step - loss: 5.4868 - val_loss: 6.8367\n","\n","Epoch 00043: loss improved from 5.63481 to 5.48680, saving model to best_model.hdf5\n","Epoch 44/100\n","16319/16319 [==============================] - 5s 290us/step - loss: 5.3791 - val_loss: 7.5753\n","\n","Epoch 00044: loss improved from 5.48680 to 5.37910, saving model to best_model.hdf5\n","Epoch 45/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 5.2302 - val_loss: 8.5630\n","\n","Epoch 00045: loss improved from 5.37910 to 5.23023, saving model to best_model.hdf5\n","Epoch 46/100\n","16319/16319 [==============================] - 5s 282us/step - loss: 5.0689 - val_loss: 7.2428\n","\n","Epoch 00046: loss improved from 5.23023 to 5.06888, saving model to best_model.hdf5\n","Epoch 47/100\n","16319/16319 [==============================] - 5s 283us/step - loss: 5.4028 - val_loss: 6.9929\n","\n","Epoch 00047: loss did not improve from 5.06888\n","Epoch 48/100\n","16319/16319 [==============================] - 5s 279us/step - loss: 5.4022 - val_loss: 8.3009\n","\n","Epoch 00048: loss did not improve from 5.06888\n","Epoch 49/100\n","16319/16319 [==============================] - 5s 292us/step - loss: 5.4488 - val_loss: 8.0962\n","\n","Epoch 00049: loss did not improve from 5.06888\n","Epoch 50/100\n","16319/16319 [==============================] - 5s 288us/step - loss: 5.5853 - val_loss: 7.9122\n","\n","Epoch 00050: loss did not improve from 5.06888\n","Epoch 51/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 5.4819 - val_loss: 7.8283\n","\n","Epoch 00051: loss did not improve from 5.06888\n","Epoch 52/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 5.0337 - val_loss: 8.8665\n","\n","Epoch 00052: loss improved from 5.06888 to 5.03366, saving model to best_model.hdf5\n","Epoch 53/100\n","16319/16319 [==============================] - 5s 284us/step - loss: 5.3995 - val_loss: 8.0202\n","\n","Epoch 00053: loss did not improve from 5.03366\n","Epoch 54/100\n","16319/16319 [==============================] - 5s 289us/step - loss: 5.6299 - val_loss: 7.6414\n","\n","Epoch 00054: loss did not improve from 5.03366\n","Epoch 55/100\n","16319/16319 [==============================] - 5s 297us/step - loss: 5.0597 - val_loss: 7.1824\n","\n","Epoch 00055: loss did not improve from 5.03366\n","Epoch 56/100\n","16319/16319 [==============================] - 5s 297us/step - loss: 5.4938 - val_loss: 7.4630\n","\n","Epoch 00056: loss did not improve from 5.03366\n","Epoch 57/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 4.9270 - val_loss: 7.8441\n","\n","Epoch 00057: loss improved from 5.03366 to 4.92697, saving model to best_model.hdf5\n","Epoch 58/100\n","16319/16319 [==============================] - 5s 291us/step - loss: 5.0009 - val_loss: 8.1552\n","\n","Epoch 00058: loss did not improve from 4.92697\n","Epoch 59/100\n","16319/16319 [==============================] - 5s 289us/step - loss: 4.9863 - val_loss: 7.4183\n","\n","Epoch 00059: loss did not improve from 4.92697\n","Epoch 60/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 5.1072 - val_loss: 7.2021\n","\n","Epoch 00060: loss did not improve from 4.92697\n","Epoch 61/100\n","16319/16319 [==============================] - 5s 288us/step - loss: 4.7734 - val_loss: 7.4280\n","\n","Epoch 00061: loss improved from 4.92697 to 4.77340, saving model to best_model.hdf5\n","Epoch 62/100\n","16319/16319 [==============================] - 5s 282us/step - loss: 4.9422 - val_loss: 7.4488\n","\n","Epoch 00062: loss did not improve from 4.77340\n","Epoch 63/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 4.9328 - val_loss: 7.3015\n","\n","Epoch 00063: loss did not improve from 4.77340\n","Epoch 64/100\n","16319/16319 [==============================] - 5s 282us/step - loss: 4.8668 - val_loss: 7.9427\n","\n","Epoch 00064: loss did not improve from 4.77340\n","Epoch 65/100\n","16319/16319 [==============================] - 5s 289us/step - loss: 4.7881 - val_loss: 7.4349\n","\n","Epoch 00065: loss did not improve from 4.77340\n","Epoch 66/100\n","16319/16319 [==============================] - 5s 283us/step - loss: 5.1330 - val_loss: 8.8082\n","\n","Epoch 00066: loss did not improve from 4.77340\n","Epoch 67/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 5.1674 - val_loss: 8.6504\n","\n","Epoch 00067: loss did not improve from 4.77340\n","Epoch 68/100\n","16319/16319 [==============================] - 5s 283us/step - loss: 5.0462 - val_loss: 7.2251\n","\n","Epoch 00068: loss did not improve from 4.77340\n","Epoch 69/100\n","16319/16319 [==============================] - 5s 291us/step - loss: 4.7459 - val_loss: 7.3785\n","\n","Epoch 00069: loss improved from 4.77340 to 4.74592, saving model to best_model.hdf5\n","Epoch 70/100\n","16319/16319 [==============================] - 5s 301us/step - loss: 4.6892 - val_loss: 8.5740\n","\n","Epoch 00070: loss improved from 4.74592 to 4.68921, saving model to best_model.hdf5\n","Epoch 71/100\n","16319/16319 [==============================] - 5s 308us/step - loss: 4.7048 - val_loss: 6.9197\n","\n","Epoch 00071: loss did not improve from 4.68921\n","Epoch 72/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 4.6849 - val_loss: 8.7258\n","\n","Epoch 00072: loss improved from 4.68921 to 4.68493, saving model to best_model.hdf5\n","Epoch 73/100\n","16319/16319 [==============================] - 5s 290us/step - loss: 4.7916 - val_loss: 8.1682\n","\n","Epoch 00073: loss did not improve from 4.68493\n","Epoch 74/100\n","16319/16319 [==============================] - 5s 290us/step - loss: 4.6664 - val_loss: 8.1922\n","\n","Epoch 00074: loss improved from 4.68493 to 4.66642, saving model to best_model.hdf5\n","Epoch 75/100\n","16319/16319 [==============================] - 5s 292us/step - loss: 4.5814 - val_loss: 8.1870\n","\n","Epoch 00075: loss improved from 4.66642 to 4.58135, saving model to best_model.hdf5\n","Epoch 76/100\n","16319/16319 [==============================] - 5s 289us/step - loss: 4.3666 - val_loss: 8.2765\n","\n","Epoch 00076: loss improved from 4.58135 to 4.36656, saving model to best_model.hdf5\n","Epoch 77/100\n","16319/16319 [==============================] - 5s 292us/step - loss: 6.3512 - val_loss: 12.4121\n","\n","Epoch 00077: loss did not improve from 4.36656\n","Epoch 78/100\n","16319/16319 [==============================] - 5s 291us/step - loss: 5.7979 - val_loss: 8.5028\n","\n","Epoch 00078: loss did not improve from 4.36656\n","Epoch 79/100\n","16319/16319 [==============================] - 5s 294us/step - loss: 4.5142 - val_loss: 8.1282\n","\n","Epoch 00079: loss did not improve from 4.36656\n","Epoch 80/100\n","16319/16319 [==============================] - 5s 291us/step - loss: 4.6683 - val_loss: 7.4411\n","\n","Epoch 00080: loss did not improve from 4.36656\n","Epoch 81/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 4.5125 - val_loss: 7.9666\n","\n","Epoch 00081: loss did not improve from 4.36656\n","Epoch 82/100\n","16319/16319 [==============================] - 5s 282us/step - loss: 4.7407 - val_loss: 9.3664\n","\n","Epoch 00082: loss did not improve from 4.36656\n","Epoch 83/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 4.6155 - val_loss: 9.0976\n","\n","Epoch 00083: loss did not improve from 4.36656\n","Epoch 84/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 4.5223 - val_loss: 10.4160\n","\n","Epoch 00084: loss did not improve from 4.36656\n","Epoch 85/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 4.6526 - val_loss: 8.4017\n","\n","Epoch 00085: loss did not improve from 4.36656\n","Epoch 86/100\n","16319/16319 [==============================] - 5s 291us/step - loss: 4.4383 - val_loss: 8.7724\n","\n","Epoch 00086: loss did not improve from 4.36656\n","Epoch 87/100\n","16319/16319 [==============================] - 5s 279us/step - loss: 4.5311 - val_loss: 9.1380\n","\n","Epoch 00087: loss did not improve from 4.36656\n","Epoch 88/100\n","16319/16319 [==============================] - 5s 288us/step - loss: 4.2525 - val_loss: 8.2584\n","\n","Epoch 00088: loss improved from 4.36656 to 4.25245, saving model to best_model.hdf5\n","Epoch 89/100\n","16319/16319 [==============================] - 5s 292us/step - loss: 4.4848 - val_loss: 8.0923\n","\n","Epoch 00089: loss did not improve from 4.25245\n","Epoch 90/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 4.7799 - val_loss: 7.9258\n","\n","Epoch 00090: loss did not improve from 4.25245\n","Epoch 91/100\n","16319/16319 [==============================] - 5s 282us/step - loss: 4.6635 - val_loss: 8.1989\n","\n","Epoch 00091: loss did not improve from 4.25245\n","Epoch 92/100\n","16319/16319 [==============================] - 5s 283us/step - loss: 4.5428 - val_loss: 10.2903\n","\n","Epoch 00092: loss did not improve from 4.25245\n","Epoch 93/100\n","16319/16319 [==============================] - 5s 295us/step - loss: 4.6826 - val_loss: 7.4479\n","\n","Epoch 00093: loss did not improve from 4.25245\n","Epoch 94/100\n","16319/16319 [==============================] - 5s 290us/step - loss: 4.4985 - val_loss: 7.6156\n","\n","Epoch 00094: loss did not improve from 4.25245\n","Epoch 95/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 4.6126 - val_loss: 7.7350\n","\n","Epoch 00095: loss did not improve from 4.25245\n","Epoch 96/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 4.5134 - val_loss: 7.7202\n","\n","Epoch 00096: loss did not improve from 4.25245\n","Epoch 97/100\n","16319/16319 [==============================] - 5s 284us/step - loss: 4.6335 - val_loss: 7.6488\n","\n","Epoch 00097: loss did not improve from 4.25245\n","Epoch 98/100\n","16319/16319 [==============================] - 5s 283us/step - loss: 4.4381 - val_loss: 8.4118\n","\n","Epoch 00098: loss did not improve from 4.25245\n","Epoch 99/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 4.2822 - val_loss: 8.1183\n","\n","Epoch 00099: loss did not improve from 4.25245\n","Epoch 100/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 4.4706 - val_loss: 7.5357\n","\n","Epoch 00100: loss did not improve from 4.25245\n","Done training. Saving weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"46YpcFGPcAC0","colab_type":"code","colab":{}},"source":["!rm -rf /content/*_optflowProcess\n","!rm -rf /content/*_optflowProcess_augmented"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zl2SDK5-Cidf","colab_type":"code","outputId":"485dc58f-3cce-45a0-f0a5-6e05a8b8841a","executionInfo":{"status":"ok","timestamp":1590095764467,"user_tz":360,"elapsed":834850,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet.py /content/train.mp4 /content/train.txt --mode=train --split=0.2 --model  finalSolution.h5 --epoch 100 --history 1 --wipe "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-21 21:02:10.482193: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet.py:184: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), input_shape=(100, 100,..., strides=(4, 4), padding=\"same\")`\n","  self.model.add(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4),input_shape=(self.DSIZE[0],self.DSIZE[1],2)))\n","2020-05-21 21:02:12.408674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-21 21:02:12.423131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:02:12.424016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-21 21:02:12.424051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-21 21:02:12.426167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-21 21:02:12.428036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-21 21:02:12.428447: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-21 21:02:12.430403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-21 21:02:12.431646: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-21 21:02:12.435653: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-21 21:02:12.435743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:02:12.436501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:02:12.437232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-21 21:02:12.442172: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199995000 Hz\n","2020-05-21 21:02:12.442562: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2f61100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-21 21:02:12.442598: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-21 21:02:12.534016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:02:12.535117: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2f612c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-21 21:02:12.535152: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-21 21:02:12.535403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:02:12.536145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-21 21:02:12.536192: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-21 21:02:12.536247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-21 21:02:12.536307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-21 21:02:12.536348: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-21 21:02:12.536412: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-21 21:02:12.536464: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-21 21:02:12.536509: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-21 21:02:12.536607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:02:12.537303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:02:12.537997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-21 21:02:12.538049: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-21 21:02:13.020846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-21 21:02:13.020904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-21 21:02:13.020921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-21 21:02:13.021157: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:02:13.021941: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:02:13.022643: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-21 21:02:13.022698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet.py:186: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  self.model.add(Convolution2D(64, 8,8 ,border_mode='same',subsample=(4,4)))\n","speednet.py:188: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (4, 4), strides=(2, 2), padding=\"same\")`\n","  self.model.add(Convolution2D(128, 4,4,border_mode='same',subsample=(2,2)))\n","speednet.py:190: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (2, 2), strides=(1, 1), padding=\"same\")`\n","  self.model.add(Convolution2D(128, 2,2,border_mode='same',subsample=(1,1)))\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400frames\n","Shuffling data\n","Done prepping data\n","Starting training\n","Train on 16319 samples, validate on 4080 samples\n","Epoch 1/100\n","2020-05-21 21:08:25.168526: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-21 21:08:25.371041: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","16319/16319 [==============================] - 6s 371us/step - loss: 37.1552 - val_loss: 18.1472\n","\n","Epoch 00001: loss improved from inf to 37.15516, saving model to finalSolution.h5\n","Epoch 2/100\n","16319/16319 [==============================] - 5s 286us/step - loss: 20.3791 - val_loss: 11.5773\n","\n","Epoch 00002: loss improved from 37.15516 to 20.37907, saving model to finalSolution.h5\n","Epoch 3/100\n","16319/16319 [==============================] - 5s 278us/step - loss: 15.5974 - val_loss: 11.4041\n","\n","Epoch 00003: loss improved from 20.37907 to 15.59737, saving model to finalSolution.h5\n","Epoch 4/100\n","16319/16319 [==============================] - 5s 280us/step - loss: 13.0680 - val_loss: 9.8149\n","\n","Epoch 00004: loss improved from 15.59737 to 13.06802, saving model to finalSolution.h5\n","Epoch 5/100\n","16319/16319 [==============================] - 5s 291us/step - loss: 11.6950 - val_loss: 9.3579\n","\n","Epoch 00005: loss improved from 13.06802 to 11.69500, saving model to finalSolution.h5\n","Epoch 6/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 11.3133 - val_loss: 11.1457\n","\n","Epoch 00006: loss improved from 11.69500 to 11.31327, saving model to finalSolution.h5\n","Epoch 7/100\n","16319/16319 [==============================] - 4s 275us/step - loss: 10.7369 - val_loss: 9.8059\n","\n","Epoch 00007: loss improved from 11.31327 to 10.73689, saving model to finalSolution.h5\n","Epoch 8/100\n","16319/16319 [==============================] - 5s 279us/step - loss: 10.5690 - val_loss: 9.2160\n","\n","Epoch 00008: loss improved from 10.73689 to 10.56905, saving model to finalSolution.h5\n","Epoch 9/100\n","16319/16319 [==============================] - 5s 282us/step - loss: 9.5713 - val_loss: 9.6957\n","\n","Epoch 00009: loss improved from 10.56905 to 9.57128, saving model to finalSolution.h5\n","Epoch 10/100\n","16319/16319 [==============================] - 5s 283us/step - loss: 9.8399 - val_loss: 9.4241\n","\n","Epoch 00010: loss did not improve from 9.57128\n","Epoch 11/100\n","16319/16319 [==============================] - 5s 278us/step - loss: 9.1594 - val_loss: 9.2532\n","\n","Epoch 00011: loss improved from 9.57128 to 9.15942, saving model to finalSolution.h5\n","Epoch 12/100\n","16319/16319 [==============================] - 4s 273us/step - loss: 9.2472 - val_loss: 8.6004\n","\n","Epoch 00012: loss did not improve from 9.15942\n","Epoch 13/100\n","16319/16319 [==============================] - 4s 276us/step - loss: 9.0895 - val_loss: 9.7798\n","\n","Epoch 00013: loss improved from 9.15942 to 9.08953, saving model to finalSolution.h5\n","Epoch 14/100\n","16319/16319 [==============================] - 5s 278us/step - loss: 9.0147 - val_loss: 9.2231\n","\n","Epoch 00014: loss improved from 9.08953 to 9.01471, saving model to finalSolution.h5\n","Epoch 15/100\n","16319/16319 [==============================] - 5s 281us/step - loss: 8.6507 - val_loss: 8.7841\n","\n","Epoch 00015: loss improved from 9.01471 to 8.65069, saving model to finalSolution.h5\n","Epoch 16/100\n","16319/16319 [==============================] - 4s 275us/step - loss: 8.4310 - val_loss: 8.6159\n","\n","Epoch 00016: loss improved from 8.65069 to 8.43097, saving model to finalSolution.h5\n","Epoch 17/100\n","16319/16319 [==============================] - 5s 280us/step - loss: 8.1285 - val_loss: 8.2620\n","\n","Epoch 00017: loss improved from 8.43097 to 8.12851, saving model to finalSolution.h5\n","Epoch 18/100\n","16319/16319 [==============================] - 4s 275us/step - loss: 7.8053 - val_loss: 7.9620\n","\n","Epoch 00018: loss improved from 8.12851 to 7.80526, saving model to finalSolution.h5\n","Epoch 19/100\n","16319/16319 [==============================] - 5s 285us/step - loss: 7.9375 - val_loss: 8.3213\n","\n","Epoch 00019: loss did not improve from 7.80526\n","Epoch 20/100\n","16319/16319 [==============================] - 4s 274us/step - loss: 7.8081 - val_loss: 8.2794\n","\n","Epoch 00020: loss did not improve from 7.80526\n","Epoch 21/100\n","16319/16319 [==============================] - 5s 276us/step - loss: 7.5229 - val_loss: 8.2881\n","\n","Epoch 00021: loss improved from 7.80526 to 7.52293, saving model to finalSolution.h5\n","Epoch 22/100\n","16319/16319 [==============================] - 5s 280us/step - loss: 7.7141 - val_loss: 7.8394\n","\n","Epoch 00022: loss did not improve from 7.52293\n","Epoch 23/100\n","16319/16319 [==============================] - 5s 281us/step - loss: 7.5367 - val_loss: 7.5401\n","\n","Epoch 00023: loss did not improve from 7.52293\n","Epoch 24/100\n","16319/16319 [==============================] - 5s 278us/step - loss: 7.1399 - val_loss: 7.6070\n","\n","Epoch 00024: loss improved from 7.52293 to 7.13989, saving model to finalSolution.h5\n","Epoch 25/100\n","16319/16319 [==============================] - 5s 281us/step - loss: 6.7941 - val_loss: 8.3766\n","\n","Epoch 00025: loss improved from 7.13989 to 6.79415, saving model to finalSolution.h5\n","Epoch 26/100\n","16319/16319 [==============================] - 5s 280us/step - loss: 6.8541 - val_loss: 8.4753\n","\n","Epoch 00026: loss did not improve from 6.79415\n","Epoch 27/100\n","16319/16319 [==============================] - 5s 279us/step - loss: 6.5962 - val_loss: 8.1004\n","\n","Epoch 00027: loss improved from 6.79415 to 6.59616, saving model to finalSolution.h5\n","Epoch 28/100\n","16319/16319 [==============================] - 5s 284us/step - loss: 6.5758 - val_loss: 7.7427\n","\n","Epoch 00028: loss improved from 6.59616 to 6.57579, saving model to finalSolution.h5\n","Epoch 29/100\n","16319/16319 [==============================] - 5s 277us/step - loss: 6.5928 - val_loss: 8.4882\n","\n","Epoch 00029: loss did not improve from 6.57579\n","Epoch 30/100\n","16319/16319 [==============================] - 4s 272us/step - loss: 6.7307 - val_loss: 7.6621\n","\n","Epoch 00030: loss did not improve from 6.57579\n","Epoch 31/100\n","16319/16319 [==============================] - 5s 280us/step - loss: 6.8144 - val_loss: 8.6055\n","\n","Epoch 00031: loss did not improve from 6.57579\n","Epoch 32/100\n","16319/16319 [==============================] - 5s 281us/step - loss: 6.5475 - val_loss: 7.8743\n","\n","Epoch 00032: loss improved from 6.57579 to 6.54745, saving model to finalSolution.h5\n","Epoch 33/100\n","16319/16319 [==============================] - 5s 279us/step - loss: 6.4450 - val_loss: 7.8956\n","\n","Epoch 00033: loss improved from 6.54745 to 6.44500, saving model to finalSolution.h5\n","Epoch 34/100\n","16319/16319 [==============================] - 5s 277us/step - loss: 6.2997 - val_loss: 7.4987\n","\n","Epoch 00034: loss improved from 6.44500 to 6.29974, saving model to finalSolution.h5\n","Epoch 35/100\n","16319/16319 [==============================] - 5s 282us/step - loss: 6.1482 - val_loss: 7.8496\n","\n","Epoch 00035: loss improved from 6.29974 to 6.14820, saving model to finalSolution.h5\n","Epoch 36/100\n","16319/16319 [==============================] - 5s 278us/step - loss: 6.0004 - val_loss: 8.2408\n","\n","Epoch 00036: loss improved from 6.14820 to 6.00041, saving model to finalSolution.h5\n","Epoch 37/100\n","16319/16319 [==============================] - 5s 280us/step - loss: 6.0753 - val_loss: 7.7706\n","\n","Epoch 00037: loss did not improve from 6.00041\n","Epoch 38/100\n","16319/16319 [==============================] - 5s 278us/step - loss: 6.4173 - val_loss: 7.8420\n","\n","Epoch 00038: loss did not improve from 6.00041\n","Epoch 39/100\n","16319/16319 [==============================] - 4s 273us/step - loss: 6.0302 - val_loss: 7.6755\n","\n","Epoch 00039: loss did not improve from 6.00041\n","Epoch 40/100\n","16319/16319 [==============================] - 5s 284us/step - loss: 6.2862 - val_loss: 7.4526\n","\n","Epoch 00040: loss did not improve from 6.00041\n","Epoch 41/100\n","16319/16319 [==============================] - 5s 282us/step - loss: 5.7606 - val_loss: 8.3142\n","\n","Epoch 00041: loss improved from 6.00041 to 5.76062, saving model to finalSolution.h5\n","Epoch 42/100\n","16319/16319 [==============================] - 4s 274us/step - loss: 5.9079 - val_loss: 8.0031\n","\n","Epoch 00042: loss did not improve from 5.76062\n","Epoch 43/100\n","16319/16319 [==============================] - 5s 277us/step - loss: 5.7982 - val_loss: 7.8842\n","\n","Epoch 00043: loss did not improve from 5.76062\n","Epoch 44/100\n","16319/16319 [==============================] - 5s 276us/step - loss: 5.6324 - val_loss: 8.1095\n","\n","Epoch 00044: loss improved from 5.76062 to 5.63243, saving model to finalSolution.h5\n","Epoch 45/100\n","16319/16319 [==============================] - 5s 280us/step - loss: 5.6401 - val_loss: 7.1453\n","\n","Epoch 00045: loss did not improve from 5.63243\n","Epoch 46/100\n","16319/16319 [==============================] - 5s 299us/step - loss: 5.5755 - val_loss: 7.6427\n","\n","Epoch 00046: loss improved from 5.63243 to 5.57546, saving model to finalSolution.h5\n","Epoch 47/100\n","16319/16319 [==============================] - 5s 277us/step - loss: 5.5907 - val_loss: 7.2473\n","\n","Epoch 00047: loss did not improve from 5.57546\n","Epoch 48/100\n","16319/16319 [==============================] - 4s 275us/step - loss: 5.3456 - val_loss: 7.7760\n","\n","Epoch 00048: loss improved from 5.57546 to 5.34559, saving model to finalSolution.h5\n","Epoch 49/100\n","16319/16319 [==============================] - 5s 282us/step - loss: 5.3256 - val_loss: 10.0993\n","\n","Epoch 00049: loss improved from 5.34559 to 5.32556, saving model to finalSolution.h5\n","Epoch 50/100\n","16319/16319 [==============================] - 4s 276us/step - loss: 5.5394 - val_loss: 8.1201\n","\n","Epoch 00050: loss did not improve from 5.32556\n","Epoch 51/100\n","16319/16319 [==============================] - 4s 273us/step - loss: 5.6660 - val_loss: 8.4140\n","\n","Epoch 00051: loss did not improve from 5.32556\n","Epoch 52/100\n","16319/16319 [==============================] - 4s 268us/step - loss: 5.4545 - val_loss: 7.4831\n","\n","Epoch 00052: loss did not improve from 5.32556\n","Epoch 53/100\n","16319/16319 [==============================] - 5s 282us/step - loss: 5.2841 - val_loss: 8.1372\n","\n","Epoch 00053: loss improved from 5.32556 to 5.28412, saving model to finalSolution.h5\n","Epoch 54/100\n","16319/16319 [==============================] - 5s 277us/step - loss: 5.3346 - val_loss: 7.0826\n","\n","Epoch 00054: loss did not improve from 5.28412\n","Epoch 55/100\n","16319/16319 [==============================] - 5s 280us/step - loss: 5.4138 - val_loss: 8.1334\n","\n","Epoch 00055: loss did not improve from 5.28412\n","Epoch 56/100\n","16319/16319 [==============================] - 4s 271us/step - loss: 5.5976 - val_loss: 8.0830\n","\n","Epoch 00056: loss did not improve from 5.28412\n","Epoch 57/100\n","16319/16319 [==============================] - 5s 282us/step - loss: 5.2262 - val_loss: 16.1491\n","\n","Epoch 00057: loss improved from 5.28412 to 5.22625, saving model to finalSolution.h5\n","Epoch 58/100\n","16319/16319 [==============================] - 5s 277us/step - loss: 5.8104 - val_loss: 8.0193\n","\n","Epoch 00058: loss did not improve from 5.22625\n","Epoch 59/100\n","16319/16319 [==============================] - 5s 281us/step - loss: 5.1721 - val_loss: 6.7124\n","\n","Epoch 00059: loss improved from 5.22625 to 5.17208, saving model to finalSolution.h5\n","Epoch 60/100\n","16319/16319 [==============================] - 5s 278us/step - loss: 5.3141 - val_loss: 7.8996\n","\n","Epoch 00060: loss did not improve from 5.17208\n","Epoch 61/100\n","16319/16319 [==============================] - 5s 278us/step - loss: 5.2238 - val_loss: 6.9742\n","\n","Epoch 00061: loss did not improve from 5.17208\n","Epoch 62/100\n","16319/16319 [==============================] - 5s 277us/step - loss: 5.1907 - val_loss: 7.3060\n","\n","Epoch 00062: loss did not improve from 5.17208\n","Epoch 63/100\n","16319/16319 [==============================] - 4s 274us/step - loss: 5.2457 - val_loss: 8.2130\n","\n","Epoch 00063: loss did not improve from 5.17208\n","Epoch 64/100\n","16319/16319 [==============================] - 5s 285us/step - loss: 5.1878 - val_loss: 7.5588\n","\n","Epoch 00064: loss did not improve from 5.17208\n","Epoch 65/100\n","16319/16319 [==============================] - 5s 296us/step - loss: 5.0634 - val_loss: 7.4477\n","\n","Epoch 00065: loss improved from 5.17208 to 5.06339, saving model to finalSolution.h5\n","Epoch 66/100\n","16319/16319 [==============================] - 5s 291us/step - loss: 4.8603 - val_loss: 7.5883\n","\n","Epoch 00066: loss improved from 5.06339 to 4.86034, saving model to finalSolution.h5\n","Epoch 67/100\n","16319/16319 [==============================] - 5s 277us/step - loss: 4.9955 - val_loss: 7.9195\n","\n","Epoch 00067: loss did not improve from 4.86034\n","Epoch 68/100\n","16319/16319 [==============================] - 5s 277us/step - loss: 5.2688 - val_loss: 8.0604\n","\n","Epoch 00068: loss did not improve from 4.86034\n","Epoch 69/100\n","16319/16319 [==============================] - 4s 273us/step - loss: 5.3365 - val_loss: 7.1465\n","\n","Epoch 00069: loss did not improve from 4.86034\n","Epoch 70/100\n","16319/16319 [==============================] - 4s 273us/step - loss: 4.9581 - val_loss: 7.3300\n","\n","Epoch 00070: loss did not improve from 4.86034\n","Epoch 71/100\n","16319/16319 [==============================] - 5s 278us/step - loss: 4.8910 - val_loss: 6.8248\n","\n","Epoch 00071: loss did not improve from 4.86034\n","Epoch 72/100\n","16319/16319 [==============================] - 5s 276us/step - loss: 4.7908 - val_loss: 6.7827\n","\n","Epoch 00072: loss improved from 4.86034 to 4.79082, saving model to finalSolution.h5\n","Epoch 73/100\n","16319/16319 [==============================] - 5s 279us/step - loss: 4.8308 - val_loss: 6.7799\n","\n","Epoch 00073: loss did not improve from 4.79082\n","Epoch 74/100\n","16319/16319 [==============================] - 5s 276us/step - loss: 5.1137 - val_loss: 7.1061\n","\n","Epoch 00074: loss did not improve from 4.79082\n","Epoch 75/100\n","16319/16319 [==============================] - 4s 275us/step - loss: 4.5302 - val_loss: 6.9213\n","\n","Epoch 00075: loss improved from 4.79082 to 4.53021, saving model to finalSolution.h5\n","Epoch 76/100\n","16319/16319 [==============================] - 4s 274us/step - loss: 5.0066 - val_loss: 8.5066\n","\n","Epoch 00076: loss did not improve from 4.53021\n","Epoch 77/100\n","16319/16319 [==============================] - 4s 274us/step - loss: 4.9831 - val_loss: 7.6966\n","\n","Epoch 00077: loss did not improve from 4.53021\n","Epoch 78/100\n","16319/16319 [==============================] - 5s 277us/step - loss: 5.0742 - val_loss: 7.7302\n","\n","Epoch 00078: loss did not improve from 4.53021\n","Epoch 79/100\n","16319/16319 [==============================] - 5s 276us/step - loss: 4.8618 - val_loss: 7.8643\n","\n","Epoch 00079: loss did not improve from 4.53021\n","Epoch 80/100\n","16319/16319 [==============================] - 5s 280us/step - loss: 4.7289 - val_loss: 8.1423\n","\n","Epoch 00080: loss did not improve from 4.53021\n","Epoch 81/100\n","16319/16319 [==============================] - 5s 278us/step - loss: 4.7116 - val_loss: 7.0357\n","\n","Epoch 00081: loss did not improve from 4.53021\n","Epoch 82/100\n","16319/16319 [==============================] - 4s 274us/step - loss: 4.7891 - val_loss: 8.2854\n","\n","Epoch 00082: loss did not improve from 4.53021\n","Epoch 83/100\n","16319/16319 [==============================] - 5s 287us/step - loss: 4.8672 - val_loss: 7.9045\n","\n","Epoch 00083: loss did not improve from 4.53021\n","Epoch 84/100\n","16319/16319 [==============================] - 5s 285us/step - loss: 4.8622 - val_loss: 7.6471\n","\n","Epoch 00084: loss did not improve from 4.53021\n","Epoch 85/100\n","16319/16319 [==============================] - 4s 272us/step - loss: 4.7714 - val_loss: 7.6264\n","\n","Epoch 00085: loss did not improve from 4.53021\n","Epoch 86/100\n","16319/16319 [==============================] - 4s 274us/step - loss: 4.7486 - val_loss: 7.0810\n","\n","Epoch 00086: loss did not improve from 4.53021\n","Epoch 87/100\n","16319/16319 [==============================] - 5s 277us/step - loss: 4.7505 - val_loss: 7.0331\n","\n","Epoch 00087: loss did not improve from 4.53021\n","Epoch 88/100\n","16319/16319 [==============================] - 5s 278us/step - loss: 4.9273 - val_loss: 7.4773\n","\n","Epoch 00088: loss did not improve from 4.53021\n","Epoch 89/100\n","16319/16319 [==============================] - 4s 276us/step - loss: 4.5229 - val_loss: 8.2524\n","\n","Epoch 00089: loss improved from 4.53021 to 4.52294, saving model to finalSolution.h5\n","Epoch 90/100\n","16319/16319 [==============================] - 5s 280us/step - loss: 4.7441 - val_loss: 7.3044\n","\n","Epoch 00090: loss did not improve from 4.52294\n","Epoch 91/100\n","16319/16319 [==============================] - 4s 275us/step - loss: 5.0749 - val_loss: 7.7182\n","\n","Epoch 00091: loss did not improve from 4.52294\n","Epoch 92/100\n","16319/16319 [==============================] - 5s 278us/step - loss: 4.8951 - val_loss: 7.1800\n","\n","Epoch 00092: loss did not improve from 4.52294\n","Epoch 93/100\n","16319/16319 [==============================] - 5s 281us/step - loss: 4.8421 - val_loss: 7.7090\n","\n","Epoch 00093: loss did not improve from 4.52294\n","Epoch 94/100\n","16319/16319 [==============================] - 4s 274us/step - loss: 4.7110 - val_loss: 6.9735\n","\n","Epoch 00094: loss did not improve from 4.52294\n","Epoch 95/100\n","16319/16319 [==============================] - 5s 276us/step - loss: 4.7830 - val_loss: 7.5999\n","\n","Epoch 00095: loss did not improve from 4.52294\n","Epoch 96/100\n","16319/16319 [==============================] - 4s 271us/step - loss: 4.6654 - val_loss: 6.7957\n","\n","Epoch 00096: loss did not improve from 4.52294\n","Epoch 97/100\n","16319/16319 [==============================] - 4s 273us/step - loss: 4.5324 - val_loss: 7.7555\n","\n","Epoch 00097: loss did not improve from 4.52294\n","Epoch 98/100\n","16319/16319 [==============================] - 5s 280us/step - loss: 4.8150 - val_loss: 8.1405\n","\n","Epoch 00098: loss did not improve from 4.52294\n","Epoch 99/100\n","16319/16319 [==============================] - 5s 283us/step - loss: 4.8015 - val_loss: 7.4875\n","\n","Epoch 00099: loss did not improve from 4.52294\n","Epoch 100/100\n","16319/16319 [==============================] - 4s 273us/step - loss: 4.7047 - val_loss: 7.3447\n","\n","Epoch 00100: loss did not improve from 4.52294\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jBjnJ1cXC1Mk","colab_type":"code","outputId":"5c212b08-6125-45ab-824d-21945b3ff382","executionInfo":{"status":"ok","timestamp":1590097287632,"user_tz":360,"elapsed":1335384,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet.py /content/train.mp4 /content/train.txt --mode=train --split=0.2 --model  finalSolution.h5 --epoch 200 --history 1 --wipe  --augment"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-21 21:19:13.633238: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet.py:181: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), input_shape=(100, 100,..., strides=(4, 4), padding=\"same\")`\n","  self.model.add(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4),input_shape=(self.DSIZE[0],self.DSIZE[1],2)))\n","2020-05-21 21:19:15.565785: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-21 21:19:15.580699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:19:15.581518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-21 21:19:15.581565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-21 21:19:15.583095: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-21 21:19:15.584983: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-21 21:19:15.585342: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-21 21:19:15.587299: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-21 21:19:15.588450: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-21 21:19:15.592556: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-21 21:19:15.592663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:19:15.593525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:19:15.594403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-21 21:19:15.606161: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2199995000 Hz\n","2020-05-21 21:19:15.606491: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b012c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-21 21:19:15.606523: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-21 21:19:15.698150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:19:15.699216: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b01480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-21 21:19:15.699260: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-21 21:19:15.699496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:19:15.700462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-21 21:19:15.700508: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-21 21:19:15.700547: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-21 21:19:15.700594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-21 21:19:15.700617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-21 21:19:15.700639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-21 21:19:15.700661: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-21 21:19:15.700684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-21 21:19:15.700742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:19:15.701548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:19:15.702284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-21 21:19:15.702344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-21 21:19:16.205755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-21 21:19:16.205809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-21 21:19:16.205819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-21 21:19:16.206000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:19:16.206791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-21 21:19:16.207491: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-21 21:19:16.207532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  self.model.add(Convolution2D(64, 8,8 ,border_mode='same',subsample=(4,4)))\n","speednet.py:185: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (4, 4), strides=(2, 2), padding=\"same\")`\n","  self.model.add(Convolution2D(128, 4,4,border_mode='same',subsample=(2,2)))\n","speednet.py:187: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (2, 2), strides=(1, 1), padding=\"same\")`\n","  self.model.add(Convolution2D(128, 2,2,border_mode='same',subsample=(1,1)))\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400frames\n","Shuffling data\n","Done prepping data\n","Starting training\n","Train on 16319 samples, validate on 4080 samples\n","Epoch 1/200\n","2020-05-21 21:28:40.073132: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-21 21:28:40.264618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","16319/16319 [==============================] - 6s 343us/step - loss: 36.7090 - val_loss: 17.2347\n","\n","Epoch 00001: loss improved from inf to 36.70905, saving model to finalSolution.h5\n","Epoch 2/200\n","16319/16319 [==============================] - 4s 241us/step - loss: 19.9161 - val_loss: 12.3007\n","\n","Epoch 00002: loss improved from 36.70905 to 19.91614, saving model to finalSolution.h5\n","Epoch 3/200\n","16319/16319 [==============================] - 4s 251us/step - loss: 16.2060 - val_loss: 11.7921\n","\n","Epoch 00003: loss improved from 19.91614 to 16.20597, saving model to finalSolution.h5\n","Epoch 4/200\n","16319/16319 [==============================] - 4s 259us/step - loss: 13.8452 - val_loss: 12.7606\n","\n","Epoch 00004: loss improved from 16.20597 to 13.84520, saving model to finalSolution.h5\n","Epoch 5/200\n","16319/16319 [==============================] - 4s 246us/step - loss: 12.4506 - val_loss: 11.6145\n","\n","Epoch 00005: loss improved from 13.84520 to 12.45063, saving model to finalSolution.h5\n","Epoch 6/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 11.7995 - val_loss: 10.3200\n","\n","Epoch 00006: loss improved from 12.45063 to 11.79951, saving model to finalSolution.h5\n","Epoch 7/200\n","16319/16319 [==============================] - 4s 250us/step - loss: 10.9474 - val_loss: 10.4094\n","\n","Epoch 00007: loss improved from 11.79951 to 10.94735, saving model to finalSolution.h5\n","Epoch 8/200\n","16319/16319 [==============================] - 4s 236us/step - loss: 10.5947 - val_loss: 11.0039\n","\n","Epoch 00008: loss improved from 10.94735 to 10.59469, saving model to finalSolution.h5\n","Epoch 9/200\n","16319/16319 [==============================] - 4s 237us/step - loss: 10.5879 - val_loss: 12.7484\n","\n","Epoch 00009: loss improved from 10.59469 to 10.58795, saving model to finalSolution.h5\n","Epoch 10/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 9.9754 - val_loss: 11.4166\n","\n","Epoch 00010: loss improved from 10.58795 to 9.97545, saving model to finalSolution.h5\n","Epoch 11/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 9.8694 - val_loss: 9.5342\n","\n","Epoch 00011: loss improved from 9.97545 to 9.86939, saving model to finalSolution.h5\n","Epoch 12/200\n","16319/16319 [==============================] - 4s 239us/step - loss: 9.3013 - val_loss: 11.4852\n","\n","Epoch 00012: loss improved from 9.86939 to 9.30128, saving model to finalSolution.h5\n","Epoch 13/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 9.7446 - val_loss: 11.1290\n","\n","Epoch 00013: loss did not improve from 9.30128\n","Epoch 14/200\n","16319/16319 [==============================] - 4s 240us/step - loss: 9.4176 - val_loss: 10.5596\n","\n","Epoch 00014: loss did not improve from 9.30128\n","Epoch 15/200\n","16319/16319 [==============================] - 4s 230us/step - loss: 9.0454 - val_loss: 13.9280\n","\n","Epoch 00015: loss improved from 9.30128 to 9.04539, saving model to finalSolution.h5\n","Epoch 16/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 8.5848 - val_loss: 9.4680\n","\n","Epoch 00016: loss improved from 9.04539 to 8.58481, saving model to finalSolution.h5\n","Epoch 17/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 8.3950 - val_loss: 10.0449\n","\n","Epoch 00017: loss improved from 8.58481 to 8.39504, saving model to finalSolution.h5\n","Epoch 18/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 8.0731 - val_loss: 10.7492\n","\n","Epoch 00018: loss improved from 8.39504 to 8.07310, saving model to finalSolution.h5\n","Epoch 19/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 7.9338 - val_loss: 10.2495\n","\n","Epoch 00019: loss improved from 8.07310 to 7.93376, saving model to finalSolution.h5\n","Epoch 20/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 8.0851 - val_loss: 10.5894\n","\n","Epoch 00020: loss did not improve from 7.93376\n","Epoch 21/200\n","16319/16319 [==============================] - 4s 239us/step - loss: 7.7349 - val_loss: 8.9828\n","\n","Epoch 00021: loss improved from 7.93376 to 7.73492, saving model to finalSolution.h5\n","Epoch 22/200\n","16319/16319 [==============================] - 4s 237us/step - loss: 7.6197 - val_loss: 10.8416\n","\n","Epoch 00022: loss improved from 7.73492 to 7.61966, saving model to finalSolution.h5\n","Epoch 23/200\n","16319/16319 [==============================] - 4s 237us/step - loss: 7.6026 - val_loss: 9.4800\n","\n","Epoch 00023: loss improved from 7.61966 to 7.60264, saving model to finalSolution.h5\n","Epoch 24/200\n","16319/16319 [==============================] - 4s 234us/step - loss: 7.4252 - val_loss: 8.9678\n","\n","Epoch 00024: loss improved from 7.60264 to 7.42520, saving model to finalSolution.h5\n","Epoch 25/200\n","16319/16319 [==============================] - 4s 238us/step - loss: 7.2428 - val_loss: 10.4475\n","\n","Epoch 00025: loss improved from 7.42520 to 7.24284, saving model to finalSolution.h5\n","Epoch 26/200\n","16319/16319 [==============================] - 4s 239us/step - loss: 7.5126 - val_loss: 9.1902\n","\n","Epoch 00026: loss did not improve from 7.24284\n","Epoch 27/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 7.0005 - val_loss: 7.8843\n","\n","Epoch 00027: loss improved from 7.24284 to 7.00051, saving model to finalSolution.h5\n","Epoch 28/200\n","16319/16319 [==============================] - 4s 241us/step - loss: 6.7641 - val_loss: 9.6480\n","\n","Epoch 00028: loss improved from 7.00051 to 6.76410, saving model to finalSolution.h5\n","Epoch 29/200\n","16319/16319 [==============================] - 4s 243us/step - loss: 6.9707 - val_loss: 13.4789\n","\n","Epoch 00029: loss did not improve from 6.76410\n","Epoch 30/200\n","16319/16319 [==============================] - 4s 230us/step - loss: 7.9404 - val_loss: 8.9163\n","\n","Epoch 00030: loss did not improve from 6.76410\n","Epoch 31/200\n","16319/16319 [==============================] - 4s 225us/step - loss: 6.6881 - val_loss: 9.5695\n","\n","Epoch 00031: loss improved from 6.76410 to 6.68815, saving model to finalSolution.h5\n","Epoch 32/200\n","16319/16319 [==============================] - 4s 237us/step - loss: 6.4887 - val_loss: 10.4346\n","\n","Epoch 00032: loss improved from 6.68815 to 6.48866, saving model to finalSolution.h5\n","Epoch 33/200\n","16319/16319 [==============================] - 4s 234us/step - loss: 6.1620 - val_loss: 8.6282\n","\n","Epoch 00033: loss improved from 6.48866 to 6.16200, saving model to finalSolution.h5\n","Epoch 34/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 6.0968 - val_loss: 8.6699\n","\n","Epoch 00034: loss improved from 6.16200 to 6.09683, saving model to finalSolution.h5\n","Epoch 35/200\n","16319/16319 [==============================] - 4s 237us/step - loss: 6.1725 - val_loss: 9.9915\n","\n","Epoch 00035: loss did not improve from 6.09683\n","Epoch 36/200\n","16319/16319 [==============================] - 4s 234us/step - loss: 6.5857 - val_loss: 9.4369\n","\n","Epoch 00036: loss did not improve from 6.09683\n","Epoch 37/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 6.2657 - val_loss: 9.3425\n","\n","Epoch 00037: loss did not improve from 6.09683\n","Epoch 38/200\n","16319/16319 [==============================] - 4s 233us/step - loss: 6.2231 - val_loss: 10.5610\n","\n","Epoch 00038: loss did not improve from 6.09683\n","Epoch 39/200\n","16319/16319 [==============================] - 4s 230us/step - loss: 6.0748 - val_loss: 10.3400\n","\n","Epoch 00039: loss improved from 6.09683 to 6.07478, saving model to finalSolution.h5\n","Epoch 40/200\n","16319/16319 [==============================] - 4s 234us/step - loss: 5.9789 - val_loss: 9.7473\n","\n","Epoch 00040: loss improved from 6.07478 to 5.97887, saving model to finalSolution.h5\n","Epoch 41/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 5.5142 - val_loss: 8.8528\n","\n","Epoch 00041: loss improved from 5.97887 to 5.51420, saving model to finalSolution.h5\n","Epoch 42/200\n","16319/16319 [==============================] - 4s 241us/step - loss: 5.5823 - val_loss: 8.7821\n","\n","Epoch 00042: loss did not improve from 5.51420\n","Epoch 43/200\n","16319/16319 [==============================] - 4s 233us/step - loss: 5.7181 - val_loss: 11.0146\n","\n","Epoch 00043: loss did not improve from 5.51420\n","Epoch 44/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 5.9604 - val_loss: 11.1627\n","\n","Epoch 00044: loss did not improve from 5.51420\n","Epoch 45/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 5.7160 - val_loss: 9.5528\n","\n","Epoch 00045: loss did not improve from 5.51420\n","Epoch 46/200\n","16319/16319 [==============================] - 4s 237us/step - loss: 5.3187 - val_loss: 8.4674\n","\n","Epoch 00046: loss improved from 5.51420 to 5.31865, saving model to finalSolution.h5\n","Epoch 47/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 5.5621 - val_loss: 9.2361\n","\n","Epoch 00047: loss did not improve from 5.31865\n","Epoch 48/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 5.5826 - val_loss: 10.2506\n","\n","Epoch 00048: loss did not improve from 5.31865\n","Epoch 49/200\n","16319/16319 [==============================] - 4s 230us/step - loss: 5.3957 - val_loss: 8.8832\n","\n","Epoch 00049: loss did not improve from 5.31865\n","Epoch 50/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 5.6128 - val_loss: 9.7849\n","\n","Epoch 00050: loss did not improve from 5.31865\n","Epoch 51/200\n","16319/16319 [==============================] - 4s 233us/step - loss: 5.2880 - val_loss: 10.0125\n","\n","Epoch 00051: loss improved from 5.31865 to 5.28805, saving model to finalSolution.h5\n","Epoch 52/200\n","16319/16319 [==============================] - 4s 225us/step - loss: 5.0803 - val_loss: 9.3336\n","\n","Epoch 00052: loss improved from 5.28805 to 5.08028, saving model to finalSolution.h5\n","Epoch 53/200\n","16319/16319 [==============================] - 4s 227us/step - loss: 5.3649 - val_loss: 10.4588\n","\n","Epoch 00053: loss did not improve from 5.08028\n","Epoch 54/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 5.5352 - val_loss: 11.8835\n","\n","Epoch 00054: loss did not improve from 5.08028\n","Epoch 55/200\n","16319/16319 [==============================] - 4s 233us/step - loss: 5.2894 - val_loss: 9.4128\n","\n","Epoch 00055: loss did not improve from 5.08028\n","Epoch 56/200\n","16319/16319 [==============================] - 4s 250us/step - loss: 5.1109 - val_loss: 9.1429\n","\n","Epoch 00056: loss did not improve from 5.08028\n","Epoch 57/200\n","16319/16319 [==============================] - 4s 239us/step - loss: 5.3441 - val_loss: 10.2509\n","\n","Epoch 00057: loss did not improve from 5.08028\n","Epoch 58/200\n","16319/16319 [==============================] - 4s 234us/step - loss: 5.2500 - val_loss: 8.8077\n","\n","Epoch 00058: loss did not improve from 5.08028\n","Epoch 59/200\n","16319/16319 [==============================] - 4s 240us/step - loss: 5.1504 - val_loss: 9.0332\n","\n","Epoch 00059: loss did not improve from 5.08028\n","Epoch 60/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 5.2935 - val_loss: 10.2396\n","\n","Epoch 00060: loss did not improve from 5.08028\n","Epoch 61/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 5.1383 - val_loss: 8.6082\n","\n","Epoch 00061: loss did not improve from 5.08028\n","Epoch 62/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 5.1657 - val_loss: 9.7837\n","\n","Epoch 00062: loss did not improve from 5.08028\n","Epoch 63/200\n","16319/16319 [==============================] - 4s 240us/step - loss: 5.1862 - val_loss: 9.8566\n","\n","Epoch 00063: loss did not improve from 5.08028\n","Epoch 64/200\n","16319/16319 [==============================] - 4s 240us/step - loss: 5.0444 - val_loss: 8.6877\n","\n","Epoch 00064: loss improved from 5.08028 to 5.04443, saving model to finalSolution.h5\n","Epoch 65/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 4.9477 - val_loss: 9.2936\n","\n","Epoch 00065: loss improved from 5.04443 to 4.94769, saving model to finalSolution.h5\n","Epoch 66/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 4.9831 - val_loss: 9.9161\n","\n","Epoch 00066: loss did not improve from 4.94769\n","Epoch 67/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 5.0223 - val_loss: 8.6859\n","\n","Epoch 00067: loss did not improve from 4.94769\n","Epoch 68/200\n","16319/16319 [==============================] - 4s 234us/step - loss: 4.8372 - val_loss: 9.9874\n","\n","Epoch 00068: loss improved from 4.94769 to 4.83722, saving model to finalSolution.h5\n","Epoch 69/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 5.0457 - val_loss: 9.6253\n","\n","Epoch 00069: loss did not improve from 4.83722\n","Epoch 70/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 4.7064 - val_loss: 8.6342\n","\n","Epoch 00070: loss improved from 4.83722 to 4.70644, saving model to finalSolution.h5\n","Epoch 71/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 5.0283 - val_loss: 8.7034\n","\n","Epoch 00071: loss did not improve from 4.70644\n","Epoch 72/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.7560 - val_loss: 9.3884\n","\n","Epoch 00072: loss did not improve from 4.70644\n","Epoch 73/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 4.9492 - val_loss: 15.2750\n","\n","Epoch 00073: loss did not improve from 4.70644\n","Epoch 74/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 5.2039 - val_loss: 8.8353\n","\n","Epoch 00074: loss did not improve from 4.70644\n","Epoch 75/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.6938 - val_loss: 8.8828\n","\n","Epoch 00075: loss improved from 4.70644 to 4.69379, saving model to finalSolution.h5\n","Epoch 76/200\n","16319/16319 [==============================] - 4s 230us/step - loss: 4.5572 - val_loss: 8.9617\n","\n","Epoch 00076: loss improved from 4.69379 to 4.55724, saving model to finalSolution.h5\n","Epoch 77/200\n","16319/16319 [==============================] - 4s 227us/step - loss: 4.8156 - val_loss: 8.5138\n","\n","Epoch 00077: loss did not improve from 4.55724\n","Epoch 78/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 5.4251 - val_loss: 9.2332\n","\n","Epoch 00078: loss did not improve from 4.55724\n","Epoch 79/200\n","16319/16319 [==============================] - 4s 225us/step - loss: 4.7793 - val_loss: 10.2043\n","\n","Epoch 00079: loss did not improve from 4.55724\n","Epoch 80/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 4.5730 - val_loss: 8.3134\n","\n","Epoch 00080: loss did not improve from 4.55724\n","Epoch 81/200\n","16319/16319 [==============================] - 4s 234us/step - loss: 4.9329 - val_loss: 10.0892\n","\n","Epoch 00081: loss did not improve from 4.55724\n","Epoch 82/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.9327 - val_loss: 8.9643\n","\n","Epoch 00082: loss did not improve from 4.55724\n","Epoch 83/200\n","16319/16319 [==============================] - 4s 245us/step - loss: 4.8597 - val_loss: 10.5252\n","\n","Epoch 00083: loss did not improve from 4.55724\n","Epoch 84/200\n","16319/16319 [==============================] - 4s 250us/step - loss: 4.7461 - val_loss: 9.6428\n","\n","Epoch 00084: loss did not improve from 4.55724\n","Epoch 85/200\n","16319/16319 [==============================] - 4s 240us/step - loss: 4.6994 - val_loss: 9.9102\n","\n","Epoch 00085: loss did not improve from 4.55724\n","Epoch 86/200\n","16319/16319 [==============================] - 4s 223us/step - loss: 4.5250 - val_loss: 9.6211\n","\n","Epoch 00086: loss improved from 4.55724 to 4.52504, saving model to finalSolution.h5\n","Epoch 87/200\n","16319/16319 [==============================] - 4s 241us/step - loss: 4.5283 - val_loss: 8.7841\n","\n","Epoch 00087: loss did not improve from 4.52504\n","Epoch 88/200\n","16319/16319 [==============================] - 4s 227us/step - loss: 4.7137 - val_loss: 9.3304\n","\n","Epoch 00088: loss did not improve from 4.52504\n","Epoch 89/200\n","16319/16319 [==============================] - 4s 242us/step - loss: 4.6001 - val_loss: 9.4765\n","\n","Epoch 00089: loss did not improve from 4.52504\n","Epoch 90/200\n","16319/16319 [==============================] - 4s 233us/step - loss: 4.7426 - val_loss: 9.6692\n","\n","Epoch 00090: loss did not improve from 4.52504\n","Epoch 91/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 4.8598 - val_loss: 9.1069\n","\n","Epoch 00091: loss did not improve from 4.52504\n","Epoch 92/200\n","16319/16319 [==============================] - 4s 226us/step - loss: 4.4913 - val_loss: 8.7687\n","\n","Epoch 00092: loss improved from 4.52504 to 4.49129, saving model to finalSolution.h5\n","Epoch 93/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 4.4369 - val_loss: 10.2418\n","\n","Epoch 00093: loss improved from 4.49129 to 4.43691, saving model to finalSolution.h5\n","Epoch 94/200\n","16319/16319 [==============================] - 4s 234us/step - loss: 4.4720 - val_loss: 9.4794\n","\n","Epoch 00094: loss did not improve from 4.43691\n","Epoch 95/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.3532 - val_loss: 8.7043\n","\n","Epoch 00095: loss improved from 4.43691 to 4.35317, saving model to finalSolution.h5\n","Epoch 96/200\n","16319/16319 [==============================] - 4s 236us/step - loss: 4.8527 - val_loss: 10.3167\n","\n","Epoch 00096: loss did not improve from 4.35317\n","Epoch 97/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 4.6664 - val_loss: 8.9561\n","\n","Epoch 00097: loss did not improve from 4.35317\n","Epoch 98/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 4.5225 - val_loss: 9.5341\n","\n","Epoch 00098: loss did not improve from 4.35317\n","Epoch 99/200\n","16319/16319 [==============================] - 4s 225us/step - loss: 5.0140 - val_loss: 9.1370\n","\n","Epoch 00099: loss did not improve from 4.35317\n","Epoch 100/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 4.7029 - val_loss: 10.2031\n","\n","Epoch 00100: loss did not improve from 4.35317\n","Epoch 101/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 4.4607 - val_loss: 8.8434\n","\n","Epoch 00101: loss did not improve from 4.35317\n","Epoch 102/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 4.5381 - val_loss: 11.6453\n","\n","Epoch 00102: loss did not improve from 4.35317\n","Epoch 103/200\n","16319/16319 [==============================] - 4s 236us/step - loss: 4.8767 - val_loss: 10.3974\n","\n","Epoch 00103: loss did not improve from 4.35317\n","Epoch 104/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 4.4325 - val_loss: 10.0802\n","\n","Epoch 00104: loss did not improve from 4.35317\n","Epoch 105/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.4110 - val_loss: 9.5409\n","\n","Epoch 00105: loss did not improve from 4.35317\n","Epoch 106/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.1637 - val_loss: 10.0263\n","\n","Epoch 00106: loss improved from 4.35317 to 4.16372, saving model to finalSolution.h5\n","Epoch 107/200\n","16319/16319 [==============================] - 4s 227us/step - loss: 4.2694 - val_loss: 10.5948\n","\n","Epoch 00107: loss did not improve from 4.16372\n","Epoch 108/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.8509 - val_loss: 9.7336\n","\n","Epoch 00108: loss did not improve from 4.16372\n","Epoch 109/200\n","16319/16319 [==============================] - 4s 230us/step - loss: 4.6219 - val_loss: 9.3149\n","\n","Epoch 00109: loss did not improve from 4.16372\n","Epoch 110/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.3824 - val_loss: 9.4100\n","\n","Epoch 00110: loss did not improve from 4.16372\n","Epoch 111/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 4.4824 - val_loss: 10.0490\n","\n","Epoch 00111: loss did not improve from 4.16372\n","Epoch 112/200\n","16319/16319 [==============================] - 4s 227us/step - loss: 4.4254 - val_loss: 10.8084\n","\n","Epoch 00112: loss did not improve from 4.16372\n","Epoch 113/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 4.3040 - val_loss: 10.5243\n","\n","Epoch 00113: loss did not improve from 4.16372\n","Epoch 114/200\n","16319/16319 [==============================] - 4s 237us/step - loss: 4.6385 - val_loss: 10.8709\n","\n","Epoch 00114: loss did not improve from 4.16372\n","Epoch 115/200\n","16319/16319 [==============================] - 4s 236us/step - loss: 4.4361 - val_loss: 10.1902\n","\n","Epoch 00115: loss did not improve from 4.16372\n","Epoch 116/200\n","16319/16319 [==============================] - 4s 237us/step - loss: 4.3896 - val_loss: 9.8802\n","\n","Epoch 00116: loss did not improve from 4.16372\n","Epoch 117/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 4.2161 - val_loss: 10.4510\n","\n","Epoch 00117: loss did not improve from 4.16372\n","Epoch 118/200\n","16319/16319 [==============================] - 4s 236us/step - loss: 4.3835 - val_loss: 10.1320\n","\n","Epoch 00118: loss did not improve from 4.16372\n","Epoch 119/200\n","16319/16319 [==============================] - 4s 239us/step - loss: 4.6997 - val_loss: 9.9674\n","\n","Epoch 00119: loss did not improve from 4.16372\n","Epoch 120/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 4.4339 - val_loss: 10.5248\n","\n","Epoch 00120: loss did not improve from 4.16372\n","Epoch 121/200\n","16319/16319 [==============================] - 4s 233us/step - loss: 4.3428 - val_loss: 9.7079\n","\n","Epoch 00121: loss did not improve from 4.16372\n","Epoch 122/200\n","16319/16319 [==============================] - 4s 236us/step - loss: 4.3620 - val_loss: 10.3618\n","\n","Epoch 00122: loss did not improve from 4.16372\n","Epoch 123/200\n","16319/16319 [==============================] - 4s 242us/step - loss: 4.3992 - val_loss: 11.4542\n","\n","Epoch 00123: loss did not improve from 4.16372\n","Epoch 124/200\n","16319/16319 [==============================] - 4s 236us/step - loss: 4.2753 - val_loss: 10.0112\n","\n","Epoch 00124: loss did not improve from 4.16372\n","Epoch 125/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.5170 - val_loss: 9.7182\n","\n","Epoch 00125: loss did not improve from 4.16372\n","Epoch 126/200\n","16319/16319 [==============================] - 4s 233us/step - loss: 4.4267 - val_loss: 10.0663\n","\n","Epoch 00126: loss did not improve from 4.16372\n","Epoch 127/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.3256 - val_loss: 10.7620\n","\n","Epoch 00127: loss did not improve from 4.16372\n","Epoch 128/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 4.1772 - val_loss: 9.9712\n","\n","Epoch 00128: loss did not improve from 4.16372\n","Epoch 129/200\n","16319/16319 [==============================] - 4s 224us/step - loss: 4.2696 - val_loss: 10.4834\n","\n","Epoch 00129: loss did not improve from 4.16372\n","Epoch 130/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 4.8586 - val_loss: 10.2440\n","\n","Epoch 00130: loss did not improve from 4.16372\n","Epoch 131/200\n","16319/16319 [==============================] - 4s 238us/step - loss: 4.2390 - val_loss: 10.1115\n","\n","Epoch 00131: loss did not improve from 4.16372\n","Epoch 132/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 4.8269 - val_loss: 10.5713\n","\n","Epoch 00132: loss did not improve from 4.16372\n","Epoch 133/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 4.2242 - val_loss: 10.2268\n","\n","Epoch 00133: loss did not improve from 4.16372\n","Epoch 134/200\n","16319/16319 [==============================] - 4s 225us/step - loss: 4.1390 - val_loss: 10.0172\n","\n","Epoch 00134: loss improved from 4.16372 to 4.13902, saving model to finalSolution.h5\n","Epoch 135/200\n","16319/16319 [==============================] - 4s 230us/step - loss: 4.4322 - val_loss: 11.5661\n","\n","Epoch 00135: loss did not improve from 4.13902\n","Epoch 136/200\n","16319/16319 [==============================] - 4s 238us/step - loss: 4.3963 - val_loss: 10.2852\n","\n","Epoch 00136: loss did not improve from 4.13902\n","Epoch 137/200\n","16319/16319 [==============================] - 4s 243us/step - loss: 4.5941 - val_loss: 10.3540\n","\n","Epoch 00137: loss did not improve from 4.13902\n","Epoch 138/200\n","16319/16319 [==============================] - 4s 234us/step - loss: 4.2605 - val_loss: 10.2454\n","\n","Epoch 00138: loss did not improve from 4.13902\n","Epoch 139/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.2214 - val_loss: 9.8394\n","\n","Epoch 00139: loss did not improve from 4.13902\n","Epoch 140/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 4.3343 - val_loss: 10.3889\n","\n","Epoch 00140: loss did not improve from 4.13902\n","Epoch 141/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 4.3661 - val_loss: 9.7644\n","\n","Epoch 00141: loss did not improve from 4.13902\n","Epoch 142/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 4.3382 - val_loss: 10.1800\n","\n","Epoch 00142: loss did not improve from 4.13902\n","Epoch 143/200\n","16319/16319 [==============================] - 4s 234us/step - loss: 4.4039 - val_loss: 9.8772\n","\n","Epoch 00143: loss did not improve from 4.13902\n","Epoch 144/200\n","16319/16319 [==============================] - 4s 241us/step - loss: 4.3318 - val_loss: 11.2142\n","\n","Epoch 00144: loss did not improve from 4.13902\n","Epoch 145/200\n","16319/16319 [==============================] - 4s 223us/step - loss: 4.3406 - val_loss: 10.3711\n","\n","Epoch 00145: loss did not improve from 4.13902\n","Epoch 146/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 4.1780 - val_loss: 11.1737\n","\n","Epoch 00146: loss did not improve from 4.13902\n","Epoch 147/200\n","16319/16319 [==============================] - 4s 230us/step - loss: 4.3254 - val_loss: 10.8445\n","\n","Epoch 00147: loss did not improve from 4.13902\n","Epoch 148/200\n","16319/16319 [==============================] - 4s 227us/step - loss: 4.3940 - val_loss: 10.4843\n","\n","Epoch 00148: loss did not improve from 4.13902\n","Epoch 149/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 4.1756 - val_loss: 10.7968\n","\n","Epoch 00149: loss did not improve from 4.13902\n","Epoch 150/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 4.2072 - val_loss: 12.9863\n","\n","Epoch 00150: loss did not improve from 4.13902\n","Epoch 151/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 4.5332 - val_loss: 11.8965\n","\n","Epoch 00151: loss did not improve from 4.13902\n","Epoch 152/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 4.3692 - val_loss: 9.7404\n","\n","Epoch 00152: loss did not improve from 4.13902\n","Epoch 153/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 5.0305 - val_loss: 10.1955\n","\n","Epoch 00153: loss did not improve from 4.13902\n","Epoch 154/200\n","16319/16319 [==============================] - 4s 236us/step - loss: 4.2583 - val_loss: 9.9736\n","\n","Epoch 00154: loss did not improve from 4.13902\n","Epoch 155/200\n","16319/16319 [==============================] - 4s 238us/step - loss: 4.1733 - val_loss: 10.0843\n","\n","Epoch 00155: loss did not improve from 4.13902\n","Epoch 156/200\n","16319/16319 [==============================] - 4s 239us/step - loss: 5.1682 - val_loss: 9.7015\n","\n","Epoch 00156: loss did not improve from 4.13902\n","Epoch 157/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 4.5055 - val_loss: 10.2278\n","\n","Epoch 00157: loss did not improve from 4.13902\n","Epoch 158/200\n","16319/16319 [==============================] - 4s 233us/step - loss: 4.2760 - val_loss: 10.4517\n","\n","Epoch 00158: loss did not improve from 4.13902\n","Epoch 159/200\n","16319/16319 [==============================] - 4s 238us/step - loss: 4.4200 - val_loss: 10.5545\n","\n","Epoch 00159: loss did not improve from 4.13902\n","Epoch 160/200\n","16319/16319 [==============================] - 4s 230us/step - loss: 4.3134 - val_loss: 10.5450\n","\n","Epoch 00160: loss did not improve from 4.13902\n","Epoch 161/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 4.2800 - val_loss: 11.6724\n","\n","Epoch 00161: loss did not improve from 4.13902\n","Epoch 162/200\n","16319/16319 [==============================] - 4s 230us/step - loss: 4.4574 - val_loss: 10.5761\n","\n","Epoch 00162: loss did not improve from 4.13902\n","Epoch 163/200\n","16319/16319 [==============================] - 4s 236us/step - loss: 4.2584 - val_loss: 10.3286\n","\n","Epoch 00163: loss did not improve from 4.13902\n","Epoch 164/200\n","16319/16319 [==============================] - 4s 236us/step - loss: 4.1097 - val_loss: 11.2087\n","\n","Epoch 00164: loss improved from 4.13902 to 4.10965, saving model to finalSolution.h5\n","Epoch 165/200\n","16319/16319 [==============================] - 4s 251us/step - loss: 4.4330 - val_loss: 10.2579\n","\n","Epoch 00165: loss did not improve from 4.10965\n","Epoch 166/200\n","16319/16319 [==============================] - 4s 243us/step - loss: 4.3539 - val_loss: 11.7560\n","\n","Epoch 00166: loss did not improve from 4.10965\n","Epoch 167/200\n","16319/16319 [==============================] - 4s 241us/step - loss: 4.6426 - val_loss: 10.1008\n","\n","Epoch 00167: loss did not improve from 4.10965\n","Epoch 168/200\n","16319/16319 [==============================] - 4s 230us/step - loss: 4.3553 - val_loss: 10.2289\n","\n","Epoch 00168: loss did not improve from 4.10965\n","Epoch 169/200\n","16319/16319 [==============================] - 4s 238us/step - loss: 4.3314 - val_loss: 10.2108\n","\n","Epoch 00169: loss did not improve from 4.10965\n","Epoch 170/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.7666 - val_loss: 12.1643\n","\n","Epoch 00170: loss did not improve from 4.10965\n","Epoch 171/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 4.4179 - val_loss: 10.3412\n","\n","Epoch 00171: loss did not improve from 4.10965\n","Epoch 172/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 4.3116 - val_loss: 10.1993\n","\n","Epoch 00172: loss did not improve from 4.10965\n","Epoch 173/200\n","16319/16319 [==============================] - 4s 233us/step - loss: 5.2650 - val_loss: 10.6245\n","\n","Epoch 00173: loss did not improve from 4.10965\n","Epoch 174/200\n","16319/16319 [==============================] - 4s 233us/step - loss: 4.6448 - val_loss: 10.4335\n","\n","Epoch 00174: loss did not improve from 4.10965\n","Epoch 175/200\n","16319/16319 [==============================] - 4s 226us/step - loss: 4.3189 - val_loss: 10.3795\n","\n","Epoch 00175: loss did not improve from 4.10965\n","Epoch 176/200\n","16319/16319 [==============================] - 4s 224us/step - loss: 4.0533 - val_loss: 10.3179\n","\n","Epoch 00176: loss improved from 4.10965 to 4.05334, saving model to finalSolution.h5\n","Epoch 177/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.2067 - val_loss: 11.1581\n","\n","Epoch 00177: loss did not improve from 4.05334\n","Epoch 178/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 4.1961 - val_loss: 9.5417\n","\n","Epoch 00178: loss did not improve from 4.05334\n","Epoch 179/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 4.4649 - val_loss: 10.2115\n","\n","Epoch 00179: loss did not improve from 4.05334\n","Epoch 180/200\n","16319/16319 [==============================] - 4s 233us/step - loss: 4.1513 - val_loss: 10.1423\n","\n","Epoch 00180: loss did not improve from 4.05334\n","Epoch 181/200\n","16319/16319 [==============================] - 4s 234us/step - loss: 3.9078 - val_loss: 10.5598\n","\n","Epoch 00181: loss improved from 4.05334 to 3.90778, saving model to finalSolution.h5\n","Epoch 182/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 5.7354 - val_loss: 13.7170\n","\n","Epoch 00182: loss did not improve from 3.90778\n","Epoch 183/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 5.2867 - val_loss: 10.6804\n","\n","Epoch 00183: loss did not improve from 3.90778\n","Epoch 184/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 4.5569 - val_loss: 10.8334\n","\n","Epoch 00184: loss did not improve from 3.90778\n","Epoch 185/200\n","16319/16319 [==============================] - 4s 228us/step - loss: 4.2831 - val_loss: 11.1936\n","\n","Epoch 00185: loss did not improve from 3.90778\n","Epoch 186/200\n","16319/16319 [==============================] - 4s 239us/step - loss: 4.2603 - val_loss: 11.7901\n","\n","Epoch 00186: loss did not improve from 3.90778\n","Epoch 187/200\n","16319/16319 [==============================] - 4s 232us/step - loss: 4.2428 - val_loss: 11.2209\n","\n","Epoch 00187: loss did not improve from 3.90778\n","Epoch 188/200\n","16319/16319 [==============================] - 4s 234us/step - loss: 4.4008 - val_loss: 9.9526\n","\n","Epoch 00188: loss did not improve from 3.90778\n","Epoch 189/200\n","16319/16319 [==============================] - 4s 231us/step - loss: 4.2585 - val_loss: 10.6132\n","\n","Epoch 00189: loss did not improve from 3.90778\n","Epoch 190/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 5.6848 - val_loss: 12.2643\n","\n","Epoch 00190: loss did not improve from 3.90778\n","Epoch 191/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 4.4090 - val_loss: 10.6827\n","\n","Epoch 00191: loss did not improve from 3.90778\n","Epoch 192/200\n","16319/16319 [==============================] - 4s 230us/step - loss: 4.6004 - val_loss: 10.5511\n","\n","Epoch 00192: loss did not improve from 3.90778\n","Epoch 193/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 4.1792 - val_loss: 10.6990\n","\n","Epoch 00193: loss did not improve from 3.90778\n","Epoch 194/200\n","16319/16319 [==============================] - 4s 237us/step - loss: 4.0586 - val_loss: 10.0059\n","\n","Epoch 00194: loss did not improve from 3.90778\n","Epoch 195/200\n","16319/16319 [==============================] - 4s 226us/step - loss: 4.4621 - val_loss: 10.4473\n","\n","Epoch 00195: loss did not improve from 3.90778\n","Epoch 196/200\n","16319/16319 [==============================] - 4s 225us/step - loss: 4.2714 - val_loss: 9.9434\n","\n","Epoch 00196: loss did not improve from 3.90778\n","Epoch 197/200\n","16319/16319 [==============================] - 4s 237us/step - loss: 4.3680 - val_loss: 10.2536\n","\n","Epoch 00197: loss did not improve from 3.90778\n","Epoch 198/200\n","16319/16319 [==============================] - 4s 226us/step - loss: 4.4310 - val_loss: 11.0245\n","\n","Epoch 00198: loss did not improve from 3.90778\n","Epoch 199/200\n","16319/16319 [==============================] - 4s 229us/step - loss: 5.0600 - val_loss: 12.2302\n","\n","Epoch 00199: loss did not improve from 3.90778\n","Epoch 200/200\n","16319/16319 [==============================] - 4s 235us/step - loss: 4.7192 - val_loss: 12.4844\n","\n","Epoch 00200: loss did not improve from 3.90778\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8yOOYOv4UuPY","colab_type":"code","outputId":"1800c247-05c1-417f-8202-2e693e4816c6","executionInfo":{"status":"ok","timestamp":1590085187538,"user_tz":360,"elapsed":3852,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["! ls train_optflowProcess/0.png\n","import cv2 as cv\n","import numpy as np\n","frame=cv.imread(\"train_optflowProcess/10.png\")\n","a=np.array(frame)\n","print(a.mean())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["train_optflowProcess/0.png\n","2.790395833333333\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"4b015017-21f5-43ad-b98e-28a2167a8f1e","executionInfo":{"status":"ok","timestamp":1590160404260,"user_tz":360,"elapsed":735312,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"id":"D6uzC4DP1OrT","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet_final.py /content/train.mp4 /content/train.txt --mode=train --split=0.2 --model  finalSolution.h5 --epoch 100 --history 1 "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-22 15:01:09.922569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet_final.py:186: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-22 15:01:11.991217: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-22 15:01:12.006113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:01:12.007039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-22 15:01:12.007079: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-22 15:01:12.008673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-22 15:01:12.010563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-22 15:01:12.011007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-22 15:01:12.012890: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-22 15:01:12.014147: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-22 15:01:12.017976: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 15:01:12.018102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:01:12.018973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:01:12.019875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-22 15:01:12.025364: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz\n","2020-05-22 15:01:12.025785: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x161f2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-22 15:01:12.025815: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-22 15:01:12.116172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:01:12.117329: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x161f480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-22 15:01:12.117406: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-22 15:01:12.117660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:01:12.118512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-22 15:01:12.118590: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-22 15:01:12.118625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-22 15:01:12.118684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-22 15:01:12.118723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-22 15:01:12.118748: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-22 15:01:12.118778: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-22 15:01:12.118807: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 15:01:12.118895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:01:12.119801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:01:12.120522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-22 15:01:12.120579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-22 15:01:12.650040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 15:01:12.650097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-22 15:01:12.650118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-22 15:01:12.650362: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:01:12.651429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 15:01:12.652289: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-22 15:01:12.652342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet_final.py:188: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","speednet_final.py:190: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","Found preprocessed data\n","tcmalloc: large alloc 2447884288 bytes == 0x1dff6000 @  0x7f0b63dad1e7 0x7f0b618935e1 0x7f0b618f7c78 0x7f0b618f7f37 0x7f0b6198ff28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f0b639aab97 0x5b250a\n","tcmalloc: large alloc 2447884288 bytes == 0xafe72000 @  0x7f0b63dad1e7 0x7f0b618935e1 0x7f0b618f7c78 0x7f0b618f7f37 0x7f0b6198ff28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f0b639aab97 0x5b250a\n","Loading frame 20398\n","done loading 20399 frames\n","Done prepping data\n","tcmalloc: large alloc 1631920128 bytes == 0x141d4e000 @  0x7f0b63dad1e7 0x7f0b618935e1 0x7f0b618f7c78 0x7f0b618f7d93 0x7f0b61982ed6 0x7f0b61983338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f0b639aab97 0x5b250a\n","tcmalloc: large alloc 1631920128 bytes == 0x1dff6000 @  0x7f0b63dad1e7 0x7f0b618935e1 0x7f0b618f7c78 0x7f0b618f7d93 0x7f0b61982ed6 0x7f0b61983338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f0b639aab97 0x5b250a\n","Starting training\n","Train on 16319 samples, validate on 4080 samples\n","Epoch 1/100\n","2020-05-22 15:01:28.050097: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-22 15:01:29.522604: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","16319/16319 [==============================] - 14s 832us/step - loss: 66.7950 - val_loss: 11.7538\n","\n","Epoch 00001: loss improved from inf to 66.79495, saving model to finalSolution.h5\n","Epoch 2/100\n","16319/16319 [==============================] - 7s 425us/step - loss: 13.4771 - val_loss: 5.6861\n","\n","Epoch 00002: loss improved from 66.79495 to 13.47709, saving model to finalSolution.h5\n","Epoch 3/100\n","16319/16319 [==============================] - 7s 428us/step - loss: 9.7353 - val_loss: 4.4575\n","\n","Epoch 00003: loss improved from 13.47709 to 9.73530, saving model to finalSolution.h5\n","Epoch 4/100\n","16319/16319 [==============================] - 7s 447us/step - loss: 8.0140 - val_loss: 3.4170\n","\n","Epoch 00004: loss improved from 9.73530 to 8.01396, saving model to finalSolution.h5\n","Epoch 5/100\n","16319/16319 [==============================] - 7s 439us/step - loss: 7.4610 - val_loss: 3.1380\n","\n","Epoch 00005: loss improved from 8.01396 to 7.46104, saving model to finalSolution.h5\n","Epoch 6/100\n","16319/16319 [==============================] - 7s 435us/step - loss: 6.5965 - val_loss: 2.7873\n","\n","Epoch 00006: loss improved from 7.46104 to 6.59645, saving model to finalSolution.h5\n","Epoch 7/100\n","16319/16319 [==============================] - 7s 422us/step - loss: 5.8097 - val_loss: 2.6701\n","\n","Epoch 00007: loss improved from 6.59645 to 5.80973, saving model to finalSolution.h5\n","Epoch 8/100\n","16319/16319 [==============================] - 7s 436us/step - loss: 5.8080 - val_loss: 2.4900\n","\n","Epoch 00008: loss improved from 5.80973 to 5.80801, saving model to finalSolution.h5\n","Epoch 9/100\n","16319/16319 [==============================] - 7s 415us/step - loss: 5.4450 - val_loss: 2.6460\n","\n","Epoch 00009: loss improved from 5.80801 to 5.44497, saving model to finalSolution.h5\n","Epoch 10/100\n","16319/16319 [==============================] - 7s 425us/step - loss: 5.1252 - val_loss: 2.3092\n","\n","Epoch 00010: loss improved from 5.44497 to 5.12524, saving model to finalSolution.h5\n","Epoch 11/100\n","16319/16319 [==============================] - 7s 430us/step - loss: 4.8720 - val_loss: 2.3262\n","\n","Epoch 00011: loss improved from 5.12524 to 4.87195, saving model to finalSolution.h5\n","Epoch 12/100\n","16319/16319 [==============================] - 7s 457us/step - loss: 4.7484 - val_loss: 2.1141\n","\n","Epoch 00012: loss improved from 4.87195 to 4.74845, saving model to finalSolution.h5\n","Epoch 13/100\n","16319/16319 [==============================] - 7s 440us/step - loss: 4.7311 - val_loss: 1.8672\n","\n","Epoch 00013: loss improved from 4.74845 to 4.73105, saving model to finalSolution.h5\n","Epoch 14/100\n","16319/16319 [==============================] - 7s 437us/step - loss: 4.5143 - val_loss: 1.9427\n","\n","Epoch 00014: loss improved from 4.73105 to 4.51428, saving model to finalSolution.h5\n","Epoch 15/100\n","16319/16319 [==============================] - 7s 441us/step - loss: 4.3968 - val_loss: 2.1192\n","\n","Epoch 00015: loss improved from 4.51428 to 4.39680, saving model to finalSolution.h5\n","Epoch 16/100\n","16319/16319 [==============================] - 7s 437us/step - loss: 4.2610 - val_loss: 1.6363\n","\n","Epoch 00016: loss improved from 4.39680 to 4.26101, saving model to finalSolution.h5\n","Epoch 17/100\n","16319/16319 [==============================] - 7s 436us/step - loss: 4.3076 - val_loss: 2.2373\n","\n","Epoch 00017: loss did not improve from 4.26101\n","Epoch 18/100\n","16319/16319 [==============================] - 7s 430us/step - loss: 4.2001 - val_loss: 1.7027\n","\n","Epoch 00018: loss improved from 4.26101 to 4.20015, saving model to finalSolution.h5\n","Epoch 19/100\n","16319/16319 [==============================] - 7s 435us/step - loss: 4.1129 - val_loss: 1.6671\n","\n","Epoch 00019: loss improved from 4.20015 to 4.11291, saving model to finalSolution.h5\n","Epoch 20/100\n","16319/16319 [==============================] - 7s 432us/step - loss: 4.0855 - val_loss: 1.7995\n","\n","Epoch 00020: loss improved from 4.11291 to 4.08545, saving model to finalSolution.h5\n","Epoch 21/100\n","16319/16319 [==============================] - 7s 442us/step - loss: 3.7698 - val_loss: 1.5651\n","\n","Epoch 00021: loss improved from 4.08545 to 3.76980, saving model to finalSolution.h5\n","Epoch 22/100\n","16319/16319 [==============================] - 7s 444us/step - loss: 3.9732 - val_loss: 1.6222\n","\n","Epoch 00022: loss did not improve from 3.76980\n","Epoch 23/100\n","16319/16319 [==============================] - 7s 432us/step - loss: 3.8341 - val_loss: 1.6306\n","\n","Epoch 00023: loss did not improve from 3.76980\n","Epoch 24/100\n","16319/16319 [==============================] - 7s 443us/step - loss: 3.8033 - val_loss: 1.8690\n","\n","Epoch 00024: loss did not improve from 3.76980\n","Epoch 25/100\n","16319/16319 [==============================] - 7s 444us/step - loss: 3.7231 - val_loss: 1.6512\n","\n","Epoch 00025: loss improved from 3.76980 to 3.72309, saving model to finalSolution.h5\n","Epoch 26/100\n","16319/16319 [==============================] - 7s 444us/step - loss: 3.6525 - val_loss: 1.4333\n","\n","Epoch 00026: loss improved from 3.72309 to 3.65246, saving model to finalSolution.h5\n","Epoch 27/100\n","16319/16319 [==============================] - 7s 426us/step - loss: 3.7535 - val_loss: 1.7596\n","\n","Epoch 00027: loss did not improve from 3.65246\n","Epoch 28/100\n","16319/16319 [==============================] - 7s 418us/step - loss: 3.6384 - val_loss: 1.8326\n","\n","Epoch 00028: loss improved from 3.65246 to 3.63842, saving model to finalSolution.h5\n","Epoch 29/100\n","16319/16319 [==============================] - 7s 420us/step - loss: 3.6448 - val_loss: 1.6721\n","\n","Epoch 00029: loss did not improve from 3.63842\n","Epoch 30/100\n","16319/16319 [==============================] - 7s 433us/step - loss: 3.5232 - val_loss: 1.4702\n","\n","Epoch 00030: loss improved from 3.63842 to 3.52322, saving model to finalSolution.h5\n","Epoch 31/100\n","16319/16319 [==============================] - 7s 433us/step - loss: 3.7958 - val_loss: 1.8062\n","\n","Epoch 00031: loss did not improve from 3.52322\n","Epoch 32/100\n","16319/16319 [==============================] - 7s 431us/step - loss: 3.4667 - val_loss: 1.3781\n","\n","Epoch 00032: loss improved from 3.52322 to 3.46672, saving model to finalSolution.h5\n","Epoch 33/100\n","16319/16319 [==============================] - 7s 427us/step - loss: 3.3905 - val_loss: 1.5282\n","\n","Epoch 00033: loss improved from 3.46672 to 3.39048, saving model to finalSolution.h5\n","Epoch 34/100\n","16319/16319 [==============================] - 7s 427us/step - loss: 3.4286 - val_loss: 1.9528\n","\n","Epoch 00034: loss did not improve from 3.39048\n","Epoch 35/100\n","16319/16319 [==============================] - 7s 424us/step - loss: 3.4570 - val_loss: 1.3371\n","\n","Epoch 00035: loss did not improve from 3.39048\n","Epoch 36/100\n","16319/16319 [==============================] - 7s 435us/step - loss: 3.4536 - val_loss: 1.4559\n","\n","Epoch 00036: loss did not improve from 3.39048\n","Epoch 37/100\n","16319/16319 [==============================] - 7s 430us/step - loss: 3.3385 - val_loss: 1.4527\n","\n","Epoch 00037: loss improved from 3.39048 to 3.33851, saving model to finalSolution.h5\n","Epoch 38/100\n","16319/16319 [==============================] - 7s 427us/step - loss: 3.3379 - val_loss: 1.5492\n","\n","Epoch 00038: loss improved from 3.33851 to 3.33790, saving model to finalSolution.h5\n","Epoch 39/100\n","16319/16319 [==============================] - 7s 424us/step - loss: 3.2814 - val_loss: 1.3454\n","\n","Epoch 00039: loss improved from 3.33790 to 3.28139, saving model to finalSolution.h5\n","Epoch 40/100\n","16319/16319 [==============================] - 7s 425us/step - loss: 3.3893 - val_loss: 1.2309\n","\n","Epoch 00040: loss did not improve from 3.28139\n","Epoch 41/100\n","16319/16319 [==============================] - 7s 427us/step - loss: 3.2926 - val_loss: 1.1997\n","\n","Epoch 00041: loss did not improve from 3.28139\n","Epoch 42/100\n","16319/16319 [==============================] - 7s 424us/step - loss: 3.2855 - val_loss: 1.3771\n","\n","Epoch 00042: loss did not improve from 3.28139\n","Epoch 43/100\n","16319/16319 [==============================] - 7s 423us/step - loss: 3.2133 - val_loss: 1.6215\n","\n","Epoch 00043: loss improved from 3.28139 to 3.21328, saving model to finalSolution.h5\n","Epoch 44/100\n","16319/16319 [==============================] - 7s 429us/step - loss: 3.3428 - val_loss: 1.6138\n","\n","Epoch 00044: loss did not improve from 3.21328\n","Epoch 45/100\n","16319/16319 [==============================] - 7s 441us/step - loss: 3.2096 - val_loss: 1.3788\n","\n","Epoch 00045: loss improved from 3.21328 to 3.20958, saving model to finalSolution.h5\n","Epoch 46/100\n","16319/16319 [==============================] - 7s 436us/step - loss: 3.2294 - val_loss: 1.3130\n","\n","Epoch 00046: loss did not improve from 3.20958\n","Epoch 47/100\n","16319/16319 [==============================] - 7s 449us/step - loss: 3.1944 - val_loss: 1.6177\n","\n","Epoch 00047: loss improved from 3.20958 to 3.19443, saving model to finalSolution.h5\n","Epoch 48/100\n","16319/16319 [==============================] - 7s 446us/step - loss: 3.1304 - val_loss: 1.3037\n","\n","Epoch 00048: loss improved from 3.19443 to 3.13044, saving model to finalSolution.h5\n","Epoch 49/100\n","16319/16319 [==============================] - 7s 420us/step - loss: 3.0884 - val_loss: 1.3673\n","\n","Epoch 00049: loss improved from 3.13044 to 3.08842, saving model to finalSolution.h5\n","Epoch 50/100\n","16319/16319 [==============================] - 7s 427us/step - loss: 3.2181 - val_loss: 1.4239\n","\n","Epoch 00050: loss did not improve from 3.08842\n","Epoch 51/100\n","16319/16319 [==============================] - 7s 428us/step - loss: 3.1394 - val_loss: 1.2527\n","\n","Epoch 00051: loss did not improve from 3.08842\n","Epoch 52/100\n","16319/16319 [==============================] - 7s 426us/step - loss: 3.0912 - val_loss: 1.2345\n","\n","Epoch 00052: loss did not improve from 3.08842\n","Epoch 53/100\n","16319/16319 [==============================] - 7s 428us/step - loss: 3.1782 - val_loss: 1.4770\n","\n","Epoch 00053: loss did not improve from 3.08842\n","Epoch 54/100\n","16319/16319 [==============================] - 7s 435us/step - loss: 3.0224 - val_loss: 1.1710\n","\n","Epoch 00054: loss improved from 3.08842 to 3.02244, saving model to finalSolution.h5\n","Epoch 55/100\n","16319/16319 [==============================] - 7s 436us/step - loss: 3.0810 - val_loss: 1.2257\n","\n","Epoch 00055: loss did not improve from 3.02244\n","Epoch 56/100\n","16319/16319 [==============================] - 7s 440us/step - loss: 3.1333 - val_loss: 1.1237\n","\n","Epoch 00056: loss did not improve from 3.02244\n","Epoch 57/100\n","16319/16319 [==============================] - 7s 426us/step - loss: 3.0957 - val_loss: 1.2808\n","\n","Epoch 00057: loss did not improve from 3.02244\n","Epoch 58/100\n","16319/16319 [==============================] - 7s 427us/step - loss: 3.0191 - val_loss: 1.2964\n","\n","Epoch 00058: loss improved from 3.02244 to 3.01915, saving model to finalSolution.h5\n","Epoch 59/100\n","16319/16319 [==============================] - 7s 429us/step - loss: 2.9389 - val_loss: 1.3317\n","\n","Epoch 00059: loss improved from 3.01915 to 2.93885, saving model to finalSolution.h5\n","Epoch 60/100\n","16319/16319 [==============================] - 7s 424us/step - loss: 2.9850 - val_loss: 1.4548\n","\n","Epoch 00060: loss did not improve from 2.93885\n","Epoch 61/100\n","16319/16319 [==============================] - 7s 424us/step - loss: 2.9651 - val_loss: 1.1796\n","\n","Epoch 00061: loss did not improve from 2.93885\n","Epoch 62/100\n","16319/16319 [==============================] - 7s 431us/step - loss: 3.0250 - val_loss: 1.2921\n","\n","Epoch 00062: loss did not improve from 2.93885\n","Epoch 63/100\n","16319/16319 [==============================] - 7s 437us/step - loss: 2.9642 - val_loss: 1.0928\n","\n","Epoch 00063: loss did not improve from 2.93885\n","Epoch 64/100\n","16319/16319 [==============================] - 7s 438us/step - loss: 3.0155 - val_loss: 1.2800\n","\n","Epoch 00064: loss did not improve from 2.93885\n","Epoch 65/100\n","16319/16319 [==============================] - 7s 429us/step - loss: 2.9073 - val_loss: 1.4684\n","\n","Epoch 00065: loss improved from 2.93885 to 2.90727, saving model to finalSolution.h5\n","Epoch 66/100\n","16319/16319 [==============================] - 7s 424us/step - loss: 3.0305 - val_loss: 1.1804\n","\n","Epoch 00066: loss did not improve from 2.90727\n","Epoch 67/100\n","16319/16319 [==============================] - 7s 428us/step - loss: 2.9197 - val_loss: 1.3169\n","\n","Epoch 00067: loss did not improve from 2.90727\n","Epoch 68/100\n","16319/16319 [==============================] - 7s 415us/step - loss: 2.9017 - val_loss: 1.2676\n","\n","Epoch 00068: loss improved from 2.90727 to 2.90174, saving model to finalSolution.h5\n","Epoch 69/100\n","16319/16319 [==============================] - 7s 429us/step - loss: 2.8393 - val_loss: 1.1624\n","\n","Epoch 00069: loss improved from 2.90174 to 2.83933, saving model to finalSolution.h5\n","Epoch 70/100\n","16319/16319 [==============================] - 7s 430us/step - loss: 2.9394 - val_loss: 1.5378\n","\n","Epoch 00070: loss did not improve from 2.83933\n","Epoch 71/100\n","16319/16319 [==============================] - 7s 439us/step - loss: 2.9282 - val_loss: 1.3888\n","\n","Epoch 00071: loss did not improve from 2.83933\n","Epoch 72/100\n","16319/16319 [==============================] - 7s 434us/step - loss: 2.9248 - val_loss: 1.3578\n","\n","Epoch 00072: loss did not improve from 2.83933\n","Epoch 73/100\n","16319/16319 [==============================] - 7s 423us/step - loss: 2.8838 - val_loss: 1.1742\n","\n","Epoch 00073: loss did not improve from 2.83933\n","Epoch 74/100\n","16319/16319 [==============================] - 7s 429us/step - loss: 2.9214 - val_loss: 1.2347\n","\n","Epoch 00074: loss did not improve from 2.83933\n","Epoch 75/100\n","16319/16319 [==============================] - 7s 431us/step - loss: 2.8328 - val_loss: 1.2723\n","\n","Epoch 00075: loss improved from 2.83933 to 2.83284, saving model to finalSolution.h5\n","Epoch 76/100\n","16319/16319 [==============================] - 7s 433us/step - loss: 2.9509 - val_loss: 1.5395\n","\n","Epoch 00076: loss did not improve from 2.83284\n","Epoch 77/100\n","16319/16319 [==============================] - 7s 429us/step - loss: 2.9182 - val_loss: 1.2435\n","\n","Epoch 00077: loss did not improve from 2.83284\n","Epoch 78/100\n","16319/16319 [==============================] - 7s 438us/step - loss: 2.8637 - val_loss: 1.2276\n","\n","Epoch 00078: loss did not improve from 2.83284\n","Epoch 79/100\n","16319/16319 [==============================] - 7s 435us/step - loss: 2.8476 - val_loss: 1.1024\n","\n","Epoch 00079: loss did not improve from 2.83284\n","Epoch 80/100\n","16319/16319 [==============================] - 7s 425us/step - loss: 2.7330 - val_loss: 1.2234\n","\n","Epoch 00080: loss improved from 2.83284 to 2.73297, saving model to finalSolution.h5\n","Epoch 81/100\n","16319/16319 [==============================] - 7s 437us/step - loss: 2.8456 - val_loss: 1.3526\n","\n","Epoch 00081: loss did not improve from 2.73297\n","Epoch 82/100\n","16319/16319 [==============================] - 7s 422us/step - loss: 2.9011 - val_loss: 1.1222\n","\n","Epoch 00082: loss did not improve from 2.73297\n","Epoch 83/100\n","16319/16319 [==============================] - 7s 421us/step - loss: 2.7729 - val_loss: 1.1624\n","\n","Epoch 00083: loss did not improve from 2.73297\n","Epoch 84/100\n","16319/16319 [==============================] - 7s 427us/step - loss: 2.7721 - val_loss: 1.4756\n","\n","Epoch 00084: loss did not improve from 2.73297\n","Epoch 85/100\n","16319/16319 [==============================] - 7s 449us/step - loss: 2.9078 - val_loss: 1.1560\n","\n","Epoch 00085: loss did not improve from 2.73297\n","Epoch 86/100\n","16319/16319 [==============================] - 7s 433us/step - loss: 2.8265 - val_loss: 1.2717\n","\n","Epoch 00086: loss did not improve from 2.73297\n","Epoch 87/100\n","16319/16319 [==============================] - 7s 419us/step - loss: 2.7244 - val_loss: 1.2739\n","\n","Epoch 00087: loss improved from 2.73297 to 2.72436, saving model to finalSolution.h5\n","Epoch 88/100\n","16319/16319 [==============================] - 7s 429us/step - loss: 2.8080 - val_loss: 1.4751\n","\n","Epoch 00088: loss did not improve from 2.72436\n","Epoch 89/100\n","16319/16319 [==============================] - 7s 441us/step - loss: 2.8486 - val_loss: 1.2844\n","\n","Epoch 00089: loss did not improve from 2.72436\n","Epoch 90/100\n","16319/16319 [==============================] - 7s 431us/step - loss: 2.7808 - val_loss: 1.4727\n","\n","Epoch 00090: loss did not improve from 2.72436\n","Epoch 91/100\n","16319/16319 [==============================] - 7s 448us/step - loss: 2.8500 - val_loss: 1.4536\n","\n","Epoch 00091: loss did not improve from 2.72436\n","Epoch 92/100\n","16319/16319 [==============================] - 7s 446us/step - loss: 2.6999 - val_loss: 1.0698\n","\n","Epoch 00092: loss improved from 2.72436 to 2.69986, saving model to finalSolution.h5\n","Epoch 93/100\n","16319/16319 [==============================] - 7s 447us/step - loss: 2.7765 - val_loss: 1.3486\n","\n","Epoch 00093: loss did not improve from 2.69986\n","Epoch 94/100\n","16319/16319 [==============================] - 7s 441us/step - loss: 2.7281 - val_loss: 1.0857\n","\n","Epoch 00094: loss did not improve from 2.69986\n","Epoch 95/100\n","16319/16319 [==============================] - 7s 433us/step - loss: 2.7021 - val_loss: 1.0607\n","\n","Epoch 00095: loss did not improve from 2.69986\n","Epoch 96/100\n","16319/16319 [==============================] - 7s 435us/step - loss: 2.7068 - val_loss: 1.0851\n","\n","Epoch 00096: loss did not improve from 2.69986\n","Epoch 97/100\n","16319/16319 [==============================] - 7s 443us/step - loss: 2.8126 - val_loss: 1.2644\n","\n","Epoch 00097: loss did not improve from 2.69986\n","Epoch 98/100\n","16319/16319 [==============================] - 7s 446us/step - loss: 2.7330 - val_loss: 1.1866\n","\n","Epoch 00098: loss did not improve from 2.69986\n","Epoch 99/100\n","16319/16319 [==============================] - 7s 442us/step - loss: 2.8312 - val_loss: 1.1348\n","\n","Epoch 00099: loss did not improve from 2.69986\n","Epoch 100/100\n","16319/16319 [==============================] - 7s 434us/step - loss: 2.7107 - val_loss: 1.0157\n","\n","Epoch 00100: loss did not improve from 2.69986\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I6ozv8wY9EKD","colab_type":"code","outputId":"aa9cc06b-3fae-46c1-c321-5809c25432df","executionInfo":{"status":"ok","timestamp":1590173293655,"user_tz":360,"elapsed":1793978,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet_final.py /content/train.mp4 /content/train.txt --mode=train --split_start=7700 --split_end=12100 --model  finalSolution.h5 --epoch 100 --history 1 --augment "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-22 18:17:49.909514: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet_final.py:176: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-22 18:17:52.848301: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-22 18:17:52.912884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 18:17:52.913856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-22 18:17:52.913913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-22 18:17:53.162971: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-22 18:17:53.302176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-22 18:17:53.337947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-22 18:17:53.605565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-22 18:17:53.648210: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-22 18:17:54.154709: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 18:17:54.154895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 18:17:54.155897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 18:17:54.156743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-22 18:17:54.182205: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-05-22 18:17:54.182552: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1be72c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-22 18:17:54.182591: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-22 18:17:54.313487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 18:17:54.314510: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1be7480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-22 18:17:54.314553: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-22 18:17:54.315834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 18:17:54.316772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-22 18:17:54.316839: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-22 18:17:54.316904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-22 18:17:54.316947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-22 18:17:54.316990: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-22 18:17:54.317034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-22 18:17:54.317077: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-22 18:17:54.317118: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-22 18:17:54.317211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 18:17:54.318203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 18:17:54.319084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-22 18:17:54.323553: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-22 18:18:00.742238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-22 18:18:00.742293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-22 18:18:00.742321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-22 18:18:00.748143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 18:18:00.749130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-22 18:18:00.750068: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-22 18:18:00.750125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet_final.py:178: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","speednet_final.py:180: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400frames\n","Done prepping data\n","Starting training\n","Train on 16319 samples, validate on 4080 samples\n","Epoch 1/100\n","2020-05-22 18:28:16.804230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-22 18:28:18.258097: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","16319/16319 [==============================] - 22s 1ms/step - loss: 76.0936 - val_loss: 68.7361\n","\n","Epoch 00001: loss improved from inf to 76.09364, saving model to finalSolution.h5\n","Epoch 2/100\n","16319/16319 [==============================] - 16s 961us/step - loss: 71.8343 - val_loss: 67.7167\n","\n","Epoch 00002: loss improved from 76.09364 to 71.83429, saving model to finalSolution.h5\n","Epoch 3/100\n","16319/16319 [==============================] - 16s 960us/step - loss: 69.8357 - val_loss: 68.9362\n","\n","Epoch 00003: loss improved from 71.83429 to 69.83574, saving model to finalSolution.h5\n","Epoch 4/100\n","16319/16319 [==============================] - 16s 961us/step - loss: 69.0390 - val_loss: 67.7287\n","\n","Epoch 00004: loss improved from 69.83574 to 69.03904, saving model to finalSolution.h5\n","Epoch 5/100\n","16319/16319 [==============================] - 16s 959us/step - loss: 68.8336 - val_loss: 67.6258\n","\n","Epoch 00005: loss improved from 69.03904 to 68.83362, saving model to finalSolution.h5\n","Epoch 6/100\n","16319/16319 [==============================] - 16s 958us/step - loss: 68.8539 - val_loss: 67.9834\n","\n","Epoch 00006: loss did not improve from 68.83362\n","Epoch 7/100\n","16319/16319 [==============================] - 16s 967us/step - loss: 68.6046 - val_loss: 67.9353\n","\n","Epoch 00007: loss improved from 68.83362 to 68.60464, saving model to finalSolution.h5\n","Epoch 8/100\n","16319/16319 [==============================] - 16s 954us/step - loss: 68.9781 - val_loss: 67.9479\n","\n","Epoch 00008: loss did not improve from 68.60464\n","Epoch 9/100\n","16319/16319 [==============================] - 16s 987us/step - loss: 68.4868 - val_loss: 67.9672\n","\n","Epoch 00009: loss improved from 68.60464 to 68.48683, saving model to finalSolution.h5\n","Epoch 10/100\n","16319/16319 [==============================] - 16s 959us/step - loss: 68.6245 - val_loss: 67.9214\n","\n","Epoch 00010: loss did not improve from 68.48683\n","Epoch 11/100\n","16319/16319 [==============================] - 16s 961us/step - loss: 68.8734 - val_loss: 67.9263\n","\n","Epoch 00011: loss did not improve from 68.48683\n","Epoch 12/100\n","16319/16319 [==============================] - 16s 956us/step - loss: 68.6381 - val_loss: 67.0860\n","\n","Epoch 00012: loss did not improve from 68.48683\n","Epoch 13/100\n","16319/16319 [==============================] - 16s 958us/step - loss: 68.4719 - val_loss: 67.6457\n","\n","Epoch 00013: loss improved from 68.48683 to 68.47185, saving model to finalSolution.h5\n","Epoch 14/100\n","16319/16319 [==============================] - 16s 952us/step - loss: 68.7138 - val_loss: 67.9404\n","\n","Epoch 00014: loss did not improve from 68.47185\n","Epoch 15/100\n","16319/16319 [==============================] - 16s 951us/step - loss: 68.6114 - val_loss: 67.9001\n","\n","Epoch 00015: loss did not improve from 68.47185\n","Epoch 16/100\n","16319/16319 [==============================] - 16s 959us/step - loss: 68.3554 - val_loss: 67.9210\n","\n","Epoch 00016: loss improved from 68.47185 to 68.35535, saving model to finalSolution.h5\n","Epoch 17/100\n","16319/16319 [==============================] - 16s 958us/step - loss: 68.3930 - val_loss: 68.2249\n","\n","Epoch 00017: loss did not improve from 68.35535\n","Epoch 18/100\n","16319/16319 [==============================] - 16s 960us/step - loss: 68.2790 - val_loss: 67.9215\n","\n","Epoch 00018: loss improved from 68.35535 to 68.27905, saving model to finalSolution.h5\n","Epoch 19/100\n","16319/16319 [==============================] - 16s 958us/step - loss: 68.1721 - val_loss: 68.5053\n","\n","Epoch 00019: loss improved from 68.27905 to 68.17214, saving model to finalSolution.h5\n","Epoch 20/100\n","16319/16319 [==============================] - 16s 964us/step - loss: 68.3087 - val_loss: 68.4008\n","\n","Epoch 00020: loss did not improve from 68.17214\n","Epoch 21/100\n","16319/16319 [==============================] - 16s 962us/step - loss: 68.7120 - val_loss: 68.0107\n","\n","Epoch 00021: loss did not improve from 68.17214\n","Epoch 22/100\n","16319/16319 [==============================] - 16s 952us/step - loss: 68.3862 - val_loss: 67.9110\n","\n","Epoch 00022: loss did not improve from 68.17214\n","Epoch 23/100\n","16319/16319 [==============================] - 15s 948us/step - loss: 68.3837 - val_loss: 68.3210\n","\n","Epoch 00023: loss did not improve from 68.17214\n","Epoch 24/100\n","16319/16319 [==============================] - 16s 960us/step - loss: 68.3275 - val_loss: 68.0560\n","\n","Epoch 00024: loss did not improve from 68.17214\n","Epoch 25/100\n","16319/16319 [==============================] - 16s 965us/step - loss: 68.7119 - val_loss: 69.3138\n","\n","Epoch 00025: loss did not improve from 68.17214\n","Epoch 26/100\n","16319/16319 [==============================] - 16s 978us/step - loss: 68.5778 - val_loss: 67.5561\n","\n","Epoch 00026: loss did not improve from 68.17214\n","Epoch 27/100\n","16319/16319 [==============================] - 16s 962us/step - loss: 68.5036 - val_loss: 67.8620\n","\n","Epoch 00027: loss did not improve from 68.17214\n","Epoch 28/100\n","16319/16319 [==============================] - 16s 973us/step - loss: 68.2335 - val_loss: 67.8765\n","\n","Epoch 00028: loss did not improve from 68.17214\n","Epoch 29/100\n","16319/16319 [==============================] - 16s 977us/step - loss: 68.1174 - val_loss: 67.7803\n","\n","Epoch 00029: loss improved from 68.17214 to 68.11740, saving model to finalSolution.h5\n","Epoch 30/100\n","16319/16319 [==============================] - 16s 970us/step - loss: 68.2959 - val_loss: 68.3384\n","\n","Epoch 00030: loss did not improve from 68.11740\n","Epoch 31/100\n","16319/16319 [==============================] - 16s 969us/step - loss: 68.1868 - val_loss: 68.2872\n","\n","Epoch 00031: loss did not improve from 68.11740\n","Epoch 32/100\n","16319/16319 [==============================] - 16s 973us/step - loss: 68.2565 - val_loss: 67.7719\n","\n","Epoch 00032: loss did not improve from 68.11740\n","Epoch 33/100\n","16319/16319 [==============================] - 16s 971us/step - loss: 67.9573 - val_loss: 67.6242\n","\n","Epoch 00033: loss improved from 68.11740 to 67.95730, saving model to finalSolution.h5\n","Epoch 34/100\n","16319/16319 [==============================] - 16s 971us/step - loss: 68.2257 - val_loss: 67.6722\n","\n","Epoch 00034: loss did not improve from 67.95730\n","Epoch 35/100\n","16319/16319 [==============================] - 16s 971us/step - loss: 68.0999 - val_loss: 67.5780\n","\n","Epoch 00035: loss did not improve from 67.95730\n","Epoch 36/100\n","16319/16319 [==============================] - 16s 970us/step - loss: 68.5146 - val_loss: 66.9753\n","\n","Epoch 00036: loss did not improve from 67.95730\n","Epoch 37/100\n","16319/16319 [==============================] - 16s 968us/step - loss: 67.6414 - val_loss: 67.7542\n","\n","Epoch 00037: loss improved from 67.95730 to 67.64145, saving model to finalSolution.h5\n","Epoch 38/100\n","16319/16319 [==============================] - 16s 968us/step - loss: 68.0922 - val_loss: 67.7346\n","\n","Epoch 00038: loss did not improve from 67.64145\n","Epoch 39/100\n","16319/16319 [==============================] - 16s 968us/step - loss: 68.0868 - val_loss: 67.8164\n","\n","Epoch 00039: loss did not improve from 67.64145\n","Epoch 40/100\n","16319/16319 [==============================] - 16s 966us/step - loss: 68.1635 - val_loss: 68.1250\n","\n","Epoch 00040: loss did not improve from 67.64145\n","Epoch 41/100\n","16319/16319 [==============================] - 16s 967us/step - loss: 67.9444 - val_loss: 67.8501\n","\n","Epoch 00041: loss did not improve from 67.64145\n","Epoch 42/100\n","16319/16319 [==============================] - 16s 958us/step - loss: 68.1252 - val_loss: 67.4746\n","\n","Epoch 00042: loss did not improve from 67.64145\n","Epoch 43/100\n","16319/16319 [==============================] - 16s 964us/step - loss: 68.0732 - val_loss: 67.3446\n","\n","Epoch 00043: loss did not improve from 67.64145\n","Epoch 44/100\n","16319/16319 [==============================] - 16s 971us/step - loss: 67.8143 - val_loss: 67.3308\n","\n","Epoch 00044: loss did not improve from 67.64145\n","Epoch 45/100\n","16319/16319 [==============================] - 16s 974us/step - loss: 67.8094 - val_loss: 67.4588\n","\n","Epoch 00045: loss did not improve from 67.64145\n","Epoch 46/100\n","16319/16319 [==============================] - 16s 974us/step - loss: 67.9545 - val_loss: 67.4126\n","\n","Epoch 00046: loss did not improve from 67.64145\n","Epoch 47/100\n","16319/16319 [==============================] - 16s 969us/step - loss: 67.9626 - val_loss: 67.3508\n","\n","Epoch 00047: loss did not improve from 67.64145\n","Epoch 48/100\n","16319/16319 [==============================] - 16s 1ms/step - loss: 68.0819 - val_loss: 68.1626\n","\n","Epoch 00048: loss did not improve from 67.64145\n","Epoch 49/100\n","16319/16319 [==============================] - 16s 966us/step - loss: 67.9130 - val_loss: 67.4834\n","\n","Epoch 00049: loss did not improve from 67.64145\n","Epoch 50/100\n","16319/16319 [==============================] - 16s 967us/step - loss: 67.7667 - val_loss: 68.5231\n","\n","Epoch 00050: loss did not improve from 67.64145\n","Epoch 51/100\n","16319/16319 [==============================] - 16s 967us/step - loss: 67.9789 - val_loss: 67.8481\n","\n","Epoch 00051: loss did not improve from 67.64145\n","Epoch 52/100\n","16319/16319 [==============================] - 16s 965us/step - loss: 67.7734 - val_loss: 67.4942\n","\n","Epoch 00052: loss did not improve from 67.64145\n","Epoch 53/100\n","16319/16319 [==============================] - 16s 968us/step - loss: 68.0362 - val_loss: 67.4906\n","\n","Epoch 00053: loss did not improve from 67.64145\n","Epoch 54/100\n","16319/16319 [==============================] - 16s 959us/step - loss: 67.7185 - val_loss: 67.8679\n","\n","Epoch 00054: loss did not improve from 67.64145\n","Epoch 55/100\n","16319/16319 [==============================] - 16s 965us/step - loss: 68.0114 - val_loss: 66.7012\n","\n","Epoch 00055: loss did not improve from 67.64145\n","Epoch 56/100\n","16319/16319 [==============================] - 16s 966us/step - loss: 67.8063 - val_loss: 67.9191\n","\n","Epoch 00056: loss did not improve from 67.64145\n","Epoch 57/100\n","16319/16319 [==============================] - 16s 973us/step - loss: 67.6109 - val_loss: 68.0170\n","\n","Epoch 00057: loss improved from 67.64145 to 67.61092, saving model to finalSolution.h5\n","Epoch 58/100\n","16319/16319 [==============================] - 16s 970us/step - loss: 67.8265 - val_loss: 67.9199\n","\n","Epoch 00058: loss did not improve from 67.61092\n","Epoch 59/100\n","16319/16319 [==============================] - 16s 969us/step - loss: 67.9920 - val_loss: 67.7634\n","\n","Epoch 00059: loss did not improve from 67.61092\n","Epoch 60/100\n","16319/16319 [==============================] - 16s 963us/step - loss: 68.1671 - val_loss: 68.4583\n","\n","Epoch 00060: loss did not improve from 67.61092\n","Epoch 61/100\n","16319/16319 [==============================] - 16s 952us/step - loss: 67.7461 - val_loss: 67.9404\n","\n","Epoch 00061: loss did not improve from 67.61092\n","Epoch 62/100\n","16319/16319 [==============================] - 16s 968us/step - loss: 67.9133 - val_loss: 68.5510\n","\n","Epoch 00062: loss did not improve from 67.61092\n","Epoch 63/100\n","16319/16319 [==============================] - 16s 1ms/step - loss: 68.0417 - val_loss: 68.0665\n","\n","Epoch 00063: loss did not improve from 67.61092\n","Epoch 64/100\n","16319/16319 [==============================] - 16s 957us/step - loss: 68.1552 - val_loss: 67.9401\n","\n","Epoch 00064: loss did not improve from 67.61092\n","Epoch 65/100\n","16319/16319 [==============================] - 16s 965us/step - loss: 68.0530 - val_loss: 68.1190\n","\n","Epoch 00065: loss did not improve from 67.61092\n","Epoch 66/100\n","16319/16319 [==============================] - 16s 1ms/step - loss: 67.9859 - val_loss: 67.9892\n","\n","Epoch 00066: loss did not improve from 67.61092\n","Epoch 67/100\n","16319/16319 [==============================] - 16s 964us/step - loss: 67.9955 - val_loss: 68.0550\n","\n","Epoch 00067: loss did not improve from 67.61092\n","Epoch 68/100\n","16319/16319 [==============================] - 16s 996us/step - loss: 67.8772 - val_loss: 68.6138\n","\n","Epoch 00068: loss did not improve from 67.61092\n","Epoch 69/100\n","16319/16319 [==============================] - 16s 973us/step - loss: 68.0669 - val_loss: 68.1142\n","\n","Epoch 00069: loss did not improve from 67.61092\n","Epoch 70/100\n","16319/16319 [==============================] - 17s 1ms/step - loss: 67.8398 - val_loss: 67.8233\n","\n","Epoch 00070: loss did not improve from 67.61092\n","Epoch 71/100\n","16319/16319 [==============================] - 16s 960us/step - loss: 68.0505 - val_loss: 67.9243\n","\n","Epoch 00071: loss did not improve from 67.61092\n","Epoch 72/100\n","16319/16319 [==============================] - 16s 961us/step - loss: 67.7808 - val_loss: 67.9666\n","\n","Epoch 00072: loss did not improve from 67.61092\n","Epoch 73/100\n","16319/16319 [==============================] - 16s 961us/step - loss: 67.7209 - val_loss: 67.9145\n","\n","Epoch 00073: loss did not improve from 67.61092\n","Epoch 74/100\n"," 3530/16319 [=====>........................] - ETA: 13s - loss: 67.9356Traceback (most recent call last):\n","  File \"speednet_final.py\", line 347, in <module>\n","    net.main(args)\n","  File \"speednet_final.py\", line 56, in main\n","    self.train(args.video_file,args.speed_file,args.split,args.wipe,self.EPOCHS,self.BATCH_SIZE,args.augment)\n","  File \"speednet_final.py\", line 268, in train\n","    callbacks=[checkpoint])\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1239, in fit\n","^C\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t68fphQFQ3eB","colab_type":"text"},"source":["## **We are using the Final version for testing**"]},{"cell_type":"markdown","metadata":{"id":"hzUjlsKCROD7","colab_type":"text"},"source":["Without LSTM and no augment"]},{"cell_type":"code","metadata":{"id":"9GOM_cF8TDgn","colab_type":"code","outputId":"d371430b-f9a3-4c15-9b31-5c5537418aa8","executionInfo":{"status":"ok","timestamp":1590267794738,"user_tz":360,"elapsed":408094,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet_final_noLSTM.py train.mp4 train.txt --mode=train --split=0.2 --model final.h5"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-23 20:55:58.632307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet_final_noLSTM.py:182: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), input_shape=(100, 100,..., strides=(4, 4), padding=\"same\")`\n","  self.model.add(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4),input_shape=(self.DSIZE[0],self.DSIZE[1],2)))\n","2020-05-23 20:56:00.342263: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-23 20:56:00.384028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 20:56:00.384607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-23 20:56:00.384645: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-23 20:56:00.631670: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-23 20:56:00.765291: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-23 20:56:00.792966: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-23 20:56:01.068061: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-23 20:56:01.126387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-23 20:56:01.648929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 20:56:01.649159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 20:56:01.649892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 20:56:01.650451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-23 20:56:01.670077: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-05-23 20:56:01.670376: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2abd100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-23 20:56:01.670408: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-23 20:56:01.839048: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 20:56:01.839779: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2abd2c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-23 20:56:01.839820: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-23 20:56:01.841051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 20:56:01.841642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-23 20:56:01.841706: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-23 20:56:01.841783: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-23 20:56:01.841810: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-23 20:56:01.841851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-23 20:56:01.841887: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-23 20:56:01.841909: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-23 20:56:01.841932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 20:56:01.842012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 20:56:01.842619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 20:56:01.843171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-23 20:56:01.847710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-23 20:56:08.237511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 20:56:08.237567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-23 20:56:08.237575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-23 20:56:08.243086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 20:56:08.243739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 20:56:08.244321: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-23 20:56:08.244362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet_final_noLSTM.py:184: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  self.model.add(Convolution2D(64, 8,8 ,border_mode='same',subsample=(4,4)))\n","speednet_final_noLSTM.py:186: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (4, 4), strides=(2, 2), padding=\"same\")`\n","  self.model.add(Convolution2D(128, 4,4,border_mode='same',subsample=(2,2)))\n","speednet_final_noLSTM.py:188: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (2, 2), strides=(1, 1), padding=\"same\")`\n","  self.model.add(Convolution2D(128, 2,2,border_mode='same',subsample=(1,1)))\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400frames\n","Done prepping data\n","Starting training\n","Train on 16319 samples, validate on 4080 samples\n","Epoch 1/50\n","2020-05-23 20:59:13.815138: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-23 20:59:15.230252: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","16319/16319 [==============================] - 10s 633us/step - loss: 34.5474 - val_loss: 14.4853\n","\n","Epoch 00001: loss improved from inf to 34.54737, saving model to final.h5\n","Epoch 2/50\n","16319/16319 [==============================] - 4s 248us/step - loss: 19.6011 - val_loss: 13.0335\n","\n","Epoch 00002: loss improved from 34.54737 to 19.60114, saving model to final.h5\n","Epoch 3/50\n","16319/16319 [==============================] - 4s 247us/step - loss: 15.4965 - val_loss: 10.8868\n","\n","Epoch 00003: loss improved from 19.60114 to 15.49647, saving model to final.h5\n","Epoch 4/50\n","16319/16319 [==============================] - 4s 248us/step - loss: 12.6730 - val_loss: 11.1051\n","\n","Epoch 00004: loss improved from 15.49647 to 12.67298, saving model to final.h5\n","Epoch 5/50\n","16319/16319 [==============================] - 4s 265us/step - loss: 11.9425 - val_loss: 10.2390\n","\n","Epoch 00005: loss improved from 12.67298 to 11.94251, saving model to final.h5\n","Epoch 6/50\n","16319/16319 [==============================] - 4s 263us/step - loss: 10.8245 - val_loss: 10.6321\n","\n","Epoch 00006: loss improved from 11.94251 to 10.82446, saving model to final.h5\n","Epoch 7/50\n","16319/16319 [==============================] - 4s 259us/step - loss: 9.9363 - val_loss: 9.5056\n","\n","Epoch 00007: loss improved from 10.82446 to 9.93632, saving model to final.h5\n","Epoch 8/50\n","16319/16319 [==============================] - 4s 249us/step - loss: 9.8493 - val_loss: 9.8327\n","\n","Epoch 00008: loss improved from 9.93632 to 9.84933, saving model to final.h5\n","Epoch 9/50\n","16319/16319 [==============================] - 4s 246us/step - loss: 9.4142 - val_loss: 9.9796\n","\n","Epoch 00009: loss improved from 9.84933 to 9.41424, saving model to final.h5\n","Epoch 10/50\n","16319/16319 [==============================] - 4s 248us/step - loss: 9.3369 - val_loss: 9.5548\n","\n","Epoch 00010: loss improved from 9.41424 to 9.33691, saving model to final.h5\n","Epoch 11/50\n","16319/16319 [==============================] - 4s 246us/step - loss: 9.2957 - val_loss: 9.5117\n","\n","Epoch 00011: loss improved from 9.33691 to 9.29570, saving model to final.h5\n","Epoch 12/50\n","16319/16319 [==============================] - 4s 247us/step - loss: 9.5238 - val_loss: 8.5513\n","\n","Epoch 00012: loss did not improve from 9.29570\n","Epoch 13/50\n","16319/16319 [==============================] - 4s 247us/step - loss: 8.7932 - val_loss: 8.5834\n","\n","Epoch 00013: loss improved from 9.29570 to 8.79322, saving model to final.h5\n","Epoch 14/50\n","16319/16319 [==============================] - 4s 252us/step - loss: 8.4878 - val_loss: 7.6578\n","\n","Epoch 00014: loss improved from 8.79322 to 8.48780, saving model to final.h5\n","Epoch 15/50\n","16319/16319 [==============================] - 4s 250us/step - loss: 8.2978 - val_loss: 9.0021\n","\n","Epoch 00015: loss improved from 8.48780 to 8.29776, saving model to final.h5\n","Epoch 16/50\n","16319/16319 [==============================] - 4s 247us/step - loss: 8.3036 - val_loss: 8.6529\n","\n","Epoch 00016: loss did not improve from 8.29776\n","Epoch 17/50\n","16319/16319 [==============================] - 4s 248us/step - loss: 7.6699 - val_loss: 8.2311\n","\n","Epoch 00017: loss improved from 8.29776 to 7.66989, saving model to final.h5\n","Epoch 18/50\n","16319/16319 [==============================] - 4s 248us/step - loss: 7.8760 - val_loss: 7.9067\n","\n","Epoch 00018: loss did not improve from 7.66989\n","Epoch 19/50\n","16319/16319 [==============================] - 4s 246us/step - loss: 7.7073 - val_loss: 9.0056\n","\n","Epoch 00019: loss did not improve from 7.66989\n","Epoch 20/50\n","16319/16319 [==============================] - 4s 245us/step - loss: 7.7423 - val_loss: 8.7424\n","\n","Epoch 00020: loss did not improve from 7.66989\n","Epoch 21/50\n","16319/16319 [==============================] - 4s 245us/step - loss: 7.3334 - val_loss: 8.5891\n","\n","Epoch 00021: loss improved from 7.66989 to 7.33339, saving model to final.h5\n","Epoch 22/50\n","16319/16319 [==============================] - 4s 244us/step - loss: 7.1034 - val_loss: 8.8243\n","\n","Epoch 00022: loss improved from 7.33339 to 7.10341, saving model to final.h5\n","Epoch 23/50\n","16319/16319 [==============================] - 4s 246us/step - loss: 6.7705 - val_loss: 8.3715\n","\n","Epoch 00023: loss improved from 7.10341 to 6.77048, saving model to final.h5\n","Epoch 24/50\n","16319/16319 [==============================] - 4s 246us/step - loss: 6.7004 - val_loss: 8.1461\n","\n","Epoch 00024: loss improved from 6.77048 to 6.70044, saving model to final.h5\n","Epoch 25/50\n","16319/16319 [==============================] - 4s 245us/step - loss: 7.1588 - val_loss: 8.2381\n","\n","Epoch 00025: loss did not improve from 6.70044\n","Epoch 26/50\n","16319/16319 [==============================] - 4s 245us/step - loss: 6.7729 - val_loss: 7.2866\n","\n","Epoch 00026: loss did not improve from 6.70044\n","Epoch 27/50\n","16319/16319 [==============================] - 4s 246us/step - loss: 7.0533 - val_loss: 8.4564\n","\n","Epoch 00027: loss did not improve from 6.70044\n","Epoch 28/50\n","16319/16319 [==============================] - 4s 244us/step - loss: 6.3938 - val_loss: 8.0723\n","\n","Epoch 00028: loss improved from 6.70044 to 6.39383, saving model to final.h5\n","Epoch 29/50\n","16319/16319 [==============================] - 4s 244us/step - loss: 6.5496 - val_loss: 7.6190\n","\n","Epoch 00029: loss did not improve from 6.39383\n","Epoch 30/50\n","16319/16319 [==============================] - 4s 246us/step - loss: 6.3536 - val_loss: 7.6328\n","\n","Epoch 00030: loss improved from 6.39383 to 6.35359, saving model to final.h5\n","Epoch 31/50\n","16319/16319 [==============================] - 4s 245us/step - loss: 6.5525 - val_loss: 8.5860\n","\n","Epoch 00031: loss did not improve from 6.35359\n","Epoch 32/50\n","16319/16319 [==============================] - 4s 244us/step - loss: 6.2270 - val_loss: 8.0650\n","\n","Epoch 00032: loss improved from 6.35359 to 6.22699, saving model to final.h5\n","Epoch 33/50\n","16319/16319 [==============================] - 4s 244us/step - loss: 6.4516 - val_loss: 7.6708\n","\n","Epoch 00033: loss did not improve from 6.22699\n","Epoch 34/50\n","16319/16319 [==============================] - 4s 249us/step - loss: 6.0686 - val_loss: 8.1583\n","\n","Epoch 00034: loss improved from 6.22699 to 6.06861, saving model to final.h5\n","Epoch 35/50\n","16319/16319 [==============================] - 4s 251us/step - loss: 5.9129 - val_loss: 7.5487\n","\n","Epoch 00035: loss improved from 6.06861 to 5.91291, saving model to final.h5\n","Epoch 36/50\n","16319/16319 [==============================] - 4s 246us/step - loss: 5.9319 - val_loss: 7.6016\n","\n","Epoch 00036: loss did not improve from 5.91291\n","Epoch 37/50\n","16319/16319 [==============================] - 4s 245us/step - loss: 6.2945 - val_loss: 10.5968\n","\n","Epoch 00037: loss did not improve from 5.91291\n","Epoch 38/50\n","16319/16319 [==============================] - 4s 246us/step - loss: 6.1307 - val_loss: 9.1046\n","\n","Epoch 00038: loss did not improve from 5.91291\n","Epoch 39/50\n","16319/16319 [==============================] - 4s 245us/step - loss: 5.6089 - val_loss: 7.4876\n","\n","Epoch 00039: loss improved from 5.91291 to 5.60886, saving model to final.h5\n","Epoch 40/50\n","16319/16319 [==============================] - 4s 246us/step - loss: 5.5042 - val_loss: 6.8743\n","\n","Epoch 00040: loss improved from 5.60886 to 5.50420, saving model to final.h5\n","Epoch 41/50\n","16319/16319 [==============================] - 4s 245us/step - loss: 5.3201 - val_loss: 7.3314\n","\n","Epoch 00041: loss improved from 5.50420 to 5.32008, saving model to final.h5\n","Epoch 42/50\n","16319/16319 [==============================] - 4s 245us/step - loss: 5.8567 - val_loss: 7.7692\n","\n","Epoch 00042: loss did not improve from 5.32008\n","Epoch 43/50\n","16319/16319 [==============================] - 4s 247us/step - loss: 5.6684 - val_loss: 7.8841\n","\n","Epoch 00043: loss did not improve from 5.32008\n","Epoch 44/50\n","16319/16319 [==============================] - 4s 249us/step - loss: 5.4179 - val_loss: 8.7691\n","\n","Epoch 00044: loss did not improve from 5.32008\n","Epoch 45/50\n","16319/16319 [==============================] - 4s 248us/step - loss: 5.4369 - val_loss: 7.3468\n","\n","Epoch 00045: loss did not improve from 5.32008\n","Epoch 46/50\n","16319/16319 [==============================] - 4s 245us/step - loss: 5.5627 - val_loss: 8.3292\n","\n","Epoch 00046: loss did not improve from 5.32008\n","Epoch 47/50\n","16319/16319 [==============================] - 4s 247us/step - loss: 5.5338 - val_loss: 8.1485\n","\n","Epoch 00047: loss did not improve from 5.32008\n","Epoch 48/50\n","16319/16319 [==============================] - 4s 246us/step - loss: 5.4722 - val_loss: 7.6791\n","\n","Epoch 00048: loss did not improve from 5.32008\n","Epoch 49/50\n","16319/16319 [==============================] - 4s 245us/step - loss: 5.1799 - val_loss: 8.5342\n","\n","Epoch 00049: loss improved from 5.32008 to 5.17991, saving model to final.h5\n","Epoch 50/50\n","16319/16319 [==============================] - 4s 247us/step - loss: 5.3295 - val_loss: 9.6136\n","\n","Epoch 00050: loss did not improve from 5.17991\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jUQYyOftU428","colab_type":"text"},"source":["Without LSTM and augment"]},{"cell_type":"code","metadata":{"id":"xFtO3IyqUhp3","colab_type":"code","outputId":"3d751945-88fa-4053-fa88-e48750eadec9","executionInfo":{"status":"ok","timestamp":1590268561272,"user_tz":360,"elapsed":716311,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet_final_noLSTM.py train.mp4 train.txt --mode=train --split=0.2 --model final.h5 --augment --wipe"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-23 21:03:37.641827: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet_final_noLSTM.py:182: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), input_shape=(100, 100,..., strides=(4, 4), padding=\"same\")`\n","  self.model.add(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4),input_shape=(self.DSIZE[0],self.DSIZE[1],2)))\n","2020-05-23 21:03:39.312312: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-23 21:03:39.324533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 21:03:39.325219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-23 21:03:39.325260: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-23 21:03:39.327154: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-23 21:03:39.328803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-23 21:03:39.329169: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-23 21:03:39.331016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-23 21:03:39.332208: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-23 21:03:39.336018: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 21:03:39.336138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 21:03:39.336783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 21:03:39.337356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-23 21:03:39.342672: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-05-23 21:03:39.342974: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1821100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-23 21:03:39.343010: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-23 21:03:39.432651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 21:03:39.433320: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18212c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-23 21:03:39.433351: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-23 21:03:39.433530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 21:03:39.434184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-23 21:03:39.434228: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-23 21:03:39.434272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-23 21:03:39.434284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-23 21:03:39.434297: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-23 21:03:39.434308: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-23 21:03:39.434322: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-23 21:03:39.434336: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 21:03:39.434407: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 21:03:39.434951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 21:03:39.435433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-23 21:03:39.435477: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-23 21:03:39.921894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 21:03:39.921958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-23 21:03:39.921967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-23 21:03:39.922176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 21:03:39.922746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 21:03:39.923277: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-23 21:03:39.923319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet_final_noLSTM.py:184: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  self.model.add(Convolution2D(64, 8,8 ,border_mode='same',subsample=(4,4)))\n","speednet_final_noLSTM.py:186: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (4, 4), strides=(2, 2), padding=\"same\")`\n","  self.model.add(Convolution2D(128, 4,4,border_mode='same',subsample=(2,2)))\n","speednet_final_noLSTM.py:188: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (2, 2), strides=(1, 1), padding=\"same\")`\n","  self.model.add(Convolution2D(128, 2,2,border_mode='same',subsample=(1,1)))\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400frames\n","Done prepping data\n","Starting training\n","Train on 16319 samples, validate on 4080 samples\n","Epoch 1/50\n","2020-05-23 21:12:42.493356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-23 21:12:42.741345: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","16319/16319 [==============================] - 5s 294us/step - loss: 38.7751 - val_loss: 14.5337\n","\n","Epoch 00001: loss improved from inf to 38.77506, saving model to final.h5\n","Epoch 2/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 21.5979 - val_loss: 14.1022\n","\n","Epoch 00002: loss improved from 38.77506 to 21.59790, saving model to final.h5\n","Epoch 3/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 16.5113 - val_loss: 11.7522\n","\n","Epoch 00003: loss improved from 21.59790 to 16.51130, saving model to final.h5\n","Epoch 4/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 13.8201 - val_loss: 11.1820\n","\n","Epoch 00004: loss improved from 16.51130 to 13.82011, saving model to final.h5\n","Epoch 5/50\n","16319/16319 [==============================] - 3s 198us/step - loss: 12.5085 - val_loss: 12.2144\n","\n","Epoch 00005: loss improved from 13.82011 to 12.50853, saving model to final.h5\n","Epoch 6/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 12.0264 - val_loss: 11.4807\n","\n","Epoch 00006: loss improved from 12.50853 to 12.02639, saving model to final.h5\n","Epoch 7/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 11.2594 - val_loss: 12.8703\n","\n","Epoch 00007: loss improved from 12.02639 to 11.25944, saving model to final.h5\n","Epoch 8/50\n","16319/16319 [==============================] - 3s 198us/step - loss: 10.9135 - val_loss: 11.0718\n","\n","Epoch 00008: loss improved from 11.25944 to 10.91347, saving model to final.h5\n","Epoch 9/50\n","16319/16319 [==============================] - 3s 200us/step - loss: 10.3679 - val_loss: 11.4599\n","\n","Epoch 00009: loss improved from 10.91347 to 10.36788, saving model to final.h5\n","Epoch 10/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 10.7672 - val_loss: 9.8730\n","\n","Epoch 00010: loss did not improve from 10.36788\n","Epoch 11/50\n","16319/16319 [==============================] - 3s 198us/step - loss: 10.4499 - val_loss: 11.5243\n","\n","Epoch 00011: loss did not improve from 10.36788\n","Epoch 12/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 10.0127 - val_loss: 11.3244\n","\n","Epoch 00012: loss improved from 10.36788 to 10.01267, saving model to final.h5\n","Epoch 13/50\n","16319/16319 [==============================] - 3s 201us/step - loss: 9.5913 - val_loss: 9.7318\n","\n","Epoch 00013: loss improved from 10.01267 to 9.59132, saving model to final.h5\n","Epoch 14/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 9.7565 - val_loss: 10.1347\n","\n","Epoch 00014: loss did not improve from 9.59132\n","Epoch 15/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 9.3892 - val_loss: 10.3045\n","\n","Epoch 00015: loss improved from 9.59132 to 9.38920, saving model to final.h5\n","Epoch 16/50\n","16319/16319 [==============================] - 3s 200us/step - loss: 9.2565 - val_loss: 10.6990\n","\n","Epoch 00016: loss improved from 9.38920 to 9.25651, saving model to final.h5\n","Epoch 17/50\n","16319/16319 [==============================] - 3s 200us/step - loss: 8.8352 - val_loss: 9.4596\n","\n","Epoch 00017: loss improved from 9.25651 to 8.83521, saving model to final.h5\n","Epoch 18/50\n","16319/16319 [==============================] - 3s 202us/step - loss: 8.7283 - val_loss: 11.1469\n","\n","Epoch 00018: loss improved from 8.83521 to 8.72831, saving model to final.h5\n","Epoch 19/50\n","16319/16319 [==============================] - 3s 201us/step - loss: 8.2707 - val_loss: 9.8035\n","\n","Epoch 00019: loss improved from 8.72831 to 8.27070, saving model to final.h5\n","Epoch 20/50\n","16319/16319 [==============================] - 3s 202us/step - loss: 8.1579 - val_loss: 12.3339\n","\n","Epoch 00020: loss improved from 8.27070 to 8.15788, saving model to final.h5\n","Epoch 21/50\n","16319/16319 [==============================] - 3s 200us/step - loss: 7.8887 - val_loss: 11.3303\n","\n","Epoch 00021: loss improved from 8.15788 to 7.88867, saving model to final.h5\n","Epoch 22/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 7.9343 - val_loss: 9.9906\n","\n","Epoch 00022: loss did not improve from 7.88867\n","Epoch 23/50\n","16319/16319 [==============================] - 3s 200us/step - loss: 7.7424 - val_loss: 9.9870\n","\n","Epoch 00023: loss improved from 7.88867 to 7.74243, saving model to final.h5\n","Epoch 24/50\n","16319/16319 [==============================] - 3s 200us/step - loss: 7.6635 - val_loss: 10.5284\n","\n","Epoch 00024: loss improved from 7.74243 to 7.66353, saving model to final.h5\n","Epoch 25/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 7.3954 - val_loss: 8.8327\n","\n","Epoch 00025: loss improved from 7.66353 to 7.39543, saving model to final.h5\n","Epoch 26/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 7.1851 - val_loss: 9.0536\n","\n","Epoch 00026: loss improved from 7.39543 to 7.18509, saving model to final.h5\n","Epoch 27/50\n","16319/16319 [==============================] - 3s 200us/step - loss: 7.3810 - val_loss: 9.4496\n","\n","Epoch 00027: loss did not improve from 7.18509\n","Epoch 28/50\n","16319/16319 [==============================] - 3s 201us/step - loss: 7.1833 - val_loss: 9.6900\n","\n","Epoch 00028: loss improved from 7.18509 to 7.18333, saving model to final.h5\n","Epoch 29/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 6.9085 - val_loss: 8.5333\n","\n","Epoch 00029: loss improved from 7.18333 to 6.90852, saving model to final.h5\n","Epoch 30/50\n","16319/16319 [==============================] - 3s 198us/step - loss: 6.6258 - val_loss: 8.7645\n","\n","Epoch 00030: loss improved from 6.90852 to 6.62582, saving model to final.h5\n","Epoch 31/50\n","16319/16319 [==============================] - 3s 200us/step - loss: 6.6462 - val_loss: 9.6450\n","\n","Epoch 00031: loss did not improve from 6.62582\n","Epoch 32/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 6.6298 - val_loss: 10.5045\n","\n","Epoch 00032: loss did not improve from 6.62582\n","Epoch 33/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 6.7586 - val_loss: 12.3685\n","\n","Epoch 00033: loss did not improve from 6.62582\n","Epoch 34/50\n","16319/16319 [==============================] - 3s 198us/step - loss: 6.5073 - val_loss: 10.0642\n","\n","Epoch 00034: loss improved from 6.62582 to 6.50731, saving model to final.h5\n","Epoch 35/50\n","16319/16319 [==============================] - 3s 200us/step - loss: 6.4376 - val_loss: 9.3902\n","\n","Epoch 00035: loss improved from 6.50731 to 6.43760, saving model to final.h5\n","Epoch 36/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 6.7205 - val_loss: 10.8098\n","\n","Epoch 00036: loss did not improve from 6.43760\n","Epoch 37/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 6.4046 - val_loss: 9.3203\n","\n","Epoch 00037: loss improved from 6.43760 to 6.40460, saving model to final.h5\n","Epoch 38/50\n","16319/16319 [==============================] - 3s 201us/step - loss: 6.0909 - val_loss: 10.1501\n","\n","Epoch 00038: loss improved from 6.40460 to 6.09090, saving model to final.h5\n","Epoch 39/50\n","16319/16319 [==============================] - 3s 200us/step - loss: 5.9330 - val_loss: 9.5323\n","\n","Epoch 00039: loss improved from 6.09090 to 5.93295, saving model to final.h5\n","Epoch 40/50\n","16319/16319 [==============================] - 3s 200us/step - loss: 6.1095 - val_loss: 9.8490\n","\n","Epoch 00040: loss did not improve from 5.93295\n","Epoch 41/50\n","16319/16319 [==============================] - 3s 200us/step - loss: 5.8356 - val_loss: 9.8725\n","\n","Epoch 00041: loss improved from 5.93295 to 5.83560, saving model to final.h5\n","Epoch 42/50\n","16319/16319 [==============================] - 3s 206us/step - loss: 6.2215 - val_loss: 10.4337\n","\n","Epoch 00042: loss did not improve from 5.83560\n","Epoch 43/50\n","16319/16319 [==============================] - 3s 211us/step - loss: 6.1267 - val_loss: 9.8234\n","\n","Epoch 00043: loss did not improve from 5.83560\n","Epoch 44/50\n","16319/16319 [==============================] - 3s 208us/step - loss: 6.2635 - val_loss: 9.5949\n","\n","Epoch 00044: loss did not improve from 5.83560\n","Epoch 45/50\n","16319/16319 [==============================] - 3s 210us/step - loss: 5.8030 - val_loss: 8.8053\n","\n","Epoch 00045: loss improved from 5.83560 to 5.80304, saving model to final.h5\n","Epoch 46/50\n","16319/16319 [==============================] - 3s 212us/step - loss: 5.8132 - val_loss: 8.7974\n","\n","Epoch 00046: loss did not improve from 5.80304\n","Epoch 47/50\n","16319/16319 [==============================] - 3s 209us/step - loss: 5.7057 - val_loss: 8.6284\n","\n","Epoch 00047: loss improved from 5.80304 to 5.70570, saving model to final.h5\n","Epoch 48/50\n","16319/16319 [==============================] - 3s 202us/step - loss: 5.2996 - val_loss: 9.1053\n","\n","Epoch 00048: loss improved from 5.70570 to 5.29956, saving model to final.h5\n","Epoch 49/50\n","16319/16319 [==============================] - 3s 202us/step - loss: 5.8121 - val_loss: 9.3017\n","\n","Epoch 00049: loss did not improve from 5.29956\n","Epoch 50/50\n","16319/16319 [==============================] - 3s 199us/step - loss: 5.6875 - val_loss: 9.3441\n","\n","Epoch 00050: loss did not improve from 5.29956\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AQR3eyovX5V3","colab_type":"text"},"source":["With LSTM and No augment changing the LR"]},{"cell_type":"code","metadata":{"id":"WeqvujF7WZdt","colab_type":"code","outputId":"fa0cf5c0-19ba-4f84-d704-aaa37397657f","executionInfo":{"status":"ok","timestamp":1590275695476,"user_tz":360,"elapsed":4053344,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet_final_with_LSTM.py train.mp4 train.txt --mode=train --split=0.2 --model final.h5 --epoch 400 --history 1 --LR 0.001"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-23 22:06:54.025285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet_final_with_LSTM.py:180: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-23 22:06:55.726743: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-23 22:06:55.739576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 22:06:55.740178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-23 22:06:55.740213: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-23 22:06:55.742082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-23 22:06:55.749612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-23 22:06:55.749955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-23 22:06:55.751619: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-23 22:06:55.752959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-23 22:06:55.756903: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 22:06:55.757023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 22:06:55.757584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 22:06:55.758075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-23 22:06:55.763424: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-05-23 22:06:55.763690: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x31732c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-23 22:06:55.763726: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-23 22:06:55.850993: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 22:06:55.851683: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3173480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-23 22:06:55.851724: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-23 22:06:55.851935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 22:06:55.852440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-23 22:06:55.852480: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-23 22:06:55.852523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-23 22:06:55.852539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-23 22:06:55.852552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-23 22:06:55.852566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-23 22:06:55.852579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-23 22:06:55.852593: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 22:06:55.852649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 22:06:55.853219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 22:06:55.853711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-23 22:06:55.853789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-23 22:06:56.352192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 22:06:56.352246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-23 22:06:56.352256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-23 22:06:56.352454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 22:06:56.353096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 22:06:56.353615: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-23 22:06:56.353653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet_final_with_LSTM.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","speednet_final_with_LSTM.py:185: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","Found preprocessed data\n","tcmalloc: large alloc 2447884288 bytes == 0x1fb5e000 @  0x7f9835ee11e7 0x7f98339c75e1 0x7f9833a2bc78 0x7f9833a2bf37 0x7f9833ac3f28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f9835adeb97 0x5b250a\n","tcmalloc: large alloc 2447884288 bytes == 0xb19da000 @  0x7f9835ee11e7 0x7f98339c75e1 0x7f9833a2bc78 0x7f9833a2bf37 0x7f9833ac3f28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f9835adeb97 0x5b250a\n","Loading frame 20398\n","done loading 20399 frames\n","Done prepping data\n","tcmalloc: large alloc 1631920128 bytes == 0x1438b2000 @  0x7f9835ee11e7 0x7f98339c75e1 0x7f9833a2bc78 0x7f9833a2bd93 0x7f9833ab6ed6 0x7f9833ab7338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f9835adeb97 0x5b250a\n","tcmalloc: large alloc 1631920128 bytes == 0x1fb5e000 @  0x7f9835ee11e7 0x7f98339c75e1 0x7f9833a2bc78 0x7f9833a2bd93 0x7f9833ab6ed6 0x7f9833ab7338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f9835adeb97 0x5b250a\n","Starting training\n","Train on 16319 samples, validate on 4080 samples\n","Epoch 1/400\n","2020-05-23 22:07:08.328240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-23 22:07:08.532576: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","16319/16319 [==============================] - 12s 718us/step - loss: 74.5606 - val_loss: 71.4308\n","\n","Epoch 00001: loss improved from inf to 74.56058, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 72.4511 - val_loss: 67.9400\n","\n","Epoch 00002: loss improved from 74.56058 to 72.45108, saving model to final.h5\n","Epoch 3/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 70.5738 - val_loss: 67.9770\n","\n","Epoch 00003: loss improved from 72.45108 to 70.57378, saving model to final.h5\n","Epoch 4/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 69.6955 - val_loss: 67.9064\n","\n","Epoch 00004: loss improved from 70.57378 to 69.69555, saving model to final.h5\n","Epoch 5/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 69.1406 - val_loss: 67.9659\n","\n","Epoch 00005: loss improved from 69.69555 to 69.14061, saving model to final.h5\n","Epoch 6/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 69.3972 - val_loss: 68.4756\n","\n","Epoch 00006: loss did not improve from 69.14061\n","Epoch 7/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 69.2463 - val_loss: 67.9564\n","\n","Epoch 00007: loss did not improve from 69.14061\n","Epoch 8/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 69.1605 - val_loss: 67.9407\n","\n","Epoch 00008: loss did not improve from 69.14061\n","Epoch 9/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 69.4063 - val_loss: 68.0018\n","\n","Epoch 00009: loss did not improve from 69.14061\n","Epoch 10/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 69.2452 - val_loss: 68.3192\n","\n","Epoch 00010: loss did not improve from 69.14061\n","Epoch 11/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 69.0375 - val_loss: 67.9299\n","\n","Epoch 00011: loss improved from 69.14061 to 69.03752, saving model to final.h5\n","Epoch 12/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 69.3894 - val_loss: 67.9769\n","\n","Epoch 00012: loss did not improve from 69.03752\n","Epoch 13/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 69.1896 - val_loss: 68.1083\n","\n","Epoch 00013: loss did not improve from 69.03752\n","Epoch 14/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 69.0565 - val_loss: 67.9254\n","\n","Epoch 00014: loss did not improve from 69.03752\n","Epoch 15/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 68.7032 - val_loss: 67.9800\n","\n","Epoch 00015: loss improved from 69.03752 to 68.70316, saving model to final.h5\n","Epoch 16/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 68.9649 - val_loss: 67.9357\n","\n","Epoch 00016: loss did not improve from 68.70316\n","Epoch 17/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 68.9112 - val_loss: 67.9401\n","\n","Epoch 00017: loss did not improve from 68.70316\n","Epoch 18/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 68.7731 - val_loss: 68.1426\n","\n","Epoch 00018: loss did not improve from 68.70316\n","Epoch 19/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 68.7934 - val_loss: 68.1620\n","\n","Epoch 00019: loss did not improve from 68.70316\n","Epoch 20/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 68.7912 - val_loss: 67.8756\n","\n","Epoch 00020: loss did not improve from 68.70316\n","Epoch 21/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 68.9203 - val_loss: 68.3163\n","\n","Epoch 00021: loss did not improve from 68.70316\n","Epoch 22/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 68.6913 - val_loss: 67.9344\n","\n","Epoch 00022: loss improved from 68.70316 to 68.69126, saving model to final.h5\n","Epoch 23/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 68.5235 - val_loss: 68.1702\n","\n","Epoch 00023: loss improved from 68.69126 to 68.52349, saving model to final.h5\n","Epoch 24/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 68.6583 - val_loss: 68.2455\n","\n","Epoch 00024: loss did not improve from 68.52349\n","Epoch 25/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 68.6675 - val_loss: 68.3484\n","\n","Epoch 00025: loss did not improve from 68.52349\n","Epoch 26/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 68.7362 - val_loss: 67.9358\n","\n","Epoch 00026: loss did not improve from 68.52349\n","Epoch 27/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 68.9183 - val_loss: 68.0219\n","\n","Epoch 00027: loss did not improve from 68.52349\n","Epoch 28/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 68.8973 - val_loss: 68.5581\n","\n","Epoch 00028: loss did not improve from 68.52349\n","Epoch 29/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 68.7166 - val_loss: 67.8663\n","\n","Epoch 00029: loss did not improve from 68.52349\n","Epoch 30/400\n","16319/16319 [==============================] - 11s 657us/step - loss: 68.4544 - val_loss: 67.8054\n","\n","Epoch 00030: loss improved from 68.52349 to 68.45445, saving model to final.h5\n","Epoch 31/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 68.5615 - val_loss: 67.9825\n","\n","Epoch 00031: loss did not improve from 68.45445\n","Epoch 32/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 68.6510 - val_loss: 68.0736\n","\n","Epoch 00032: loss did not improve from 68.45445\n","Epoch 33/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 68.6388 - val_loss: 68.1418\n","\n","Epoch 00033: loss did not improve from 68.45445\n","Epoch 34/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 68.6075 - val_loss: 68.2957\n","\n","Epoch 00034: loss did not improve from 68.45445\n","Epoch 35/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 68.3876 - val_loss: 67.9897\n","\n","Epoch 00035: loss improved from 68.45445 to 68.38758, saving model to final.h5\n","Epoch 36/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 68.4795 - val_loss: 68.4006\n","\n","Epoch 00036: loss did not improve from 68.38758\n","Epoch 37/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 68.3993 - val_loss: 67.9097\n","\n","Epoch 00037: loss did not improve from 68.38758\n","Epoch 38/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 68.6593 - val_loss: 67.9250\n","\n","Epoch 00038: loss did not improve from 68.38758\n","Epoch 39/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 68.2241 - val_loss: 68.3067\n","\n","Epoch 00039: loss improved from 68.38758 to 68.22409, saving model to final.h5\n","Epoch 40/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 68.7079 - val_loss: 67.8905\n","\n","Epoch 00040: loss did not improve from 68.22409\n","Epoch 41/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 68.1920 - val_loss: 68.7121\n","\n","Epoch 00041: loss improved from 68.22409 to 68.19199, saving model to final.h5\n","Epoch 42/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 68.2859 - val_loss: 68.1393\n","\n","Epoch 00042: loss did not improve from 68.19199\n","Epoch 43/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 68.2216 - val_loss: 68.0446\n","\n","Epoch 00043: loss did not improve from 68.19199\n","Epoch 44/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 68.3753 - val_loss: 67.9743\n","\n","Epoch 00044: loss did not improve from 68.19199\n","Epoch 45/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 68.4787 - val_loss: 68.0914\n","\n","Epoch 00045: loss did not improve from 68.19199\n","Epoch 46/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 68.3626 - val_loss: 68.1066\n","\n","Epoch 00046: loss did not improve from 68.19199\n","Epoch 47/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 68.4522 - val_loss: 67.7280\n","\n","Epoch 00047: loss did not improve from 68.19199\n","Epoch 48/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 68.4260 - val_loss: 68.2792\n","\n","Epoch 00048: loss did not improve from 68.19199\n","Epoch 49/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 68.3267 - val_loss: 67.9529\n","\n","Epoch 00049: loss did not improve from 68.19199\n","Epoch 50/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 68.3612 - val_loss: 68.0339\n","\n","Epoch 00050: loss did not improve from 68.19199\n","Epoch 51/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 68.2464 - val_loss: 68.0209\n","\n","Epoch 00051: loss did not improve from 68.19199\n","Epoch 52/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 68.0611 - val_loss: 67.7528\n","\n","Epoch 00052: loss improved from 68.19199 to 68.06108, saving model to final.h5\n","Epoch 53/400\n","16319/16319 [==============================] - 10s 633us/step - loss: 68.3181 - val_loss: 67.6212\n","\n","Epoch 00053: loss did not improve from 68.06108\n","Epoch 54/400\n","16319/16319 [==============================] - 10s 631us/step - loss: 68.2939 - val_loss: 67.9920\n","\n","Epoch 00054: loss did not improve from 68.06108\n","Epoch 55/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 68.1877 - val_loss: 67.5822\n","\n","Epoch 00055: loss did not improve from 68.06108\n","Epoch 56/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 68.2863 - val_loss: 68.0152\n","\n","Epoch 00056: loss did not improve from 68.06108\n","Epoch 57/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 68.3710 - val_loss: 68.5048\n","\n","Epoch 00057: loss did not improve from 68.06108\n","Epoch 58/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 68.3547 - val_loss: 68.1081\n","\n","Epoch 00058: loss did not improve from 68.06108\n","Epoch 59/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 68.3300 - val_loss: 68.2324\n","\n","Epoch 00059: loss did not improve from 68.06108\n","Epoch 60/400\n","16319/16319 [==============================] - 10s 643us/step - loss: 68.2077 - val_loss: 67.9586\n","\n","Epoch 00060: loss did not improve from 68.06108\n","Epoch 61/400\n","16319/16319 [==============================] - 10s 639us/step - loss: 68.2641 - val_loss: 67.6787\n","\n","Epoch 00061: loss did not improve from 68.06108\n","Epoch 62/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 68.3506 - val_loss: 68.0192\n","\n","Epoch 00062: loss did not improve from 68.06108\n","Epoch 63/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 68.1899 - val_loss: 68.0403\n","\n","Epoch 00063: loss did not improve from 68.06108\n","Epoch 64/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 68.1614 - val_loss: 68.0310\n","\n","Epoch 00064: loss did not improve from 68.06108\n","Epoch 65/400\n","16319/16319 [==============================] - 10s 631us/step - loss: 68.2268 - val_loss: 68.3739\n","\n","Epoch 00065: loss did not improve from 68.06108\n","Epoch 66/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 68.3219 - val_loss: 68.5666\n","\n","Epoch 00066: loss did not improve from 68.06108\n","Epoch 67/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 68.2835 - val_loss: 67.9182\n","\n","Epoch 00067: loss did not improve from 68.06108\n","Epoch 68/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 68.2834 - val_loss: 67.9277\n","\n","Epoch 00068: loss did not improve from 68.06108\n","Epoch 69/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 68.1790 - val_loss: 68.0323\n","\n","Epoch 00069: loss did not improve from 68.06108\n","Epoch 70/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 68.1741 - val_loss: 68.2086\n","\n","Epoch 00070: loss did not improve from 68.06108\n","Epoch 71/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 68.1003 - val_loss: 68.6002\n","\n","Epoch 00071: loss did not improve from 68.06108\n","Epoch 72/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 68.4260 - val_loss: 67.9484\n","\n","Epoch 00072: loss did not improve from 68.06108\n","Epoch 73/400\n","16319/16319 [==============================] - 10s 632us/step - loss: 68.2020 - val_loss: 67.9347\n","\n","Epoch 00073: loss did not improve from 68.06108\n","Epoch 74/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 68.1285 - val_loss: 67.9399\n","\n","Epoch 00074: loss did not improve from 68.06108\n","Epoch 75/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 68.1559 - val_loss: 67.9489\n","\n","Epoch 00075: loss did not improve from 68.06108\n","Epoch 76/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 68.1397 - val_loss: 68.2060\n","\n","Epoch 00076: loss did not improve from 68.06108\n","Epoch 77/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 68.0614 - val_loss: 68.0592\n","\n","Epoch 00077: loss did not improve from 68.06108\n","Epoch 78/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 68.2124 - val_loss: 68.0318\n","\n","Epoch 00078: loss did not improve from 68.06108\n","Epoch 79/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 68.1433 - val_loss: 67.9758\n","\n","Epoch 00079: loss did not improve from 68.06108\n","Epoch 80/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 68.1291 - val_loss: 68.0799\n","\n","Epoch 00080: loss did not improve from 68.06108\n","Epoch 81/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 68.0323 - val_loss: 67.9343\n","\n","Epoch 00081: loss improved from 68.06108 to 68.03232, saving model to final.h5\n","Epoch 82/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 67.9855 - val_loss: 67.9424\n","\n","Epoch 00082: loss improved from 68.03232 to 67.98549, saving model to final.h5\n","Epoch 83/400\n","16319/16319 [==============================] - 10s 628us/step - loss: 68.1819 - val_loss: 67.9992\n","\n","Epoch 00083: loss did not improve from 67.98549\n","Epoch 84/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 68.1730 - val_loss: 68.4339\n","\n","Epoch 00084: loss did not improve from 67.98549\n","Epoch 85/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 68.0731 - val_loss: 68.0446\n","\n","Epoch 00085: loss did not improve from 67.98549\n","Epoch 86/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 68.1173 - val_loss: 67.9535\n","\n","Epoch 00086: loss did not improve from 67.98549\n","Epoch 87/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 68.2372 - val_loss: 68.0468\n","\n","Epoch 00087: loss did not improve from 67.98549\n","Epoch 88/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 67.9836 - val_loss: 68.0221\n","\n","Epoch 00088: loss improved from 67.98549 to 67.98364, saving model to final.h5\n","Epoch 89/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 68.2408 - val_loss: 68.1437\n","\n","Epoch 00089: loss did not improve from 67.98364\n","Epoch 90/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 67.9962 - val_loss: 68.0643\n","\n","Epoch 00090: loss did not improve from 67.98364\n","Epoch 91/400\n","16319/16319 [==============================] - 11s 661us/step - loss: 68.0371 - val_loss: 68.2992\n","\n","Epoch 00091: loss did not improve from 67.98364\n","Epoch 92/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 68.1044 - val_loss: 67.9354\n","\n","Epoch 00092: loss did not improve from 67.98364\n","Epoch 93/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 67.9108 - val_loss: 68.0815\n","\n","Epoch 00093: loss improved from 67.98364 to 67.91083, saving model to final.h5\n","Epoch 94/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 67.9412 - val_loss: 67.9287\n","\n","Epoch 00094: loss did not improve from 67.91083\n","Epoch 95/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 67.9652 - val_loss: 68.1552\n","\n","Epoch 00095: loss did not improve from 67.91083\n","Epoch 96/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 67.9134 - val_loss: 68.0415\n","\n","Epoch 00096: loss did not improve from 67.91083\n","Epoch 97/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 68.0191 - val_loss: 67.9571\n","\n","Epoch 00097: loss did not improve from 67.91083\n","Epoch 98/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 68.0729 - val_loss: 67.9483\n","\n","Epoch 00098: loss did not improve from 67.91083\n","Epoch 99/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 68.0268 - val_loss: 67.9941\n","\n","Epoch 00099: loss did not improve from 67.91083\n","Epoch 100/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 67.9376 - val_loss: 67.7766\n","\n","Epoch 00100: loss did not improve from 67.91083\n","Epoch 101/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.9142 - val_loss: 68.0894\n","\n","Epoch 00101: loss did not improve from 67.91083\n","Epoch 102/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.8397 - val_loss: 67.8803\n","\n","Epoch 00102: loss improved from 67.91083 to 67.83965, saving model to final.h5\n","Epoch 103/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 67.6631 - val_loss: 67.8738\n","\n","Epoch 00103: loss improved from 67.83965 to 67.66312, saving model to final.h5\n","Epoch 104/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 67.9399 - val_loss: 67.9529\n","\n","Epoch 00104: loss did not improve from 67.66312\n","Epoch 105/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 67.7894 - val_loss: 68.0001\n","\n","Epoch 00105: loss did not improve from 67.66312\n","Epoch 106/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.9389 - val_loss: 67.9974\n","\n","Epoch 00106: loss did not improve from 67.66312\n","Epoch 107/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.7740 - val_loss: 67.9629\n","\n","Epoch 00107: loss did not improve from 67.66312\n","Epoch 108/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.8250 - val_loss: 68.2018\n","\n","Epoch 00108: loss did not improve from 67.66312\n","Epoch 109/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 67.8850 - val_loss: 68.2064\n","\n","Epoch 00109: loss did not improve from 67.66312\n","Epoch 110/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.7869 - val_loss: 67.8882\n","\n","Epoch 00110: loss did not improve from 67.66312\n","Epoch 111/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.8199 - val_loss: 67.9921\n","\n","Epoch 00111: loss did not improve from 67.66312\n","Epoch 112/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 67.8101 - val_loss: 67.9462\n","\n","Epoch 00112: loss did not improve from 67.66312\n","Epoch 113/400\n","16319/16319 [==============================] - 10s 630us/step - loss: 67.7354 - val_loss: 67.8716\n","\n","Epoch 00113: loss did not improve from 67.66312\n","Epoch 114/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 67.6970 - val_loss: 68.3473\n","\n","Epoch 00114: loss did not improve from 67.66312\n","Epoch 115/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.8617 - val_loss: 68.4413\n","\n","Epoch 00115: loss did not improve from 67.66312\n","Epoch 116/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.4211 - val_loss: 67.9282\n","\n","Epoch 00116: loss improved from 67.66312 to 67.42108, saving model to final.h5\n","Epoch 117/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 67.6292 - val_loss: 67.9840\n","\n","Epoch 00117: loss did not improve from 67.42108\n","Epoch 118/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.7603 - val_loss: 67.8573\n","\n","Epoch 00118: loss did not improve from 67.42108\n","Epoch 119/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 67.8198 - val_loss: 67.9583\n","\n","Epoch 00119: loss did not improve from 67.42108\n","Epoch 120/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 67.6288 - val_loss: 67.8911\n","\n","Epoch 00120: loss did not improve from 67.42108\n","Epoch 121/400\n","16319/16319 [==============================] - 10s 635us/step - loss: 67.7846 - val_loss: 67.9573\n","\n","Epoch 00121: loss did not improve from 67.42108\n","Epoch 122/400\n","16319/16319 [==============================] - 10s 642us/step - loss: 67.7389 - val_loss: 67.9842\n","\n","Epoch 00122: loss did not improve from 67.42108\n","Epoch 123/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.8359 - val_loss: 67.8834\n","\n","Epoch 00123: loss did not improve from 67.42108\n","Epoch 124/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.5664 - val_loss: 67.9128\n","\n","Epoch 00124: loss did not improve from 67.42108\n","Epoch 125/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.7571 - val_loss: 67.7711\n","\n","Epoch 00125: loss did not improve from 67.42108\n","Epoch 126/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.6921 - val_loss: 68.1221\n","\n","Epoch 00126: loss did not improve from 67.42108\n","Epoch 127/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.5943 - val_loss: 68.0552\n","\n","Epoch 00127: loss did not improve from 67.42108\n","Epoch 128/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 67.6537 - val_loss: 68.0232\n","\n","Epoch 00128: loss did not improve from 67.42108\n","Epoch 129/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 67.6152 - val_loss: 68.0977\n","\n","Epoch 00129: loss did not improve from 67.42108\n","Epoch 130/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.6095 - val_loss: 67.7407\n","\n","Epoch 00130: loss did not improve from 67.42108\n","Epoch 131/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.5703 - val_loss: 67.5338\n","\n","Epoch 00131: loss did not improve from 67.42108\n","Epoch 132/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.6540 - val_loss: 67.7644\n","\n","Epoch 00132: loss did not improve from 67.42108\n","Epoch 133/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.5926 - val_loss: 68.0015\n","\n","Epoch 00133: loss did not improve from 67.42108\n","Epoch 134/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.5810 - val_loss: 67.9716\n","\n","Epoch 00134: loss did not improve from 67.42108\n","Epoch 135/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.5379 - val_loss: 67.8517\n","\n","Epoch 00135: loss did not improve from 67.42108\n","Epoch 136/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.7578 - val_loss: 67.9066\n","\n","Epoch 00136: loss did not improve from 67.42108\n","Epoch 137/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.6725 - val_loss: 68.2458\n","\n","Epoch 00137: loss did not improve from 67.42108\n","Epoch 138/400\n","16319/16319 [==============================] - 10s 610us/step - loss: 67.8588 - val_loss: 67.9339\n","\n","Epoch 00138: loss did not improve from 67.42108\n","Epoch 139/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.6006 - val_loss: 67.9399\n","\n","Epoch 00139: loss did not improve from 67.42108\n","Epoch 140/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.6682 - val_loss: 67.9619\n","\n","Epoch 00140: loss did not improve from 67.42108\n","Epoch 141/400\n","16319/16319 [==============================] - 10s 610us/step - loss: 67.4763 - val_loss: 67.9206\n","\n","Epoch 00141: loss did not improve from 67.42108\n","Epoch 142/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.7152 - val_loss: 67.7038\n","\n","Epoch 00142: loss did not improve from 67.42108\n","Epoch 143/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.5609 - val_loss: 67.8973\n","\n","Epoch 00143: loss did not improve from 67.42108\n","Epoch 144/400\n","16319/16319 [==============================] - 10s 630us/step - loss: 67.6418 - val_loss: 67.9859\n","\n","Epoch 00144: loss did not improve from 67.42108\n","Epoch 145/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.5777 - val_loss: 67.9131\n","\n","Epoch 00145: loss did not improve from 67.42108\n","Epoch 146/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.6341 - val_loss: 67.9170\n","\n","Epoch 00146: loss did not improve from 67.42108\n","Epoch 147/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.7457 - val_loss: 67.9701\n","\n","Epoch 00147: loss did not improve from 67.42108\n","Epoch 148/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.5561 - val_loss: 67.3279\n","\n","Epoch 00148: loss did not improve from 67.42108\n","Epoch 149/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 67.6123 - val_loss: 67.7117\n","\n","Epoch 00149: loss did not improve from 67.42108\n","Epoch 150/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4879 - val_loss: 68.0183\n","\n","Epoch 00150: loss did not improve from 67.42108\n","Epoch 151/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.6489 - val_loss: 67.7244\n","\n","Epoch 00151: loss did not improve from 67.42108\n","Epoch 152/400\n","16319/16319 [==============================] - 11s 645us/step - loss: 67.5283 - val_loss: 67.9636\n","\n","Epoch 00152: loss did not improve from 67.42108\n","Epoch 153/400\n","16319/16319 [==============================] - 10s 636us/step - loss: 67.5641 - val_loss: 68.2736\n","\n","Epoch 00153: loss did not improve from 67.42108\n","Epoch 154/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.5097 - val_loss: 67.9638\n","\n","Epoch 00154: loss did not improve from 67.42108\n","Epoch 155/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 67.4221 - val_loss: 68.5631\n","\n","Epoch 00155: loss did not improve from 67.42108\n","Epoch 156/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.5553 - val_loss: 67.9223\n","\n","Epoch 00156: loss did not improve from 67.42108\n","Epoch 157/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 67.6232 - val_loss: 67.9365\n","\n","Epoch 00157: loss did not improve from 67.42108\n","Epoch 158/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.6138 - val_loss: 67.9365\n","\n","Epoch 00158: loss did not improve from 67.42108\n","Epoch 159/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4653 - val_loss: 67.9926\n","\n","Epoch 00159: loss did not improve from 67.42108\n","Epoch 160/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 67.4184 - val_loss: 67.9901\n","\n","Epoch 00160: loss improved from 67.42108 to 67.41838, saving model to final.h5\n","Epoch 161/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4067 - val_loss: 67.9081\n","\n","Epoch 00161: loss improved from 67.41838 to 67.40675, saving model to final.h5\n","Epoch 162/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.5138 - val_loss: 68.1732\n","\n","Epoch 00162: loss did not improve from 67.40675\n","Epoch 163/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.6127 - val_loss: 67.9356\n","\n","Epoch 00163: loss did not improve from 67.40675\n","Epoch 164/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4244 - val_loss: 67.8555\n","\n","Epoch 00164: loss did not improve from 67.40675\n","Epoch 165/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.4200 - val_loss: 67.8300\n","\n","Epoch 00165: loss did not improve from 67.40675\n","Epoch 166/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3843 - val_loss: 67.9203\n","\n","Epoch 00166: loss improved from 67.40675 to 67.38433, saving model to final.h5\n","Epoch 167/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.5339 - val_loss: 67.8191\n","\n","Epoch 00167: loss did not improve from 67.38433\n","Epoch 168/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 67.4195 - val_loss: 67.7457\n","\n","Epoch 00168: loss did not improve from 67.38433\n","Epoch 169/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.4281 - val_loss: 67.9375\n","\n","Epoch 00169: loss did not improve from 67.38433\n","Epoch 170/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.4761 - val_loss: 67.9437\n","\n","Epoch 00170: loss did not improve from 67.38433\n","Epoch 171/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 67.5213 - val_loss: 68.0675\n","\n","Epoch 00171: loss did not improve from 67.38433\n","Epoch 172/400\n","16319/16319 [==============================] - 10s 610us/step - loss: 67.4471 - val_loss: 67.8065\n","\n","Epoch 00172: loss did not improve from 67.38433\n","Epoch 173/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4517 - val_loss: 67.8102\n","\n","Epoch 00173: loss did not improve from 67.38433\n","Epoch 174/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 67.3986 - val_loss: 67.7243\n","\n","Epoch 00174: loss did not improve from 67.38433\n","Epoch 175/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 67.5070 - val_loss: 68.2094\n","\n","Epoch 00175: loss did not improve from 67.38433\n","Epoch 176/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.4021 - val_loss: 67.7096\n","\n","Epoch 00176: loss did not improve from 67.38433\n","Epoch 177/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3996 - val_loss: 67.9208\n","\n","Epoch 00177: loss did not improve from 67.38433\n","Epoch 178/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.5337 - val_loss: 67.9330\n","\n","Epoch 00178: loss did not improve from 67.38433\n","Epoch 179/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 67.5250 - val_loss: 67.9273\n","\n","Epoch 00179: loss did not improve from 67.38433\n","Epoch 180/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.4679 - val_loss: 68.1821\n","\n","Epoch 00180: loss did not improve from 67.38433\n","Epoch 181/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.5082 - val_loss: 67.8798\n","\n","Epoch 00181: loss did not improve from 67.38433\n","Epoch 182/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.4871 - val_loss: 68.0632\n","\n","Epoch 00182: loss did not improve from 67.38433\n","Epoch 183/400\n","16319/16319 [==============================] - 10s 641us/step - loss: 67.3312 - val_loss: 67.9699\n","\n","Epoch 00183: loss improved from 67.38433 to 67.33121, saving model to final.h5\n","Epoch 184/400\n","16319/16319 [==============================] - 10s 631us/step - loss: 67.2492 - val_loss: 67.7186\n","\n","Epoch 00184: loss improved from 67.33121 to 67.24924, saving model to final.h5\n","Epoch 185/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.4331 - val_loss: 67.9349\n","\n","Epoch 00185: loss did not improve from 67.24924\n","Epoch 186/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.4362 - val_loss: 67.7705\n","\n","Epoch 00186: loss did not improve from 67.24924\n","Epoch 187/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.4667 - val_loss: 68.0885\n","\n","Epoch 00187: loss did not improve from 67.24924\n","Epoch 188/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4044 - val_loss: 67.7795\n","\n","Epoch 00188: loss did not improve from 67.24924\n","Epoch 189/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3641 - val_loss: 67.8656\n","\n","Epoch 00189: loss did not improve from 67.24924\n","Epoch 190/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.5207 - val_loss: 67.9745\n","\n","Epoch 00190: loss did not improve from 67.24924\n","Epoch 191/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.5004 - val_loss: 67.8403\n","\n","Epoch 00191: loss did not improve from 67.24924\n","Epoch 192/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 67.4479 - val_loss: 67.6087\n","\n","Epoch 00192: loss did not improve from 67.24924\n","Epoch 193/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.4970 - val_loss: 67.6753\n","\n","Epoch 00193: loss did not improve from 67.24924\n","Epoch 194/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.2741 - val_loss: 68.5745\n","\n","Epoch 00194: loss did not improve from 67.24924\n","Epoch 195/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3805 - val_loss: 67.8283\n","\n","Epoch 00195: loss did not improve from 67.24924\n","Epoch 196/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3594 - val_loss: 68.2931\n","\n","Epoch 00196: loss did not improve from 67.24924\n","Epoch 197/400\n","16319/16319 [==============================] - 10s 609us/step - loss: 67.4641 - val_loss: 67.8269\n","\n","Epoch 00197: loss did not improve from 67.24924\n","Epoch 198/400\n","16319/16319 [==============================] - 10s 608us/step - loss: 67.2955 - val_loss: 67.9581\n","\n","Epoch 00198: loss did not improve from 67.24924\n","Epoch 199/400\n","16319/16319 [==============================] - 10s 604us/step - loss: 67.3810 - val_loss: 67.9604\n","\n","Epoch 00199: loss did not improve from 67.24924\n","Epoch 200/400\n","16319/16319 [==============================] - 10s 610us/step - loss: 67.4678 - val_loss: 67.9332\n","\n","Epoch 00200: loss did not improve from 67.24924\n","Epoch 201/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.3785 - val_loss: 67.8965\n","\n","Epoch 00201: loss did not improve from 67.24924\n","Epoch 202/400\n","16319/16319 [==============================] - 10s 607us/step - loss: 67.3957 - val_loss: 67.9991\n","\n","Epoch 00202: loss did not improve from 67.24924\n","Epoch 203/400\n","16319/16319 [==============================] - 10s 605us/step - loss: 67.3703 - val_loss: 67.8365\n","\n","Epoch 00203: loss did not improve from 67.24924\n","Epoch 204/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.3853 - val_loss: 68.3287\n","\n","Epoch 00204: loss did not improve from 67.24924\n","Epoch 205/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3243 - val_loss: 67.9246\n","\n","Epoch 00205: loss did not improve from 67.24924\n","Epoch 206/400\n","16319/16319 [==============================] - 10s 610us/step - loss: 67.3561 - val_loss: 68.1511\n","\n","Epoch 00206: loss did not improve from 67.24924\n","Epoch 207/400\n","16319/16319 [==============================] - 10s 606us/step - loss: 67.3497 - val_loss: 67.9060\n","\n","Epoch 00207: loss did not improve from 67.24924\n","Epoch 208/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 67.2905 - val_loss: 68.2700\n","\n","Epoch 00208: loss did not improve from 67.24924\n","Epoch 209/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 67.3161 - val_loss: 67.8393\n","\n","Epoch 00209: loss did not improve from 67.24924\n","Epoch 210/400\n","16319/16319 [==============================] - 10s 608us/step - loss: 67.4606 - val_loss: 67.8174\n","\n","Epoch 00210: loss did not improve from 67.24924\n","Epoch 211/400\n","16319/16319 [==============================] - 10s 606us/step - loss: 67.3900 - val_loss: 67.5914\n","\n","Epoch 00211: loss did not improve from 67.24924\n","Epoch 212/400\n","16319/16319 [==============================] - 10s 607us/step - loss: 67.4831 - val_loss: 67.5952\n","\n","Epoch 00212: loss did not improve from 67.24924\n","Epoch 213/400\n","16319/16319 [==============================] - 10s 606us/step - loss: 67.5638 - val_loss: 68.1772\n","\n","Epoch 00213: loss did not improve from 67.24924\n","Epoch 214/400\n","16319/16319 [==============================] - 10s 631us/step - loss: 67.3683 - val_loss: 67.5518\n","\n","Epoch 00214: loss did not improve from 67.24924\n","Epoch 215/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 67.4173 - val_loss: 67.9641\n","\n","Epoch 00215: loss did not improve from 67.24924\n","Epoch 216/400\n","16319/16319 [==============================] - 10s 609us/step - loss: 67.5344 - val_loss: 67.9606\n","\n","Epoch 00216: loss did not improve from 67.24924\n","Epoch 217/400\n","16319/16319 [==============================] - 10s 609us/step - loss: 67.4052 - val_loss: 67.8153\n","\n","Epoch 00217: loss did not improve from 67.24924\n","Epoch 218/400\n","16319/16319 [==============================] - 10s 607us/step - loss: 67.3648 - val_loss: 68.0778\n","\n","Epoch 00218: loss did not improve from 67.24924\n","Epoch 219/400\n","16319/16319 [==============================] - 10s 607us/step - loss: 67.3795 - val_loss: 67.9045\n","\n","Epoch 00219: loss did not improve from 67.24924\n","Epoch 220/400\n","16319/16319 [==============================] - 10s 606us/step - loss: 67.3428 - val_loss: 67.8105\n","\n","Epoch 00220: loss did not improve from 67.24924\n","Epoch 221/400\n","16319/16319 [==============================] - 10s 609us/step - loss: 67.2994 - val_loss: 67.9605\n","\n","Epoch 00221: loss did not improve from 67.24924\n","Epoch 222/400\n","16319/16319 [==============================] - 10s 610us/step - loss: 67.2552 - val_loss: 67.9608\n","\n","Epoch 00222: loss did not improve from 67.24924\n","Epoch 223/400\n","16319/16319 [==============================] - 10s 609us/step - loss: 67.3872 - val_loss: 67.7544\n","\n","Epoch 00223: loss did not improve from 67.24924\n","Epoch 224/400\n","16319/16319 [==============================] - 10s 610us/step - loss: 67.2410 - val_loss: 68.0538\n","\n","Epoch 00224: loss improved from 67.24924 to 67.24096, saving model to final.h5\n","Epoch 225/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.3458 - val_loss: 67.9118\n","\n","Epoch 00225: loss did not improve from 67.24096\n","Epoch 226/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 67.2772 - val_loss: 67.8382\n","\n","Epoch 00226: loss did not improve from 67.24096\n","Epoch 227/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4113 - val_loss: 67.8867\n","\n","Epoch 00227: loss did not improve from 67.24096\n","Epoch 228/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3572 - val_loss: 67.9402\n","\n","Epoch 00228: loss did not improve from 67.24096\n","Epoch 229/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4830 - val_loss: 67.8796\n","\n","Epoch 00229: loss did not improve from 67.24096\n","Epoch 230/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4143 - val_loss: 67.7692\n","\n","Epoch 00230: loss did not improve from 67.24096\n","Epoch 231/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4200 - val_loss: 67.7418\n","\n","Epoch 00231: loss did not improve from 67.24096\n","Epoch 232/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.3563 - val_loss: 67.7009\n","\n","Epoch 00232: loss did not improve from 67.24096\n","Epoch 233/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 67.3339 - val_loss: 67.6405\n","\n","Epoch 00233: loss did not improve from 67.24096\n","Epoch 234/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.3665 - val_loss: 67.9871\n","\n","Epoch 00234: loss did not improve from 67.24096\n","Epoch 235/400\n","16319/16319 [==============================] - 10s 632us/step - loss: 67.3559 - val_loss: 67.7362\n","\n","Epoch 00235: loss did not improve from 67.24096\n","Epoch 236/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3909 - val_loss: 67.8048\n","\n","Epoch 00236: loss did not improve from 67.24096\n","Epoch 237/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.5012 - val_loss: 68.0907\n","\n","Epoch 00237: loss did not improve from 67.24096\n","Epoch 238/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3783 - val_loss: 67.8350\n","\n","Epoch 00238: loss did not improve from 67.24096\n","Epoch 239/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3080 - val_loss: 67.9599\n","\n","Epoch 00239: loss did not improve from 67.24096\n","Epoch 240/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3232 - val_loss: 67.9462\n","\n","Epoch 00240: loss did not improve from 67.24096\n","Epoch 241/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 67.4203 - val_loss: 68.0291\n","\n","Epoch 00241: loss did not improve from 67.24096\n","Epoch 242/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.4207 - val_loss: 67.8580\n","\n","Epoch 00242: loss did not improve from 67.24096\n","Epoch 243/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.3102 - val_loss: 68.0482\n","\n","Epoch 00243: loss did not improve from 67.24096\n","Epoch 244/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3329 - val_loss: 67.8134\n","\n","Epoch 00244: loss did not improve from 67.24096\n","Epoch 245/400\n","16319/16319 [==============================] - 11s 644us/step - loss: 67.3339 - val_loss: 67.5235\n","\n","Epoch 00245: loss did not improve from 67.24096\n","Epoch 246/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 67.3731 - val_loss: 67.9309\n","\n","Epoch 00246: loss did not improve from 67.24096\n","Epoch 247/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3617 - val_loss: 68.0586\n","\n","Epoch 00247: loss did not improve from 67.24096\n","Epoch 248/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 67.4103 - val_loss: 67.8692\n","\n","Epoch 00248: loss did not improve from 67.24096\n","Epoch 249/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 67.3198 - val_loss: 67.9988\n","\n","Epoch 00249: loss did not improve from 67.24096\n","Epoch 250/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4104 - val_loss: 67.9113\n","\n","Epoch 00250: loss did not improve from 67.24096\n","Epoch 251/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.4273 - val_loss: 67.9087\n","\n","Epoch 00251: loss did not improve from 67.24096\n","Epoch 252/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 67.4284 - val_loss: 67.8683\n","\n","Epoch 00252: loss did not improve from 67.24096\n","Epoch 253/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3994 - val_loss: 67.9317\n","\n","Epoch 00253: loss did not improve from 67.24096\n","Epoch 254/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3061 - val_loss: 68.0272\n","\n","Epoch 00254: loss did not improve from 67.24096\n","Epoch 255/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3876 - val_loss: 67.8179\n","\n","Epoch 00255: loss did not improve from 67.24096\n","Epoch 256/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.3547 - val_loss: 67.8352\n","\n","Epoch 00256: loss did not improve from 67.24096\n","Epoch 257/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 67.2595 - val_loss: 68.0465\n","\n","Epoch 00257: loss did not improve from 67.24096\n","Epoch 258/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.3393 - val_loss: 67.9054\n","\n","Epoch 00258: loss did not improve from 67.24096\n","Epoch 259/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3731 - val_loss: 68.0475\n","\n","Epoch 00259: loss did not improve from 67.24096\n","Epoch 260/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3532 - val_loss: 67.9493\n","\n","Epoch 00260: loss did not improve from 67.24096\n","Epoch 261/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.2939 - val_loss: 67.9464\n","\n","Epoch 00261: loss did not improve from 67.24096\n","Epoch 262/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.3487 - val_loss: 68.1051\n","\n","Epoch 00262: loss did not improve from 67.24096\n","Epoch 263/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.4373 - val_loss: 67.9455\n","\n","Epoch 00263: loss did not improve from 67.24096\n","Epoch 264/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.3029 - val_loss: 67.8916\n","\n","Epoch 00264: loss did not improve from 67.24096\n","Epoch 265/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 67.3553 - val_loss: 67.7503\n","\n","Epoch 00265: loss did not improve from 67.24096\n","Epoch 266/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.3716 - val_loss: 67.7750\n","\n","Epoch 00266: loss did not improve from 67.24096\n","Epoch 267/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.3972 - val_loss: 67.7506\n","\n","Epoch 00267: loss did not improve from 67.24096\n","Epoch 268/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.2855 - val_loss: 67.6845\n","\n","Epoch 00268: loss did not improve from 67.24096\n","Epoch 269/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4595 - val_loss: 67.9344\n","\n","Epoch 00269: loss did not improve from 67.24096\n","Epoch 270/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.3914 - val_loss: 67.9460\n","\n","Epoch 00270: loss did not improve from 67.24096\n","Epoch 271/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.2904 - val_loss: 67.9721\n","\n","Epoch 00271: loss did not improve from 67.24096\n","Epoch 272/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3647 - val_loss: 68.0595\n","\n","Epoch 00272: loss did not improve from 67.24096\n","Epoch 273/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 67.3438 - val_loss: 68.1371\n","\n","Epoch 00273: loss did not improve from 67.24096\n","Epoch 274/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.5003 - val_loss: 67.9452\n","\n","Epoch 00274: loss did not improve from 67.24096\n","Epoch 275/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.2312 - val_loss: 67.9391\n","\n","Epoch 00275: loss improved from 67.24096 to 67.23116, saving model to final.h5\n","Epoch 276/400\n","16319/16319 [==============================] - 10s 642us/step - loss: 67.4192 - val_loss: 67.9788\n","\n","Epoch 00276: loss did not improve from 67.23116\n","Epoch 277/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 67.3048 - val_loss: 67.9382\n","\n","Epoch 00277: loss did not improve from 67.23116\n","Epoch 278/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.2374 - val_loss: 67.7742\n","\n","Epoch 00278: loss did not improve from 67.23116\n","Epoch 279/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.2772 - val_loss: 67.7468\n","\n","Epoch 00279: loss did not improve from 67.23116\n","Epoch 280/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.2391 - val_loss: 67.8669\n","\n","Epoch 00280: loss did not improve from 67.23116\n","Epoch 281/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.4011 - val_loss: 67.9368\n","\n","Epoch 00281: loss did not improve from 67.23116\n","Epoch 282/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3300 - val_loss: 67.8639\n","\n","Epoch 00282: loss did not improve from 67.23116\n","Epoch 283/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.4232 - val_loss: 67.8807\n","\n","Epoch 00283: loss did not improve from 67.23116\n","Epoch 284/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3970 - val_loss: 67.8151\n","\n","Epoch 00284: loss did not improve from 67.23116\n","Epoch 285/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.4036 - val_loss: 68.0641\n","\n","Epoch 00285: loss did not improve from 67.23116\n","Epoch 286/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4261 - val_loss: 67.8666\n","\n","Epoch 00286: loss did not improve from 67.23116\n","Epoch 287/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.3259 - val_loss: 67.7581\n","\n","Epoch 00287: loss did not improve from 67.23116\n","Epoch 288/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.3190 - val_loss: 67.7096\n","\n","Epoch 00288: loss did not improve from 67.23116\n","Epoch 289/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 67.4305 - val_loss: 67.9011\n","\n","Epoch 00289: loss did not improve from 67.23116\n","Epoch 290/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3753 - val_loss: 68.1244\n","\n","Epoch 00290: loss did not improve from 67.23116\n","Epoch 291/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.3548 - val_loss: 67.9837\n","\n","Epoch 00291: loss did not improve from 67.23116\n","Epoch 292/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3625 - val_loss: 67.9903\n","\n","Epoch 00292: loss did not improve from 67.23116\n","Epoch 293/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.3711 - val_loss: 68.1572\n","\n","Epoch 00293: loss did not improve from 67.23116\n","Epoch 294/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.3646 - val_loss: 67.7765\n","\n","Epoch 00294: loss did not improve from 67.23116\n","Epoch 295/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3895 - val_loss: 67.8754\n","\n","Epoch 00295: loss did not improve from 67.23116\n","Epoch 296/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 67.3187 - val_loss: 67.9571\n","\n","Epoch 00296: loss did not improve from 67.23116\n","Epoch 297/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.3919 - val_loss: 68.0300\n","\n","Epoch 00297: loss did not improve from 67.23116\n","Epoch 298/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3573 - val_loss: 67.9445\n","\n","Epoch 00298: loss did not improve from 67.23116\n","Epoch 299/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3844 - val_loss: 67.9019\n","\n","Epoch 00299: loss did not improve from 67.23116\n","Epoch 300/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3945 - val_loss: 67.9340\n","\n","Epoch 00300: loss did not improve from 67.23116\n","Epoch 301/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.2598 - val_loss: 68.1558\n","\n","Epoch 00301: loss did not improve from 67.23116\n","Epoch 302/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3861 - val_loss: 67.9526\n","\n","Epoch 00302: loss did not improve from 67.23116\n","Epoch 303/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.3646 - val_loss: 67.8354\n","\n","Epoch 00303: loss did not improve from 67.23116\n","Epoch 304/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.3816 - val_loss: 67.8424\n","\n","Epoch 00304: loss did not improve from 67.23116\n","Epoch 305/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 67.3806 - val_loss: 67.9791\n","\n","Epoch 00305: loss did not improve from 67.23116\n","Epoch 306/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3004 - val_loss: 67.7109\n","\n","Epoch 00306: loss did not improve from 67.23116\n","Epoch 307/400\n","16319/16319 [==============================] - 11s 649us/step - loss: 67.4366 - val_loss: 67.7112\n","\n","Epoch 00307: loss did not improve from 67.23116\n","Epoch 308/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4179 - val_loss: 67.7267\n","\n","Epoch 00308: loss did not improve from 67.23116\n","Epoch 309/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.4258 - val_loss: 67.8485\n","\n","Epoch 00309: loss did not improve from 67.23116\n","Epoch 310/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.4112 - val_loss: 67.8839\n","\n","Epoch 00310: loss did not improve from 67.23116\n","Epoch 311/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4416 - val_loss: 68.0770\n","\n","Epoch 00311: loss did not improve from 67.23116\n","Epoch 312/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.4210 - val_loss: 68.0332\n","\n","Epoch 00312: loss did not improve from 67.23116\n","Epoch 313/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4700 - val_loss: 67.9276\n","\n","Epoch 00313: loss did not improve from 67.23116\n","Epoch 314/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4414 - val_loss: 67.9961\n","\n","Epoch 00314: loss did not improve from 67.23116\n","Epoch 315/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.3440 - val_loss: 67.9589\n","\n","Epoch 00315: loss did not improve from 67.23116\n","Epoch 316/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3889 - val_loss: 68.1753\n","\n","Epoch 00316: loss did not improve from 67.23116\n","Epoch 317/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 67.3993 - val_loss: 67.9094\n","\n","Epoch 00317: loss did not improve from 67.23116\n","Epoch 318/400\n","16319/16319 [==============================] - 10s 610us/step - loss: 67.3699 - val_loss: 67.9096\n","\n","Epoch 00318: loss did not improve from 67.23116\n","Epoch 319/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 67.4535 - val_loss: 67.9695\n","\n","Epoch 00319: loss did not improve from 67.23116\n","Epoch 320/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4134 - val_loss: 67.9362\n","\n","Epoch 00320: loss did not improve from 67.23116\n","Epoch 321/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.3991 - val_loss: 68.0212\n","\n","Epoch 00321: loss did not improve from 67.23116\n","Epoch 322/400\n","16319/16319 [==============================] - 10s 610us/step - loss: 67.3588 - val_loss: 67.9542\n","\n","Epoch 00322: loss did not improve from 67.23116\n","Epoch 323/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4598 - val_loss: 67.7630\n","\n","Epoch 00323: loss did not improve from 67.23116\n","Epoch 324/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4266 - val_loss: 67.5897\n","\n","Epoch 00324: loss did not improve from 67.23116\n","Epoch 325/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 67.3695 - val_loss: 67.9870\n","\n","Epoch 00325: loss did not improve from 67.23116\n","Epoch 326/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 67.3964 - val_loss: 67.9368\n","\n","Epoch 00326: loss did not improve from 67.23116\n","Epoch 327/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4211 - val_loss: 67.9401\n","\n","Epoch 00327: loss did not improve from 67.23116\n","Epoch 328/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.3695 - val_loss: 67.9646\n","\n","Epoch 00328: loss did not improve from 67.23116\n","Epoch 329/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 67.4003 - val_loss: 67.9689\n","\n","Epoch 00329: loss did not improve from 67.23116\n","Epoch 330/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4167 - val_loss: 68.0683\n","\n","Epoch 00330: loss did not improve from 67.23116\n","Epoch 331/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4014 - val_loss: 68.0093\n","\n","Epoch 00331: loss did not improve from 67.23116\n","Epoch 332/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.4336 - val_loss: 67.9450\n","\n","Epoch 00332: loss did not improve from 67.23116\n","Epoch 333/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3314 - val_loss: 68.1068\n","\n","Epoch 00333: loss did not improve from 67.23116\n","Epoch 334/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.5335 - val_loss: 67.9261\n","\n","Epoch 00334: loss did not improve from 67.23116\n","Epoch 335/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3960 - val_loss: 67.9375\n","\n","Epoch 00335: loss did not improve from 67.23116\n","Epoch 336/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.3537 - val_loss: 67.9435\n","\n","Epoch 00336: loss did not improve from 67.23116\n","Epoch 337/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 67.4002 - val_loss: 68.0146\n","\n","Epoch 00337: loss did not improve from 67.23116\n","Epoch 338/400\n","16319/16319 [==============================] - 11s 644us/step - loss: 67.4267 - val_loss: 68.0163\n","\n","Epoch 00338: loss did not improve from 67.23116\n","Epoch 339/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 67.4142 - val_loss: 67.9693\n","\n","Epoch 00339: loss did not improve from 67.23116\n","Epoch 340/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4261 - val_loss: 67.9305\n","\n","Epoch 00340: loss did not improve from 67.23116\n","Epoch 341/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3397 - val_loss: 67.9376\n","\n","Epoch 00341: loss did not improve from 67.23116\n","Epoch 342/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.4792 - val_loss: 67.8961\n","\n","Epoch 00342: loss did not improve from 67.23116\n","Epoch 343/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3093 - val_loss: 68.2734\n","\n","Epoch 00343: loss did not improve from 67.23116\n","Epoch 344/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 67.4926 - val_loss: 67.9569\n","\n","Epoch 00344: loss did not improve from 67.23116\n","Epoch 345/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.4258 - val_loss: 67.9685\n","\n","Epoch 00345: loss did not improve from 67.23116\n","Epoch 346/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.4360 - val_loss: 67.9960\n","\n","Epoch 00346: loss did not improve from 67.23116\n","Epoch 347/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.4234 - val_loss: 67.9621\n","\n","Epoch 00347: loss did not improve from 67.23116\n","Epoch 348/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.3892 - val_loss: 67.9322\n","\n","Epoch 00348: loss did not improve from 67.23116\n","Epoch 349/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.3544 - val_loss: 67.9665\n","\n","Epoch 00349: loss did not improve from 67.23116\n","Epoch 350/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3639 - val_loss: 67.9514\n","\n","Epoch 00350: loss did not improve from 67.23116\n","Epoch 351/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3959 - val_loss: 67.9853\n","\n","Epoch 00351: loss did not improve from 67.23116\n","Epoch 352/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 67.4382 - val_loss: 67.9376\n","\n","Epoch 00352: loss did not improve from 67.23116\n","Epoch 353/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.4276 - val_loss: 67.9822\n","\n","Epoch 00353: loss did not improve from 67.23116\n","Epoch 354/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.4553 - val_loss: 67.9421\n","\n","Epoch 00354: loss did not improve from 67.23116\n","Epoch 355/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4157 - val_loss: 68.0663\n","\n","Epoch 00355: loss did not improve from 67.23116\n","Epoch 356/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 67.4142 - val_loss: 67.9354\n","\n","Epoch 00356: loss did not improve from 67.23116\n","Epoch 357/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.3832 - val_loss: 67.9364\n","\n","Epoch 00357: loss did not improve from 67.23116\n","Epoch 358/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4105 - val_loss: 67.9567\n","\n","Epoch 00358: loss did not improve from 67.23116\n","Epoch 359/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.4133 - val_loss: 67.9245\n","\n","Epoch 00359: loss did not improve from 67.23116\n","Epoch 360/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.4280 - val_loss: 67.9596\n","\n","Epoch 00360: loss did not improve from 67.23116\n","Epoch 361/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.4296 - val_loss: 67.9441\n","\n","Epoch 00361: loss did not improve from 67.23116\n","Epoch 362/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.3767 - val_loss: 67.9646\n","\n","Epoch 00362: loss did not improve from 67.23116\n","Epoch 363/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3672 - val_loss: 68.0159\n","\n","Epoch 00363: loss did not improve from 67.23116\n","Epoch 364/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4321 - val_loss: 67.9344\n","\n","Epoch 00364: loss did not improve from 67.23116\n","Epoch 365/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3216 - val_loss: 68.1060\n","\n","Epoch 00365: loss did not improve from 67.23116\n","Epoch 366/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.3868 - val_loss: 67.9412\n","\n","Epoch 00366: loss did not improve from 67.23116\n","Epoch 367/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3799 - val_loss: 67.9957\n","\n","Epoch 00367: loss did not improve from 67.23116\n","Epoch 368/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 67.4077 - val_loss: 67.9767\n","\n","Epoch 00368: loss did not improve from 67.23116\n","Epoch 369/400\n","16319/16319 [==============================] - 11s 646us/step - loss: 67.3290 - val_loss: 67.9160\n","\n","Epoch 00369: loss did not improve from 67.23116\n","Epoch 370/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4339 - val_loss: 67.9407\n","\n","Epoch 00370: loss did not improve from 67.23116\n","Epoch 371/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.4397 - val_loss: 68.1280\n","\n","Epoch 00371: loss did not improve from 67.23116\n","Epoch 372/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4214 - val_loss: 67.9534\n","\n","Epoch 00372: loss did not improve from 67.23116\n","Epoch 373/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.4392 - val_loss: 67.9419\n","\n","Epoch 00373: loss did not improve from 67.23116\n","Epoch 374/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.3238 - val_loss: 68.1033\n","\n","Epoch 00374: loss did not improve from 67.23116\n","Epoch 375/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3913 - val_loss: 68.0102\n","\n","Epoch 00375: loss did not improve from 67.23116\n","Epoch 376/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 67.3923 - val_loss: 67.9692\n","\n","Epoch 00376: loss did not improve from 67.23116\n","Epoch 377/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 67.4169 - val_loss: 67.9893\n","\n","Epoch 00377: loss did not improve from 67.23116\n","Epoch 378/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4115 - val_loss: 67.9111\n","\n","Epoch 00378: loss did not improve from 67.23116\n","Epoch 379/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3252 - val_loss: 67.9572\n","\n","Epoch 00379: loss did not improve from 67.23116\n","Epoch 380/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.4330 - val_loss: 67.9660\n","\n","Epoch 00380: loss did not improve from 67.23116\n","Epoch 381/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3726 - val_loss: 68.0554\n","\n","Epoch 00381: loss did not improve from 67.23116\n","Epoch 382/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.4438 - val_loss: 68.1524\n","\n","Epoch 00382: loss did not improve from 67.23116\n","Epoch 383/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.4468 - val_loss: 67.9478\n","\n","Epoch 00383: loss did not improve from 67.23116\n","Epoch 384/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 67.3898 - val_loss: 67.9837\n","\n","Epoch 00384: loss did not improve from 67.23116\n","Epoch 385/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 67.4283 - val_loss: 67.9434\n","\n","Epoch 00385: loss did not improve from 67.23116\n","Epoch 386/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3523 - val_loss: 68.1720\n","\n","Epoch 00386: loss did not improve from 67.23116\n","Epoch 387/400\n","16319/16319 [==============================] - 10s 628us/step - loss: 67.4228 - val_loss: 68.0600\n","\n","Epoch 00387: loss did not improve from 67.23116\n","Epoch 388/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.3743 - val_loss: 67.9392\n","\n","Epoch 00388: loss did not improve from 67.23116\n","Epoch 389/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.3917 - val_loss: 67.9365\n","\n","Epoch 00389: loss did not improve from 67.23116\n","Epoch 390/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.3734 - val_loss: 67.9407\n","\n","Epoch 00390: loss did not improve from 67.23116\n","Epoch 391/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.3300 - val_loss: 67.9349\n","\n","Epoch 00391: loss did not improve from 67.23116\n","Epoch 392/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 67.3738 - val_loss: 67.9347\n","\n","Epoch 00392: loss did not improve from 67.23116\n","Epoch 393/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 67.3233 - val_loss: 67.9805\n","\n","Epoch 00393: loss did not improve from 67.23116\n","Epoch 394/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 67.4278 - val_loss: 68.0100\n","\n","Epoch 00394: loss did not improve from 67.23116\n","Epoch 395/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 67.3980 - val_loss: 67.9401\n","\n","Epoch 00395: loss did not improve from 67.23116\n","Epoch 396/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 67.3539 - val_loss: 67.9366\n","\n","Epoch 00396: loss did not improve from 67.23116\n","Epoch 397/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 67.3341 - val_loss: 67.9346\n","\n","Epoch 00397: loss did not improve from 67.23116\n","Epoch 398/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 67.4086 - val_loss: 67.9505\n","\n","Epoch 00398: loss did not improve from 67.23116\n","Epoch 399/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 67.3921 - val_loss: 68.0167\n","\n","Epoch 00399: loss did not improve from 67.23116\n","Epoch 400/400\n","16319/16319 [==============================] - 10s 640us/step - loss: 67.3858 - val_loss: 67.9347\n","\n","Epoch 00400: loss did not improve from 67.23116\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VS-aFBZ4lGiu","colab_type":"code","outputId":"714cae0a-1bcc-4113-aac1-4a6c94faf20b","executionInfo":{"status":"ok","timestamp":1590279776759,"user_tz":360,"elapsed":4081269,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet_final_with_LSTM.py train.mp4 train.txt --mode=train --split=0.2 --model final.h5 --epoch 400 --history 1 --LR 0.0001"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-23 23:14:27.840603: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet_final_with_LSTM.py:180: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-23 23:14:29.540643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-23 23:14:29.554723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 23:14:29.555396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-23 23:14:29.555431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-23 23:14:29.557300: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-23 23:14:29.559516: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-23 23:14:29.559898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-23 23:14:29.561659: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-23 23:14:29.562809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-23 23:14:29.566696: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 23:14:29.566795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 23:14:29.567349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 23:14:29.567829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-23 23:14:29.572858: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-05-23 23:14:29.573153: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2d3b2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-23 23:14:29.573185: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-23 23:14:29.661091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 23:14:29.661736: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2d3b480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-23 23:14:29.661765: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-23 23:14:29.661971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 23:14:29.662478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-23 23:14:29.662516: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-23 23:14:29.662558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-23 23:14:29.662580: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-23 23:14:29.662594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-23 23:14:29.662607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-23 23:14:29.662620: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-23 23:14:29.662631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-23 23:14:29.662692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 23:14:29.663251: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 23:14:29.663752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-23 23:14:29.663836: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-23 23:14:30.163387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-23 23:14:30.163442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-23 23:14:30.163451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-23 23:14:30.163669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 23:14:30.164275: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-23 23:14:30.164775: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-23 23:14:30.164813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet_final_with_LSTM.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","speednet_final_with_LSTM.py:185: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","Found preprocessed data\n","tcmalloc: large alloc 2447884288 bytes == 0x1f72a000 @  0x7f57286741e7 0x7f572615a5e1 0x7f57261bec78 0x7f57261bef37 0x7f5726256f28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f5728271b97 0x5b250a\n","tcmalloc: large alloc 2447884288 bytes == 0xb15a6000 @  0x7f57286741e7 0x7f572615a5e1 0x7f57261bec78 0x7f57261bef37 0x7f5726256f28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f5728271b97 0x5b250a\n","Loading frame 20398\n","done loading 20399 frames\n","Done prepping data\n","tcmalloc: large alloc 1631920128 bytes == 0x14347e000 @  0x7f57286741e7 0x7f572615a5e1 0x7f57261bec78 0x7f57261bed93 0x7f5726249ed6 0x7f572624a338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f5728271b97 0x5b250a\n","tcmalloc: large alloc 1631920128 bytes == 0x1f72a000 @  0x7f57286741e7 0x7f572615a5e1 0x7f57261bec78 0x7f57261bed93 0x7f5726249ed6 0x7f572624a338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f5728271b97 0x5b250a\n","Starting training\n","Train on 16319 samples, validate on 4080 samples\n","Epoch 1/400\n","2020-05-23 23:14:41.960620: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-23 23:14:42.165044: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","16319/16319 [==============================] - 12s 711us/step - loss: 76.3860 - val_loss: 68.6622\n","\n","Epoch 00001: loss improved from inf to 76.38601, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 32.7971 - val_loss: 9.5289\n","\n","Epoch 00002: loss improved from 76.38601 to 32.79709, saving model to final.h5\n","Epoch 3/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 14.1593 - val_loss: 6.6812\n","\n","Epoch 00003: loss improved from 32.79709 to 14.15931, saving model to final.h5\n","Epoch 4/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 11.3162 - val_loss: 5.4322\n","\n","Epoch 00004: loss improved from 14.15931 to 11.31620, saving model to final.h5\n","Epoch 5/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 9.7231 - val_loss: 4.4866\n","\n","Epoch 00005: loss improved from 11.31620 to 9.72312, saving model to final.h5\n","Epoch 6/400\n","16319/16319 [==============================] - 10s 638us/step - loss: 8.7865 - val_loss: 3.9232\n","\n","Epoch 00006: loss improved from 9.72312 to 8.78647, saving model to final.h5\n","Epoch 7/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 8.0854 - val_loss: 3.4426\n","\n","Epoch 00007: loss improved from 8.78647 to 8.08537, saving model to final.h5\n","Epoch 8/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 7.5874 - val_loss: 3.1555\n","\n","Epoch 00008: loss improved from 8.08537 to 7.58736, saving model to final.h5\n","Epoch 9/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 6.8328 - val_loss: 2.9581\n","\n","Epoch 00009: loss improved from 7.58736 to 6.83275, saving model to final.h5\n","Epoch 10/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 6.7916 - val_loss: 2.9711\n","\n","Epoch 00010: loss improved from 6.83275 to 6.79160, saving model to final.h5\n","Epoch 11/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 6.2958 - val_loss: 2.4638\n","\n","Epoch 00011: loss improved from 6.79160 to 6.29583, saving model to final.h5\n","Epoch 12/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 6.0893 - val_loss: 2.3840\n","\n","Epoch 00012: loss improved from 6.29583 to 6.08932, saving model to final.h5\n","Epoch 13/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 5.8385 - val_loss: 3.5586\n","\n","Epoch 00013: loss improved from 6.08932 to 5.83850, saving model to final.h5\n","Epoch 14/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 5.6101 - val_loss: 2.0478\n","\n","Epoch 00014: loss improved from 5.83850 to 5.61009, saving model to final.h5\n","Epoch 15/400\n","16319/16319 [==============================] - 10s 642us/step - loss: 5.4855 - val_loss: 2.0423\n","\n","Epoch 00015: loss improved from 5.61009 to 5.48549, saving model to final.h5\n","Epoch 16/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 5.3455 - val_loss: 1.9648\n","\n","Epoch 00016: loss improved from 5.48549 to 5.34547, saving model to final.h5\n","Epoch 17/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 5.0807 - val_loss: 2.0775\n","\n","Epoch 00017: loss improved from 5.34547 to 5.08068, saving model to final.h5\n","Epoch 18/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 4.9677 - val_loss: 1.7134\n","\n","Epoch 00018: loss improved from 5.08068 to 4.96765, saving model to final.h5\n","Epoch 19/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 4.9970 - val_loss: 1.6633\n","\n","Epoch 00019: loss did not improve from 4.96765\n","Epoch 20/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 4.9318 - val_loss: 1.8207\n","\n","Epoch 00020: loss improved from 4.96765 to 4.93181, saving model to final.h5\n","Epoch 21/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 4.6735 - val_loss: 1.8193\n","\n","Epoch 00021: loss improved from 4.93181 to 4.67347, saving model to final.h5\n","Epoch 22/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 4.7321 - val_loss: 1.4578\n","\n","Epoch 00022: loss did not improve from 4.67347\n","Epoch 23/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 4.5974 - val_loss: 1.5175\n","\n","Epoch 00023: loss improved from 4.67347 to 4.59740, saving model to final.h5\n","Epoch 24/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 4.4360 - val_loss: 1.5559\n","\n","Epoch 00024: loss improved from 4.59740 to 4.43602, saving model to final.h5\n","Epoch 25/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 4.5114 - val_loss: 1.5223\n","\n","Epoch 00025: loss did not improve from 4.43602\n","Epoch 26/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 4.4289 - val_loss: 1.5584\n","\n","Epoch 00026: loss improved from 4.43602 to 4.42892, saving model to final.h5\n","Epoch 27/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 4.4971 - val_loss: 1.4048\n","\n","Epoch 00027: loss did not improve from 4.42892\n","Epoch 28/400\n","16319/16319 [==============================] - 11s 650us/step - loss: 4.2554 - val_loss: 1.4222\n","\n","Epoch 00028: loss improved from 4.42892 to 4.25544, saving model to final.h5\n","Epoch 29/400\n","16319/16319 [==============================] - 10s 631us/step - loss: 4.2378 - val_loss: 1.3550\n","\n","Epoch 00029: loss improved from 4.25544 to 4.23783, saving model to final.h5\n","Epoch 30/400\n","16319/16319 [==============================] - 10s 628us/step - loss: 4.2737 - val_loss: 1.2993\n","\n","Epoch 00030: loss did not improve from 4.23783\n","Epoch 31/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 4.1937 - val_loss: 1.3078\n","\n","Epoch 00031: loss improved from 4.23783 to 4.19371, saving model to final.h5\n","Epoch 32/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 4.0911 - val_loss: 1.4744\n","\n","Epoch 00032: loss improved from 4.19371 to 4.09111, saving model to final.h5\n","Epoch 33/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 4.1212 - val_loss: 1.3283\n","\n","Epoch 00033: loss did not improve from 4.09111\n","Epoch 34/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 4.0760 - val_loss: 1.5452\n","\n","Epoch 00034: loss improved from 4.09111 to 4.07598, saving model to final.h5\n","Epoch 35/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 4.0190 - val_loss: 1.3233\n","\n","Epoch 00035: loss improved from 4.07598 to 4.01903, saving model to final.h5\n","Epoch 36/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 4.0000 - val_loss: 1.3719\n","\n","Epoch 00036: loss improved from 4.01903 to 4.00003, saving model to final.h5\n","Epoch 37/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 4.0295 - val_loss: 1.1606\n","\n","Epoch 00037: loss did not improve from 4.00003\n","Epoch 38/400\n","16319/16319 [==============================] - 10s 628us/step - loss: 3.9578 - val_loss: 1.2447\n","\n","Epoch 00038: loss improved from 4.00003 to 3.95780, saving model to final.h5\n","Epoch 39/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 3.9838 - val_loss: 1.3130\n","\n","Epoch 00039: loss did not improve from 3.95780\n","Epoch 40/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 3.7717 - val_loss: 1.3113\n","\n","Epoch 00040: loss improved from 3.95780 to 3.77168, saving model to final.h5\n","Epoch 41/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 3.8834 - val_loss: 1.2393\n","\n","Epoch 00041: loss did not improve from 3.77168\n","Epoch 42/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 3.8290 - val_loss: 1.1604\n","\n","Epoch 00042: loss did not improve from 3.77168\n","Epoch 43/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 3.7967 - val_loss: 1.1583\n","\n","Epoch 00043: loss did not improve from 3.77168\n","Epoch 44/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 3.8252 - val_loss: 1.1639\n","\n","Epoch 00044: loss did not improve from 3.77168\n","Epoch 45/400\n","16319/16319 [==============================] - 10s 635us/step - loss: 3.9007 - val_loss: 1.1418\n","\n","Epoch 00045: loss did not improve from 3.77168\n","Epoch 46/400\n","16319/16319 [==============================] - 10s 630us/step - loss: 3.7867 - val_loss: 1.1557\n","\n","Epoch 00046: loss did not improve from 3.77168\n","Epoch 47/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 3.6320 - val_loss: 1.0895\n","\n","Epoch 00047: loss improved from 3.77168 to 3.63203, saving model to final.h5\n","Epoch 48/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 3.7626 - val_loss: 1.2238\n","\n","Epoch 00048: loss did not improve from 3.63203\n","Epoch 49/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 3.6994 - val_loss: 1.3364\n","\n","Epoch 00049: loss did not improve from 3.63203\n","Epoch 50/400\n","16319/16319 [==============================] - 10s 634us/step - loss: 3.5962 - val_loss: 1.0726\n","\n","Epoch 00050: loss improved from 3.63203 to 3.59618, saving model to final.h5\n","Epoch 51/400\n","16319/16319 [==============================] - 10s 628us/step - loss: 3.6565 - val_loss: 0.9717\n","\n","Epoch 00051: loss did not improve from 3.59618\n","Epoch 52/400\n","16319/16319 [==============================] - 10s 635us/step - loss: 3.7294 - val_loss: 1.1223\n","\n","Epoch 00052: loss did not improve from 3.59618\n","Epoch 53/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 3.5707 - val_loss: 1.0336\n","\n","Epoch 00053: loss improved from 3.59618 to 3.57075, saving model to final.h5\n","Epoch 54/400\n","16319/16319 [==============================] - 10s 639us/step - loss: 3.5539 - val_loss: 1.1861\n","\n","Epoch 00054: loss improved from 3.57075 to 3.55393, saving model to final.h5\n","Epoch 55/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 3.5713 - val_loss: 1.0066\n","\n","Epoch 00055: loss did not improve from 3.55393\n","Epoch 56/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 3.5488 - val_loss: 1.1006\n","\n","Epoch 00056: loss improved from 3.55393 to 3.54880, saving model to final.h5\n","Epoch 57/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 3.5105 - val_loss: 1.0094\n","\n","Epoch 00057: loss improved from 3.54880 to 3.51055, saving model to final.h5\n","Epoch 58/400\n","16319/16319 [==============================] - 11s 644us/step - loss: 3.5632 - val_loss: 1.0026\n","\n","Epoch 00058: loss did not improve from 3.51055\n","Epoch 59/400\n","16319/16319 [==============================] - 11s 650us/step - loss: 3.6122 - val_loss: 1.0290\n","\n","Epoch 00059: loss did not improve from 3.51055\n","Epoch 60/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 3.4491 - val_loss: 0.9395\n","\n","Epoch 00060: loss improved from 3.51055 to 3.44909, saving model to final.h5\n","Epoch 61/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 3.4791 - val_loss: 1.0365\n","\n","Epoch 00061: loss did not improve from 3.44909\n","Epoch 62/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 3.4372 - val_loss: 1.0546\n","\n","Epoch 00062: loss improved from 3.44909 to 3.43722, saving model to final.h5\n","Epoch 63/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 3.4810 - val_loss: 1.0812\n","\n","Epoch 00063: loss did not improve from 3.43722\n","Epoch 64/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 3.4039 - val_loss: 0.9587\n","\n","Epoch 00064: loss improved from 3.43722 to 3.40387, saving model to final.h5\n","Epoch 65/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 3.5127 - val_loss: 0.9518\n","\n","Epoch 00065: loss did not improve from 3.40387\n","Epoch 66/400\n","16319/16319 [==============================] - 10s 628us/step - loss: 3.4627 - val_loss: 0.9114\n","\n","Epoch 00066: loss did not improve from 3.40387\n","Epoch 67/400\n","16319/16319 [==============================] - 10s 632us/step - loss: 3.3530 - val_loss: 0.9649\n","\n","Epoch 00067: loss improved from 3.40387 to 3.35297, saving model to final.h5\n","Epoch 68/400\n","16319/16319 [==============================] - 10s 636us/step - loss: 3.4055 - val_loss: 0.9412\n","\n","Epoch 00068: loss did not improve from 3.35297\n","Epoch 69/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 3.3201 - val_loss: 0.9178\n","\n","Epoch 00069: loss improved from 3.35297 to 3.32008, saving model to final.h5\n","Epoch 70/400\n","16319/16319 [==============================] - 10s 633us/step - loss: 3.3178 - val_loss: 1.0972\n","\n","Epoch 00070: loss improved from 3.32008 to 3.31779, saving model to final.h5\n","Epoch 71/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 3.4125 - val_loss: 1.0747\n","\n","Epoch 00071: loss did not improve from 3.31779\n","Epoch 72/400\n","16319/16319 [==============================] - 10s 630us/step - loss: 3.3629 - val_loss: 1.0511\n","\n","Epoch 00072: loss did not improve from 3.31779\n","Epoch 73/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 3.3239 - val_loss: 1.0561\n","\n","Epoch 00073: loss did not improve from 3.31779\n","Epoch 74/400\n","16319/16319 [==============================] - 10s 635us/step - loss: 3.2469 - val_loss: 0.9028\n","\n","Epoch 00074: loss improved from 3.31779 to 3.24693, saving model to final.h5\n","Epoch 75/400\n","16319/16319 [==============================] - 10s 641us/step - loss: 3.3136 - val_loss: 0.8929\n","\n","Epoch 00075: loss did not improve from 3.24693\n","Epoch 76/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 3.3317 - val_loss: 0.8727\n","\n","Epoch 00076: loss did not improve from 3.24693\n","Epoch 77/400\n","16319/16319 [==============================] - 10s 634us/step - loss: 3.3605 - val_loss: 0.9606\n","\n","Epoch 00077: loss did not improve from 3.24693\n","Epoch 78/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 3.2924 - val_loss: 0.8923\n","\n","Epoch 00078: loss did not improve from 3.24693\n","Epoch 79/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 3.2495 - val_loss: 0.9186\n","\n","Epoch 00079: loss did not improve from 3.24693\n","Epoch 80/400\n","16319/16319 [==============================] - 10s 632us/step - loss: 3.2981 - val_loss: 0.8907\n","\n","Epoch 00080: loss did not improve from 3.24693\n","Epoch 81/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 3.2389 - val_loss: 0.9103\n","\n","Epoch 00081: loss improved from 3.24693 to 3.23886, saving model to final.h5\n","Epoch 82/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 3.2686 - val_loss: 0.8965\n","\n","Epoch 00082: loss did not improve from 3.23886\n","Epoch 83/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 3.2128 - val_loss: 0.9495\n","\n","Epoch 00083: loss improved from 3.23886 to 3.21283, saving model to final.h5\n","Epoch 84/400\n","16319/16319 [==============================] - 10s 628us/step - loss: 3.1622 - val_loss: 0.8164\n","\n","Epoch 00084: loss improved from 3.21283 to 3.16222, saving model to final.h5\n","Epoch 85/400\n","16319/16319 [==============================] - 10s 636us/step - loss: 3.1993 - val_loss: 0.8203\n","\n","Epoch 00085: loss did not improve from 3.16222\n","Epoch 86/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 3.1992 - val_loss: 0.9755\n","\n","Epoch 00086: loss did not improve from 3.16222\n","Epoch 87/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 3.2324 - val_loss: 0.8294\n","\n","Epoch 00087: loss did not improve from 3.16222\n","Epoch 88/400\n","16319/16319 [==============================] - 10s 640us/step - loss: 3.1584 - val_loss: 0.8437\n","\n","Epoch 00088: loss improved from 3.16222 to 3.15843, saving model to final.h5\n","Epoch 89/400\n","16319/16319 [==============================] - 11s 657us/step - loss: 3.1313 - val_loss: 0.9791\n","\n","Epoch 00089: loss improved from 3.15843 to 3.13134, saving model to final.h5\n","Epoch 90/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 3.1934 - val_loss: 1.0579\n","\n","Epoch 00090: loss did not improve from 3.13134\n","Epoch 91/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 3.2081 - val_loss: 0.8317\n","\n","Epoch 00091: loss did not improve from 3.13134\n","Epoch 92/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 3.1591 - val_loss: 0.8815\n","\n","Epoch 00092: loss did not improve from 3.13134\n","Epoch 93/400\n","16319/16319 [==============================] - 10s 637us/step - loss: 3.1834 - val_loss: 0.7839\n","\n","Epoch 00093: loss did not improve from 3.13134\n","Epoch 94/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 3.0851 - val_loss: 0.8179\n","\n","Epoch 00094: loss improved from 3.13134 to 3.08508, saving model to final.h5\n","Epoch 95/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 3.1600 - val_loss: 0.8010\n","\n","Epoch 00095: loss did not improve from 3.08508\n","Epoch 96/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 3.2100 - val_loss: 0.8994\n","\n","Epoch 00096: loss did not improve from 3.08508\n","Epoch 97/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 3.1641 - val_loss: 0.7733\n","\n","Epoch 00097: loss did not improve from 3.08508\n","Epoch 98/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 3.0674 - val_loss: 0.7670\n","\n","Epoch 00098: loss improved from 3.08508 to 3.06744, saving model to final.h5\n","Epoch 99/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 3.0549 - val_loss: 0.8994\n","\n","Epoch 00099: loss improved from 3.06744 to 3.05488, saving model to final.h5\n","Epoch 100/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 3.1636 - val_loss: 0.8619\n","\n","Epoch 00100: loss did not improve from 3.05488\n","Epoch 101/400\n","16319/16319 [==============================] - 10s 632us/step - loss: 3.0948 - val_loss: 0.8323\n","\n","Epoch 00101: loss did not improve from 3.05488\n","Epoch 102/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 3.1279 - val_loss: 0.8738\n","\n","Epoch 00102: loss did not improve from 3.05488\n","Epoch 103/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 3.1805 - val_loss: 0.9918\n","\n","Epoch 00103: loss did not improve from 3.05488\n","Epoch 104/400\n","16319/16319 [==============================] - 10s 631us/step - loss: 3.1350 - val_loss: 1.2397\n","\n","Epoch 00104: loss did not improve from 3.05488\n","Epoch 105/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 3.0225 - val_loss: 0.7511\n","\n","Epoch 00105: loss improved from 3.05488 to 3.02250, saving model to final.h5\n","Epoch 106/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 3.0511 - val_loss: 0.8129\n","\n","Epoch 00106: loss did not improve from 3.02250\n","Epoch 107/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 3.0043 - val_loss: 0.8349\n","\n","Epoch 00107: loss improved from 3.02250 to 3.00434, saving model to final.h5\n","Epoch 108/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 3.0315 - val_loss: 0.9017\n","\n","Epoch 00108: loss did not improve from 3.00434\n","Epoch 109/400\n","16319/16319 [==============================] - 10s 628us/step - loss: 3.0453 - val_loss: 0.8770\n","\n","Epoch 00109: loss did not improve from 3.00434\n","Epoch 110/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 3.0588 - val_loss: 0.8160\n","\n","Epoch 00110: loss did not improve from 3.00434\n","Epoch 111/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.9079 - val_loss: 0.8045\n","\n","Epoch 00111: loss improved from 3.00434 to 2.90789, saving model to final.h5\n","Epoch 112/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 3.0476 - val_loss: 0.9097\n","\n","Epoch 00112: loss did not improve from 2.90789\n","Epoch 113/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 3.0074 - val_loss: 0.7674\n","\n","Epoch 00113: loss did not improve from 2.90789\n","Epoch 114/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 3.0062 - val_loss: 1.2100\n","\n","Epoch 00114: loss did not improve from 2.90789\n","Epoch 115/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.9650 - val_loss: 0.8148\n","\n","Epoch 00115: loss did not improve from 2.90789\n","Epoch 116/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 3.0373 - val_loss: 0.7784\n","\n","Epoch 00116: loss did not improve from 2.90789\n","Epoch 117/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 2.9469 - val_loss: 0.7479\n","\n","Epoch 00117: loss did not improve from 2.90789\n","Epoch 118/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.9123 - val_loss: 0.7393\n","\n","Epoch 00118: loss did not improve from 2.90789\n","Epoch 119/400\n","16319/16319 [==============================] - 11s 655us/step - loss: 2.8943 - val_loss: 0.8026\n","\n","Epoch 00119: loss improved from 2.90789 to 2.89429, saving model to final.h5\n","Epoch 120/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.9324 - val_loss: 0.8462\n","\n","Epoch 00120: loss did not improve from 2.89429\n","Epoch 121/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.8683 - val_loss: 0.7997\n","\n","Epoch 00121: loss improved from 2.89429 to 2.86831, saving model to final.h5\n","Epoch 122/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.9146 - val_loss: 0.9019\n","\n","Epoch 00122: loss did not improve from 2.86831\n","Epoch 123/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.9397 - val_loss: 0.7715\n","\n","Epoch 00123: loss did not improve from 2.86831\n","Epoch 124/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 2.8157 - val_loss: 0.7661\n","\n","Epoch 00124: loss improved from 2.86831 to 2.81572, saving model to final.h5\n","Epoch 125/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 2.9407 - val_loss: 0.6893\n","\n","Epoch 00125: loss did not improve from 2.81572\n","Epoch 126/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.9934 - val_loss: 0.7431\n","\n","Epoch 00126: loss did not improve from 2.81572\n","Epoch 127/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.9542 - val_loss: 0.7647\n","\n","Epoch 00127: loss did not improve from 2.81572\n","Epoch 128/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.8809 - val_loss: 0.8769\n","\n","Epoch 00128: loss did not improve from 2.81572\n","Epoch 129/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.8580 - val_loss: 0.7218\n","\n","Epoch 00129: loss did not improve from 2.81572\n","Epoch 130/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.8873 - val_loss: 0.7387\n","\n","Epoch 00130: loss did not improve from 2.81572\n","Epoch 131/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.8606 - val_loss: 0.7748\n","\n","Epoch 00131: loss did not improve from 2.81572\n","Epoch 132/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 2.8933 - val_loss: 0.9395\n","\n","Epoch 00132: loss did not improve from 2.81572\n","Epoch 133/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 2.9462 - val_loss: 0.8199\n","\n","Epoch 00133: loss did not improve from 2.81572\n","Epoch 134/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 2.8907 - val_loss: 0.7760\n","\n","Epoch 00134: loss did not improve from 2.81572\n","Epoch 135/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.9735 - val_loss: 0.7701\n","\n","Epoch 00135: loss did not improve from 2.81572\n","Epoch 136/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.8290 - val_loss: 0.9328\n","\n","Epoch 00136: loss did not improve from 2.81572\n","Epoch 137/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.9123 - val_loss: 0.7719\n","\n","Epoch 00137: loss did not improve from 2.81572\n","Epoch 138/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.8677 - val_loss: 0.9310\n","\n","Epoch 00138: loss did not improve from 2.81572\n","Epoch 139/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.8591 - val_loss: 0.9080\n","\n","Epoch 00139: loss did not improve from 2.81572\n","Epoch 140/400\n","16319/16319 [==============================] - 10s 628us/step - loss: 2.8448 - val_loss: 0.7862\n","\n","Epoch 00140: loss did not improve from 2.81572\n","Epoch 141/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.7936 - val_loss: 0.7364\n","\n","Epoch 00141: loss improved from 2.81572 to 2.79356, saving model to final.h5\n","Epoch 142/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 2.8441 - val_loss: 0.8468\n","\n","Epoch 00142: loss did not improve from 2.79356\n","Epoch 143/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.8556 - val_loss: 0.8660\n","\n","Epoch 00143: loss did not improve from 2.79356\n","Epoch 144/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.8497 - val_loss: 0.8108\n","\n","Epoch 00144: loss did not improve from 2.79356\n","Epoch 145/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 2.8281 - val_loss: 0.7798\n","\n","Epoch 00145: loss did not improve from 2.79356\n","Epoch 146/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.8841 - val_loss: 0.7426\n","\n","Epoch 00146: loss did not improve from 2.79356\n","Epoch 147/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.8596 - val_loss: 0.7308\n","\n","Epoch 00147: loss did not improve from 2.79356\n","Epoch 148/400\n","16319/16319 [==============================] - 10s 631us/step - loss: 2.7756 - val_loss: 0.6868\n","\n","Epoch 00148: loss improved from 2.79356 to 2.77563, saving model to final.h5\n","Epoch 149/400\n","16319/16319 [==============================] - 11s 644us/step - loss: 2.7792 - val_loss: 0.7929\n","\n","Epoch 00149: loss did not improve from 2.77563\n","Epoch 150/400\n","16319/16319 [==============================] - 11s 645us/step - loss: 2.8101 - val_loss: 0.8497\n","\n","Epoch 00150: loss did not improve from 2.77563\n","Epoch 151/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 2.8445 - val_loss: 0.7913\n","\n","Epoch 00151: loss did not improve from 2.77563\n","Epoch 152/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.8272 - val_loss: 0.7553\n","\n","Epoch 00152: loss did not improve from 2.77563\n","Epoch 153/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.8488 - val_loss: 0.9089\n","\n","Epoch 00153: loss did not improve from 2.77563\n","Epoch 154/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 2.7758 - val_loss: 0.7859\n","\n","Epoch 00154: loss did not improve from 2.77563\n","Epoch 155/400\n","16319/16319 [==============================] - 10s 632us/step - loss: 2.6893 - val_loss: 0.7407\n","\n","Epoch 00155: loss improved from 2.77563 to 2.68934, saving model to final.h5\n","Epoch 156/400\n","16319/16319 [==============================] - 10s 634us/step - loss: 2.7487 - val_loss: 0.8449\n","\n","Epoch 00156: loss did not improve from 2.68934\n","Epoch 157/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 2.7383 - val_loss: 0.7784\n","\n","Epoch 00157: loss did not improve from 2.68934\n","Epoch 158/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 2.7324 - val_loss: 0.8847\n","\n","Epoch 00158: loss did not improve from 2.68934\n","Epoch 159/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 2.7915 - val_loss: 0.7903\n","\n","Epoch 00159: loss did not improve from 2.68934\n","Epoch 160/400\n","16319/16319 [==============================] - 10s 630us/step - loss: 2.7599 - val_loss: 0.9114\n","\n","Epoch 00160: loss did not improve from 2.68934\n","Epoch 161/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 2.7843 - val_loss: 0.7358\n","\n","Epoch 00161: loss did not improve from 2.68934\n","Epoch 162/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 2.7871 - val_loss: 0.7351\n","\n","Epoch 00162: loss did not improve from 2.68934\n","Epoch 163/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 2.8028 - val_loss: 0.7462\n","\n","Epoch 00163: loss did not improve from 2.68934\n","Epoch 164/400\n","16319/16319 [==============================] - 11s 644us/step - loss: 2.6737 - val_loss: 0.7238\n","\n","Epoch 00164: loss improved from 2.68934 to 2.67372, saving model to final.h5\n","Epoch 165/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 2.6822 - val_loss: 0.9008\n","\n","Epoch 00165: loss did not improve from 2.67372\n","Epoch 166/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 2.7630 - val_loss: 0.7379\n","\n","Epoch 00166: loss did not improve from 2.67372\n","Epoch 167/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.7717 - val_loss: 0.7840\n","\n","Epoch 00167: loss did not improve from 2.67372\n","Epoch 168/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.6676 - val_loss: 0.9111\n","\n","Epoch 00168: loss improved from 2.67372 to 2.66758, saving model to final.h5\n","Epoch 169/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.7342 - val_loss: 0.7033\n","\n","Epoch 00169: loss did not improve from 2.66758\n","Epoch 170/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.7057 - val_loss: 0.8702\n","\n","Epoch 00170: loss did not improve from 2.66758\n","Epoch 171/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.6854 - val_loss: 0.6633\n","\n","Epoch 00171: loss did not improve from 2.66758\n","Epoch 172/400\n","16319/16319 [==============================] - 10s 630us/step - loss: 2.6558 - val_loss: 0.6687\n","\n","Epoch 00172: loss improved from 2.66758 to 2.65582, saving model to final.h5\n","Epoch 173/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 2.7629 - val_loss: 1.0221\n","\n","Epoch 00173: loss did not improve from 2.65582\n","Epoch 174/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.7460 - val_loss: 0.7456\n","\n","Epoch 00174: loss did not improve from 2.65582\n","Epoch 175/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 2.6669 - val_loss: 0.7710\n","\n","Epoch 00175: loss did not improve from 2.65582\n","Epoch 176/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 2.7289 - val_loss: 0.8172\n","\n","Epoch 00176: loss did not improve from 2.65582\n","Epoch 177/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.7747 - val_loss: 0.6691\n","\n","Epoch 00177: loss did not improve from 2.65582\n","Epoch 178/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.7272 - val_loss: 0.6866\n","\n","Epoch 00178: loss did not improve from 2.65582\n","Epoch 179/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.7225 - val_loss: 0.6892\n","\n","Epoch 00179: loss did not improve from 2.65582\n","Epoch 180/400\n","16319/16319 [==============================] - 11s 660us/step - loss: 2.6851 - val_loss: 0.6584\n","\n","Epoch 00180: loss did not improve from 2.65582\n","Epoch 181/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.6895 - val_loss: 0.7239\n","\n","Epoch 00181: loss did not improve from 2.65582\n","Epoch 182/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.6805 - val_loss: 0.7786\n","\n","Epoch 00182: loss did not improve from 2.65582\n","Epoch 183/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 2.6859 - val_loss: 0.7577\n","\n","Epoch 00183: loss did not improve from 2.65582\n","Epoch 184/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.7271 - val_loss: 0.7285\n","\n","Epoch 00184: loss did not improve from 2.65582\n","Epoch 185/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 2.6902 - val_loss: 0.6942\n","\n","Epoch 00185: loss did not improve from 2.65582\n","Epoch 186/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 2.6711 - val_loss: 0.7123\n","\n","Epoch 00186: loss did not improve from 2.65582\n","Epoch 187/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 2.6738 - val_loss: 0.6999\n","\n","Epoch 00187: loss did not improve from 2.65582\n","Epoch 188/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 2.6509 - val_loss: 0.7256\n","\n","Epoch 00188: loss improved from 2.65582 to 2.65085, saving model to final.h5\n","Epoch 189/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 2.5680 - val_loss: 0.7188\n","\n","Epoch 00189: loss improved from 2.65085 to 2.56799, saving model to final.h5\n","Epoch 190/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 2.6234 - val_loss: 0.6878\n","\n","Epoch 00190: loss did not improve from 2.56799\n","Epoch 191/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.6182 - val_loss: 0.7009\n","\n","Epoch 00191: loss did not improve from 2.56799\n","Epoch 192/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.6557 - val_loss: 0.6441\n","\n","Epoch 00192: loss did not improve from 2.56799\n","Epoch 193/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.6106 - val_loss: 0.6935\n","\n","Epoch 00193: loss did not improve from 2.56799\n","Epoch 194/400\n","16319/16319 [==============================] - 10s 636us/step - loss: 2.6195 - val_loss: 0.7718\n","\n","Epoch 00194: loss did not improve from 2.56799\n","Epoch 195/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 2.6047 - val_loss: 0.7203\n","\n","Epoch 00195: loss did not improve from 2.56799\n","Epoch 196/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 2.6432 - val_loss: 0.9145\n","\n","Epoch 00196: loss did not improve from 2.56799\n","Epoch 197/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.6284 - val_loss: 0.7179\n","\n","Epoch 00197: loss did not improve from 2.56799\n","Epoch 198/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.6653 - val_loss: 0.7342\n","\n","Epoch 00198: loss did not improve from 2.56799\n","Epoch 199/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.6362 - val_loss: 0.6526\n","\n","Epoch 00199: loss did not improve from 2.56799\n","Epoch 200/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.7018 - val_loss: 0.6596\n","\n","Epoch 00200: loss did not improve from 2.56799\n","Epoch 201/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.6877 - val_loss: 0.6553\n","\n","Epoch 00201: loss did not improve from 2.56799\n","Epoch 202/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.6058 - val_loss: 0.6963\n","\n","Epoch 00202: loss did not improve from 2.56799\n","Epoch 203/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.6385 - val_loss: 0.5938\n","\n","Epoch 00203: loss did not improve from 2.56799\n","Epoch 204/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.6525 - val_loss: 0.6916\n","\n","Epoch 00204: loss did not improve from 2.56799\n","Epoch 205/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.6569 - val_loss: 0.8064\n","\n","Epoch 00205: loss did not improve from 2.56799\n","Epoch 206/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.6497 - val_loss: 0.9142\n","\n","Epoch 00206: loss did not improve from 2.56799\n","Epoch 207/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.5897 - val_loss: 0.7056\n","\n","Epoch 00207: loss did not improve from 2.56799\n","Epoch 208/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.6331 - val_loss: 0.8513\n","\n","Epoch 00208: loss did not improve from 2.56799\n","Epoch 209/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.5439 - val_loss: 0.6901\n","\n","Epoch 00209: loss improved from 2.56799 to 2.54387, saving model to final.h5\n","Epoch 210/400\n","16319/16319 [==============================] - 10s 637us/step - loss: 2.6023 - val_loss: 0.6885\n","\n","Epoch 00210: loss did not improve from 2.54387\n","Epoch 211/400\n","16319/16319 [==============================] - 10s 641us/step - loss: 2.6004 - val_loss: 0.6942\n","\n","Epoch 00211: loss did not improve from 2.54387\n","Epoch 212/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.6757 - val_loss: 0.7593\n","\n","Epoch 00212: loss did not improve from 2.54387\n","Epoch 213/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.5409 - val_loss: 0.7605\n","\n","Epoch 00213: loss improved from 2.54387 to 2.54091, saving model to final.h5\n","Epoch 214/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.5644 - val_loss: 0.6626\n","\n","Epoch 00214: loss did not improve from 2.54091\n","Epoch 215/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.6161 - val_loss: 0.6460\n","\n","Epoch 00215: loss did not improve from 2.54091\n","Epoch 216/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.6055 - val_loss: 0.6950\n","\n","Epoch 00216: loss did not improve from 2.54091\n","Epoch 217/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.5602 - val_loss: 0.5721\n","\n","Epoch 00217: loss did not improve from 2.54091\n","Epoch 218/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.5712 - val_loss: 0.6897\n","\n","Epoch 00218: loss did not improve from 2.54091\n","Epoch 219/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.5710 - val_loss: 0.6723\n","\n","Epoch 00219: loss did not improve from 2.54091\n","Epoch 220/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.5747 - val_loss: 0.7556\n","\n","Epoch 00220: loss did not improve from 2.54091\n","Epoch 221/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.6137 - val_loss: 0.7364\n","\n","Epoch 00221: loss did not improve from 2.54091\n","Epoch 222/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.6037 - val_loss: 0.6809\n","\n","Epoch 00222: loss did not improve from 2.54091\n","Epoch 223/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.5079 - val_loss: 0.7746\n","\n","Epoch 00223: loss improved from 2.54091 to 2.50791, saving model to final.h5\n","Epoch 224/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 2.5369 - val_loss: 0.6479\n","\n","Epoch 00224: loss did not improve from 2.50791\n","Epoch 225/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 2.5308 - val_loss: 0.6543\n","\n","Epoch 00225: loss did not improve from 2.50791\n","Epoch 226/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.5678 - val_loss: 0.7560\n","\n","Epoch 00226: loss did not improve from 2.50791\n","Epoch 227/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 2.5304 - val_loss: 0.6315\n","\n","Epoch 00227: loss did not improve from 2.50791\n","Epoch 228/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.5990 - val_loss: 0.5790\n","\n","Epoch 00228: loss did not improve from 2.50791\n","Epoch 229/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 2.5332 - val_loss: 0.7225\n","\n","Epoch 00229: loss did not improve from 2.50791\n","Epoch 230/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 2.5429 - val_loss: 0.7453\n","\n","Epoch 00230: loss did not improve from 2.50791\n","Epoch 231/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.5033 - val_loss: 0.6827\n","\n","Epoch 00231: loss improved from 2.50791 to 2.50329, saving model to final.h5\n","Epoch 232/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.4845 - val_loss: 0.6377\n","\n","Epoch 00232: loss improved from 2.50329 to 2.48450, saving model to final.h5\n","Epoch 233/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.5684 - val_loss: 1.0120\n","\n","Epoch 00233: loss did not improve from 2.48450\n","Epoch 234/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.5860 - val_loss: 0.6712\n","\n","Epoch 00234: loss did not improve from 2.48450\n","Epoch 235/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 2.5454 - val_loss: 0.6913\n","\n","Epoch 00235: loss did not improve from 2.48450\n","Epoch 236/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.5579 - val_loss: 0.9936\n","\n","Epoch 00236: loss did not improve from 2.48450\n","Epoch 237/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 2.5020 - val_loss: 0.6017\n","\n","Epoch 00237: loss did not improve from 2.48450\n","Epoch 238/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.5450 - val_loss: 0.8294\n","\n","Epoch 00238: loss did not improve from 2.48450\n","Epoch 239/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.5486 - val_loss: 0.5929\n","\n","Epoch 00239: loss did not improve from 2.48450\n","Epoch 240/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.5657 - val_loss: 0.9488\n","\n","Epoch 00240: loss did not improve from 2.48450\n","Epoch 241/400\n","16319/16319 [==============================] - 10s 642us/step - loss: 2.5307 - val_loss: 0.6288\n","\n","Epoch 00241: loss did not improve from 2.48450\n","Epoch 242/400\n","16319/16319 [==============================] - 10s 628us/step - loss: 2.5217 - val_loss: 0.5919\n","\n","Epoch 00242: loss did not improve from 2.48450\n","Epoch 243/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.4862 - val_loss: 0.9454\n","\n","Epoch 00243: loss did not improve from 2.48450\n","Epoch 244/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.5176 - val_loss: 0.6420\n","\n","Epoch 00244: loss did not improve from 2.48450\n","Epoch 245/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 2.4514 - val_loss: 0.8006\n","\n","Epoch 00245: loss improved from 2.48450 to 2.45136, saving model to final.h5\n","Epoch 246/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 2.5577 - val_loss: 0.5957\n","\n","Epoch 00246: loss did not improve from 2.45136\n","Epoch 247/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 2.4975 - val_loss: 0.7872\n","\n","Epoch 00247: loss did not improve from 2.45136\n","Epoch 248/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 2.5842 - val_loss: 0.6328\n","\n","Epoch 00248: loss did not improve from 2.45136\n","Epoch 249/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 2.5836 - val_loss: 0.7494\n","\n","Epoch 00249: loss did not improve from 2.45136\n","Epoch 250/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.4822 - val_loss: 0.7279\n","\n","Epoch 00250: loss did not improve from 2.45136\n","Epoch 251/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.4596 - val_loss: 0.7344\n","\n","Epoch 00251: loss did not improve from 2.45136\n","Epoch 252/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.4665 - val_loss: 0.7124\n","\n","Epoch 00252: loss did not improve from 2.45136\n","Epoch 253/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 2.5455 - val_loss: 0.7057\n","\n","Epoch 00253: loss did not improve from 2.45136\n","Epoch 254/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 2.5021 - val_loss: 0.7445\n","\n","Epoch 00254: loss did not improve from 2.45136\n","Epoch 255/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.5407 - val_loss: 0.6467\n","\n","Epoch 00255: loss did not improve from 2.45136\n","Epoch 256/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.4728 - val_loss: 0.6358\n","\n","Epoch 00256: loss did not improve from 2.45136\n","Epoch 257/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.5143 - val_loss: 0.7581\n","\n","Epoch 00257: loss did not improve from 2.45136\n","Epoch 258/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.5213 - val_loss: 0.6762\n","\n","Epoch 00258: loss did not improve from 2.45136\n","Epoch 259/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.4707 - val_loss: 0.7126\n","\n","Epoch 00259: loss did not improve from 2.45136\n","Epoch 260/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.5350 - val_loss: 0.6673\n","\n","Epoch 00260: loss did not improve from 2.45136\n","Epoch 261/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.5044 - val_loss: 0.6903\n","\n","Epoch 00261: loss did not improve from 2.45136\n","Epoch 262/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.5183 - val_loss: 0.6484\n","\n","Epoch 00262: loss did not improve from 2.45136\n","Epoch 263/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.4425 - val_loss: 0.7541\n","\n","Epoch 00263: loss improved from 2.45136 to 2.44254, saving model to final.h5\n","Epoch 264/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 2.5058 - val_loss: 0.7156\n","\n","Epoch 00264: loss did not improve from 2.44254\n","Epoch 265/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.4945 - val_loss: 0.7111\n","\n","Epoch 00265: loss did not improve from 2.44254\n","Epoch 266/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.4329 - val_loss: 0.7778\n","\n","Epoch 00266: loss improved from 2.44254 to 2.43285, saving model to final.h5\n","Epoch 267/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.4135 - val_loss: 0.7314\n","\n","Epoch 00267: loss improved from 2.43285 to 2.41354, saving model to final.h5\n","Epoch 268/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.4378 - val_loss: 0.7552\n","\n","Epoch 00268: loss did not improve from 2.41354\n","Epoch 269/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.4535 - val_loss: 0.5840\n","\n","Epoch 00269: loss did not improve from 2.41354\n","Epoch 270/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.4479 - val_loss: 0.7114\n","\n","Epoch 00270: loss did not improve from 2.41354\n","Epoch 271/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.4876 - val_loss: 0.8626\n","\n","Epoch 00271: loss did not improve from 2.41354\n","Epoch 272/400\n","16319/16319 [==============================] - 11s 644us/step - loss: 2.3772 - val_loss: 0.8315\n","\n","Epoch 00272: loss improved from 2.41354 to 2.37715, saving model to final.h5\n","Epoch 273/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.4296 - val_loss: 1.0991\n","\n","Epoch 00273: loss did not improve from 2.37715\n","Epoch 274/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 2.4173 - val_loss: 0.7040\n","\n","Epoch 00274: loss did not improve from 2.37715\n","Epoch 275/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.4419 - val_loss: 0.6616\n","\n","Epoch 00275: loss did not improve from 2.37715\n","Epoch 276/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.4726 - val_loss: 0.6905\n","\n","Epoch 00276: loss did not improve from 2.37715\n","Epoch 277/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.4450 - val_loss: 0.8181\n","\n","Epoch 00277: loss did not improve from 2.37715\n","Epoch 278/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.4614 - val_loss: 0.6218\n","\n","Epoch 00278: loss did not improve from 2.37715\n","Epoch 279/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.4577 - val_loss: 0.8321\n","\n","Epoch 00279: loss did not improve from 2.37715\n","Epoch 280/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.3914 - val_loss: 0.6316\n","\n","Epoch 00280: loss did not improve from 2.37715\n","Epoch 281/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.4505 - val_loss: 0.6736\n","\n","Epoch 00281: loss did not improve from 2.37715\n","Epoch 282/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.4468 - val_loss: 0.6663\n","\n","Epoch 00282: loss did not improve from 2.37715\n","Epoch 283/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.4368 - val_loss: 0.7195\n","\n","Epoch 00283: loss did not improve from 2.37715\n","Epoch 284/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.4582 - val_loss: 0.7021\n","\n","Epoch 00284: loss did not improve from 2.37715\n","Epoch 285/400\n","16319/16319 [==============================] - 10s 628us/step - loss: 2.3847 - val_loss: 0.6678\n","\n","Epoch 00285: loss did not improve from 2.37715\n","Epoch 286/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.3951 - val_loss: 0.7092\n","\n","Epoch 00286: loss did not improve from 2.37715\n","Epoch 287/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.4127 - val_loss: 0.6930\n","\n","Epoch 00287: loss did not improve from 2.37715\n","Epoch 288/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 2.4065 - val_loss: 0.6191\n","\n","Epoch 00288: loss did not improve from 2.37715\n","Epoch 289/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.4399 - val_loss: 0.7842\n","\n","Epoch 00289: loss did not improve from 2.37715\n","Epoch 290/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.3545 - val_loss: 0.7493\n","\n","Epoch 00290: loss improved from 2.37715 to 2.35452, saving model to final.h5\n","Epoch 291/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.3788 - val_loss: 0.6737\n","\n","Epoch 00291: loss did not improve from 2.35452\n","Epoch 292/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.4258 - val_loss: 0.7936\n","\n","Epoch 00292: loss did not improve from 2.35452\n","Epoch 293/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.3598 - val_loss: 0.6169\n","\n","Epoch 00293: loss did not improve from 2.35452\n","Epoch 294/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.3928 - val_loss: 0.8579\n","\n","Epoch 00294: loss did not improve from 2.35452\n","Epoch 295/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.3935 - val_loss: 0.9899\n","\n","Epoch 00295: loss did not improve from 2.35452\n","Epoch 296/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.3784 - val_loss: 0.6739\n","\n","Epoch 00296: loss did not improve from 2.35452\n","Epoch 297/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.3981 - val_loss: 0.8175\n","\n","Epoch 00297: loss did not improve from 2.35452\n","Epoch 298/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.4811 - val_loss: 0.6635\n","\n","Epoch 00298: loss did not improve from 2.35452\n","Epoch 299/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.4322 - val_loss: 0.6933\n","\n","Epoch 00299: loss did not improve from 2.35452\n","Epoch 300/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.4186 - val_loss: 0.6124\n","\n","Epoch 00300: loss did not improve from 2.35452\n","Epoch 301/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.3798 - val_loss: 0.5934\n","\n","Epoch 00301: loss did not improve from 2.35452\n","Epoch 302/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 2.4256 - val_loss: 0.7860\n","\n","Epoch 00302: loss did not improve from 2.35452\n","Epoch 303/400\n","16319/16319 [==============================] - 11s 653us/step - loss: 2.3270 - val_loss: 0.6256\n","\n","Epoch 00303: loss improved from 2.35452 to 2.32698, saving model to final.h5\n","Epoch 304/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.3773 - val_loss: 0.6757\n","\n","Epoch 00304: loss did not improve from 2.32698\n","Epoch 305/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.4235 - val_loss: 0.6286\n","\n","Epoch 00305: loss did not improve from 2.32698\n","Epoch 306/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.4029 - val_loss: 0.6559\n","\n","Epoch 00306: loss did not improve from 2.32698\n","Epoch 307/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 2.4158 - val_loss: 0.7068\n","\n","Epoch 00307: loss did not improve from 2.32698\n","Epoch 308/400\n","16319/16319 [==============================] - 10s 630us/step - loss: 2.4203 - val_loss: 0.6920\n","\n","Epoch 00308: loss did not improve from 2.32698\n","Epoch 309/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 2.3818 - val_loss: 0.6479\n","\n","Epoch 00309: loss did not improve from 2.32698\n","Epoch 310/400\n","16319/16319 [==============================] - 10s 634us/step - loss: 2.4353 - val_loss: 0.9044\n","\n","Epoch 00310: loss did not improve from 2.32698\n","Epoch 311/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 2.3345 - val_loss: 0.6947\n","\n","Epoch 00311: loss did not improve from 2.32698\n","Epoch 312/400\n","16319/16319 [==============================] - 10s 626us/step - loss: 2.4087 - val_loss: 0.6517\n","\n","Epoch 00312: loss did not improve from 2.32698\n","Epoch 313/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 2.3561 - val_loss: 0.6453\n","\n","Epoch 00313: loss did not improve from 2.32698\n","Epoch 314/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.3929 - val_loss: 0.6686\n","\n","Epoch 00314: loss did not improve from 2.32698\n","Epoch 315/400\n","16319/16319 [==============================] - 10s 636us/step - loss: 2.3483 - val_loss: 0.6136\n","\n","Epoch 00315: loss did not improve from 2.32698\n","Epoch 316/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.3638 - val_loss: 0.6783\n","\n","Epoch 00316: loss did not improve from 2.32698\n","Epoch 317/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.3875 - val_loss: 0.7088\n","\n","Epoch 00317: loss did not improve from 2.32698\n","Epoch 318/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.3396 - val_loss: 0.7526\n","\n","Epoch 00318: loss did not improve from 2.32698\n","Epoch 319/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.4222 - val_loss: 0.6258\n","\n","Epoch 00319: loss did not improve from 2.32698\n","Epoch 320/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.3479 - val_loss: 0.7536\n","\n","Epoch 00320: loss did not improve from 2.32698\n","Epoch 321/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 2.3951 - val_loss: 0.7466\n","\n","Epoch 00321: loss did not improve from 2.32698\n","Epoch 322/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.3638 - val_loss: 0.6538\n","\n","Epoch 00322: loss did not improve from 2.32698\n","Epoch 323/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 2.2977 - val_loss: 0.8411\n","\n","Epoch 00323: loss improved from 2.32698 to 2.29775, saving model to final.h5\n","Epoch 324/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.3278 - val_loss: 0.6517\n","\n","Epoch 00324: loss did not improve from 2.29775\n","Epoch 325/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.3303 - val_loss: 0.6741\n","\n","Epoch 00325: loss did not improve from 2.29775\n","Epoch 326/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 2.3292 - val_loss: 0.5937\n","\n","Epoch 00326: loss did not improve from 2.29775\n","Epoch 327/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.3669 - val_loss: 0.7312\n","\n","Epoch 00327: loss did not improve from 2.29775\n","Epoch 328/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.3706 - val_loss: 0.6295\n","\n","Epoch 00328: loss did not improve from 2.29775\n","Epoch 329/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.3504 - val_loss: 0.6609\n","\n","Epoch 00329: loss did not improve from 2.29775\n","Epoch 330/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.3105 - val_loss: 0.6846\n","\n","Epoch 00330: loss did not improve from 2.29775\n","Epoch 331/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 2.3115 - val_loss: 0.7157\n","\n","Epoch 00331: loss did not improve from 2.29775\n","Epoch 332/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.3585 - val_loss: 0.6789\n","\n","Epoch 00332: loss did not improve from 2.29775\n","Epoch 333/400\n","16319/16319 [==============================] - 10s 634us/step - loss: 2.3877 - val_loss: 0.7954\n","\n","Epoch 00333: loss did not improve from 2.29775\n","Epoch 334/400\n","16319/16319 [==============================] - 10s 636us/step - loss: 2.3039 - val_loss: 0.8703\n","\n","Epoch 00334: loss did not improve from 2.29775\n","Epoch 335/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.3822 - val_loss: 0.6514\n","\n","Epoch 00335: loss did not improve from 2.29775\n","Epoch 336/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.3496 - val_loss: 0.7259\n","\n","Epoch 00336: loss did not improve from 2.29775\n","Epoch 337/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.4381 - val_loss: 0.6306\n","\n","Epoch 00337: loss did not improve from 2.29775\n","Epoch 338/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.4069 - val_loss: 0.5546\n","\n","Epoch 00338: loss did not improve from 2.29775\n","Epoch 339/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.4015 - val_loss: 0.6771\n","\n","Epoch 00339: loss did not improve from 2.29775\n","Epoch 340/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.3472 - val_loss: 0.6923\n","\n","Epoch 00340: loss did not improve from 2.29775\n","Epoch 341/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.3485 - val_loss: 0.5973\n","\n","Epoch 00341: loss did not improve from 2.29775\n","Epoch 342/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.3316 - val_loss: 0.6775\n","\n","Epoch 00342: loss did not improve from 2.29775\n","Epoch 343/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.2809 - val_loss: 0.6554\n","\n","Epoch 00343: loss improved from 2.29775 to 2.28088, saving model to final.h5\n","Epoch 344/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.3212 - val_loss: 0.9144\n","\n","Epoch 00344: loss did not improve from 2.28088\n","Epoch 345/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 2.3391 - val_loss: 0.6586\n","\n","Epoch 00345: loss did not improve from 2.28088\n","Epoch 346/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 2.2822 - val_loss: 0.7220\n","\n","Epoch 00346: loss did not improve from 2.28088\n","Epoch 347/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 2.3409 - val_loss: 0.8043\n","\n","Epoch 00347: loss did not improve from 2.28088\n","Epoch 348/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.2932 - val_loss: 0.7254\n","\n","Epoch 00348: loss did not improve from 2.28088\n","Epoch 349/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.2816 - val_loss: 0.8325\n","\n","Epoch 00349: loss did not improve from 2.28088\n","Epoch 350/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.3656 - val_loss: 0.7497\n","\n","Epoch 00350: loss did not improve from 2.28088\n","Epoch 351/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.3720 - val_loss: 0.6479\n","\n","Epoch 00351: loss did not improve from 2.28088\n","Epoch 352/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.3623 - val_loss: 0.8617\n","\n","Epoch 00352: loss did not improve from 2.28088\n","Epoch 353/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.3220 - val_loss: 0.7297\n","\n","Epoch 00353: loss did not improve from 2.28088\n","Epoch 354/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.4345 - val_loss: 0.8285\n","\n","Epoch 00354: loss did not improve from 2.28088\n","Epoch 355/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2.3173 - val_loss: 0.7355\n","\n","Epoch 00355: loss did not improve from 2.28088\n","Epoch 356/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 2.3029 - val_loss: 0.7062\n","\n","Epoch 00356: loss did not improve from 2.28088\n","Epoch 357/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 2.3080 - val_loss: 0.8130\n","\n","Epoch 00357: loss did not improve from 2.28088\n","Epoch 358/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 2.2714 - val_loss: 0.6735\n","\n","Epoch 00358: loss improved from 2.28088 to 2.27138, saving model to final.h5\n","Epoch 359/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.2688 - val_loss: 0.6012\n","\n","Epoch 00359: loss improved from 2.27138 to 2.26876, saving model to final.h5\n","Epoch 360/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.2709 - val_loss: 0.7188\n","\n","Epoch 00360: loss did not improve from 2.26876\n","Epoch 361/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.3390 - val_loss: 0.9094\n","\n","Epoch 00361: loss did not improve from 2.26876\n","Epoch 362/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.3097 - val_loss: 0.6951\n","\n","Epoch 00362: loss did not improve from 2.26876\n","Epoch 363/400\n","16319/16319 [==============================] - 10s 620us/step - loss: 2.3184 - val_loss: 0.8071\n","\n","Epoch 00363: loss did not improve from 2.26876\n","Epoch 364/400\n","16319/16319 [==============================] - 10s 639us/step - loss: 2.3044 - val_loss: 0.6326\n","\n","Epoch 00364: loss did not improve from 2.26876\n","Epoch 365/400\n","16319/16319 [==============================] - 10s 627us/step - loss: 2.2800 - val_loss: 0.9384\n","\n","Epoch 00365: loss did not improve from 2.26876\n","Epoch 366/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.3009 - val_loss: 0.7723\n","\n","Epoch 00366: loss did not improve from 2.26876\n","Epoch 367/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.3269 - val_loss: 0.7157\n","\n","Epoch 00367: loss did not improve from 2.26876\n","Epoch 368/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.2682 - val_loss: 0.6692\n","\n","Epoch 00368: loss improved from 2.26876 to 2.26821, saving model to final.h5\n","Epoch 369/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.3294 - val_loss: 0.9172\n","\n","Epoch 00369: loss did not improve from 2.26821\n","Epoch 370/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.2518 - val_loss: 0.7935\n","\n","Epoch 00370: loss improved from 2.26821 to 2.25181, saving model to final.h5\n","Epoch 371/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.2859 - val_loss: 0.5912\n","\n","Epoch 00371: loss did not improve from 2.25181\n","Epoch 372/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.3139 - val_loss: 0.7882\n","\n","Epoch 00372: loss did not improve from 2.25181\n","Epoch 373/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.2650 - val_loss: 0.6634\n","\n","Epoch 00373: loss did not improve from 2.25181\n","Epoch 374/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.2628 - val_loss: 0.7055\n","\n","Epoch 00374: loss did not improve from 2.25181\n","Epoch 375/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.2023 - val_loss: 0.7350\n","\n","Epoch 00375: loss improved from 2.25181 to 2.20231, saving model to final.h5\n","Epoch 376/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 2.2577 - val_loss: 0.7720\n","\n","Epoch 00376: loss did not improve from 2.20231\n","Epoch 377/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.2941 - val_loss: 0.8703\n","\n","Epoch 00377: loss did not improve from 2.20231\n","Epoch 378/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.2597 - val_loss: 0.8018\n","\n","Epoch 00378: loss did not improve from 2.20231\n","Epoch 379/400\n","16319/16319 [==============================] - 10s 622us/step - loss: 2.2625 - val_loss: 0.8658\n","\n","Epoch 00379: loss did not improve from 2.20231\n","Epoch 380/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 2.2502 - val_loss: 0.6888\n","\n","Epoch 00380: loss did not improve from 2.20231\n","Epoch 381/400\n","16319/16319 [==============================] - 10s 610us/step - loss: 2.2495 - val_loss: 0.9717\n","\n","Epoch 00381: loss did not improve from 2.20231\n","Epoch 382/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.2988 - val_loss: 0.6179\n","\n","Epoch 00382: loss did not improve from 2.20231\n","Epoch 383/400\n","16319/16319 [==============================] - 10s 612us/step - loss: 2.2776 - val_loss: 0.6718\n","\n","Epoch 00383: loss did not improve from 2.20231\n","Epoch 384/400\n","16319/16319 [==============================] - 10s 613us/step - loss: 2.3032 - val_loss: 0.7967\n","\n","Epoch 00384: loss did not improve from 2.20231\n","Epoch 385/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.2365 - val_loss: 0.6296\n","\n","Epoch 00385: loss did not improve from 2.20231\n","Epoch 386/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.3159 - val_loss: 0.8631\n","\n","Epoch 00386: loss did not improve from 2.20231\n","Epoch 387/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 2.2747 - val_loss: 0.6758\n","\n","Epoch 00387: loss did not improve from 2.20231\n","Epoch 388/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.3153 - val_loss: 0.8618\n","\n","Epoch 00388: loss did not improve from 2.20231\n","Epoch 389/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.3069 - val_loss: 0.6808\n","\n","Epoch 00389: loss did not improve from 2.20231\n","Epoch 390/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.2651 - val_loss: 0.8285\n","\n","Epoch 00390: loss did not improve from 2.20231\n","Epoch 391/400\n","16319/16319 [==============================] - 10s 617us/step - loss: 2.2744 - val_loss: 0.7488\n","\n","Epoch 00391: loss did not improve from 2.20231\n","Epoch 392/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.2895 - val_loss: 0.8074\n","\n","Epoch 00392: loss did not improve from 2.20231\n","Epoch 393/400\n","16319/16319 [==============================] - 10s 611us/step - loss: 2.3479 - val_loss: 0.6838\n","\n","Epoch 00393: loss did not improve from 2.20231\n","Epoch 394/400\n","16319/16319 [==============================] - 10s 616us/step - loss: 2.2992 - val_loss: 0.6518\n","\n","Epoch 00394: loss did not improve from 2.20231\n","Epoch 395/400\n","16319/16319 [==============================] - 11s 652us/step - loss: 2.2796 - val_loss: 0.7978\n","\n","Epoch 00395: loss did not improve from 2.20231\n","Epoch 396/400\n","16319/16319 [==============================] - 10s 619us/step - loss: 2.2484 - val_loss: 0.7751\n","\n","Epoch 00396: loss did not improve from 2.20231\n","Epoch 397/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.2330 - val_loss: 0.7185\n","\n","Epoch 00397: loss did not improve from 2.20231\n","Epoch 398/400\n","16319/16319 [==============================] - 10s 614us/step - loss: 2.2883 - val_loss: 0.6864\n","\n","Epoch 00398: loss did not improve from 2.20231\n","Epoch 399/400\n","16319/16319 [==============================] - 10s 615us/step - loss: 2.2737 - val_loss: 0.7350\n","\n","Epoch 00399: loss did not improve from 2.20231\n","Epoch 400/400\n","16319/16319 [==============================] - 10s 618us/step - loss: 2.2817 - val_loss: 0.7705\n","\n","Epoch 00400: loss did not improve from 2.20231\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1AQGE2NUzQ1j","colab_type":"code","outputId":"2ee031c9-543d-4f66-d2c2-82c09ec86808","executionInfo":{"status":"ok","timestamp":1590279902996,"user_tz":360,"elapsed":126212,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet_final_with_LSTM.py train.mp4 train.txt --mode=train --split=0.2 --model final.h5 --epoch 400 --history 1 --LR 0.1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-24 00:22:29.461708: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet_final_with_LSTM.py:180: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-24 00:22:31.157118: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-24 00:22:31.170299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:22:31.170840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 00:22:31.170896: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 00:22:31.172607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 00:22:31.174482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 00:22:31.174802: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 00:22:31.176455: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 00:22:31.177548: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 00:22:31.181372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 00:22:31.181478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:22:31.182044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:22:31.182522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 00:22:31.187558: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-05-24 00:22:31.187797: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x29612c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 00:22:31.187833: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-24 00:22:31.276898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:22:31.277547: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2961480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 00:22:31.277577: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-24 00:22:31.277745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:22:31.278285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 00:22:31.278332: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 00:22:31.278377: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 00:22:31.278392: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 00:22:31.278406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 00:22:31.278420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 00:22:31.278432: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 00:22:31.278446: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 00:22:31.278518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:22:31.279060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:22:31.279534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 00:22:31.279615: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 00:22:31.777340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 00:22:31.777400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-24 00:22:31.777409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-24 00:22:31.777630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:22:31.778276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:22:31.778761: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-24 00:22:31.778797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet_final_with_LSTM.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","speednet_final_with_LSTM.py:185: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","Found preprocessed data\n","tcmalloc: large alloc 2447884288 bytes == 0x1f350000 @  0x7efd6a3781e7 0x7efd67e5e5e1 0x7efd67ec2c78 0x7efd67ec2f37 0x7efd67f5af28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7efd69f75b97 0x5b250a\n","tcmalloc: large alloc 2447884288 bytes == 0xb11cc000 @  0x7efd6a3781e7 0x7efd67e5e5e1 0x7efd67ec2c78 0x7efd67ec2f37 0x7efd67f5af28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7efd69f75b97 0x5b250a\n","Loading frame 20398\n","done loading 20399 frames\n","Done prepping data\n","tcmalloc: large alloc 1631920128 bytes == 0x1430a4000 @  0x7efd6a3781e7 0x7efd67e5e5e1 0x7efd67ec2c78 0x7efd67ec2d93 0x7efd67f4ded6 0x7efd67f4e338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7efd69f75b97 0x5b250a\n","tcmalloc: large alloc 1631920128 bytes == 0x1f350000 @  0x7efd6a3781e7 0x7efd67e5e5e1 0x7efd67ec2c78 0x7efd67ec2d93 0x7efd67f4ded6 0x7efd67f4e338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7efd69f75b97 0x5b250a\n","Starting training\n","Train on 16319 samples, validate on 4080 samples\n","Epoch 1/400\n","2020-05-24 00:22:43.687830: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 00:22:43.888702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","16319/16319 [==============================] - 12s 724us/step - loss: 943.3585 - val_loss: 68.4345\n","\n","Epoch 00001: loss improved from inf to 943.35853, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/400\n","16319/16319 [==============================] - 10s 629us/step - loss: 263.5430 - val_loss: 75.8357\n","\n","Epoch 00002: loss improved from 943.35853 to 263.54298, saving model to final.h5\n","Epoch 3/400\n","16319/16319 [==============================] - 10s 623us/step - loss: 2931.9158 - val_loss: 69.1943\n","\n","Epoch 00003: loss did not improve from 263.54298\n","Epoch 4/400\n","16319/16319 [==============================] - 10s 637us/step - loss: 255.5737 - val_loss: 72.7531\n","\n","Epoch 00004: loss improved from 263.54298 to 255.57370, saving model to final.h5\n","Epoch 5/400\n","16319/16319 [==============================] - 10s 628us/step - loss: 432.2463 - val_loss: 153.3780\n","\n","Epoch 00005: loss did not improve from 255.57370\n","Epoch 6/400\n","16319/16319 [==============================] - 10s 621us/step - loss: 3775.3621 - val_loss: 69.6282\n","\n","Epoch 00006: loss did not improve from 255.57370\n","Epoch 7/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 172.9162 - val_loss: 75.7056\n","\n","Epoch 00007: loss improved from 255.57370 to 172.91616, saving model to final.h5\n","Epoch 8/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 572.5261 - val_loss: 397.6266\n","\n","Epoch 00008: loss did not improve from 172.91616\n","Epoch 9/400\n","16319/16319 [==============================] - 10s 625us/step - loss: 380.4935 - val_loss: 480.4227\n","\n","Epoch 00009: loss did not improve from 172.91616\n","Epoch 10/400\n","16319/16319 [==============================] - 10s 624us/step - loss: 4144.0457 - val_loss: 69.7354\n","\n","Epoch 00010: loss did not improve from 172.91616\n","Epoch 11/400\n"," 8240/16319 [==============>...............] - ETA: 4s - loss: 134.3783\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v__VP4RI45fj","colab_type":"text"},"source":["Added Data Generator -- training no augmentation normal splt and seeing what happens"]},{"cell_type":"code","metadata":{"id":"NwSj3dC55IOT","colab_type":"code","outputId":"ffe15b06-ab2d-4cee-caa4-76c4a5c15014","executionInfo":{"status":"ok","timestamp":1590284533890,"user_tz":360,"elapsed":3904555,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet_final_with_LSTM_testtobereplaced.py train.mp4 train.txt --mode=train --split=0.2 --model final.h5 --epoch 400 --history 1 --LR 0.0001"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-24 00:36:41.481598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet_final_with_LSTM_testtobereplaced.py:180: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-24 00:36:43.168716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-24 00:36:43.181788: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:36:43.182370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 00:36:43.182409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 00:36:43.184081: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 00:36:43.185863: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 00:36:43.186217: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 00:36:43.187834: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 00:36:43.188987: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 00:36:43.192665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 00:36:43.192777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:36:43.193396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:36:43.193906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 00:36:43.198840: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-05-24 00:36:43.199124: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x208b2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 00:36:43.199157: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-24 00:36:43.293406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:36:43.294412: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x208b480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 00:36:43.294444: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-24 00:36:43.294630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:36:43.295201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 00:36:43.295249: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 00:36:43.295305: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 00:36:43.295330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 00:36:43.295357: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 00:36:43.295378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 00:36:43.295399: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 00:36:43.295420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 00:36:43.295487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:36:43.296043: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:36:43.296527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 00:36:43.296575: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 00:36:43.793049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 00:36:43.793106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-24 00:36:43.793118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-24 00:36:43.793342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:36:43.793959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 00:36:43.794453: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-24 00:36:43.794496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet_final_with_LSTM_testtobereplaced.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","speednet_final_with_LSTM_testtobereplaced.py:185: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","Found preprocessed data\n","tcmalloc: large alloc 2447884288 bytes == 0x1ea84000 @  0x7f7159e781e7 0x7f715791e5e1 0x7f7157982c78 0x7f7157982f37 0x7f7157a1af28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f7159a75b97 0x5b250a\n","tcmalloc: large alloc 2447884288 bytes == 0xb0900000 @  0x7f7159e781e7 0x7f715791e5e1 0x7f7157982c78 0x7f7157982f37 0x7f7157a1af28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f7159a75b97 0x5b250a\n","Loading frame 20398\n","done loading 20399 frames\n","Done prepping data\n","tcmalloc: large alloc 1631920128 bytes == 0x1427d8000 @  0x7f7159e781e7 0x7f715791e5e1 0x7f7157982c78 0x7f7157982d93 0x7f7157a0ded6 0x7f7157a0e338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f7159a75b97 0x5b250a\n","tcmalloc: large alloc 1631920128 bytes == 0x1ea84000 @  0x7f7159e781e7 0x7f715791e5e1 0x7f7157982c78 0x7f7157982d93 0x7f7157a0ded6 0x7f7157a0e338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f7159a75b97 0x5b250a\n","20399\n","20399 Training data size per Aug\n","16319 Train indices size\n","4080 Val indices size\n","Starting training\n","shuffling\n","shuffling\n","Epoch 1/400\n","2020-05-24 00:36:55.186173: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 00:36:55.368076: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","1632/1632 [==============================] - 12s 7ms/step - loss: 78.1571 - val_loss: 105.6420\n","\n","Epoch 00001: loss improved from inf to 78.15817, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 31.9301 - val_loss: 10.4053\n","\n","Epoch 00002: loss improved from 78.15817 to 31.92867, saving model to final.h5\n","Epoch 3/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 14.1999 - val_loss: 16.5617\n","\n","Epoch 00003: loss improved from 31.92867 to 14.19991, saving model to final.h5\n","Epoch 4/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 12.0319 - val_loss: 25.3706\n","\n","Epoch 00004: loss improved from 14.19991 to 12.03218, saving model to final.h5\n","Epoch 5/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.4839 - val_loss: 7.8457\n","\n","Epoch 00005: loss improved from 12.03218 to 10.48337, saving model to final.h5\n","Epoch 6/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.7333 - val_loss: 10.2467\n","\n","Epoch 00006: loss improved from 10.48337 to 9.73335, saving model to final.h5\n","Epoch 7/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.8161 - val_loss: 13.5159\n","\n","Epoch 00007: loss improved from 9.73335 to 8.81631, saving model to final.h5\n","Epoch 8/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.4032 - val_loss: 7.3013\n","\n","Epoch 00008: loss improved from 8.81631 to 8.40289, saving model to final.h5\n","Epoch 9/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.9622 - val_loss: 13.3594\n","\n","Epoch 00009: loss improved from 8.40289 to 7.96209, saving model to final.h5\n","Epoch 10/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.4283 - val_loss: 10.9179\n","\n","Epoch 00010: loss improved from 7.96209 to 7.42821, saving model to final.h5\n","Epoch 11/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.9847 - val_loss: 8.5371\n","\n","Epoch 00011: loss improved from 7.42821 to 6.98505, saving model to final.h5\n","Epoch 12/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.9947 - val_loss: 1.4856\n","\n","Epoch 00012: loss did not improve from 6.98505\n","Epoch 13/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.6319 - val_loss: 10.4798\n","\n","Epoch 00013: loss improved from 6.98505 to 6.63206, saving model to final.h5\n","Epoch 14/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.5178 - val_loss: 1.3131\n","\n","Epoch 00014: loss improved from 6.63206 to 6.51791, saving model to final.h5\n","Epoch 15/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.3300 - val_loss: 3.8110\n","\n","Epoch 00015: loss improved from 6.51791 to 6.33023, saving model to final.h5\n","Epoch 16/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.2488 - val_loss: 6.0302\n","\n","Epoch 00016: loss improved from 6.33023 to 6.24910, saving model to final.h5\n","Epoch 17/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.9451 - val_loss: 3.1294\n","\n","Epoch 00017: loss improved from 6.24910 to 5.94511, saving model to final.h5\n","Epoch 18/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.7566 - val_loss: 7.9508\n","\n","Epoch 00018: loss improved from 5.94511 to 5.75668, saving model to final.h5\n","Epoch 19/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.7054 - val_loss: 2.4286\n","\n","Epoch 00019: loss improved from 5.75668 to 5.70534, saving model to final.h5\n","Epoch 20/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.4579 - val_loss: 7.8469\n","\n","Epoch 00020: loss improved from 5.70534 to 5.45792, saving model to final.h5\n","Epoch 21/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.3544 - val_loss: 2.3826\n","\n","Epoch 00021: loss improved from 5.45792 to 5.35460, saving model to final.h5\n","Epoch 22/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.3830 - val_loss: 2.4681\n","\n","Epoch 00022: loss did not improve from 5.35460\n","Epoch 23/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.2192 - val_loss: 14.9669\n","\n","Epoch 00023: loss improved from 5.35460 to 5.21909, saving model to final.h5\n","Epoch 24/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.1531 - val_loss: 3.6981\n","\n","Epoch 00024: loss improved from 5.21909 to 5.15331, saving model to final.h5\n","Epoch 25/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.0358 - val_loss: 7.1669\n","\n","Epoch 00025: loss improved from 5.15331 to 5.03583, saving model to final.h5\n","Epoch 26/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.0276 - val_loss: 2.4185\n","\n","Epoch 00026: loss improved from 5.03583 to 5.02773, saving model to final.h5\n","Epoch 27/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.8857 - val_loss: 3.2983\n","\n","Epoch 00027: loss improved from 5.02773 to 4.88565, saving model to final.h5\n","Epoch 28/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.8729 - val_loss: 7.6245\n","\n","Epoch 00028: loss improved from 4.88565 to 4.87297, saving model to final.h5\n","Epoch 29/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.8537 - val_loss: 5.8579\n","\n","Epoch 00029: loss improved from 4.87297 to 4.85387, saving model to final.h5\n","Epoch 30/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.7709 - val_loss: 5.1747\n","\n","Epoch 00030: loss improved from 4.85387 to 4.77112, saving model to final.h5\n","Epoch 31/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.7532 - val_loss: 1.5245\n","\n","Epoch 00031: loss improved from 4.77112 to 4.75327, saving model to final.h5\n","Epoch 32/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.6476 - val_loss: 13.5845\n","\n","Epoch 00032: loss improved from 4.75327 to 4.64733, saving model to final.h5\n","Epoch 33/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.5952 - val_loss: 2.4681\n","\n","Epoch 00033: loss improved from 4.64733 to 4.59537, saving model to final.h5\n","Epoch 34/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.5343 - val_loss: 7.0305\n","\n","Epoch 00034: loss improved from 4.59537 to 4.53440, saving model to final.h5\n","Epoch 35/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.6129 - val_loss: 12.0184\n","\n","Epoch 00035: loss did not improve from 4.53440\n","Epoch 36/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.4192 - val_loss: 2.7048\n","\n","Epoch 00036: loss improved from 4.53440 to 4.41902, saving model to final.h5\n","Epoch 37/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.4693 - val_loss: 4.3582\n","\n","Epoch 00037: loss did not improve from 4.41902\n","Epoch 38/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.4127 - val_loss: 4.1498\n","\n","Epoch 00038: loss improved from 4.41902 to 4.41199, saving model to final.h5\n","Epoch 39/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.4271 - val_loss: 6.8997\n","\n","Epoch 00039: loss did not improve from 4.41199\n","Epoch 40/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.2608 - val_loss: 6.1506\n","\n","Epoch 00040: loss improved from 4.41199 to 4.26080, saving model to final.h5\n","Epoch 41/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.3099 - val_loss: 4.3830\n","\n","Epoch 00041: loss did not improve from 4.26080\n","Epoch 42/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.2977 - val_loss: 4.1646\n","\n","Epoch 00042: loss did not improve from 4.26080\n","Epoch 43/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.2258 - val_loss: 3.6317\n","\n","Epoch 00043: loss improved from 4.26080 to 4.22595, saving model to final.h5\n","Epoch 44/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.3010 - val_loss: 2.3896\n","\n","Epoch 00044: loss did not improve from 4.22595\n","Epoch 45/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.2127 - val_loss: 10.9763\n","\n","Epoch 00045: loss improved from 4.22595 to 4.21268, saving model to final.h5\n","Epoch 46/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.2238 - val_loss: 10.2766\n","\n","Epoch 00046: loss did not improve from 4.21268\n","Epoch 47/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.1511 - val_loss: 8.9637\n","\n","Epoch 00047: loss improved from 4.21268 to 4.15128, saving model to final.h5\n","Epoch 48/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.2061 - val_loss: 7.2395\n","\n","Epoch 00048: loss did not improve from 4.15128\n","Epoch 49/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0565 - val_loss: 3.3570\n","\n","Epoch 00049: loss improved from 4.15128 to 4.05652, saving model to final.h5\n","Epoch 50/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.2156 - val_loss: 6.9857\n","\n","Epoch 00050: loss did not improve from 4.05652\n","Epoch 51/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9605 - val_loss: 3.2048\n","\n","Epoch 00051: loss improved from 4.05652 to 3.96068, saving model to final.h5\n","Epoch 52/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0164 - val_loss: 1.3540\n","\n","Epoch 00052: loss did not improve from 3.96068\n","Epoch 53/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0328 - val_loss: 1.4303\n","\n","Epoch 00053: loss did not improve from 3.96068\n","Epoch 54/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9460 - val_loss: 3.1565\n","\n","Epoch 00054: loss improved from 3.96068 to 3.94620, saving model to final.h5\n","Epoch 55/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9497 - val_loss: 2.2632\n","\n","Epoch 00055: loss did not improve from 3.94620\n","Epoch 56/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9834 - val_loss: 2.2357\n","\n","Epoch 00056: loss did not improve from 3.94620\n","Epoch 57/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0347 - val_loss: 13.5088\n","\n","Epoch 00057: loss did not improve from 3.94620\n","Epoch 58/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8883 - val_loss: 5.2245\n","\n","Epoch 00058: loss improved from 3.94620 to 3.88811, saving model to final.h5\n","Epoch 59/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9017 - val_loss: 2.2761\n","\n","Epoch 00059: loss did not improve from 3.88811\n","Epoch 60/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9064 - val_loss: 2.7591\n","\n","Epoch 00060: loss did not improve from 3.88811\n","Epoch 61/400\n","1632/1632 [==============================] - 11s 6ms/step - loss: 3.8400 - val_loss: 4.5932\n","\n","Epoch 00061: loss improved from 3.88811 to 3.84009, saving model to final.h5\n","Epoch 62/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8787 - val_loss: 6.0037\n","\n","Epoch 00062: loss did not improve from 3.84009\n","Epoch 63/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8962 - val_loss: 3.4179\n","\n","Epoch 00063: loss did not improve from 3.84009\n","Epoch 64/400\n","1632/1632 [==============================] - 11s 7ms/step - loss: 3.7278 - val_loss: 4.8968\n","\n","Epoch 00064: loss improved from 3.84009 to 3.72784, saving model to final.h5\n","Epoch 65/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8090 - val_loss: 4.8322\n","\n","Epoch 00065: loss did not improve from 3.72784\n","Epoch 66/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8807 - val_loss: 9.5835\n","\n","Epoch 00066: loss did not improve from 3.72784\n","Epoch 67/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7425 - val_loss: 1.5745\n","\n","Epoch 00067: loss did not improve from 3.72784\n","Epoch 68/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8302 - val_loss: 1.5511\n","\n","Epoch 00068: loss did not improve from 3.72784\n","Epoch 69/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7680 - val_loss: 6.3048\n","\n","Epoch 00069: loss did not improve from 3.72784\n","Epoch 70/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6367 - val_loss: 5.1804\n","\n","Epoch 00070: loss improved from 3.72784 to 3.63681, saving model to final.h5\n","Epoch 71/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7762 - val_loss: 1.7338\n","\n","Epoch 00071: loss did not improve from 3.63681\n","Epoch 72/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6974 - val_loss: 5.2954\n","\n","Epoch 00072: loss did not improve from 3.63681\n","Epoch 73/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5725 - val_loss: 1.3676\n","\n","Epoch 00073: loss improved from 3.63681 to 3.57256, saving model to final.h5\n","Epoch 74/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6697 - val_loss: 1.9666\n","\n","Epoch 00074: loss did not improve from 3.57256\n","Epoch 75/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7748 - val_loss: 4.4892\n","\n","Epoch 00075: loss did not improve from 3.57256\n","Epoch 76/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6278 - val_loss: 6.5605\n","\n","Epoch 00076: loss did not improve from 3.57256\n","Epoch 77/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5885 - val_loss: 2.5661\n","\n","Epoch 00077: loss did not improve from 3.57256\n","Epoch 78/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6134 - val_loss: 2.6720\n","\n","Epoch 00078: loss did not improve from 3.57256\n","Epoch 79/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5952 - val_loss: 3.3017\n","\n","Epoch 00079: loss did not improve from 3.57256\n","Epoch 80/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6652 - val_loss: 1.6472\n","\n","Epoch 00080: loss did not improve from 3.57256\n","Epoch 81/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6317 - val_loss: 9.6927\n","\n","Epoch 00081: loss did not improve from 3.57256\n","Epoch 82/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6266 - val_loss: 2.5994\n","\n","Epoch 00082: loss did not improve from 3.57256\n","Epoch 83/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4928 - val_loss: 8.0281\n","\n","Epoch 00083: loss improved from 3.57256 to 3.49288, saving model to final.h5\n","Epoch 84/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5632 - val_loss: 1.8102\n","\n","Epoch 00084: loss did not improve from 3.49288\n","Epoch 85/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5112 - val_loss: 7.1894\n","\n","Epoch 00085: loss did not improve from 3.49288\n","Epoch 86/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6015 - val_loss: 3.9100\n","\n","Epoch 00086: loss did not improve from 3.49288\n","Epoch 87/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5776 - val_loss: 3.7357\n","\n","Epoch 00087: loss did not improve from 3.49288\n","Epoch 88/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4744 - val_loss: 5.9463\n","\n","Epoch 00088: loss improved from 3.49288 to 3.47447, saving model to final.h5\n","Epoch 89/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5684 - val_loss: 1.2500\n","\n","Epoch 00089: loss did not improve from 3.47447\n","Epoch 90/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4609 - val_loss: 2.4177\n","\n","Epoch 00090: loss improved from 3.47447 to 3.46090, saving model to final.h5\n","Epoch 91/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4848 - val_loss: 2.0238\n","\n","Epoch 00091: loss did not improve from 3.46090\n","Epoch 92/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4847 - val_loss: 2.1032\n","\n","Epoch 00092: loss did not improve from 3.46090\n","Epoch 93/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4965 - val_loss: 7.8350\n","\n","Epoch 00093: loss did not improve from 3.46090\n","Epoch 94/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4615 - val_loss: 14.5255\n","\n","Epoch 00094: loss did not improve from 3.46090\n","Epoch 95/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3890 - val_loss: 2.2063\n","\n","Epoch 00095: loss improved from 3.46090 to 3.38897, saving model to final.h5\n","Epoch 96/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4444 - val_loss: 0.4927\n","\n","Epoch 00096: loss did not improve from 3.38897\n","Epoch 97/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4650 - val_loss: 7.3020\n","\n","Epoch 00097: loss did not improve from 3.38897\n","Epoch 98/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4221 - val_loss: 2.1142\n","\n","Epoch 00098: loss did not improve from 3.38897\n","Epoch 99/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3937 - val_loss: 2.3345\n","\n","Epoch 00099: loss did not improve from 3.38897\n","Epoch 100/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4846 - val_loss: 3.4867\n","\n","Epoch 00100: loss did not improve from 3.38897\n","Epoch 101/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4902 - val_loss: 4.0181\n","\n","Epoch 00101: loss did not improve from 3.38897\n","Epoch 102/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4234 - val_loss: 2.0285\n","\n","Epoch 00102: loss did not improve from 3.38897\n","Epoch 103/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4046 - val_loss: 2.1487\n","\n","Epoch 00103: loss did not improve from 3.38897\n","Epoch 104/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.4167 - val_loss: 1.0321\n","\n","Epoch 00104: loss did not improve from 3.38897\n","Epoch 105/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3453 - val_loss: 5.5741\n","\n","Epoch 00105: loss improved from 3.38897 to 3.34514, saving model to final.h5\n","Epoch 106/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3584 - val_loss: 2.0563\n","\n","Epoch 00106: loss did not improve from 3.34514\n","Epoch 107/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3590 - val_loss: 1.9974\n","\n","Epoch 00107: loss did not improve from 3.34514\n","Epoch 108/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3616 - val_loss: 3.2865\n","\n","Epoch 00108: loss did not improve from 3.34514\n","Epoch 109/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3604 - val_loss: 3.5151\n","\n","Epoch 00109: loss did not improve from 3.34514\n","Epoch 110/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3354 - val_loss: 16.0952\n","\n","Epoch 00110: loss improved from 3.34514 to 3.33548, saving model to final.h5\n","Epoch 111/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3552 - val_loss: 3.0655\n","\n","Epoch 00111: loss did not improve from 3.33548\n","Epoch 112/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.2456 - val_loss: 2.6373\n","\n","Epoch 00112: loss improved from 3.33548 to 3.24555, saving model to final.h5\n","Epoch 113/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3312 - val_loss: 5.3409\n","\n","Epoch 00113: loss did not improve from 3.24555\n","Epoch 114/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.2498 - val_loss: 5.1370\n","\n","Epoch 00114: loss did not improve from 3.24555\n","Epoch 115/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3158 - val_loss: 6.4574\n","\n","Epoch 00115: loss did not improve from 3.24555\n","Epoch 116/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.2644 - val_loss: 1.2274\n","\n","Epoch 00116: loss did not improve from 3.24555\n","Epoch 117/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.2943 - val_loss: 2.8937\n","\n","Epoch 00117: loss did not improve from 3.24555\n","Epoch 118/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3258 - val_loss: 3.1046\n","\n","Epoch 00118: loss did not improve from 3.24555\n","Epoch 119/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3007 - val_loss: 2.5540\n","\n","Epoch 00119: loss did not improve from 3.24555\n","Epoch 120/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.2412 - val_loss: 3.3460\n","\n","Epoch 00120: loss improved from 3.24555 to 3.24132, saving model to final.h5\n","Epoch 121/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.2834 - val_loss: 4.2052\n","\n","Epoch 00121: loss did not improve from 3.24132\n","Epoch 122/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.2635 - val_loss: 1.8561\n","\n","Epoch 00122: loss did not improve from 3.24132\n","Epoch 123/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.3411 - val_loss: 2.0290\n","\n","Epoch 00123: loss did not improve from 3.24132\n","Epoch 124/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1180 - val_loss: 9.7826\n","\n","Epoch 00124: loss improved from 3.24132 to 3.11779, saving model to final.h5\n","Epoch 125/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.2339 - val_loss: 4.8194\n","\n","Epoch 00125: loss did not improve from 3.11779\n","Epoch 126/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.2586 - val_loss: 2.0845\n","\n","Epoch 00126: loss did not improve from 3.11779\n","Epoch 127/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.2229 - val_loss: 3.5699\n","\n","Epoch 00127: loss did not improve from 3.11779\n","Epoch 128/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1679 - val_loss: 4.8915\n","\n","Epoch 00128: loss did not improve from 3.11779\n","Epoch 129/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1879 - val_loss: 3.4722\n","\n","Epoch 00129: loss did not improve from 3.11779\n","Epoch 130/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1782 - val_loss: 4.2073\n","\n","Epoch 00130: loss did not improve from 3.11779\n","Epoch 131/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.2033 - val_loss: 8.4191\n","\n","Epoch 00131: loss did not improve from 3.11779\n","Epoch 132/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1684 - val_loss: 1.5192\n","\n","Epoch 00132: loss did not improve from 3.11779\n","Epoch 133/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1458 - val_loss: 5.2502\n","\n","Epoch 00133: loss did not improve from 3.11779\n","Epoch 134/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1962 - val_loss: 2.2032\n","\n","Epoch 00134: loss did not improve from 3.11779\n","Epoch 135/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1798 - val_loss: 1.2712\n","\n","Epoch 00135: loss did not improve from 3.11779\n","Epoch 136/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.2020 - val_loss: 4.6442\n","\n","Epoch 00136: loss did not improve from 3.11779\n","Epoch 137/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1063 - val_loss: 3.2541\n","\n","Epoch 00137: loss improved from 3.11779 to 3.10644, saving model to final.h5\n","Epoch 138/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1420 - val_loss: 10.7472\n","\n","Epoch 00138: loss did not improve from 3.10644\n","Epoch 139/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1100 - val_loss: 1.3933\n","\n","Epoch 00139: loss did not improve from 3.10644\n","Epoch 140/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1139 - val_loss: 2.2341\n","\n","Epoch 00140: loss did not improve from 3.10644\n","Epoch 141/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0865 - val_loss: 2.8244\n","\n","Epoch 00141: loss improved from 3.10644 to 3.08643, saving model to final.h5\n","Epoch 142/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1156 - val_loss: 1.7103\n","\n","Epoch 00142: loss did not improve from 3.08643\n","Epoch 143/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1182 - val_loss: 1.4182\n","\n","Epoch 00143: loss did not improve from 3.08643\n","Epoch 144/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0889 - val_loss: 9.6202\n","\n","Epoch 00144: loss did not improve from 3.08643\n","Epoch 145/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0739 - val_loss: 1.6545\n","\n","Epoch 00145: loss improved from 3.08643 to 3.07407, saving model to final.h5\n","Epoch 146/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0674 - val_loss: 5.2487\n","\n","Epoch 00146: loss improved from 3.07407 to 3.06745, saving model to final.h5\n","Epoch 147/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1251 - val_loss: 5.5371\n","\n","Epoch 00147: loss did not improve from 3.06745\n","Epoch 148/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0421 - val_loss: 1.5359\n","\n","Epoch 00148: loss improved from 3.06745 to 3.04208, saving model to final.h5\n","Epoch 149/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0884 - val_loss: 4.4319\n","\n","Epoch 00149: loss did not improve from 3.04208\n","Epoch 150/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0702 - val_loss: 2.9533\n","\n","Epoch 00150: loss did not improve from 3.04208\n","Epoch 151/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1085 - val_loss: 10.9176\n","\n","Epoch 00151: loss did not improve from 3.04208\n","Epoch 152/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0226 - val_loss: 10.9612\n","\n","Epoch 00152: loss improved from 3.04208 to 3.02249, saving model to final.h5\n","Epoch 153/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0562 - val_loss: 1.4639\n","\n","Epoch 00153: loss did not improve from 3.02249\n","Epoch 154/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0367 - val_loss: 1.2685\n","\n","Epoch 00154: loss did not improve from 3.02249\n","Epoch 155/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0566 - val_loss: 2.7092\n","\n","Epoch 00155: loss did not improve from 3.02249\n","Epoch 156/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9909 - val_loss: 2.3914\n","\n","Epoch 00156: loss improved from 3.02249 to 2.99086, saving model to final.h5\n","Epoch 157/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9916 - val_loss: 2.3540\n","\n","Epoch 00157: loss did not improve from 2.99086\n","Epoch 158/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0348 - val_loss: 3.7822\n","\n","Epoch 00158: loss did not improve from 2.99086\n","Epoch 159/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.1152 - val_loss: 1.9667\n","\n","Epoch 00159: loss did not improve from 2.99086\n","Epoch 160/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9338 - val_loss: 2.0466\n","\n","Epoch 00160: loss improved from 2.99086 to 2.93381, saving model to final.h5\n","Epoch 161/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0142 - val_loss: 5.7919\n","\n","Epoch 00161: loss did not improve from 2.93381\n","Epoch 162/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9658 - val_loss: 2.1720\n","\n","Epoch 00162: loss did not improve from 2.93381\n","Epoch 163/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0041 - val_loss: 2.1435\n","\n","Epoch 00163: loss did not improve from 2.93381\n","Epoch 164/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9717 - val_loss: 13.9939\n","\n","Epoch 00164: loss did not improve from 2.93381\n","Epoch 165/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9555 - val_loss: 3.2537\n","\n","Epoch 00165: loss did not improve from 2.93381\n","Epoch 166/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9595 - val_loss: 3.4448\n","\n","Epoch 00166: loss did not improve from 2.93381\n","Epoch 167/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.0207 - val_loss: 2.3669\n","\n","Epoch 00167: loss did not improve from 2.93381\n","Epoch 168/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9338 - val_loss: 8.0840\n","\n","Epoch 00168: loss did not improve from 2.93381\n","Epoch 169/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9567 - val_loss: 1.8914\n","\n","Epoch 00169: loss did not improve from 2.93381\n","Epoch 170/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9441 - val_loss: 0.7495\n","\n","Epoch 00170: loss did not improve from 2.93381\n","Epoch 171/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9063 - val_loss: 1.2615\n","\n","Epoch 00171: loss improved from 2.93381 to 2.90639, saving model to final.h5\n","Epoch 172/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9510 - val_loss: 7.8703\n","\n","Epoch 00172: loss did not improve from 2.90639\n","Epoch 173/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9208 - val_loss: 3.5137\n","\n","Epoch 00173: loss did not improve from 2.90639\n","Epoch 174/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8130 - val_loss: 2.4185\n","\n","Epoch 00174: loss improved from 2.90639 to 2.81287, saving model to final.h5\n","Epoch 175/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9982 - val_loss: 1.5435\n","\n","Epoch 00175: loss did not improve from 2.81287\n","Epoch 176/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9418 - val_loss: 1.0077\n","\n","Epoch 00176: loss did not improve from 2.81287\n","Epoch 177/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9323 - val_loss: 1.7999\n","\n","Epoch 00177: loss did not improve from 2.81287\n","Epoch 178/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9993 - val_loss: 1.5730\n","\n","Epoch 00178: loss did not improve from 2.81287\n","Epoch 179/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9078 - val_loss: 3.7117\n","\n","Epoch 00179: loss did not improve from 2.81287\n","Epoch 180/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8593 - val_loss: 2.5892\n","\n","Epoch 00180: loss did not improve from 2.81287\n","Epoch 181/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9083 - val_loss: 0.8920\n","\n","Epoch 00181: loss did not improve from 2.81287\n","Epoch 182/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8810 - val_loss: 2.5499\n","\n","Epoch 00182: loss did not improve from 2.81287\n","Epoch 183/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9741 - val_loss: 6.6464\n","\n","Epoch 00183: loss did not improve from 2.81287\n","Epoch 184/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8991 - val_loss: 1.8654\n","\n","Epoch 00184: loss did not improve from 2.81287\n","Epoch 185/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8891 - val_loss: 3.1368\n","\n","Epoch 00185: loss did not improve from 2.81287\n","Epoch 186/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8186 - val_loss: 2.0505\n","\n","Epoch 00186: loss did not improve from 2.81287\n","Epoch 187/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8698 - val_loss: 2.2381\n","\n","Epoch 00187: loss did not improve from 2.81287\n","Epoch 188/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8483 - val_loss: 4.5088\n","\n","Epoch 00188: loss did not improve from 2.81287\n","Epoch 189/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.9401 - val_loss: 0.6565\n","\n","Epoch 00189: loss did not improve from 2.81287\n","Epoch 190/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8430 - val_loss: 13.8887\n","\n","Epoch 00190: loss did not improve from 2.81287\n","Epoch 191/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8278 - val_loss: 1.4586\n","\n","Epoch 00191: loss did not improve from 2.81287\n","Epoch 192/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8434 - val_loss: 1.2496\n","\n","Epoch 00192: loss did not improve from 2.81287\n","Epoch 193/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7500 - val_loss: 2.1081\n","\n","Epoch 00193: loss improved from 2.81287 to 2.74986, saving model to final.h5\n","Epoch 194/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7929 - val_loss: 1.8577\n","\n","Epoch 00194: loss did not improve from 2.74986\n","Epoch 195/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8448 - val_loss: 1.8128\n","\n","Epoch 00195: loss did not improve from 2.74986\n","Epoch 196/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7825 - val_loss: 5.4257\n","\n","Epoch 00196: loss did not improve from 2.74986\n","Epoch 197/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8305 - val_loss: 4.3481\n","\n","Epoch 00197: loss did not improve from 2.74986\n","Epoch 198/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7792 - val_loss: 3.5589\n","\n","Epoch 00198: loss did not improve from 2.74986\n","Epoch 199/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8272 - val_loss: 2.1296\n","\n","Epoch 00199: loss did not improve from 2.74986\n","Epoch 200/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7863 - val_loss: 1.8207\n","\n","Epoch 00200: loss did not improve from 2.74986\n","Epoch 201/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8064 - val_loss: 1.9718\n","\n","Epoch 00201: loss did not improve from 2.74986\n","Epoch 202/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7937 - val_loss: 2.5174\n","\n","Epoch 00202: loss did not improve from 2.74986\n","Epoch 203/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8094 - val_loss: 5.4752\n","\n","Epoch 00203: loss did not improve from 2.74986\n","Epoch 204/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8218 - val_loss: 1.5505\n","\n","Epoch 00204: loss did not improve from 2.74986\n","Epoch 205/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6766 - val_loss: 1.2324\n","\n","Epoch 00205: loss improved from 2.74986 to 2.67661, saving model to final.h5\n","Epoch 206/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7713 - val_loss: 6.7218\n","\n","Epoch 00206: loss did not improve from 2.67661\n","Epoch 207/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7653 - val_loss: 2.4776\n","\n","Epoch 00207: loss did not improve from 2.67661\n","Epoch 208/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7578 - val_loss: 1.2649\n","\n","Epoch 00208: loss did not improve from 2.67661\n","Epoch 209/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7843 - val_loss: 8.4016\n","\n","Epoch 00209: loss did not improve from 2.67661\n","Epoch 210/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7486 - val_loss: 3.3335\n","\n","Epoch 00210: loss did not improve from 2.67661\n","Epoch 211/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8577 - val_loss: 3.2648\n","\n","Epoch 00211: loss did not improve from 2.67661\n","Epoch 212/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7987 - val_loss: 2.9466\n","\n","Epoch 00212: loss did not improve from 2.67661\n","Epoch 213/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7700 - val_loss: 2.6775\n","\n","Epoch 00213: loss did not improve from 2.67661\n","Epoch 214/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7108 - val_loss: 1.9990\n","\n","Epoch 00214: loss did not improve from 2.67661\n","Epoch 215/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8277 - val_loss: 1.1506\n","\n","Epoch 00215: loss did not improve from 2.67661\n","Epoch 216/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.8216 - val_loss: 1.7463\n","\n","Epoch 00216: loss did not improve from 2.67661\n","Epoch 217/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6667 - val_loss: 2.1235\n","\n","Epoch 00217: loss improved from 2.67661 to 2.66661, saving model to final.h5\n","Epoch 218/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7394 - val_loss: 4.0629\n","\n","Epoch 00218: loss did not improve from 2.66661\n","Epoch 219/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7236 - val_loss: 2.5766\n","\n","Epoch 00219: loss did not improve from 2.66661\n","Epoch 220/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7501 - val_loss: 1.8980\n","\n","Epoch 00220: loss did not improve from 2.66661\n","Epoch 221/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6874 - val_loss: 1.3074\n","\n","Epoch 00221: loss did not improve from 2.66661\n","Epoch 222/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7254 - val_loss: 4.2311\n","\n","Epoch 00222: loss did not improve from 2.66661\n","Epoch 223/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6953 - val_loss: 2.5572\n","\n","Epoch 00223: loss did not improve from 2.66661\n","Epoch 224/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7983 - val_loss: 2.7056\n","\n","Epoch 00224: loss did not improve from 2.66661\n","Epoch 225/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7163 - val_loss: 2.7487\n","\n","Epoch 00225: loss did not improve from 2.66661\n","Epoch 226/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7392 - val_loss: 1.6414\n","\n","Epoch 00226: loss did not improve from 2.66661\n","Epoch 227/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7052 - val_loss: 8.9486\n","\n","Epoch 00227: loss did not improve from 2.66661\n","Epoch 228/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7433 - val_loss: 1.2214\n","\n","Epoch 00228: loss did not improve from 2.66661\n","Epoch 229/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7783 - val_loss: 2.3424\n","\n","Epoch 00229: loss did not improve from 2.66661\n","Epoch 230/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6890 - val_loss: 5.6140\n","\n","Epoch 00230: loss did not improve from 2.66661\n","Epoch 231/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7339 - val_loss: 4.4628\n","\n","Epoch 00231: loss did not improve from 2.66661\n","Epoch 232/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7722 - val_loss: 1.7231\n","\n","Epoch 00232: loss did not improve from 2.66661\n","Epoch 233/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7578 - val_loss: 6.7027\n","\n","Epoch 00233: loss did not improve from 2.66661\n","Epoch 234/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6518 - val_loss: 1.8795\n","\n","Epoch 00234: loss improved from 2.66661 to 2.65187, saving model to final.h5\n","Epoch 235/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6846 - val_loss: 1.8483\n","\n","Epoch 00235: loss did not improve from 2.65187\n","Epoch 236/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6784 - val_loss: 9.9441\n","\n","Epoch 00236: loss did not improve from 2.65187\n","Epoch 237/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6937 - val_loss: 7.3478\n","\n","Epoch 00237: loss did not improve from 2.65187\n","Epoch 238/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6934 - val_loss: 1.9004\n","\n","Epoch 00238: loss did not improve from 2.65187\n","Epoch 239/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6413 - val_loss: 2.5309\n","\n","Epoch 00239: loss improved from 2.65187 to 2.64126, saving model to final.h5\n","Epoch 240/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6251 - val_loss: 0.9569\n","\n","Epoch 00240: loss improved from 2.64126 to 2.62502, saving model to final.h5\n","Epoch 241/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7038 - val_loss: 1.7570\n","\n","Epoch 00241: loss did not improve from 2.62502\n","Epoch 242/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6868 - val_loss: 2.6457\n","\n","Epoch 00242: loss did not improve from 2.62502\n","Epoch 243/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7204 - val_loss: 8.5437\n","\n","Epoch 00243: loss did not improve from 2.62502\n","Epoch 244/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6953 - val_loss: 2.9304\n","\n","Epoch 00244: loss did not improve from 2.62502\n","Epoch 245/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6204 - val_loss: 3.1942\n","\n","Epoch 00245: loss improved from 2.62502 to 2.62045, saving model to final.h5\n","Epoch 246/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6553 - val_loss: 2.2977\n","\n","Epoch 00246: loss did not improve from 2.62045\n","Epoch 247/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6179 - val_loss: 1.2433\n","\n","Epoch 00247: loss improved from 2.62045 to 2.61799, saving model to final.h5\n","Epoch 248/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6375 - val_loss: 3.1210\n","\n","Epoch 00248: loss did not improve from 2.61799\n","Epoch 249/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6601 - val_loss: 12.7889\n","\n","Epoch 00249: loss did not improve from 2.61799\n","Epoch 250/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.7310 - val_loss: 1.1007\n","\n","Epoch 00250: loss did not improve from 2.61799\n","Epoch 251/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6310 - val_loss: 1.4561\n","\n","Epoch 00251: loss did not improve from 2.61799\n","Epoch 252/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6251 - val_loss: 3.3540\n","\n","Epoch 00252: loss did not improve from 2.61799\n","Epoch 253/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5954 - val_loss: 1.6045\n","\n","Epoch 00253: loss improved from 2.61799 to 2.59531, saving model to final.h5\n","Epoch 254/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6646 - val_loss: 3.4001\n","\n","Epoch 00254: loss did not improve from 2.59531\n","Epoch 255/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5970 - val_loss: 3.3792\n","\n","Epoch 00255: loss did not improve from 2.59531\n","Epoch 256/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5936 - val_loss: 1.2173\n","\n","Epoch 00256: loss improved from 2.59531 to 2.59378, saving model to final.h5\n","Epoch 257/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6096 - val_loss: 2.6444\n","\n","Epoch 00257: loss did not improve from 2.59378\n","Epoch 258/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6292 - val_loss: 2.4103\n","\n","Epoch 00258: loss did not improve from 2.59378\n","Epoch 259/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6489 - val_loss: 1.8743\n","\n","Epoch 00259: loss did not improve from 2.59378\n","Epoch 260/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6426 - val_loss: 1.1986\n","\n","Epoch 00260: loss did not improve from 2.59378\n","Epoch 261/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6216 - val_loss: 2.3962\n","\n","Epoch 00261: loss did not improve from 2.59378\n","Epoch 262/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6795 - val_loss: 6.5964\n","\n","Epoch 00262: loss did not improve from 2.59378\n","Epoch 263/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6178 - val_loss: 2.1186\n","\n","Epoch 00263: loss did not improve from 2.59378\n","Epoch 264/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6668 - val_loss: 1.9770\n","\n","Epoch 00264: loss did not improve from 2.59378\n","Epoch 265/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6239 - val_loss: 3.7679\n","\n","Epoch 00265: loss did not improve from 2.59378\n","Epoch 266/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6954 - val_loss: 3.5638\n","\n","Epoch 00266: loss did not improve from 2.59378\n","Epoch 267/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6264 - val_loss: 0.7692\n","\n","Epoch 00267: loss did not improve from 2.59378\n","Epoch 268/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5961 - val_loss: 2.1060\n","\n","Epoch 00268: loss did not improve from 2.59378\n","Epoch 269/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5600 - val_loss: 3.3757\n","\n","Epoch 00269: loss improved from 2.59378 to 2.56005, saving model to final.h5\n","Epoch 270/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6412 - val_loss: 3.7443\n","\n","Epoch 00270: loss did not improve from 2.56005\n","Epoch 271/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6733 - val_loss: 2.3520\n","\n","Epoch 00271: loss did not improve from 2.56005\n","Epoch 272/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6387 - val_loss: 1.5909\n","\n","Epoch 00272: loss did not improve from 2.56005\n","Epoch 273/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6717 - val_loss: 1.3155\n","\n","Epoch 00273: loss did not improve from 2.56005\n","Epoch 274/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5835 - val_loss: 2.0645\n","\n","Epoch 00274: loss did not improve from 2.56005\n","Epoch 275/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5244 - val_loss: 4.0876\n","\n","Epoch 00275: loss improved from 2.56005 to 2.52429, saving model to final.h5\n","Epoch 276/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5653 - val_loss: 7.8188\n","\n","Epoch 00276: loss did not improve from 2.52429\n","Epoch 277/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5916 - val_loss: 2.9085\n","\n","Epoch 00277: loss did not improve from 2.52429\n","Epoch 278/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6091 - val_loss: 1.8120\n","\n","Epoch 00278: loss did not improve from 2.52429\n","Epoch 279/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5673 - val_loss: 5.6092\n","\n","Epoch 00279: loss did not improve from 2.52429\n","Epoch 280/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6338 - val_loss: 4.8147\n","\n","Epoch 00280: loss did not improve from 2.52429\n","Epoch 281/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5600 - val_loss: 3.8101\n","\n","Epoch 00281: loss did not improve from 2.52429\n","Epoch 282/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5590 - val_loss: 3.8877\n","\n","Epoch 00282: loss did not improve from 2.52429\n","Epoch 283/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5363 - val_loss: 4.5919\n","\n","Epoch 00283: loss did not improve from 2.52429\n","Epoch 284/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5711 - val_loss: 1.0497\n","\n","Epoch 00284: loss did not improve from 2.52429\n","Epoch 285/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5003 - val_loss: 2.2187\n","\n","Epoch 00285: loss improved from 2.52429 to 2.50031, saving model to final.h5\n","Epoch 286/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5431 - val_loss: 2.6136\n","\n","Epoch 00286: loss did not improve from 2.50031\n","Epoch 287/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4865 - val_loss: 2.2550\n","\n","Epoch 00287: loss improved from 2.50031 to 2.48651, saving model to final.h5\n","Epoch 288/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5937 - val_loss: 2.4326\n","\n","Epoch 00288: loss did not improve from 2.48651\n","Epoch 289/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5537 - val_loss: 1.6360\n","\n","Epoch 00289: loss did not improve from 2.48651\n","Epoch 290/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.6060 - val_loss: 5.7224\n","\n","Epoch 00290: loss did not improve from 2.48651\n","Epoch 291/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5405 - val_loss: 2.8717\n","\n","Epoch 00291: loss did not improve from 2.48651\n","Epoch 292/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5380 - val_loss: 1.0852\n","\n","Epoch 00292: loss did not improve from 2.48651\n","Epoch 293/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5449 - val_loss: 2.1926\n","\n","Epoch 00293: loss did not improve from 2.48651\n","Epoch 294/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5748 - val_loss: 1.7092\n","\n","Epoch 00294: loss did not improve from 2.48651\n","Epoch 295/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5538 - val_loss: 1.9233\n","\n","Epoch 00295: loss did not improve from 2.48651\n","Epoch 296/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5175 - val_loss: 6.1873\n","\n","Epoch 00296: loss did not improve from 2.48651\n","Epoch 297/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5650 - val_loss: 5.2265\n","\n","Epoch 00297: loss did not improve from 2.48651\n","Epoch 298/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5114 - val_loss: 3.2952\n","\n","Epoch 00298: loss did not improve from 2.48651\n","Epoch 299/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5898 - val_loss: 1.9973\n","\n","Epoch 00299: loss did not improve from 2.48651\n","Epoch 300/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5120 - val_loss: 1.7948\n","\n","Epoch 00300: loss did not improve from 2.48651\n","Epoch 301/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5431 - val_loss: 4.6214\n","\n","Epoch 00301: loss did not improve from 2.48651\n","Epoch 302/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5957 - val_loss: 1.4679\n","\n","Epoch 00302: loss did not improve from 2.48651\n","Epoch 303/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4904 - val_loss: 3.2888\n","\n","Epoch 00303: loss did not improve from 2.48651\n","Epoch 304/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4990 - val_loss: 1.4794\n","\n","Epoch 00304: loss did not improve from 2.48651\n","Epoch 305/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4671 - val_loss: 4.7443\n","\n","Epoch 00305: loss improved from 2.48651 to 2.46711, saving model to final.h5\n","Epoch 306/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5090 - val_loss: 1.9718\n","\n","Epoch 00306: loss did not improve from 2.46711\n","Epoch 307/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4585 - val_loss: 2.7198\n","\n","Epoch 00307: loss improved from 2.46711 to 2.45846, saving model to final.h5\n","Epoch 308/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4706 - val_loss: 0.7003\n","\n","Epoch 00308: loss did not improve from 2.45846\n","Epoch 309/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4993 - val_loss: 3.3145\n","\n","Epoch 00309: loss did not improve from 2.45846\n","Epoch 310/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5278 - val_loss: 2.3440\n","\n","Epoch 00310: loss did not improve from 2.45846\n","Epoch 311/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5277 - val_loss: 2.8461\n","\n","Epoch 00311: loss did not improve from 2.45846\n","Epoch 312/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5803 - val_loss: 1.7012\n","\n","Epoch 00312: loss did not improve from 2.45846\n","Epoch 313/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4505 - val_loss: 1.5221\n","\n","Epoch 00313: loss improved from 2.45846 to 2.45062, saving model to final.h5\n","Epoch 314/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4949 - val_loss: 0.8770\n","\n","Epoch 00314: loss did not improve from 2.45062\n","Epoch 315/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4892 - val_loss: 1.7269\n","\n","Epoch 00315: loss did not improve from 2.45062\n","Epoch 316/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4318 - val_loss: 2.3667\n","\n","Epoch 00316: loss improved from 2.45062 to 2.43188, saving model to final.h5\n","Epoch 317/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5012 - val_loss: 1.5643\n","\n","Epoch 00317: loss did not improve from 2.43188\n","Epoch 318/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5316 - val_loss: 2.8009\n","\n","Epoch 00318: loss did not improve from 2.43188\n","Epoch 319/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4972 - val_loss: 1.1970\n","\n","Epoch 00319: loss did not improve from 2.43188\n","Epoch 320/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5140 - val_loss: 1.6072\n","\n","Epoch 00320: loss did not improve from 2.43188\n","Epoch 321/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5129 - val_loss: 1.9147\n","\n","Epoch 00321: loss did not improve from 2.43188\n","Epoch 322/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5430 - val_loss: 2.3134\n","\n","Epoch 00322: loss did not improve from 2.43188\n","Epoch 323/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4354 - val_loss: 3.6402\n","\n","Epoch 00323: loss did not improve from 2.43188\n","Epoch 324/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4823 - val_loss: 2.1482\n","\n","Epoch 00324: loss did not improve from 2.43188\n","Epoch 325/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4604 - val_loss: 2.5422\n","\n","Epoch 00325: loss did not improve from 2.43188\n","Epoch 326/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4435 - val_loss: 2.3606\n","\n","Epoch 00326: loss did not improve from 2.43188\n","Epoch 327/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4985 - val_loss: 2.6713\n","\n","Epoch 00327: loss did not improve from 2.43188\n","Epoch 328/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4602 - val_loss: 1.0540\n","\n","Epoch 00328: loss did not improve from 2.43188\n","Epoch 329/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4350 - val_loss: 1.0387\n","\n","Epoch 00329: loss did not improve from 2.43188\n","Epoch 330/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4646 - val_loss: 1.5610\n","\n","Epoch 00330: loss did not improve from 2.43188\n","Epoch 331/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5170 - val_loss: 0.9138\n","\n","Epoch 00331: loss did not improve from 2.43188\n","Epoch 332/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4532 - val_loss: 2.5070\n","\n","Epoch 00332: loss did not improve from 2.43188\n","Epoch 333/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3618 - val_loss: 1.6552\n","\n","Epoch 00333: loss improved from 2.43188 to 2.36184, saving model to final.h5\n","Epoch 334/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4388 - val_loss: 2.1381\n","\n","Epoch 00334: loss did not improve from 2.36184\n","Epoch 335/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5148 - val_loss: 1.3547\n","\n","Epoch 00335: loss did not improve from 2.36184\n","Epoch 336/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4635 - val_loss: 4.6769\n","\n","Epoch 00336: loss did not improve from 2.36184\n","Epoch 337/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4475 - val_loss: 0.8581\n","\n","Epoch 00337: loss did not improve from 2.36184\n","Epoch 338/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4211 - val_loss: 2.4138\n","\n","Epoch 00338: loss did not improve from 2.36184\n","Epoch 339/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4332 - val_loss: 0.9636\n","\n","Epoch 00339: loss did not improve from 2.36184\n","Epoch 340/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4930 - val_loss: 2.5167\n","\n","Epoch 00340: loss did not improve from 2.36184\n","Epoch 341/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3731 - val_loss: 1.5338\n","\n","Epoch 00341: loss did not improve from 2.36184\n","Epoch 342/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3887 - val_loss: 1.7040\n","\n","Epoch 00342: loss did not improve from 2.36184\n","Epoch 343/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.5030 - val_loss: 3.5127\n","\n","Epoch 00343: loss did not improve from 2.36184\n","Epoch 344/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4425 - val_loss: 2.1744\n","\n","Epoch 00344: loss did not improve from 2.36184\n","Epoch 345/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4491 - val_loss: 3.8350\n","\n","Epoch 00345: loss did not improve from 2.36184\n","Epoch 346/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4740 - val_loss: 4.0629\n","\n","Epoch 00346: loss did not improve from 2.36184\n","Epoch 347/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3820 - val_loss: 3.6107\n","\n","Epoch 00347: loss did not improve from 2.36184\n","Epoch 348/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4259 - val_loss: 2.5092\n","\n","Epoch 00348: loss did not improve from 2.36184\n","Epoch 349/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4952 - val_loss: 1.8315\n","\n","Epoch 00349: loss did not improve from 2.36184\n","Epoch 350/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4088 - val_loss: 6.1322\n","\n","Epoch 00350: loss did not improve from 2.36184\n","Epoch 351/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4702 - val_loss: 2.4969\n","\n","Epoch 00351: loss did not improve from 2.36184\n","Epoch 352/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4038 - val_loss: 1.1361\n","\n","Epoch 00352: loss did not improve from 2.36184\n","Epoch 353/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4553 - val_loss: 8.1478\n","\n","Epoch 00353: loss did not improve from 2.36184\n","Epoch 354/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4457 - val_loss: 3.7094\n","\n","Epoch 00354: loss did not improve from 2.36184\n","Epoch 355/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4534 - val_loss: 1.4768\n","\n","Epoch 00355: loss did not improve from 2.36184\n","Epoch 356/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4484 - val_loss: 2.3953\n","\n","Epoch 00356: loss did not improve from 2.36184\n","Epoch 357/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4379 - val_loss: 4.4739\n","\n","Epoch 00357: loss did not improve from 2.36184\n","Epoch 358/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4110 - val_loss: 3.7499\n","\n","Epoch 00358: loss did not improve from 2.36184\n","Epoch 359/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4061 - val_loss: 4.7833\n","\n","Epoch 00359: loss did not improve from 2.36184\n","Epoch 360/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3917 - val_loss: 4.3828\n","\n","Epoch 00360: loss did not improve from 2.36184\n","Epoch 361/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4687 - val_loss: 0.5495\n","\n","Epoch 00361: loss did not improve from 2.36184\n","Epoch 362/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4201 - val_loss: 2.7128\n","\n","Epoch 00362: loss did not improve from 2.36184\n","Epoch 363/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4024 - val_loss: 2.5099\n","\n","Epoch 00363: loss did not improve from 2.36184\n","Epoch 364/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4493 - val_loss: 1.7723\n","\n","Epoch 00364: loss did not improve from 2.36184\n","Epoch 365/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4631 - val_loss: 10.2249\n","\n","Epoch 00365: loss did not improve from 2.36184\n","Epoch 366/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4165 - val_loss: 3.1898\n","\n","Epoch 00366: loss did not improve from 2.36184\n","Epoch 367/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4127 - val_loss: 3.4530\n","\n","Epoch 00367: loss did not improve from 2.36184\n","Epoch 368/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4374 - val_loss: 1.1710\n","\n","Epoch 00368: loss did not improve from 2.36184\n","Epoch 369/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4354 - val_loss: 3.1820\n","\n","Epoch 00369: loss did not improve from 2.36184\n","Epoch 370/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3791 - val_loss: 1.1459\n","\n","Epoch 00370: loss did not improve from 2.36184\n","Epoch 371/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4111 - val_loss: 1.5623\n","\n","Epoch 00371: loss did not improve from 2.36184\n","Epoch 372/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3887 - val_loss: 2.3013\n","\n","Epoch 00372: loss did not improve from 2.36184\n","Epoch 373/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3844 - val_loss: 2.4133\n","\n","Epoch 00373: loss did not improve from 2.36184\n","Epoch 374/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3723 - val_loss: 1.9861\n","\n","Epoch 00374: loss did not improve from 2.36184\n","Epoch 375/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3415 - val_loss: 3.9443\n","\n","Epoch 00375: loss improved from 2.36184 to 2.34152, saving model to final.h5\n","Epoch 376/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3794 - val_loss: 2.7251\n","\n","Epoch 00376: loss did not improve from 2.34152\n","Epoch 377/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4476 - val_loss: 3.0515\n","\n","Epoch 00377: loss did not improve from 2.34152\n","Epoch 378/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3653 - val_loss: 1.1081\n","\n","Epoch 00378: loss did not improve from 2.34152\n","Epoch 379/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3595 - val_loss: 1.4743\n","\n","Epoch 00379: loss did not improve from 2.34152\n","Epoch 380/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3829 - val_loss: 1.3246\n","\n","Epoch 00380: loss did not improve from 2.34152\n","Epoch 381/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.4184 - val_loss: 3.4395\n","\n","Epoch 00381: loss did not improve from 2.34152\n","Epoch 382/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3627 - val_loss: 1.9782\n","\n","Epoch 00382: loss did not improve from 2.34152\n","Epoch 383/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3761 - val_loss: 1.4440\n","\n","Epoch 00383: loss did not improve from 2.34152\n","Epoch 384/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3315 - val_loss: 4.1728\n","\n","Epoch 00384: loss improved from 2.34152 to 2.33129, saving model to final.h5\n","Epoch 385/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3360 - val_loss: 1.8773\n","\n","Epoch 00385: loss did not improve from 2.33129\n","Epoch 386/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3442 - val_loss: 0.3824\n","\n","Epoch 00386: loss did not improve from 2.33129\n","Epoch 387/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3209 - val_loss: 3.2147\n","\n","Epoch 00387: loss improved from 2.33129 to 2.32069, saving model to final.h5\n","Epoch 388/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3484 - val_loss: 2.9890\n","\n","Epoch 00388: loss did not improve from 2.32069\n","Epoch 389/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3677 - val_loss: 4.2191\n","\n","Epoch 00389: loss did not improve from 2.32069\n","Epoch 390/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3999 - val_loss: 1.8032\n","\n","Epoch 00390: loss did not improve from 2.32069\n","Epoch 391/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3818 - val_loss: 1.6966\n","\n","Epoch 00391: loss did not improve from 2.32069\n","Epoch 392/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3337 - val_loss: 6.1330\n","\n","Epoch 00392: loss did not improve from 2.32069\n","Epoch 393/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3571 - val_loss: 1.7535\n","\n","Epoch 00393: loss did not improve from 2.32069\n","Epoch 394/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3283 - val_loss: 1.9565\n","\n","Epoch 00394: loss did not improve from 2.32069\n","Epoch 395/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3022 - val_loss: 4.0383\n","\n","Epoch 00395: loss improved from 2.32069 to 2.30228, saving model to final.h5\n","Epoch 396/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3244 - val_loss: 2.9676\n","\n","Epoch 00396: loss did not improve from 2.30228\n","Epoch 397/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3520 - val_loss: 2.2672\n","\n","Epoch 00397: loss did not improve from 2.30228\n","Epoch 398/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3713 - val_loss: 3.2861\n","\n","Epoch 00398: loss did not improve from 2.30228\n","Epoch 399/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3388 - val_loss: 3.2675\n","\n","Epoch 00399: loss did not improve from 2.30228\n","Epoch 400/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 2.3132 - val_loss: 1.4480\n","\n","Epoch 00400: loss did not improve from 2.30228\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x62wm18QOdFB","colab_type":"code","outputId":"d69e4c75-9208-4636-8aae-c603d9c4e5b5","executionInfo":{"status":"ok","timestamp":1590288385564,"user_tz":360,"elapsed":3851657,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet_final_with_LSTM_testtobereplaced.py train.mp4 train.txt --mode=train --split=0.2 --model final.h5 --epoch 400 --history 1 --LR 0.00001"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-24 01:41:46.546011: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet_final_with_LSTM_testtobereplaced.py:180: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-24 01:41:48.216497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-24 01:41:48.230063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 01:41:48.230597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 01:41:48.230624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 01:41:48.232341: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 01:41:48.234633: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 01:41:48.234968: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 01:41:48.236655: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 01:41:48.237787: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 01:41:48.241891: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 01:41:48.242014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 01:41:48.242641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 01:41:48.243128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 01:41:48.248209: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-05-24 01:41:48.248444: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e152c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 01:41:48.248473: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-24 01:41:48.337313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 01:41:48.337982: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e15480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 01:41:48.338015: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-24 01:41:48.338188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 01:41:48.338701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 01:41:48.338739: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 01:41:48.338780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 01:41:48.338795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 01:41:48.338809: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 01:41:48.338822: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 01:41:48.338835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 01:41:48.338849: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 01:41:48.338931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 01:41:48.339436: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 01:41:48.339932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 01:41:48.340016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 01:41:48.836788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 01:41:48.836843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-24 01:41:48.836857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-24 01:41:48.837100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 01:41:48.837688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 01:41:48.838213: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-24 01:41:48.838257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet_final_with_LSTM_testtobereplaced.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","speednet_final_with_LSTM_testtobereplaced.py:185: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","Found preprocessed data\n","tcmalloc: large alloc 2447884288 bytes == 0x1e804000 @  0x7ff9592161e7 0x7ff956cfc5e1 0x7ff956d60c78 0x7ff956d60f37 0x7ff956df8f28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7ff958e13b97 0x5b250a\n","tcmalloc: large alloc 2447884288 bytes == 0xb0680000 @  0x7ff9592161e7 0x7ff956cfc5e1 0x7ff956d60c78 0x7ff956d60f37 0x7ff956df8f28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7ff958e13b97 0x5b250a\n","Loading frame 20398\n","done loading 20399 frames\n","Done prepping data\n","tcmalloc: large alloc 1631920128 bytes == 0x142558000 @  0x7ff9592161e7 0x7ff956cfc5e1 0x7ff956d60c78 0x7ff956d60d93 0x7ff956debed6 0x7ff956dec338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7ff958e13b97 0x5b250a\n","tcmalloc: large alloc 1631920128 bytes == 0x1e804000 @  0x7ff9592161e7 0x7ff956cfc5e1 0x7ff956d60c78 0x7ff956d60d93 0x7ff956debed6 0x7ff956dec338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7ff958e13b97 0x5b250a\n","20399\n","20399 Training data size per Aug\n","16319 Train indices size\n","4080 Val indices size\n","Starting training\n","shuffling\n","Epoch 1/400\n","2020-05-24 01:42:00.324198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 01:42:00.510512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","1632/1632 [==============================] - 11s 7ms/step - loss: 118.6744 - val_loss: 108.0773\n","\n","Epoch 00001: loss improved from inf to 118.67718, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 71.8793 - val_loss: 138.3982\n","\n","Epoch 00002: loss improved from 118.67718 to 71.87915, saving model to final.h5\n","Epoch 3/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 70.8938 - val_loss: 143.0492\n","\n","Epoch 00003: loss improved from 71.87915 to 70.89338, saving model to final.h5\n","Epoch 4/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 71.2390 - val_loss: 147.2697\n","\n","Epoch 00004: loss did not improve from 70.89338\n","Epoch 5/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 71.9524 - val_loss: 151.0246\n","\n","Epoch 00005: loss did not improve from 70.89338\n","Epoch 6/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 71.3509 - val_loss: 149.0260\n","\n","Epoch 00006: loss did not improve from 70.89338\n","Epoch 7/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 70.7818 - val_loss: 143.7250\n","\n","Epoch 00007: loss improved from 70.89338 to 70.78268, saving model to final.h5\n","Epoch 8/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 69.7281 - val_loss: 148.1838\n","\n","Epoch 00008: loss improved from 70.78268 to 69.72887, saving model to final.h5\n","Epoch 9/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 68.3475 - val_loss: 144.3478\n","\n","Epoch 00009: loss improved from 69.72887 to 68.34993, saving model to final.h5\n","Epoch 10/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 54.9356 - val_loss: 55.0015\n","\n","Epoch 00010: loss improved from 68.34993 to 54.93412, saving model to final.h5\n","Epoch 11/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 33.1932 - val_loss: 9.9837\n","\n","Epoch 00011: loss improved from 54.93412 to 33.19365, saving model to final.h5\n","Epoch 12/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 24.2168 - val_loss: 9.3753\n","\n","Epoch 00012: loss improved from 33.19365 to 24.21720, saving model to final.h5\n","Epoch 13/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 21.1352 - val_loss: 8.6874\n","\n","Epoch 00013: loss improved from 24.21720 to 21.13619, saving model to final.h5\n","Epoch 14/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 19.2194 - val_loss: 6.0342\n","\n","Epoch 00014: loss improved from 21.13619 to 19.22007, saving model to final.h5\n","Epoch 15/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 18.3438 - val_loss: 9.3232\n","\n","Epoch 00015: loss improved from 19.22007 to 18.34440, saving model to final.h5\n","Epoch 16/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 17.1254 - val_loss: 7.7838\n","\n","Epoch 00016: loss improved from 18.34440 to 17.12635, saving model to final.h5\n","Epoch 17/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 16.3167 - val_loss: 5.1798\n","\n","Epoch 00017: loss improved from 17.12635 to 16.31667, saving model to final.h5\n","Epoch 18/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 15.5268 - val_loss: 5.3808\n","\n","Epoch 00018: loss improved from 16.31667 to 15.52688, saving model to final.h5\n","Epoch 19/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 15.1164 - val_loss: 5.0754\n","\n","Epoch 00019: loss improved from 15.52688 to 15.11333, saving model to final.h5\n","Epoch 20/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 14.6028 - val_loss: 5.3142\n","\n","Epoch 00020: loss improved from 15.11333 to 14.60325, saving model to final.h5\n","Epoch 21/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 14.2090 - val_loss: 4.7810\n","\n","Epoch 00021: loss improved from 14.60325 to 14.20905, saving model to final.h5\n","Epoch 22/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 13.7253 - val_loss: 4.6444\n","\n","Epoch 00022: loss improved from 14.20905 to 13.72585, saving model to final.h5\n","Epoch 23/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 13.3953 - val_loss: 4.2116\n","\n","Epoch 00023: loss improved from 13.72585 to 13.39508, saving model to final.h5\n","Epoch 24/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 12.9785 - val_loss: 4.7241\n","\n","Epoch 00024: loss improved from 13.39508 to 12.97880, saving model to final.h5\n","Epoch 25/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 12.5463 - val_loss: 4.8821\n","\n","Epoch 00025: loss improved from 12.97880 to 12.54582, saving model to final.h5\n","Epoch 26/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 12.2425 - val_loss: 5.2078\n","\n","Epoch 00026: loss improved from 12.54582 to 12.24258, saving model to final.h5\n","Epoch 27/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 12.0027 - val_loss: 5.9983\n","\n","Epoch 00027: loss improved from 12.24258 to 12.00280, saving model to final.h5\n","Epoch 28/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.6603 - val_loss: 4.7803\n","\n","Epoch 00028: loss improved from 12.00280 to 11.66049, saving model to final.h5\n","Epoch 29/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.8901 - val_loss: 3.6472\n","\n","Epoch 00029: loss did not improve from 11.66049\n","Epoch 30/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.4965 - val_loss: 4.5350\n","\n","Epoch 00030: loss improved from 11.66049 to 11.49697, saving model to final.h5\n","Epoch 31/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.0489 - val_loss: 3.3446\n","\n","Epoch 00031: loss improved from 11.49697 to 11.04872, saving model to final.h5\n","Epoch 32/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.1056 - val_loss: 3.2296\n","\n","Epoch 00032: loss did not improve from 11.04872\n","Epoch 33/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.8546 - val_loss: 4.0098\n","\n","Epoch 00033: loss improved from 11.04872 to 10.85493, saving model to final.h5\n","Epoch 34/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.6365 - val_loss: 4.0244\n","\n","Epoch 00034: loss improved from 10.85493 to 10.63688, saving model to final.h5\n","Epoch 35/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.3938 - val_loss: 3.3485\n","\n","Epoch 00035: loss improved from 10.63688 to 10.39416, saving model to final.h5\n","Epoch 36/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.2466 - val_loss: 2.7174\n","\n","Epoch 00036: loss improved from 10.39416 to 10.24628, saving model to final.h5\n","Epoch 37/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.2286 - val_loss: 3.0933\n","\n","Epoch 00037: loss improved from 10.24628 to 10.22882, saving model to final.h5\n","Epoch 38/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.1710 - val_loss: 1.9431\n","\n","Epoch 00038: loss improved from 10.22882 to 10.17122, saving model to final.h5\n","Epoch 39/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.8731 - val_loss: 1.4271\n","\n","Epoch 00039: loss improved from 10.17122 to 9.87338, saving model to final.h5\n","Epoch 40/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.7870 - val_loss: 3.4443\n","\n","Epoch 00040: loss improved from 9.87338 to 9.78725, saving model to final.h5\n","Epoch 41/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.4059 - val_loss: 2.5551\n","\n","Epoch 00041: loss improved from 9.78725 to 9.40629, saving model to final.h5\n","Epoch 42/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.6861 - val_loss: 2.9755\n","\n","Epoch 00042: loss did not improve from 9.40629\n","Epoch 43/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.3211 - val_loss: 1.1121\n","\n","Epoch 00043: loss improved from 9.40629 to 9.32073, saving model to final.h5\n","Epoch 44/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.2456 - val_loss: 1.2435\n","\n","Epoch 00044: loss improved from 9.32073 to 9.24554, saving model to final.h5\n","Epoch 45/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.0892 - val_loss: 2.9178\n","\n","Epoch 00045: loss improved from 9.24554 to 9.08942, saving model to final.h5\n","Epoch 46/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.0377 - val_loss: 1.3916\n","\n","Epoch 00046: loss improved from 9.08942 to 9.03724, saving model to final.h5\n","Epoch 47/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.1322 - val_loss: 2.0877\n","\n","Epoch 00047: loss did not improve from 9.03724\n","Epoch 48/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.8957 - val_loss: 0.7467\n","\n","Epoch 00048: loss improved from 9.03724 to 8.89603, saving model to final.h5\n","Epoch 49/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.7255 - val_loss: 1.1529\n","\n","Epoch 00049: loss improved from 8.89603 to 8.72550, saving model to final.h5\n","Epoch 50/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.6491 - val_loss: 1.4354\n","\n","Epoch 00050: loss improved from 8.72550 to 8.64918, saving model to final.h5\n","Epoch 51/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.5350 - val_loss: 0.8974\n","\n","Epoch 00051: loss improved from 8.64918 to 8.53508, saving model to final.h5\n","Epoch 52/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.3513 - val_loss: 0.9801\n","\n","Epoch 00052: loss improved from 8.53508 to 8.35161, saving model to final.h5\n","Epoch 53/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.5148 - val_loss: 0.7502\n","\n","Epoch 00053: loss did not improve from 8.35161\n","Epoch 54/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.5083 - val_loss: 0.7881\n","\n","Epoch 00054: loss did not improve from 8.35161\n","Epoch 55/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.2374 - val_loss: 0.6101\n","\n","Epoch 00055: loss improved from 8.35161 to 8.23733, saving model to final.h5\n","Epoch 56/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.2050 - val_loss: 0.5378\n","\n","Epoch 00056: loss improved from 8.23733 to 8.20440, saving model to final.h5\n","Epoch 57/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.2280 - val_loss: 0.7133\n","\n","Epoch 00057: loss did not improve from 8.20440\n","Epoch 58/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.0180 - val_loss: 0.6386\n","\n","Epoch 00058: loss improved from 8.20440 to 8.01786, saving model to final.h5\n","Epoch 59/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.9320 - val_loss: 0.5872\n","\n","Epoch 00059: loss improved from 8.01786 to 7.93155, saving model to final.h5\n","Epoch 60/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.8274 - val_loss: 0.7253\n","\n","Epoch 00060: loss improved from 7.93155 to 7.82752, saving model to final.h5\n","Epoch 61/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.7852 - val_loss: 1.0659\n","\n","Epoch 00061: loss improved from 7.82752 to 7.78521, saving model to final.h5\n","Epoch 62/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.7440 - val_loss: 0.7309\n","\n","Epoch 00062: loss improved from 7.78521 to 7.74415, saving model to final.h5\n","Epoch 63/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.5615 - val_loss: 0.5076\n","\n","Epoch 00063: loss improved from 7.74415 to 7.56109, saving model to final.h5\n","Epoch 64/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.5904 - val_loss: 0.7199\n","\n","Epoch 00064: loss did not improve from 7.56109\n","Epoch 65/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.4845 - val_loss: 0.7008\n","\n","Epoch 00065: loss improved from 7.56109 to 7.48485, saving model to final.h5\n","Epoch 66/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.5668 - val_loss: 0.6488\n","\n","Epoch 00066: loss did not improve from 7.48485\n","Epoch 67/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.4352 - val_loss: 0.6520\n","\n","Epoch 00067: loss improved from 7.48485 to 7.43510, saving model to final.h5\n","Epoch 68/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.3950 - val_loss: 0.5899\n","\n","Epoch 00068: loss improved from 7.43510 to 7.39508, saving model to final.h5\n","Epoch 69/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.3529 - val_loss: 0.4799\n","\n","Epoch 00069: loss improved from 7.39508 to 7.35310, saving model to final.h5\n","Epoch 70/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.3739 - val_loss: 0.4695\n","\n","Epoch 00070: loss did not improve from 7.35310\n","Epoch 71/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.3084 - val_loss: 0.3457\n","\n","Epoch 00071: loss improved from 7.35310 to 7.30838, saving model to final.h5\n","Epoch 72/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.2661 - val_loss: 0.3908\n","\n","Epoch 00072: loss improved from 7.30838 to 7.26630, saving model to final.h5\n","Epoch 73/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.0665 - val_loss: 0.4425\n","\n","Epoch 00073: loss improved from 7.26630 to 7.06686, saving model to final.h5\n","Epoch 74/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.0696 - val_loss: 0.5196\n","\n","Epoch 00074: loss did not improve from 7.06686\n","Epoch 75/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.8654 - val_loss: 0.4042\n","\n","Epoch 00075: loss improved from 7.06686 to 6.86534, saving model to final.h5\n","Epoch 76/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.9995 - val_loss: 0.4234\n","\n","Epoch 00076: loss did not improve from 6.86534\n","Epoch 77/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.8249 - val_loss: 0.4733\n","\n","Epoch 00077: loss improved from 6.86534 to 6.82494, saving model to final.h5\n","Epoch 78/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.8496 - val_loss: 0.4272\n","\n","Epoch 00078: loss did not improve from 6.82494\n","Epoch 79/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.6948 - val_loss: 0.4634\n","\n","Epoch 00079: loss improved from 6.82494 to 6.69478, saving model to final.h5\n","Epoch 80/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.7124 - val_loss: 0.4576\n","\n","Epoch 00080: loss did not improve from 6.69478\n","Epoch 81/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.7044 - val_loss: 0.4613\n","\n","Epoch 00081: loss did not improve from 6.69478\n","Epoch 82/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.6637 - val_loss: 0.4756\n","\n","Epoch 00082: loss improved from 6.69478 to 6.66314, saving model to final.h5\n","Epoch 83/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.6550 - val_loss: 0.4454\n","\n","Epoch 00083: loss improved from 6.66314 to 6.65521, saving model to final.h5\n","Epoch 84/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.6308 - val_loss: 0.4222\n","\n","Epoch 00084: loss improved from 6.65521 to 6.63075, saving model to final.h5\n","Epoch 85/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.7275 - val_loss: 0.3756\n","\n","Epoch 00085: loss did not improve from 6.63075\n","Epoch 86/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.5971 - val_loss: 0.3626\n","\n","Epoch 00086: loss improved from 6.63075 to 6.59719, saving model to final.h5\n","Epoch 87/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.5393 - val_loss: 0.3783\n","\n","Epoch 00087: loss improved from 6.59719 to 6.53911, saving model to final.h5\n","Epoch 88/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.6763 - val_loss: 0.3916\n","\n","Epoch 00088: loss did not improve from 6.53911\n","Epoch 89/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.5991 - val_loss: 0.4335\n","\n","Epoch 00089: loss did not improve from 6.53911\n","Epoch 90/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.5701 - val_loss: 0.3691\n","\n","Epoch 00090: loss did not improve from 6.53911\n","Epoch 91/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.3197 - val_loss: 0.4944\n","\n","Epoch 00091: loss improved from 6.53911 to 6.31988, saving model to final.h5\n","Epoch 92/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.5145 - val_loss: 0.4627\n","\n","Epoch 00092: loss did not improve from 6.31988\n","Epoch 93/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.3950 - val_loss: 0.3316\n","\n","Epoch 00093: loss did not improve from 6.31988\n","Epoch 94/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.2848 - val_loss: 0.3481\n","\n","Epoch 00094: loss improved from 6.31988 to 6.28465, saving model to final.h5\n","Epoch 95/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.3081 - val_loss: 0.4553\n","\n","Epoch 00095: loss did not improve from 6.28465\n","Epoch 96/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.2991 - val_loss: 0.3205\n","\n","Epoch 00096: loss did not improve from 6.28465\n","Epoch 97/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.3654 - val_loss: 0.3465\n","\n","Epoch 00097: loss did not improve from 6.28465\n","Epoch 98/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.0889 - val_loss: 0.3831\n","\n","Epoch 00098: loss improved from 6.28465 to 6.08924, saving model to final.h5\n","Epoch 99/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.1127 - val_loss: 0.3075\n","\n","Epoch 00099: loss did not improve from 6.08924\n","Epoch 100/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.1164 - val_loss: 0.2633\n","\n","Epoch 00100: loss did not improve from 6.08924\n","Epoch 101/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.1358 - val_loss: 0.4749\n","\n","Epoch 00101: loss did not improve from 6.08924\n","Epoch 102/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.1087 - val_loss: 0.3831\n","\n","Epoch 00102: loss did not improve from 6.08924\n","Epoch 103/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.1984 - val_loss: 0.3111\n","\n","Epoch 00103: loss did not improve from 6.08924\n","Epoch 104/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.1231 - val_loss: 0.3974\n","\n","Epoch 00104: loss did not improve from 6.08924\n","Epoch 105/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 6.1079 - val_loss: 0.3134\n","\n","Epoch 00105: loss did not improve from 6.08924\n","Epoch 106/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.9159 - val_loss: 0.3186\n","\n","Epoch 00106: loss improved from 6.08924 to 5.91585, saving model to final.h5\n","Epoch 107/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.9532 - val_loss: 0.3392\n","\n","Epoch 00107: loss did not improve from 5.91585\n","Epoch 108/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.9814 - val_loss: 0.3493\n","\n","Epoch 00108: loss did not improve from 5.91585\n","Epoch 109/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.8498 - val_loss: 0.2790\n","\n","Epoch 00109: loss improved from 5.91585 to 5.84952, saving model to final.h5\n","Epoch 110/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.9329 - val_loss: 0.2463\n","\n","Epoch 00110: loss did not improve from 5.84952\n","Epoch 111/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.7896 - val_loss: 0.2599\n","\n","Epoch 00111: loss improved from 5.84952 to 5.78989, saving model to final.h5\n","Epoch 112/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.8735 - val_loss: 0.3075\n","\n","Epoch 00112: loss did not improve from 5.78989\n","Epoch 113/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.6999 - val_loss: 0.2556\n","\n","Epoch 00113: loss improved from 5.78989 to 5.69985, saving model to final.h5\n","Epoch 114/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.6367 - val_loss: 0.3190\n","\n","Epoch 00114: loss improved from 5.69985 to 5.63696, saving model to final.h5\n","Epoch 115/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.7111 - val_loss: 0.2758\n","\n","Epoch 00115: loss did not improve from 5.63696\n","Epoch 116/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.6437 - val_loss: 0.3040\n","\n","Epoch 00116: loss did not improve from 5.63696\n","Epoch 117/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.5625 - val_loss: 0.2559\n","\n","Epoch 00117: loss improved from 5.63696 to 5.56220, saving model to final.h5\n","Epoch 118/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.6109 - val_loss: 0.2433\n","\n","Epoch 00118: loss did not improve from 5.56220\n","Epoch 119/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.5831 - val_loss: 0.2416\n","\n","Epoch 00119: loss did not improve from 5.56220\n","Epoch 120/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.6031 - val_loss: 0.3019\n","\n","Epoch 00120: loss did not improve from 5.56220\n","Epoch 121/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.6077 - val_loss: 0.2538\n","\n","Epoch 00121: loss did not improve from 5.56220\n","Epoch 122/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.5813 - val_loss: 0.2280\n","\n","Epoch 00122: loss did not improve from 5.56220\n","Epoch 123/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.5305 - val_loss: 0.2472\n","\n","Epoch 00123: loss improved from 5.56220 to 5.53064, saving model to final.h5\n","Epoch 124/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.5376 - val_loss: 0.2190\n","\n","Epoch 00124: loss did not improve from 5.53064\n","Epoch 125/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.4608 - val_loss: 0.2720\n","\n","Epoch 00125: loss improved from 5.53064 to 5.46105, saving model to final.h5\n","Epoch 126/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.5493 - val_loss: 0.3098\n","\n","Epoch 00126: loss did not improve from 5.46105\n","Epoch 127/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.5491 - val_loss: 0.2267\n","\n","Epoch 00127: loss did not improve from 5.46105\n","Epoch 128/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.4205 - val_loss: 0.2348\n","\n","Epoch 00128: loss improved from 5.46105 to 5.42068, saving model to final.h5\n","Epoch 129/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.4481 - val_loss: 0.2057\n","\n","Epoch 00129: loss did not improve from 5.42068\n","Epoch 130/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.3134 - val_loss: 0.2028\n","\n","Epoch 00130: loss improved from 5.42068 to 5.31360, saving model to final.h5\n","Epoch 131/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.2849 - val_loss: 0.2029\n","\n","Epoch 00131: loss improved from 5.31360 to 5.28492, saving model to final.h5\n","Epoch 132/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.2600 - val_loss: 0.2123\n","\n","Epoch 00132: loss improved from 5.28492 to 5.25984, saving model to final.h5\n","Epoch 133/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.3525 - val_loss: 0.1721\n","\n","Epoch 00133: loss did not improve from 5.25984\n","Epoch 134/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.4420 - val_loss: 0.1837\n","\n","Epoch 00134: loss did not improve from 5.25984\n","Epoch 135/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.2745 - val_loss: 0.1850\n","\n","Epoch 00135: loss did not improve from 5.25984\n","Epoch 136/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.2623 - val_loss: 0.1736\n","\n","Epoch 00136: loss did not improve from 5.25984\n","Epoch 137/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.2089 - val_loss: 0.1540\n","\n","Epoch 00137: loss improved from 5.25984 to 5.20899, saving model to final.h5\n","Epoch 138/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.3104 - val_loss: 0.2048\n","\n","Epoch 00138: loss did not improve from 5.20899\n","Epoch 139/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.2677 - val_loss: 0.1786\n","\n","Epoch 00139: loss did not improve from 5.20899\n","Epoch 140/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.1394 - val_loss: 0.1486\n","\n","Epoch 00140: loss improved from 5.20899 to 5.13947, saving model to final.h5\n","Epoch 141/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.1101 - val_loss: 0.1283\n","\n","Epoch 00141: loss improved from 5.13947 to 5.10997, saving model to final.h5\n","Epoch 142/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.1432 - val_loss: 0.1281\n","\n","Epoch 00142: loss did not improve from 5.10997\n","Epoch 143/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.1271 - val_loss: 0.1449\n","\n","Epoch 00143: loss did not improve from 5.10997\n","Epoch 144/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.0743 - val_loss: 0.1532\n","\n","Epoch 00144: loss improved from 5.10997 to 5.07446, saving model to final.h5\n","Epoch 145/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.0584 - val_loss: 0.1530\n","\n","Epoch 00145: loss improved from 5.07446 to 5.05802, saving model to final.h5\n","Epoch 146/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.0670 - val_loss: 0.1442\n","\n","Epoch 00146: loss did not improve from 5.05802\n","Epoch 147/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.0049 - val_loss: 0.1363\n","\n","Epoch 00147: loss improved from 5.05802 to 5.00496, saving model to final.h5\n","Epoch 148/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.1146 - val_loss: 0.1244\n","\n","Epoch 00148: loss did not improve from 5.00496\n","Epoch 149/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 5.0531 - val_loss: 0.1395\n","\n","Epoch 00149: loss did not improve from 5.00496\n","Epoch 150/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.0614 - val_loss: 0.1153\n","\n","Epoch 00150: loss did not improve from 5.00496\n","Epoch 151/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 5.0198 - val_loss: 0.1409\n","\n","Epoch 00151: loss did not improve from 5.00496\n","Epoch 152/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.9792 - val_loss: 0.1688\n","\n","Epoch 00152: loss improved from 5.00496 to 4.97935, saving model to final.h5\n","Epoch 153/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.9879 - val_loss: 0.1389\n","\n","Epoch 00153: loss did not improve from 4.97935\n","Epoch 154/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.9385 - val_loss: 0.1324\n","\n","Epoch 00154: loss improved from 4.97935 to 4.93873, saving model to final.h5\n","Epoch 155/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.9619 - val_loss: 0.1509\n","\n","Epoch 00155: loss did not improve from 4.93873\n","Epoch 156/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.9253 - val_loss: 0.1374\n","\n","Epoch 00156: loss improved from 4.93873 to 4.92519, saving model to final.h5\n","Epoch 157/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.9219 - val_loss: 0.1689\n","\n","Epoch 00157: loss improved from 4.92519 to 4.92196, saving model to final.h5\n","Epoch 158/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.8467 - val_loss: 0.1470\n","\n","Epoch 00158: loss improved from 4.92196 to 4.84666, saving model to final.h5\n","Epoch 159/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.9341 - val_loss: 0.1265\n","\n","Epoch 00159: loss did not improve from 4.84666\n","Epoch 160/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.9395 - val_loss: 0.1488\n","\n","Epoch 00160: loss did not improve from 4.84666\n","Epoch 161/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.8806 - val_loss: 0.1339\n","\n","Epoch 00161: loss did not improve from 4.84666\n","Epoch 162/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.9678 - val_loss: 0.1470\n","\n","Epoch 00162: loss did not improve from 4.84666\n","Epoch 163/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.7727 - val_loss: 0.1068\n","\n","Epoch 00163: loss improved from 4.84666 to 4.77281, saving model to final.h5\n","Epoch 164/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.7846 - val_loss: 0.1369\n","\n","Epoch 00164: loss did not improve from 4.77281\n","Epoch 165/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.7924 - val_loss: 0.1413\n","\n","Epoch 00165: loss did not improve from 4.77281\n","Epoch 166/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.9287 - val_loss: 0.1408\n","\n","Epoch 00166: loss did not improve from 4.77281\n","Epoch 167/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.8600 - val_loss: 0.1448\n","\n","Epoch 00167: loss did not improve from 4.77281\n","Epoch 168/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.8059 - val_loss: 0.1282\n","\n","Epoch 00168: loss did not improve from 4.77281\n","Epoch 169/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.7835 - val_loss: 0.1193\n","\n","Epoch 00169: loss did not improve from 4.77281\n","Epoch 170/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.6659 - val_loss: 0.1436\n","\n","Epoch 00170: loss improved from 4.77281 to 4.66614, saving model to final.h5\n","Epoch 171/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.6994 - val_loss: 0.1152\n","\n","Epoch 00171: loss did not improve from 4.66614\n","Epoch 172/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.7498 - val_loss: 0.1081\n","\n","Epoch 00172: loss did not improve from 4.66614\n","Epoch 173/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.7124 - val_loss: 0.1314\n","\n","Epoch 00173: loss did not improve from 4.66614\n","Epoch 174/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.7743 - val_loss: 0.1357\n","\n","Epoch 00174: loss did not improve from 4.66614\n","Epoch 175/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.7367 - val_loss: 0.1403\n","\n","Epoch 00175: loss did not improve from 4.66614\n","Epoch 176/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.8012 - val_loss: 0.1237\n","\n","Epoch 00176: loss did not improve from 4.66614\n","Epoch 177/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.6368 - val_loss: 0.1257\n","\n","Epoch 00177: loss improved from 4.66614 to 4.63652, saving model to final.h5\n","Epoch 178/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.6025 - val_loss: 0.1267\n","\n","Epoch 00178: loss improved from 4.63652 to 4.60260, saving model to final.h5\n","Epoch 179/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.6479 - val_loss: 0.1249\n","\n","Epoch 00179: loss did not improve from 4.60260\n","Epoch 180/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.7081 - val_loss: 0.1053\n","\n","Epoch 00180: loss did not improve from 4.60260\n","Epoch 181/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.5795 - val_loss: 0.1142\n","\n","Epoch 00181: loss improved from 4.60260 to 4.57937, saving model to final.h5\n","Epoch 182/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.6274 - val_loss: 0.1209\n","\n","Epoch 00182: loss did not improve from 4.57937\n","Epoch 183/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.5528 - val_loss: 0.0825\n","\n","Epoch 00183: loss improved from 4.57937 to 4.55297, saving model to final.h5\n","Epoch 184/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.7294 - val_loss: 0.0866\n","\n","Epoch 00184: loss did not improve from 4.55297\n","Epoch 185/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.5968 - val_loss: 0.1084\n","\n","Epoch 00185: loss did not improve from 4.55297\n","Epoch 186/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.6220 - val_loss: 0.1230\n","\n","Epoch 00186: loss did not improve from 4.55297\n","Epoch 187/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.4515 - val_loss: 0.1931\n","\n","Epoch 00187: loss improved from 4.55297 to 4.45158, saving model to final.h5\n","Epoch 188/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.5305 - val_loss: 0.1102\n","\n","Epoch 00188: loss did not improve from 4.45158\n","Epoch 189/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.5124 - val_loss: 0.0976\n","\n","Epoch 00189: loss did not improve from 4.45158\n","Epoch 190/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.5386 - val_loss: 0.1872\n","\n","Epoch 00190: loss did not improve from 4.45158\n","Epoch 191/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.5154 - val_loss: 0.1236\n","\n","Epoch 00191: loss did not improve from 4.45158\n","Epoch 192/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.4495 - val_loss: 0.1309\n","\n","Epoch 00192: loss improved from 4.45158 to 4.44960, saving model to final.h5\n","Epoch 193/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.5422 - val_loss: 0.2128\n","\n","Epoch 00193: loss did not improve from 4.44960\n","Epoch 194/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.5289 - val_loss: 0.1790\n","\n","Epoch 00194: loss did not improve from 4.44960\n","Epoch 195/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.4489 - val_loss: 0.1033\n","\n","Epoch 00195: loss improved from 4.44960 to 4.44903, saving model to final.h5\n","Epoch 196/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.5659 - val_loss: 0.1023\n","\n","Epoch 00196: loss did not improve from 4.44903\n","Epoch 197/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.4830 - val_loss: 0.1016\n","\n","Epoch 00197: loss did not improve from 4.44903\n","Epoch 198/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.3569 - val_loss: 0.1140\n","\n","Epoch 00198: loss improved from 4.44903 to 4.35695, saving model to final.h5\n","Epoch 199/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.4172 - val_loss: 0.1596\n","\n","Epoch 00199: loss did not improve from 4.35695\n","Epoch 200/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.4925 - val_loss: 0.1252\n","\n","Epoch 00200: loss did not improve from 4.35695\n","Epoch 201/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.3878 - val_loss: 0.1124\n","\n","Epoch 00201: loss did not improve from 4.35695\n","Epoch 202/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.4890 - val_loss: 0.1884\n","\n","Epoch 00202: loss did not improve from 4.35695\n","Epoch 203/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.3832 - val_loss: 0.0973\n","\n","Epoch 00203: loss did not improve from 4.35695\n","Epoch 204/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.3459 - val_loss: 0.1102\n","\n","Epoch 00204: loss improved from 4.35695 to 4.34542, saving model to final.h5\n","Epoch 205/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.2856 - val_loss: 0.0903\n","\n","Epoch 00205: loss improved from 4.34542 to 4.28575, saving model to final.h5\n","Epoch 206/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.3212 - val_loss: 0.1747\n","\n","Epoch 00206: loss did not improve from 4.28575\n","Epoch 207/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.2983 - val_loss: 0.1216\n","\n","Epoch 00207: loss did not improve from 4.28575\n","Epoch 208/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.4345 - val_loss: 0.1052\n","\n","Epoch 00208: loss did not improve from 4.28575\n","Epoch 209/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.4172 - val_loss: 0.1323\n","\n","Epoch 00209: loss did not improve from 4.28575\n","Epoch 210/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.3625 - val_loss: 0.1216\n","\n","Epoch 00210: loss did not improve from 4.28575\n","Epoch 211/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.3828 - val_loss: 0.2341\n","\n","Epoch 00211: loss did not improve from 4.28575\n","Epoch 212/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.4034 - val_loss: 0.1820\n","\n","Epoch 00212: loss did not improve from 4.28575\n","Epoch 213/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.3054 - val_loss: 0.1216\n","\n","Epoch 00213: loss did not improve from 4.28575\n","Epoch 214/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.3602 - val_loss: 0.1568\n","\n","Epoch 00214: loss did not improve from 4.28575\n","Epoch 215/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.4077 - val_loss: 0.1367\n","\n","Epoch 00215: loss did not improve from 4.28575\n","Epoch 216/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.3043 - val_loss: 0.1311\n","\n","Epoch 00216: loss did not improve from 4.28575\n","Epoch 217/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.3032 - val_loss: 0.0889\n","\n","Epoch 00217: loss did not improve from 4.28575\n","Epoch 218/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.3097 - val_loss: 0.1186\n","\n","Epoch 00218: loss did not improve from 4.28575\n","Epoch 219/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.3968 - val_loss: 0.1241\n","\n","Epoch 00219: loss did not improve from 4.28575\n","Epoch 220/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.3921 - val_loss: 0.1632\n","\n","Epoch 00220: loss did not improve from 4.28575\n","Epoch 221/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.3289 - val_loss: 0.0839\n","\n","Epoch 00221: loss did not improve from 4.28575\n","Epoch 222/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.2506 - val_loss: 0.1390\n","\n","Epoch 00222: loss improved from 4.28575 to 4.25070, saving model to final.h5\n","Epoch 223/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.3366 - val_loss: 0.0997\n","\n","Epoch 00223: loss did not improve from 4.25070\n","Epoch 224/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.2473 - val_loss: 0.2139\n","\n","Epoch 00224: loss improved from 4.25070 to 4.24744, saving model to final.h5\n","Epoch 225/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.2680 - val_loss: 0.1262\n","\n","Epoch 00225: loss did not improve from 4.24744\n","Epoch 226/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.2290 - val_loss: 0.1008\n","\n","Epoch 00226: loss improved from 4.24744 to 4.22887, saving model to final.h5\n","Epoch 227/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.1913 - val_loss: 0.0870\n","\n","Epoch 00227: loss improved from 4.22887 to 4.19120, saving model to final.h5\n","Epoch 228/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.3320 - val_loss: 0.1069\n","\n","Epoch 00228: loss did not improve from 4.19120\n","Epoch 229/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.2306 - val_loss: 0.1034\n","\n","Epoch 00229: loss did not improve from 4.19120\n","Epoch 230/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.3167 - val_loss: 0.0922\n","\n","Epoch 00230: loss did not improve from 4.19120\n","Epoch 231/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.1585 - val_loss: 0.2131\n","\n","Epoch 00231: loss improved from 4.19120 to 4.15865, saving model to final.h5\n","Epoch 232/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.1808 - val_loss: 0.0983\n","\n","Epoch 00232: loss did not improve from 4.15865\n","Epoch 233/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0992 - val_loss: 0.1048\n","\n","Epoch 00233: loss improved from 4.15865 to 4.09897, saving model to final.h5\n","Epoch 234/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.1573 - val_loss: 0.1392\n","\n","Epoch 00234: loss did not improve from 4.09897\n","Epoch 235/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.1676 - val_loss: 0.1614\n","\n","Epoch 00235: loss did not improve from 4.09897\n","Epoch 236/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.1946 - val_loss: 0.1271\n","\n","Epoch 00236: loss did not improve from 4.09897\n","Epoch 237/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.0526 - val_loss: 0.1774\n","\n","Epoch 00237: loss improved from 4.09897 to 4.05276, saving model to final.h5\n","Epoch 238/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.1172 - val_loss: 0.1824\n","\n","Epoch 00238: loss did not improve from 4.05276\n","Epoch 239/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.1099 - val_loss: 0.1155\n","\n","Epoch 00239: loss did not improve from 4.05276\n","Epoch 240/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.1784 - val_loss: 0.1418\n","\n","Epoch 00240: loss did not improve from 4.05276\n","Epoch 241/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0901 - val_loss: 0.1114\n","\n","Epoch 00241: loss did not improve from 4.05276\n","Epoch 242/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0424 - val_loss: 0.1247\n","\n","Epoch 00242: loss improved from 4.05276 to 4.04254, saving model to final.h5\n","Epoch 243/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.1643 - val_loss: 0.1158\n","\n","Epoch 00243: loss did not improve from 4.04254\n","Epoch 244/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.2205 - val_loss: 0.1178\n","\n","Epoch 00244: loss did not improve from 4.04254\n","Epoch 245/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.1720 - val_loss: 0.1135\n","\n","Epoch 00245: loss did not improve from 4.04254\n","Epoch 246/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.1686 - val_loss: 0.1489\n","\n","Epoch 00246: loss did not improve from 4.04254\n","Epoch 247/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9809 - val_loss: 0.1389\n","\n","Epoch 00247: loss improved from 4.04254 to 3.98104, saving model to final.h5\n","Epoch 248/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0560 - val_loss: 0.1113\n","\n","Epoch 00248: loss did not improve from 3.98104\n","Epoch 249/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.1166 - val_loss: 0.1363\n","\n","Epoch 00249: loss did not improve from 3.98104\n","Epoch 250/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0560 - val_loss: 0.1696\n","\n","Epoch 00250: loss did not improve from 3.98104\n","Epoch 251/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0978 - val_loss: 0.1996\n","\n","Epoch 00251: loss did not improve from 3.98104\n","Epoch 252/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.1748 - val_loss: 0.0726\n","\n","Epoch 00252: loss did not improve from 3.98104\n","Epoch 253/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.1372 - val_loss: 0.1177\n","\n","Epoch 00253: loss did not improve from 3.98104\n","Epoch 254/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.0309 - val_loss: 0.2003\n","\n","Epoch 00254: loss did not improve from 3.98104\n","Epoch 255/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.0524 - val_loss: 0.2516\n","\n","Epoch 00255: loss did not improve from 3.98104\n","Epoch 256/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.9955 - val_loss: 0.1105\n","\n","Epoch 00256: loss did not improve from 3.98104\n","Epoch 257/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.0129 - val_loss: 0.1057\n","\n","Epoch 00257: loss did not improve from 3.98104\n","Epoch 258/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.0542 - val_loss: 0.1210\n","\n","Epoch 00258: loss did not improve from 3.98104\n","Epoch 259/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0971 - val_loss: 0.1180\n","\n","Epoch 00259: loss did not improve from 3.98104\n","Epoch 260/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0409 - val_loss: 0.1983\n","\n","Epoch 00260: loss did not improve from 3.98104\n","Epoch 261/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9526 - val_loss: 0.0906\n","\n","Epoch 00261: loss improved from 3.98104 to 3.95234, saving model to final.h5\n","Epoch 262/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9910 - val_loss: 0.0991\n","\n","Epoch 00262: loss did not improve from 3.95234\n","Epoch 263/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.0887 - val_loss: 0.1299\n","\n","Epoch 00263: loss did not improve from 3.95234\n","Epoch 264/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0684 - val_loss: 0.0869\n","\n","Epoch 00264: loss did not improve from 3.95234\n","Epoch 265/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9919 - val_loss: 0.0955\n","\n","Epoch 00265: loss did not improve from 3.95234\n","Epoch 266/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.0657 - val_loss: 0.1298\n","\n","Epoch 00266: loss did not improve from 3.95234\n","Epoch 267/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9995 - val_loss: 0.0935\n","\n","Epoch 00267: loss did not improve from 3.95234\n","Epoch 268/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0411 - val_loss: 0.0854\n","\n","Epoch 00268: loss did not improve from 3.95234\n","Epoch 269/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.0508 - val_loss: 0.1742\n","\n","Epoch 00269: loss did not improve from 3.95234\n","Epoch 270/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9849 - val_loss: 0.1340\n","\n","Epoch 00270: loss did not improve from 3.95234\n","Epoch 271/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9643 - val_loss: 0.1094\n","\n","Epoch 00271: loss did not improve from 3.95234\n","Epoch 272/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9978 - val_loss: 0.1066\n","\n","Epoch 00272: loss did not improve from 3.95234\n","Epoch 273/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8635 - val_loss: 0.1203\n","\n","Epoch 00273: loss improved from 3.95234 to 3.86368, saving model to final.h5\n","Epoch 274/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9580 - val_loss: 0.1588\n","\n","Epoch 00274: loss did not improve from 3.86368\n","Epoch 275/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 4.0188 - val_loss: 0.0800\n","\n","Epoch 00275: loss did not improve from 3.86368\n","Epoch 276/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0011 - val_loss: 0.0929\n","\n","Epoch 00276: loss did not improve from 3.86368\n","Epoch 277/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0262 - val_loss: 0.0834\n","\n","Epoch 00277: loss did not improve from 3.86368\n","Epoch 278/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9600 - val_loss: 0.1261\n","\n","Epoch 00278: loss did not improve from 3.86368\n","Epoch 279/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9765 - val_loss: 0.1173\n","\n","Epoch 00279: loss did not improve from 3.86368\n","Epoch 280/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.9668 - val_loss: 0.1383\n","\n","Epoch 00280: loss did not improve from 3.86368\n","Epoch 281/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.8989 - val_loss: 0.1039\n","\n","Epoch 00281: loss did not improve from 3.86368\n","Epoch 282/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.9921 - val_loss: 0.1038\n","\n","Epoch 00282: loss did not improve from 3.86368\n","Epoch 283/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.9863 - val_loss: 0.0976\n","\n","Epoch 00283: loss did not improve from 3.86368\n","Epoch 284/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.9463 - val_loss: 0.1234\n","\n","Epoch 00284: loss did not improve from 3.86368\n","Epoch 285/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9245 - val_loss: 0.1301\n","\n","Epoch 00285: loss did not improve from 3.86368\n","Epoch 286/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.8414 - val_loss: 0.1130\n","\n","Epoch 00286: loss improved from 3.86368 to 3.84111, saving model to final.h5\n","Epoch 287/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9999 - val_loss: 0.1730\n","\n","Epoch 00287: loss did not improve from 3.84111\n","Epoch 288/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0368 - val_loss: 0.0782\n","\n","Epoch 00288: loss did not improve from 3.84111\n","Epoch 289/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.9298 - val_loss: 0.1289\n","\n","Epoch 00289: loss did not improve from 3.84111\n","Epoch 290/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 4.0098 - val_loss: 0.1533\n","\n","Epoch 00290: loss did not improve from 3.84111\n","Epoch 291/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9790 - val_loss: 0.1092\n","\n","Epoch 00291: loss did not improve from 3.84111\n","Epoch 292/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8811 - val_loss: 0.2376\n","\n","Epoch 00292: loss did not improve from 3.84111\n","Epoch 293/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9317 - val_loss: 0.1216\n","\n","Epoch 00293: loss did not improve from 3.84111\n","Epoch 294/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8989 - val_loss: 0.1471\n","\n","Epoch 00294: loss did not improve from 3.84111\n","Epoch 295/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.8847 - val_loss: 0.1204\n","\n","Epoch 00295: loss did not improve from 3.84111\n","Epoch 296/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.8508 - val_loss: 0.1442\n","\n","Epoch 00296: loss did not improve from 3.84111\n","Epoch 297/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8931 - val_loss: 0.1992\n","\n","Epoch 00297: loss did not improve from 3.84111\n","Epoch 298/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9139 - val_loss: 0.0664\n","\n","Epoch 00298: loss did not improve from 3.84111\n","Epoch 299/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.8350 - val_loss: 0.0978\n","\n","Epoch 00299: loss improved from 3.84111 to 3.83497, saving model to final.h5\n","Epoch 300/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.9140 - val_loss: 0.1243\n","\n","Epoch 00300: loss did not improve from 3.83497\n","Epoch 301/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9858 - val_loss: 0.2325\n","\n","Epoch 00301: loss did not improve from 3.83497\n","Epoch 302/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.9592 - val_loss: 0.1059\n","\n","Epoch 00302: loss did not improve from 3.83497\n","Epoch 303/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8824 - val_loss: 0.0818\n","\n","Epoch 00303: loss did not improve from 3.83497\n","Epoch 304/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8330 - val_loss: 0.1092\n","\n","Epoch 00304: loss improved from 3.83497 to 3.83291, saving model to final.h5\n","Epoch 305/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.8244 - val_loss: 0.1101\n","\n","Epoch 00305: loss improved from 3.83291 to 3.82433, saving model to final.h5\n","Epoch 306/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8098 - val_loss: 0.2083\n","\n","Epoch 00306: loss improved from 3.82433 to 3.80982, saving model to final.h5\n","Epoch 307/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8913 - val_loss: 0.2245\n","\n","Epoch 00307: loss did not improve from 3.80982\n","Epoch 308/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.8621 - val_loss: 0.2035\n","\n","Epoch 00308: loss did not improve from 3.80982\n","Epoch 309/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8654 - val_loss: 0.1306\n","\n","Epoch 00309: loss did not improve from 3.80982\n","Epoch 310/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8608 - val_loss: 0.0910\n","\n","Epoch 00310: loss did not improve from 3.80982\n","Epoch 311/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7658 - val_loss: 0.0955\n","\n","Epoch 00311: loss improved from 3.80982 to 3.76549, saving model to final.h5\n","Epoch 312/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.8151 - val_loss: 0.1278\n","\n","Epoch 00312: loss did not improve from 3.76549\n","Epoch 313/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.9016 - val_loss: 0.1443\n","\n","Epoch 00313: loss did not improve from 3.76549\n","Epoch 314/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7686 - val_loss: 0.0712\n","\n","Epoch 00314: loss did not improve from 3.76549\n","Epoch 315/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7439 - val_loss: 0.1079\n","\n","Epoch 00315: loss improved from 3.76549 to 3.74396, saving model to final.h5\n","Epoch 316/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.8312 - val_loss: 0.1044\n","\n","Epoch 00316: loss did not improve from 3.74396\n","Epoch 317/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7154 - val_loss: 0.1044\n","\n","Epoch 00317: loss improved from 3.74396 to 3.71552, saving model to final.h5\n","Epoch 318/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7954 - val_loss: 0.0770\n","\n","Epoch 00318: loss did not improve from 3.71552\n","Epoch 319/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7445 - val_loss: 0.1214\n","\n","Epoch 00319: loss did not improve from 3.71552\n","Epoch 320/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7729 - val_loss: 0.1115\n","\n","Epoch 00320: loss did not improve from 3.71552\n","Epoch 321/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.8634 - val_loss: 0.2031\n","\n","Epoch 00321: loss did not improve from 3.71552\n","Epoch 322/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7891 - val_loss: 0.2165\n","\n","Epoch 00322: loss did not improve from 3.71552\n","Epoch 323/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7612 - val_loss: 0.1878\n","\n","Epoch 00323: loss did not improve from 3.71552\n","Epoch 324/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7774 - val_loss: 0.1336\n","\n","Epoch 00324: loss did not improve from 3.71552\n","Epoch 325/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7780 - val_loss: 0.1122\n","\n","Epoch 00325: loss did not improve from 3.71552\n","Epoch 326/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8636 - val_loss: 0.1854\n","\n","Epoch 00326: loss did not improve from 3.71552\n","Epoch 327/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8506 - val_loss: 0.1638\n","\n","Epoch 00327: loss did not improve from 3.71552\n","Epoch 328/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7362 - val_loss: 0.1044\n","\n","Epoch 00328: loss did not improve from 3.71552\n","Epoch 329/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7773 - val_loss: 0.1363\n","\n","Epoch 00329: loss did not improve from 3.71552\n","Epoch 330/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7800 - val_loss: 0.3154\n","\n","Epoch 00330: loss did not improve from 3.71552\n","Epoch 331/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7395 - val_loss: 0.1509\n","\n","Epoch 00331: loss did not improve from 3.71552\n","Epoch 332/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7379 - val_loss: 0.2191\n","\n","Epoch 00332: loss did not improve from 3.71552\n","Epoch 333/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7036 - val_loss: 0.1807\n","\n","Epoch 00333: loss improved from 3.71552 to 3.70366, saving model to final.h5\n","Epoch 334/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.6954 - val_loss: 0.1112\n","\n","Epoch 00334: loss improved from 3.70366 to 3.69539, saving model to final.h5\n","Epoch 335/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6270 - val_loss: 0.2086\n","\n","Epoch 00335: loss improved from 3.69539 to 3.62685, saving model to final.h5\n","Epoch 336/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7711 - val_loss: 0.1929\n","\n","Epoch 00336: loss did not improve from 3.62685\n","Epoch 337/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7882 - val_loss: 0.1842\n","\n","Epoch 00337: loss did not improve from 3.62685\n","Epoch 338/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7008 - val_loss: 0.1604\n","\n","Epoch 00338: loss did not improve from 3.62685\n","Epoch 339/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8400 - val_loss: 0.1705\n","\n","Epoch 00339: loss did not improve from 3.62685\n","Epoch 340/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7652 - val_loss: 0.1902\n","\n","Epoch 00340: loss did not improve from 3.62685\n","Epoch 341/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.8003 - val_loss: 0.1690\n","\n","Epoch 00341: loss did not improve from 3.62685\n","Epoch 342/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7526 - val_loss: 0.0990\n","\n","Epoch 00342: loss did not improve from 3.62685\n","Epoch 343/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.6712 - val_loss: 0.1959\n","\n","Epoch 00343: loss did not improve from 3.62685\n","Epoch 344/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7191 - val_loss: 0.0773\n","\n","Epoch 00344: loss did not improve from 3.62685\n","Epoch 345/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.8075 - val_loss: 0.1151\n","\n","Epoch 00345: loss did not improve from 3.62685\n","Epoch 346/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7596 - val_loss: 0.1012\n","\n","Epoch 00346: loss did not improve from 3.62685\n","Epoch 347/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7144 - val_loss: 0.1655\n","\n","Epoch 00347: loss did not improve from 3.62685\n","Epoch 348/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7730 - val_loss: 0.2266\n","\n","Epoch 00348: loss did not improve from 3.62685\n","Epoch 349/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.6732 - val_loss: 0.1408\n","\n","Epoch 00349: loss did not improve from 3.62685\n","Epoch 350/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7169 - val_loss: 0.1251\n","\n","Epoch 00350: loss did not improve from 3.62685\n","Epoch 351/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7254 - val_loss: 0.1828\n","\n","Epoch 00351: loss did not improve from 3.62685\n","Epoch 352/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7041 - val_loss: 0.2442\n","\n","Epoch 00352: loss did not improve from 3.62685\n","Epoch 353/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7162 - val_loss: 0.2140\n","\n","Epoch 00353: loss did not improve from 3.62685\n","Epoch 354/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7434 - val_loss: 0.3365\n","\n","Epoch 00354: loss did not improve from 3.62685\n","Epoch 355/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.6790 - val_loss: 0.1531\n","\n","Epoch 00355: loss did not improve from 3.62685\n","Epoch 356/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.6938 - val_loss: 0.1550\n","\n","Epoch 00356: loss did not improve from 3.62685\n","Epoch 357/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7685 - val_loss: 0.1467\n","\n","Epoch 00357: loss did not improve from 3.62685\n","Epoch 358/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7339 - val_loss: 0.1723\n","\n","Epoch 00358: loss did not improve from 3.62685\n","Epoch 359/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7089 - val_loss: 0.2939\n","\n","Epoch 00359: loss did not improve from 3.62685\n","Epoch 360/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7471 - val_loss: 0.1178\n","\n","Epoch 00360: loss did not improve from 3.62685\n","Epoch 361/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6575 - val_loss: 0.0792\n","\n","Epoch 00361: loss did not improve from 3.62685\n","Epoch 362/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.6861 - val_loss: 0.2025\n","\n","Epoch 00362: loss did not improve from 3.62685\n","Epoch 363/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6966 - val_loss: 0.0848\n","\n","Epoch 00363: loss did not improve from 3.62685\n","Epoch 364/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7013 - val_loss: 0.0924\n","\n","Epoch 00364: loss did not improve from 3.62685\n","Epoch 365/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6135 - val_loss: 0.1041\n","\n","Epoch 00365: loss improved from 3.62685 to 3.61325, saving model to final.h5\n","Epoch 366/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6672 - val_loss: 0.1334\n","\n","Epoch 00366: loss did not improve from 3.61325\n","Epoch 367/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6727 - val_loss: 0.2297\n","\n","Epoch 00367: loss did not improve from 3.61325\n","Epoch 368/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6381 - val_loss: 0.1371\n","\n","Epoch 00368: loss did not improve from 3.61325\n","Epoch 369/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7202 - val_loss: 0.1667\n","\n","Epoch 00369: loss did not improve from 3.61325\n","Epoch 370/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.7072 - val_loss: 0.1327\n","\n","Epoch 00370: loss did not improve from 3.61325\n","Epoch 371/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6355 - val_loss: 0.1233\n","\n","Epoch 00371: loss did not improve from 3.61325\n","Epoch 372/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5281 - val_loss: 0.1646\n","\n","Epoch 00372: loss improved from 3.61325 to 3.52808, saving model to final.h5\n","Epoch 373/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6060 - val_loss: 0.1306\n","\n","Epoch 00373: loss did not improve from 3.52808\n","Epoch 374/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6238 - val_loss: 0.1054\n","\n","Epoch 00374: loss did not improve from 3.52808\n","Epoch 375/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6467 - val_loss: 0.1481\n","\n","Epoch 00375: loss did not improve from 3.52808\n","Epoch 376/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.6361 - val_loss: 0.0891\n","\n","Epoch 00376: loss did not improve from 3.52808\n","Epoch 377/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6015 - val_loss: 0.1624\n","\n","Epoch 00377: loss did not improve from 3.52808\n","Epoch 378/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6964 - val_loss: 0.1438\n","\n","Epoch 00378: loss did not improve from 3.52808\n","Epoch 379/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6622 - val_loss: 0.0885\n","\n","Epoch 00379: loss did not improve from 3.52808\n","Epoch 380/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6618 - val_loss: 0.1562\n","\n","Epoch 00380: loss did not improve from 3.52808\n","Epoch 381/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5680 - val_loss: 0.1481\n","\n","Epoch 00381: loss did not improve from 3.52808\n","Epoch 382/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6768 - val_loss: 0.0481\n","\n","Epoch 00382: loss did not improve from 3.52808\n","Epoch 383/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.6566 - val_loss: 0.1238\n","\n","Epoch 00383: loss did not improve from 3.52808\n","Epoch 384/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.5880 - val_loss: 0.1185\n","\n","Epoch 00384: loss did not improve from 3.52808\n","Epoch 385/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6261 - val_loss: 0.1664\n","\n","Epoch 00385: loss did not improve from 3.52808\n","Epoch 386/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5856 - val_loss: 0.2545\n","\n","Epoch 00386: loss did not improve from 3.52808\n","Epoch 387/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5225 - val_loss: 0.1620\n","\n","Epoch 00387: loss improved from 3.52808 to 3.52249, saving model to final.h5\n","Epoch 388/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.7157 - val_loss: 0.1902\n","\n","Epoch 00388: loss did not improve from 3.52249\n","Epoch 389/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6012 - val_loss: 0.1292\n","\n","Epoch 00389: loss did not improve from 3.52249\n","Epoch 390/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6178 - val_loss: 0.3874\n","\n","Epoch 00390: loss did not improve from 3.52249\n","Epoch 391/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5866 - val_loss: 0.2183\n","\n","Epoch 00391: loss did not improve from 3.52249\n","Epoch 392/400\n","1632/1632 [==============================] - 9s 6ms/step - loss: 3.5554 - val_loss: 0.1233\n","\n","Epoch 00392: loss did not improve from 3.52249\n","Epoch 393/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5664 - val_loss: 0.2135\n","\n","Epoch 00393: loss did not improve from 3.52249\n","Epoch 394/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6713 - val_loss: 0.1267\n","\n","Epoch 00394: loss did not improve from 3.52249\n","Epoch 395/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6746 - val_loss: 0.1502\n","\n","Epoch 00395: loss did not improve from 3.52249\n","Epoch 396/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6822 - val_loss: 0.1841\n","\n","Epoch 00396: loss did not improve from 3.52249\n","Epoch 397/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5805 - val_loss: 0.4089\n","\n","Epoch 00397: loss did not improve from 3.52249\n","Epoch 398/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6522 - val_loss: 0.1130\n","\n","Epoch 00398: loss did not improve from 3.52249\n","Epoch 399/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.5911 - val_loss: 0.1071\n","\n","Epoch 00399: loss did not improve from 3.52249\n","Epoch 400/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 3.6858 - val_loss: 0.2512\n","\n","Epoch 00400: loss did not improve from 3.52249\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T6PzQ9kAW199","colab_type":"text"},"source":["Found the Optimum LR .. now adding augment"]},{"cell_type":"code","metadata":{"id":"JzQYPJmnOgJH","colab_type":"code","outputId":"c386f4f7-56da-4a0b-f331-fe60052ae349","executionInfo":{"status":"ok","timestamp":1590290247778,"user_tz":360,"elapsed":1810054,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet_final_with_LSTM_testtobereplaced.py train.mp4 train.txt --mode=train --split=0.2 --model final.h5 --epoch 400 --history 1 --LR 0.00001 --augment --wipe"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-24 02:46:50.394732: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet_final_with_LSTM_testtobereplaced.py:180: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-24 02:46:52.094287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-24 02:46:52.107095: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 02:46:52.107635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 02:46:52.107660: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 02:46:52.109339: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 02:46:52.111151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 02:46:52.111463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 02:46:52.113147: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 02:46:52.114278: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 02:46:52.118095: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 02:46:52.118208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 02:46:52.118787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 02:46:52.119293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 02:46:52.124394: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-05-24 02:46:52.124658: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d392c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 02:46:52.124703: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-24 02:46:52.214239: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 02:46:52.214927: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1d39480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 02:46:52.214958: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-24 02:46:52.215130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 02:46:52.215637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 02:46:52.215675: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 02:46:52.215715: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 02:46:52.215731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 02:46:52.215757: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 02:46:52.215778: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 02:46:52.215799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 02:46:52.215821: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 02:46:52.215911: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 02:46:52.216452: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 02:46:52.217039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 02:46:52.217112: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 02:46:52.711577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 02:46:52.711633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-24 02:46:52.711642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-24 02:46:52.711833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 02:46:52.712439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 02:46:52.712943: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-24 02:46:52.712981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet_final_with_LSTM_testtobereplaced.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","speednet_final_with_LSTM_testtobereplaced.py:185: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400frames\n","Done prepping data\n","20399\n","20399 Training data size per Aug\n","16319 Train indices size\n","4080 Val indices size\n","Starting training\n","shuffling\n","Epoch 1/400\n","2020-05-24 02:55:58.372051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 02:55:58.560388: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","1632/1632 [==============================] - 11s 7ms/step - loss: 142.9727 - val_loss: 91.1571\n","\n","Epoch 00001: loss improved from inf to 142.97585, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 72.2912 - val_loss: 165.9864\n","\n","Epoch 00002: loss improved from 142.97585 to 72.29148, saving model to final.h5\n","Epoch 3/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 63.9278 - val_loss: 123.6144\n","\n","Epoch 00003: loss improved from 72.29148 to 63.92721, saving model to final.h5\n","Epoch 4/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 53.8128 - val_loss: 77.0750\n","\n","Epoch 00004: loss improved from 63.92721 to 53.81279, saving model to final.h5\n","Epoch 5/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 45.3159 - val_loss: 32.3837\n","\n","Epoch 00005: loss improved from 53.81279 to 45.31631, saving model to final.h5\n","Epoch 6/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 40.6949 - val_loss: 21.8940\n","\n","Epoch 00006: loss improved from 45.31631 to 40.69514, saving model to final.h5\n","Epoch 7/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 37.2978 - val_loss: 24.4998\n","\n","Epoch 00007: loss improved from 40.69514 to 37.29717, saving model to final.h5\n","Epoch 8/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 35.3925 - val_loss: 15.7349\n","\n","Epoch 00008: loss improved from 37.29717 to 35.39300, saving model to final.h5\n","Epoch 9/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 33.5537 - val_loss: 28.9742\n","\n","Epoch 00009: loss improved from 35.39300 to 33.55258, saving model to final.h5\n","Epoch 10/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 31.8946 - val_loss: 26.2911\n","\n","Epoch 00010: loss improved from 33.55258 to 31.89541, saving model to final.h5\n","Epoch 11/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 30.2651 - val_loss: 18.2655\n","\n","Epoch 00011: loss improved from 31.89541 to 30.26532, saving model to final.h5\n","Epoch 12/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 29.9236 - val_loss: 29.2087\n","\n","Epoch 00012: loss improved from 30.26532 to 29.92350, saving model to final.h5\n","Epoch 13/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 29.2855 - val_loss: 31.7866\n","\n","Epoch 00013: loss improved from 29.92350 to 29.28582, saving model to final.h5\n","Epoch 14/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 27.9333 - val_loss: 25.3027\n","\n","Epoch 00014: loss improved from 29.28582 to 27.93289, saving model to final.h5\n","Epoch 15/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 27.2024 - val_loss: 37.9905\n","\n","Epoch 00015: loss improved from 27.93289 to 27.20215, saving model to final.h5\n","Epoch 16/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 26.6721 - val_loss: 47.1160\n","\n","Epoch 00016: loss improved from 27.20215 to 26.67212, saving model to final.h5\n","Epoch 17/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 25.6475 - val_loss: 54.6271\n","\n","Epoch 00017: loss improved from 26.67212 to 25.64751, saving model to final.h5\n","Epoch 18/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 25.1568 - val_loss: 56.8034\n","\n","Epoch 00018: loss improved from 25.64751 to 25.15792, saving model to final.h5\n","Epoch 19/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 24.9097 - val_loss: 64.6298\n","\n","Epoch 00019: loss improved from 25.15792 to 24.90961, saving model to final.h5\n","Epoch 20/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 24.5061 - val_loss: 50.5154\n","\n","Epoch 00020: loss improved from 24.90961 to 24.50591, saving model to final.h5\n","Epoch 21/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 23.4825 - val_loss: 46.8430\n","\n","Epoch 00021: loss improved from 24.50591 to 23.48118, saving model to final.h5\n","Epoch 22/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 23.5852 - val_loss: 48.0878\n","\n","Epoch 00022: loss did not improve from 23.48118\n","Epoch 23/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 22.8367 - val_loss: 66.3851\n","\n","Epoch 00023: loss improved from 23.48118 to 22.83701, saving model to final.h5\n","Epoch 24/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 22.2995 - val_loss: 69.4043\n","\n","Epoch 00024: loss improved from 22.83701 to 22.29975, saving model to final.h5\n","Epoch 25/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 21.7353 - val_loss: 58.1476\n","\n","Epoch 00025: loss improved from 22.29975 to 21.73475, saving model to final.h5\n","Epoch 26/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 21.5126 - val_loss: 74.1901\n","\n","Epoch 00026: loss improved from 21.73475 to 21.50889, saving model to final.h5\n","Epoch 27/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 21.0459 - val_loss: 86.9649\n","\n","Epoch 00027: loss improved from 21.50889 to 21.04448, saving model to final.h5\n","Epoch 28/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 20.9120 - val_loss: 66.2650\n","\n","Epoch 00028: loss improved from 21.04448 to 20.91251, saving model to final.h5\n","Epoch 29/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 20.5435 - val_loss: 75.6976\n","\n","Epoch 00029: loss improved from 20.91251 to 20.54439, saving model to final.h5\n","Epoch 30/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 19.8566 - val_loss: 83.9718\n","\n","Epoch 00030: loss improved from 20.54439 to 19.85692, saving model to final.h5\n","Epoch 31/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 20.0209 - val_loss: 75.9520\n","\n","Epoch 00031: loss did not improve from 19.85692\n","Epoch 32/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 19.5420 - val_loss: 81.7678\n","\n","Epoch 00032: loss improved from 19.85692 to 19.54146, saving model to final.h5\n","Epoch 33/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 19.3916 - val_loss: 87.8040\n","\n","Epoch 00033: loss improved from 19.54146 to 19.38981, saving model to final.h5\n","Epoch 34/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 18.3656 - val_loss: 93.1697\n","\n","Epoch 00034: loss improved from 19.38981 to 18.36567, saving model to final.h5\n","Epoch 35/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 18.1664 - val_loss: 85.9378\n","\n","Epoch 00035: loss improved from 18.36567 to 18.16716, saving model to final.h5\n","Epoch 36/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 18.3473 - val_loss: 91.6622\n","\n","Epoch 00036: loss did not improve from 18.16716\n","Epoch 37/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 17.8637 - val_loss: 93.0583\n","\n","Epoch 00037: loss improved from 18.16716 to 17.86440, saving model to final.h5\n","Epoch 38/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 17.7650 - val_loss: 96.2553\n","\n","Epoch 00038: loss improved from 17.86440 to 17.76580, saving model to final.h5\n","Epoch 39/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 17.2251 - val_loss: 89.5990\n","\n","Epoch 00039: loss improved from 17.76580 to 17.22435, saving model to final.h5\n","Epoch 40/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 17.1402 - val_loss: 72.0954\n","\n","Epoch 00040: loss improved from 17.22435 to 17.14059, saving model to final.h5\n","Epoch 41/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 16.7398 - val_loss: 80.0343\n","\n","Epoch 00041: loss improved from 17.14059 to 16.74031, saving model to final.h5\n","Epoch 42/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 16.5372 - val_loss: 53.2923\n","\n","Epoch 00042: loss improved from 16.74031 to 16.53654, saving model to final.h5\n","Epoch 43/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 16.0738 - val_loss: 79.8054\n","\n","Epoch 00043: loss improved from 16.53654 to 16.07398, saving model to final.h5\n","Epoch 44/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 16.0330 - val_loss: 85.9347\n","\n","Epoch 00044: loss improved from 16.07398 to 16.03233, saving model to final.h5\n","Epoch 45/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 15.9316 - val_loss: 54.6879\n","\n","Epoch 00045: loss improved from 16.03233 to 15.93202, saving model to final.h5\n","Epoch 46/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 15.4003 - val_loss: 77.9962\n","\n","Epoch 00046: loss improved from 15.93202 to 15.40111, saving model to final.h5\n","Epoch 47/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 15.1871 - val_loss: 70.4595\n","\n","Epoch 00047: loss improved from 15.40111 to 15.18732, saving model to final.h5\n","Epoch 48/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 15.1529 - val_loss: 57.8897\n","\n","Epoch 00048: loss improved from 15.18732 to 15.15294, saving model to final.h5\n","Epoch 49/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 14.7034 - val_loss: 55.6554\n","\n","Epoch 00049: loss improved from 15.15294 to 14.70323, saving model to final.h5\n","Epoch 50/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 14.4768 - val_loss: 61.8138\n","\n","Epoch 00050: loss improved from 14.70323 to 14.47661, saving model to final.h5\n","Epoch 51/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 14.7045 - val_loss: 58.9424\n","\n","Epoch 00051: loss did not improve from 14.47661\n","Epoch 52/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 14.3774 - val_loss: 48.4305\n","\n","Epoch 00052: loss improved from 14.47661 to 14.37722, saving model to final.h5\n","Epoch 53/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 14.2671 - val_loss: 51.2060\n","\n","Epoch 00053: loss improved from 14.37722 to 14.26716, saving model to final.h5\n","Epoch 54/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 14.2814 - val_loss: 33.0813\n","\n","Epoch 00054: loss did not improve from 14.26716\n","Epoch 55/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 13.5636 - val_loss: 48.4583\n","\n","Epoch 00055: loss improved from 14.26716 to 13.56391, saving model to final.h5\n","Epoch 56/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 13.6361 - val_loss: 44.4720\n","\n","Epoch 00056: loss did not improve from 13.56391\n","Epoch 57/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 13.4385 - val_loss: 50.9921\n","\n","Epoch 00057: loss improved from 13.56391 to 13.43911, saving model to final.h5\n","Epoch 58/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 13.1657 - val_loss: 50.0360\n","\n","Epoch 00058: loss improved from 13.43911 to 13.16436, saving model to final.h5\n","Epoch 59/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 13.2976 - val_loss: 50.0917\n","\n","Epoch 00059: loss did not improve from 13.16436\n","Epoch 60/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 13.1561 - val_loss: 53.7195\n","\n","Epoch 00060: loss improved from 13.16436 to 13.15634, saving model to final.h5\n","Epoch 61/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 12.6685 - val_loss: 56.6332\n","\n","Epoch 00061: loss improved from 13.15634 to 12.66746, saving model to final.h5\n","Epoch 62/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 13.0683 - val_loss: 46.8888\n","\n","Epoch 00062: loss did not improve from 12.66746\n","Epoch 63/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 12.6970 - val_loss: 49.6120\n","\n","Epoch 00063: loss did not improve from 12.66746\n","Epoch 64/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 12.5541 - val_loss: 56.2403\n","\n","Epoch 00064: loss improved from 12.66746 to 12.55327, saving model to final.h5\n","Epoch 65/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 12.5127 - val_loss: 57.1398\n","\n","Epoch 00065: loss improved from 12.55327 to 12.51320, saving model to final.h5\n","Epoch 66/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 12.2838 - val_loss: 50.0364\n","\n","Epoch 00066: loss improved from 12.51320 to 12.28425, saving model to final.h5\n","Epoch 67/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 12.1702 - val_loss: 36.7789\n","\n","Epoch 00067: loss improved from 12.28425 to 12.16983, saving model to final.h5\n","Epoch 68/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.9466 - val_loss: 47.4190\n","\n","Epoch 00068: loss improved from 12.16983 to 11.94673, saving model to final.h5\n","Epoch 69/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.9614 - val_loss: 39.2493\n","\n","Epoch 00069: loss did not improve from 11.94673\n","Epoch 70/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.4512 - val_loss: 42.9071\n","\n","Epoch 00070: loss improved from 11.94673 to 11.45154, saving model to final.h5\n","Epoch 71/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.5877 - val_loss: 47.2101\n","\n","Epoch 00071: loss did not improve from 11.45154\n","Epoch 72/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.5053 - val_loss: 41.3408\n","\n","Epoch 00072: loss did not improve from 11.45154\n","Epoch 73/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.4726 - val_loss: 38.6356\n","\n","Epoch 00073: loss did not improve from 11.45154\n","Epoch 74/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.7117 - val_loss: 39.2978\n","\n","Epoch 00074: loss did not improve from 11.45154\n","Epoch 75/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.1830 - val_loss: 42.8529\n","\n","Epoch 00075: loss improved from 11.45154 to 11.18309, saving model to final.h5\n","Epoch 76/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.2566 - val_loss: 36.6276\n","\n","Epoch 00076: loss did not improve from 11.18309\n","Epoch 77/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 11.0333 - val_loss: 37.0606\n","\n","Epoch 00077: loss improved from 11.18309 to 11.03329, saving model to final.h5\n","Epoch 78/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.8181 - val_loss: 33.1222\n","\n","Epoch 00078: loss improved from 11.03329 to 10.81797, saving model to final.h5\n","Epoch 79/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.7651 - val_loss: 34.6259\n","\n","Epoch 00079: loss improved from 10.81797 to 10.76549, saving model to final.h5\n","Epoch 80/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.8645 - val_loss: 41.3622\n","\n","Epoch 00080: loss did not improve from 10.76549\n","Epoch 81/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.5225 - val_loss: 34.7160\n","\n","Epoch 00081: loss improved from 10.76549 to 10.52288, saving model to final.h5\n","Epoch 82/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.2786 - val_loss: 33.4309\n","\n","Epoch 00082: loss improved from 10.52288 to 10.27867, saving model to final.h5\n","Epoch 83/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.4950 - val_loss: 33.2499\n","\n","Epoch 00083: loss did not improve from 10.27867\n","Epoch 84/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.4583 - val_loss: 30.6100\n","\n","Epoch 00084: loss did not improve from 10.27867\n","Epoch 85/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.3034 - val_loss: 46.9234\n","\n","Epoch 00085: loss did not improve from 10.27867\n","Epoch 86/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.0127 - val_loss: 42.3800\n","\n","Epoch 00086: loss improved from 10.27867 to 10.01260, saving model to final.h5\n","Epoch 87/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.1808 - val_loss: 45.4430\n","\n","Epoch 00087: loss did not improve from 10.01260\n","Epoch 88/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 10.0357 - val_loss: 45.1288\n","\n","Epoch 00088: loss did not improve from 10.01260\n","Epoch 89/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.8821 - val_loss: 51.4521\n","\n","Epoch 00089: loss improved from 10.01260 to 9.88220, saving model to final.h5\n","Epoch 90/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.9214 - val_loss: 42.0408\n","\n","Epoch 00090: loss did not improve from 9.88220\n","Epoch 91/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.7840 - val_loss: 35.5467\n","\n","Epoch 00091: loss improved from 9.88220 to 9.78377, saving model to final.h5\n","Epoch 92/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.4867 - val_loss: 36.3799\n","\n","Epoch 00092: loss improved from 9.78377 to 9.48672, saving model to final.h5\n","Epoch 93/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.4046 - val_loss: 37.6837\n","\n","Epoch 00093: loss improved from 9.48672 to 9.40417, saving model to final.h5\n","Epoch 94/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.6350 - val_loss: 48.4132\n","\n","Epoch 00094: loss did not improve from 9.40417\n","Epoch 95/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.5645 - val_loss: 49.5931\n","\n","Epoch 00095: loss did not improve from 9.40417\n","Epoch 96/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.3773 - val_loss: 48.7269\n","\n","Epoch 00096: loss improved from 9.40417 to 9.37714, saving model to final.h5\n","Epoch 97/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.2500 - val_loss: 46.4307\n","\n","Epoch 00097: loss improved from 9.37714 to 9.25009, saving model to final.h5\n","Epoch 98/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.3038 - val_loss: 54.1120\n","\n","Epoch 00098: loss did not improve from 9.25009\n","Epoch 99/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.0762 - val_loss: 40.7218\n","\n","Epoch 00099: loss improved from 9.25009 to 9.07657, saving model to final.h5\n","Epoch 100/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.2187 - val_loss: 60.6873\n","\n","Epoch 00100: loss did not improve from 9.07657\n","Epoch 101/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.1959 - val_loss: 48.2667\n","\n","Epoch 00101: loss did not improve from 9.07657\n","Epoch 102/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 9.0641 - val_loss: 56.3745\n","\n","Epoch 00102: loss improved from 9.07657 to 9.06383, saving model to final.h5\n","Epoch 103/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.9721 - val_loss: 48.0102\n","\n","Epoch 00103: loss improved from 9.06383 to 8.97212, saving model to final.h5\n","Epoch 104/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.8874 - val_loss: 48.4512\n","\n","Epoch 00104: loss improved from 8.97212 to 8.88768, saving model to final.h5\n","Epoch 105/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.9003 - val_loss: 49.0991\n","\n","Epoch 00105: loss did not improve from 8.88768\n","Epoch 106/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.6373 - val_loss: 50.3370\n","\n","Epoch 00106: loss improved from 8.88768 to 8.63736, saving model to final.h5\n","Epoch 107/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.7838 - val_loss: 54.0340\n","\n","Epoch 00107: loss did not improve from 8.63736\n","Epoch 108/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.6667 - val_loss: 49.7615\n","\n","Epoch 00108: loss did not improve from 8.63736\n","Epoch 109/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.5582 - val_loss: 43.4032\n","\n","Epoch 00109: loss improved from 8.63736 to 8.55813, saving model to final.h5\n","Epoch 110/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.4765 - val_loss: 31.2909\n","\n","Epoch 00110: loss improved from 8.55813 to 8.47653, saving model to final.h5\n","Epoch 111/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.5133 - val_loss: 40.8120\n","\n","Epoch 00111: loss did not improve from 8.47653\n","Epoch 112/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.4241 - val_loss: 53.5476\n","\n","Epoch 00112: loss improved from 8.47653 to 8.42431, saving model to final.h5\n","Epoch 113/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.5300 - val_loss: 60.6974\n","\n","Epoch 00113: loss did not improve from 8.42431\n","Epoch 114/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.1945 - val_loss: 42.1629\n","\n","Epoch 00114: loss improved from 8.42431 to 8.19395, saving model to final.h5\n","Epoch 115/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.2758 - val_loss: 47.6198\n","\n","Epoch 00115: loss did not improve from 8.19395\n","Epoch 116/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.3239 - val_loss: 40.3711\n","\n","Epoch 00116: loss did not improve from 8.19395\n","Epoch 117/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.1708 - val_loss: 58.3524\n","\n","Epoch 00117: loss improved from 8.19395 to 8.17063, saving model to final.h5\n","Epoch 118/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.0913 - val_loss: 52.4299\n","\n","Epoch 00118: loss improved from 8.17063 to 8.09143, saving model to final.h5\n","Epoch 119/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.1101 - val_loss: 53.7943\n","\n","Epoch 00119: loss did not improve from 8.09143\n","Epoch 120/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.9261 - val_loss: 49.5520\n","\n","Epoch 00120: loss improved from 8.09143 to 7.92645, saving model to final.h5\n","Epoch 121/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 8.1051 - val_loss: 45.2322\n","\n","Epoch 00121: loss did not improve from 7.92645\n","Epoch 122/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.9946 - val_loss: 51.3424\n","\n","Epoch 00122: loss did not improve from 7.92645\n","Epoch 123/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.7052 - val_loss: 52.7503\n","\n","Epoch 00123: loss improved from 7.92645 to 7.70501, saving model to final.h5\n","Epoch 124/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.9412 - val_loss: 50.5432\n","\n","Epoch 00124: loss did not improve from 7.70501\n","Epoch 125/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.8465 - val_loss: 46.5299\n","\n","Epoch 00125: loss did not improve from 7.70501\n","Epoch 126/400\n","1632/1632 [==============================] - 10s 6ms/step - loss: 7.8677 - val_loss: 46.7352\n","\n","Epoch 00126: loss did not improve from 7.70501\n","Epoch 127/400\n","1615/1632 [============================>.] - ETA: 0s - loss: 7.7548Traceback (most recent call last):\n","  File \"speednet_final_with_LSTM_testtobereplaced.py\", line 346, in <module>\n","    net.main(args)\n","  File \"speednet_final_with_LSTM_testtobereplaced.py\", line 60, in main\n","    self.train(args.video_file,args.speed_file,args.wipe,self.EPOCHS,self.BATCH_SIZE,args.augment)\n","  File \"speednet_final_with_LSTM_testtobereplaced.py\", line 260, in train\n","    callbacks=[checkpoint])\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1732, in fit_generator\n","    initial_epoch=initial_epoch)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\", line 220, in fit_generator\n","    reset_metrics=False)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1514, in train_on_batch\n","    outputs = self.train_function(ins)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\", line 3792, in __call__\n","    outputs = self._graph_fn(*converted_inputs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1605, in __call__\n","    return self._call_impl(args, kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1645, in _call_impl\n","    return self._call_flat(args, self.captured_inputs, cancellation_manager)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\n","    ctx, args, cancellation_manager=cancellation_manager))\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 598, in call\n","    ctx=ctx)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n","    inputs, attrs, num_outputs)\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sHDw5qshdkXb","colab_type":"text"},"source":["Changing the history"]},{"cell_type":"code","metadata":{"id":"L_BlVa1MpJ70","colab_type":"code","outputId":"d2449e07-a6ea-44a5-9d2f-6630f890eaba","executionInfo":{"status":"ok","timestamp":1590292095541,"user_tz":360,"elapsed":1843683,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet_final_with_LSTM_testtobereplaced.py train.mp4 train.txt --mode=train --split=0.2 --model final.h5 --epoch 400 --history 2 --LR 0.00001 --wipe"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-24 03:17:04.022029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet_final_with_LSTM_testtobereplaced.py:180: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-24 03:17:05.708382: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-24 03:17:05.721347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:17:05.721902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 03:17:05.721941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 03:17:05.723514: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 03:17:05.725346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 03:17:05.725650: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 03:17:05.727326: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 03:17:05.728425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 03:17:05.732199: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 03:17:05.732306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:17:05.732838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:17:05.733390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 03:17:05.738603: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-05-24 03:17:05.738828: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2fab2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 03:17:05.738858: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-24 03:17:05.827897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:17:05.828540: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2fab480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 03:17:05.828568: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-24 03:17:05.828731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:17:05.829280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 03:17:05.829323: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 03:17:05.829370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 03:17:05.829386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 03:17:05.829407: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 03:17:05.829427: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 03:17:05.829442: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 03:17:05.829456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 03:17:05.829517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:17:05.830072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:17:05.830561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 03:17:05.830640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 03:17:06.333060: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 03:17:06.333114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-24 03:17:06.333123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-24 03:17:06.333401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:17:06.334086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:17:06.334649: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-24 03:17:06.334686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet_final_with_LSTM_testtobereplaced.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","speednet_final_with_LSTM_testtobereplaced.py:185: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400frames\n","Done prepping data\n","20399\n","20398 Training data size per Aug\n","16318 Train indices size\n","4080 Val indices size\n","Starting training\n","shuffling\n","Epoch 1/400\n","2020-05-24 03:20:07.328035: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 03:20:07.512277: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","1632/1632 [==============================] - 15s 9ms/step - loss: 146.8096 - val_loss: 105.8120\n","\n","Epoch 00001: loss improved from inf to 146.79036, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 68.4419 - val_loss: 153.5058\n","\n","Epoch 00002: loss improved from 146.79036 to 68.44425, saving model to final.h5\n","Epoch 3/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 52.4888 - val_loss: 36.5902\n","\n","Epoch 00003: loss improved from 68.44425 to 52.48841, saving model to final.h5\n","Epoch 4/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 39.2254 - val_loss: 10.8277\n","\n","Epoch 00004: loss improved from 52.48841 to 39.22788, saving model to final.h5\n","Epoch 5/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 34.4318 - val_loss: 8.7878\n","\n","Epoch 00005: loss improved from 39.22788 to 34.43494, saving model to final.h5\n","Epoch 6/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 30.8832 - val_loss: 9.3519\n","\n","Epoch 00006: loss improved from 34.43494 to 30.88526, saving model to final.h5\n","Epoch 7/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 29.0054 - val_loss: 8.2101\n","\n","Epoch 00007: loss improved from 30.88526 to 29.00475, saving model to final.h5\n","Epoch 8/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 27.4666 - val_loss: 18.3244\n","\n","Epoch 00008: loss improved from 29.00475 to 27.46667, saving model to final.h5\n","Epoch 9/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 26.4927 - val_loss: 21.3148\n","\n","Epoch 00009: loss improved from 27.46667 to 26.49358, saving model to final.h5\n","Epoch 10/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 25.0955 - val_loss: 17.9750\n","\n","Epoch 00010: loss improved from 26.49358 to 25.09675, saving model to final.h5\n","Epoch 11/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 24.2216 - val_loss: 26.1753\n","\n","Epoch 00011: loss improved from 25.09675 to 24.21949, saving model to final.h5\n","Epoch 12/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 23.2344 - val_loss: 24.0658\n","\n","Epoch 00012: loss improved from 24.21949 to 23.23201, saving model to final.h5\n","Epoch 13/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 22.3062 - val_loss: 38.1037\n","\n","Epoch 00013: loss improved from 23.23201 to 22.30774, saving model to final.h5\n","Epoch 14/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 21.9383 - val_loss: 50.4287\n","\n","Epoch 00014: loss improved from 22.30774 to 21.93698, saving model to final.h5\n","Epoch 15/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 20.5620 - val_loss: 62.1635\n","\n","Epoch 00015: loss improved from 21.93698 to 20.56133, saving model to final.h5\n","Epoch 16/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 19.8571 - val_loss: 92.4592\n","\n","Epoch 00016: loss improved from 20.56133 to 19.85877, saving model to final.h5\n","Epoch 17/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 19.0433 - val_loss: 61.4749\n","\n","Epoch 00017: loss improved from 19.85877 to 19.04458, saving model to final.h5\n","Epoch 18/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 18.4240 - val_loss: 55.5519\n","\n","Epoch 00018: loss improved from 19.04458 to 18.42528, saving model to final.h5\n","Epoch 19/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 18.1342 - val_loss: 62.8214\n","\n","Epoch 00019: loss improved from 18.42528 to 18.13567, saving model to final.h5\n","Epoch 20/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 17.8966 - val_loss: 45.9098\n","\n","Epoch 00020: loss improved from 18.13567 to 17.89483, saving model to final.h5\n","Epoch 21/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 17.0813 - val_loss: 82.7618\n","\n","Epoch 00021: loss improved from 17.89483 to 17.08258, saving model to final.h5\n","Epoch 22/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 16.4451 - val_loss: 74.7826\n","\n","Epoch 00022: loss improved from 17.08258 to 16.44642, saving model to final.h5\n","Epoch 23/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 16.2317 - val_loss: 55.5516\n","\n","Epoch 00023: loss improved from 16.44642 to 16.23205, saving model to final.h5\n","Epoch 24/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 15.7616 - val_loss: 60.3676\n","\n","Epoch 00024: loss improved from 16.23205 to 15.76246, saving model to final.h5\n","Epoch 25/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 15.1345 - val_loss: 54.1455\n","\n","Epoch 00025: loss improved from 15.76246 to 15.13362, saving model to final.h5\n","Epoch 26/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 14.9701 - val_loss: 56.2608\n","\n","Epoch 00026: loss improved from 15.13362 to 14.97148, saving model to final.h5\n","Epoch 27/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 14.5579 - val_loss: 42.2993\n","\n","Epoch 00027: loss improved from 14.97148 to 14.55860, saving model to final.h5\n","Epoch 28/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 14.2208 - val_loss: 52.1265\n","\n","Epoch 00028: loss improved from 14.55860 to 14.22045, saving model to final.h5\n","Epoch 29/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 13.9828 - val_loss: 56.7621\n","\n","Epoch 00029: loss improved from 14.22045 to 13.97968, saving model to final.h5\n","Epoch 30/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 13.5268 - val_loss: 48.8573\n","\n","Epoch 00030: loss improved from 13.97968 to 13.52714, saving model to final.h5\n","Epoch 31/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 13.4353 - val_loss: 48.5598\n","\n","Epoch 00031: loss improved from 13.52714 to 13.43556, saving model to final.h5\n","Epoch 32/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 13.0264 - val_loss: 29.8678\n","\n","Epoch 00032: loss improved from 13.43556 to 13.02688, saving model to final.h5\n","Epoch 33/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 12.5269 - val_loss: 37.3290\n","\n","Epoch 00033: loss improved from 13.02688 to 12.52546, saving model to final.h5\n","Epoch 34/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 12.5895 - val_loss: 34.6487\n","\n","Epoch 00034: loss did not improve from 12.52546\n","Epoch 35/400\n","1632/1632 [==============================] - 14s 9ms/step - loss: 12.2572 - val_loss: 41.1864\n","\n","Epoch 00035: loss improved from 12.52546 to 12.25788, saving model to final.h5\n","Epoch 36/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 11.9589 - val_loss: 29.1144\n","\n","Epoch 00036: loss improved from 12.25788 to 11.95849, saving model to final.h5\n","Epoch 37/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 11.7305 - val_loss: 37.5397\n","\n","Epoch 00037: loss improved from 11.95849 to 11.73017, saving model to final.h5\n","Epoch 38/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 11.3544 - val_loss: 41.5119\n","\n","Epoch 00038: loss improved from 11.73017 to 11.35398, saving model to final.h5\n","Epoch 39/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 11.3420 - val_loss: 40.0433\n","\n","Epoch 00039: loss improved from 11.35398 to 11.34242, saving model to final.h5\n","Epoch 40/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 11.2652 - val_loss: 31.3738\n","\n","Epoch 00040: loss improved from 11.34242 to 11.26595, saving model to final.h5\n","Epoch 41/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 10.8667 - val_loss: 30.4500\n","\n","Epoch 00041: loss improved from 11.26595 to 10.86647, saving model to final.h5\n","Epoch 42/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 10.7598 - val_loss: 23.8318\n","\n","Epoch 00042: loss improved from 10.86647 to 10.76011, saving model to final.h5\n","Epoch 43/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 10.5250 - val_loss: 23.2740\n","\n","Epoch 00043: loss improved from 10.76011 to 10.52486, saving model to final.h5\n","Epoch 44/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 10.2249 - val_loss: 34.0308\n","\n","Epoch 00044: loss improved from 10.52486 to 10.22504, saving model to final.h5\n","Epoch 45/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 10.0944 - val_loss: 23.4462\n","\n","Epoch 00045: loss improved from 10.22504 to 10.09462, saving model to final.h5\n","Epoch 46/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 9.9894 - val_loss: 28.8384\n","\n","Epoch 00046: loss improved from 10.09462 to 9.98947, saving model to final.h5\n","Epoch 47/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 9.7177 - val_loss: 25.0989\n","\n","Epoch 00047: loss improved from 9.98947 to 9.71785, saving model to final.h5\n","Epoch 48/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 9.8871 - val_loss: 25.1693\n","\n","Epoch 00048: loss did not improve from 9.71785\n","Epoch 49/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 9.4356 - val_loss: 15.2520\n","\n","Epoch 00049: loss improved from 9.71785 to 9.43648, saving model to final.h5\n","Epoch 50/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 9.3614 - val_loss: 29.8047\n","\n","Epoch 00050: loss improved from 9.43648 to 9.36104, saving model to final.h5\n","Epoch 51/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 9.5081 - val_loss: 41.5477\n","\n","Epoch 00051: loss did not improve from 9.36104\n","Epoch 52/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 9.1055 - val_loss: 31.7792\n","\n","Epoch 00052: loss improved from 9.36104 to 9.10569, saving model to final.h5\n","Epoch 53/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 8.9062 - val_loss: 29.6031\n","\n","Epoch 00053: loss improved from 9.10569 to 8.90698, saving model to final.h5\n","Epoch 54/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 8.7448 - val_loss: 25.6296\n","\n","Epoch 00054: loss improved from 8.90698 to 8.74370, saving model to final.h5\n","Epoch 55/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 8.6925 - val_loss: 53.7134\n","\n","Epoch 00055: loss improved from 8.74370 to 8.69293, saving model to final.h5\n","Epoch 56/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 8.7357 - val_loss: 26.7346\n","\n","Epoch 00056: loss did not improve from 8.69293\n","Epoch 57/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 8.6485 - val_loss: 31.1709\n","\n","Epoch 00057: loss improved from 8.69293 to 8.64839, saving model to final.h5\n","Epoch 58/400\n","1632/1632 [==============================] - 14s 9ms/step - loss: 8.4660 - val_loss: 25.0484\n","\n","Epoch 00058: loss improved from 8.64839 to 8.46490, saving model to final.h5\n","Epoch 59/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 8.3224 - val_loss: 28.2728\n","\n","Epoch 00059: loss improved from 8.46490 to 8.32253, saving model to final.h5\n","Epoch 60/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 8.3790 - val_loss: 36.5751\n","\n","Epoch 00060: loss did not improve from 8.32253\n","Epoch 61/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 8.1906 - val_loss: 36.1768\n","\n","Epoch 00061: loss improved from 8.32253 to 8.19046, saving model to final.h5\n","Epoch 62/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.9947 - val_loss: 55.7843\n","\n","Epoch 00062: loss improved from 8.19046 to 7.99372, saving model to final.h5\n","Epoch 63/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 8.0011 - val_loss: 53.1629\n","\n","Epoch 00063: loss did not improve from 7.99372\n","Epoch 64/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.8847 - val_loss: 33.1834\n","\n","Epoch 00064: loss improved from 7.99372 to 7.88529, saving model to final.h5\n","Epoch 65/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 7.8496 - val_loss: 34.7574\n","\n","Epoch 00065: loss improved from 7.88529 to 7.85040, saving model to final.h5\n","Epoch 66/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.5775 - val_loss: 33.0958\n","\n","Epoch 00066: loss improved from 7.85040 to 7.57780, saving model to final.h5\n","Epoch 67/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.5455 - val_loss: 43.3616\n","\n","Epoch 00067: loss improved from 7.57780 to 7.54592, saving model to final.h5\n","Epoch 68/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.5974 - val_loss: 35.3502\n","\n","Epoch 00068: loss did not improve from 7.54592\n","Epoch 69/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.3585 - val_loss: 39.7187\n","\n","Epoch 00069: loss improved from 7.54592 to 7.35683, saving model to final.h5\n","Epoch 70/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.3531 - val_loss: 34.3356\n","\n","Epoch 00070: loss improved from 7.35683 to 7.35301, saving model to final.h5\n","Epoch 71/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.5058 - val_loss: 29.5666\n","\n","Epoch 00071: loss did not improve from 7.35301\n","Epoch 72/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.1734 - val_loss: 39.2086\n","\n","Epoch 00072: loss improved from 7.35301 to 7.17337, saving model to final.h5\n","Epoch 73/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.1386 - val_loss: 43.8063\n","\n","Epoch 00073: loss improved from 7.17337 to 7.13935, saving model to final.h5\n","Epoch 74/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.0644 - val_loss: 37.9776\n","\n","Epoch 00074: loss improved from 7.13935 to 7.06422, saving model to final.h5\n","Epoch 75/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.0136 - val_loss: 48.6173\n","\n","Epoch 00075: loss improved from 7.06422 to 7.01417, saving model to final.h5\n","Epoch 76/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.0142 - val_loss: 38.5764\n","\n","Epoch 00076: loss improved from 7.01417 to 7.01271, saving model to final.h5\n","Epoch 77/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 7.1052 - val_loss: 44.5039\n","\n","Epoch 00077: loss did not improve from 7.01271\n","Epoch 78/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.9832 - val_loss: 43.3731\n","\n","Epoch 00078: loss improved from 7.01271 to 6.98332, saving model to final.h5\n","Epoch 79/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.8257 - val_loss: 50.7006\n","\n","Epoch 00079: loss improved from 6.98332 to 6.82559, saving model to final.h5\n","Epoch 80/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.9418 - val_loss: 55.2584\n","\n","Epoch 00080: loss did not improve from 6.82559\n","Epoch 81/400\n","1632/1632 [==============================] - 14s 9ms/step - loss: 6.7766 - val_loss: 55.6264\n","\n","Epoch 00081: loss improved from 6.82559 to 6.77644, saving model to final.h5\n","Epoch 82/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.7615 - val_loss: 43.7872\n","\n","Epoch 00082: loss improved from 6.77644 to 6.76150, saving model to final.h5\n","Epoch 83/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.4597 - val_loss: 43.9249\n","\n","Epoch 00083: loss improved from 6.76150 to 6.46008, saving model to final.h5\n","Epoch 84/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.7367 - val_loss: 42.0784\n","\n","Epoch 00084: loss did not improve from 6.46008\n","Epoch 85/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.5616 - val_loss: 39.7530\n","\n","Epoch 00085: loss did not improve from 6.46008\n","Epoch 86/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.4400 - val_loss: 39.3874\n","\n","Epoch 00086: loss improved from 6.46008 to 6.44038, saving model to final.h5\n","Epoch 87/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.3973 - val_loss: 44.4815\n","\n","Epoch 00087: loss improved from 6.44038 to 6.39663, saving model to final.h5\n","Epoch 88/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.3587 - val_loss: 42.2798\n","\n","Epoch 00088: loss improved from 6.39663 to 6.35864, saving model to final.h5\n","Epoch 89/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.2969 - val_loss: 38.8048\n","\n","Epoch 00089: loss improved from 6.35864 to 6.29736, saving model to final.h5\n","Epoch 90/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.3588 - val_loss: 60.7090\n","\n","Epoch 00090: loss did not improve from 6.29736\n","Epoch 91/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.4277 - val_loss: 37.6293\n","\n","Epoch 00091: loss did not improve from 6.29736\n","Epoch 92/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.4000 - val_loss: 53.3768\n","\n","Epoch 00092: loss did not improve from 6.29736\n","Epoch 93/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.0971 - val_loss: 52.4794\n","\n","Epoch 00093: loss improved from 6.29736 to 6.09747, saving model to final.h5\n","Epoch 94/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.2554 - val_loss: 40.8003\n","\n","Epoch 00094: loss did not improve from 6.09747\n","Epoch 95/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.0886 - val_loss: 47.2075\n","\n","Epoch 00095: loss improved from 6.09747 to 6.08797, saving model to final.h5\n","Epoch 96/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.1601 - val_loss: 61.4408\n","\n","Epoch 00096: loss did not improve from 6.08797\n","Epoch 97/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 6.0799 - val_loss: 55.4212\n","\n","Epoch 00097: loss improved from 6.08797 to 6.07944, saving model to final.h5\n","Epoch 98/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.0933 - val_loss: 48.3286\n","\n","Epoch 00098: loss did not improve from 6.07944\n","Epoch 99/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.0014 - val_loss: 38.2275\n","\n","Epoch 00099: loss improved from 6.07944 to 6.00151, saving model to final.h5\n","Epoch 100/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 6.0307 - val_loss: 47.1503\n","\n","Epoch 00100: loss did not improve from 6.00151\n","Epoch 101/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.9978 - val_loss: 43.5480\n","\n","Epoch 00101: loss improved from 6.00151 to 5.99590, saving model to final.h5\n","Epoch 102/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.8642 - val_loss: 53.2736\n","\n","Epoch 00102: loss improved from 5.99590 to 5.86465, saving model to final.h5\n","Epoch 103/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.7378 - val_loss: 49.0755\n","\n","Epoch 00103: loss improved from 5.86465 to 5.73658, saving model to final.h5\n","Epoch 104/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 5.7883 - val_loss: 53.7358\n","\n","Epoch 00104: loss did not improve from 5.73658\n","Epoch 105/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 5.7224 - val_loss: 64.8347\n","\n","Epoch 00105: loss improved from 5.73658 to 5.72269, saving model to final.h5\n","Epoch 106/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.7714 - val_loss: 54.2620\n","\n","Epoch 00106: loss did not improve from 5.72269\n","Epoch 107/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.8679 - val_loss: 53.0937\n","\n","Epoch 00107: loss did not improve from 5.72269\n","Epoch 108/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.6152 - val_loss: 49.2431\n","\n","Epoch 00108: loss improved from 5.72269 to 5.61534, saving model to final.h5\n","Epoch 109/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.7018 - val_loss: 60.2098\n","\n","Epoch 00109: loss did not improve from 5.61534\n","Epoch 110/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.7466 - val_loss: 52.1661\n","\n","Epoch 00110: loss did not improve from 5.61534\n","Epoch 111/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.5596 - val_loss: 42.5131\n","\n","Epoch 00111: loss improved from 5.61534 to 5.55990, saving model to final.h5\n","Epoch 112/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.4963 - val_loss: 46.9284\n","\n","Epoch 00112: loss improved from 5.55990 to 5.49645, saving model to final.h5\n","Epoch 113/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 5.5354 - val_loss: 57.7393\n","\n","Epoch 00113: loss did not improve from 5.49645\n","Epoch 114/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.5409 - val_loss: 43.9970\n","\n","Epoch 00114: loss did not improve from 5.49645\n","Epoch 115/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.5047 - val_loss: 40.3885\n","\n","Epoch 00115: loss did not improve from 5.49645\n","Epoch 116/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.4797 - val_loss: 39.1455\n","\n","Epoch 00116: loss improved from 5.49645 to 5.47987, saving model to final.h5\n","Epoch 117/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.4260 - val_loss: 51.8745\n","\n","Epoch 00117: loss improved from 5.47987 to 5.42584, saving model to final.h5\n","Epoch 118/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.3880 - val_loss: 41.6905\n","\n","Epoch 00118: loss improved from 5.42584 to 5.38847, saving model to final.h5\n","Epoch 119/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 5.4151 - val_loss: 38.1361\n","\n","Epoch 00119: loss did not improve from 5.38847\n","Epoch 120/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 5.3844 - val_loss: 48.2053\n","\n","Epoch 00120: loss improved from 5.38847 to 5.38460, saving model to final.h5\n","Epoch 121/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 5.3958 - val_loss: 58.5188\n","\n","Epoch 00121: loss did not improve from 5.38460\n","Epoch 122/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.3741 - val_loss: 51.3950\n","\n","Epoch 00122: loss improved from 5.38460 to 5.37404, saving model to final.h5\n","Epoch 123/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.3308 - val_loss: 53.3494\n","\n","Epoch 00123: loss improved from 5.37404 to 5.33024, saving model to final.h5\n","Epoch 124/400\n"," 454/1632 [=======>......................] - ETA: 8s - loss: 5.3313Traceback (most recent call last):\n","  File \"speednet_final_with_LSTM_testtobereplaced.py\", line 346, in <module>\n","    net.main(args)\n","  File \"speednet_final_with_LSTM_testtobereplaced.py\", line 60, in main\n","    self.train(args.video_file,args.speed_file,args.wipe,self.EPOCHS,self.BATCH_SIZE,args.augment)\n","  File \"speednet_final_with_LSTM_testtobereplaced.py\", line 260, in train\n","    callbacks=[checkpoint])\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1732, in fit_generator\n","    initial_epoch=initial_epoch)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\", line 220, in fit_generator\n","    reset_metrics=False)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1514, in train_on_batch\n","    outputs = self.train_function(ins)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\", line 3800, in __call__\n","    [x._numpy() for x in outputs],  # pylint: disable=protected-access\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\", line 3800, in <listcomp>\n","    [x._numpy() for x in outputs],  # pylint: disable=protected-access\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 927, in _numpy\n","    return self._numpy_internal()\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XqRTHeJ_tFwE","colab_type":"code","outputId":"9657794d-0623-462b-fe59-fe52961f2332","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speednet_final_with_LSTM_testtobereplaced.py train.mp4 train.txt --mode=train --split=0.2 --model final.h5 --epoch 400 --history 2 --LR 0.00001 --resume --augment --wipe"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-24 03:47:49.664780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speednet_final_with_LSTM_testtobereplaced.py:180: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-24 03:47:51.355038: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-24 03:47:51.367747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:47:51.368327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 03:47:51.368359: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 03:47:51.369920: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 03:47:51.371703: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 03:47:51.372013: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 03:47:51.373571: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 03:47:51.374641: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 03:47:51.378196: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 03:47:51.378298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:47:51.378823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:47:51.379312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 03:47:51.384278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-05-24 03:47:51.384552: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14cd2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 03:47:51.384580: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-24 03:47:51.471695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:47:51.472361: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14cd480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 03:47:51.472392: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-24 03:47:51.472557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:47:51.473197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 03:47:51.473249: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 03:47:51.473291: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 03:47:51.473307: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 03:47:51.473320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 03:47:51.473336: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 03:47:51.473349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 03:47:51.473364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 03:47:51.473427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:47:51.473963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:47:51.474443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 03:47:51.474485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 03:47:51.967746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 03:47:51.967803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-24 03:47:51.967813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-24 03:47:51.968033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:47:51.968618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 03:47:51.969117: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-24 03:47:51.969153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speednet_final_with_LSTM_testtobereplaced.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","speednet_final_with_LSTM_testtobereplaced.py:185: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","loading weights\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400frames\n","Done prepping data\n","20399\n","20398 Training data size per Aug\n","16318 Train indices size\n","4080 Val indices size\n","Starting training\n","shuffling\n","Epoch 1/400\n","2020-05-24 03:56:51.890422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 03:56:52.078540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","1632/1632 [==============================] - 15s 9ms/step - loss: 6.0705 - val_loss: 46.5451\n","\n","Epoch 00001: loss improved from inf to 6.07015, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 6.0631 - val_loss: 45.7601\n","\n","Epoch 00002: loss improved from 6.07015 to 6.06323, saving model to final.h5\n","Epoch 3/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 5.9074 - val_loss: 53.0204\n","\n","Epoch 00003: loss improved from 6.06323 to 5.90747, saving model to final.h5\n","Epoch 4/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.8692 - val_loss: 46.0143\n","\n","Epoch 00004: loss improved from 5.90747 to 5.86942, saving model to final.h5\n","Epoch 5/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.9193 - val_loss: 44.7019\n","\n","Epoch 00005: loss did not improve from 5.86942\n","Epoch 6/400\n","1632/1632 [==============================] - 14s 8ms/step - loss: 5.7805 - val_loss: 54.7546\n","\n","Epoch 00006: loss improved from 5.86942 to 5.78075, saving model to final.h5\n","Epoch 7/400\n","1632/1632 [==============================] - 13s 8ms/step - loss: 5.7189 - val_loss: 60.5656\n","\n","Epoch 00007: loss improved from 5.78075 to 5.71939, saving model to final.h5\n","Epoch 8/400\n","1465/1632 [=========================>....] - ETA: 1s - loss: 5.5798"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ez66TUzil8vP","colab_type":"text"},"source":["Fixing overfiting adding batch normalisation layers"]},{"cell_type":"code","metadata":{"id":"coqcnSiGmBl9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"a1a1f16b-5dbb-4a2b-a4f7-045ec9b36244","executionInfo":{"status":"ok","timestamp":1590309041821,"user_tz":360,"elapsed":3039462,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}}},"source":["!python speedchallenge.py train.mp4 train.txt --epoch 200 --history 1 --model final.h5 --split_start 7700 --split_end=12100 --LR 0.00001 --mode=train"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-24 07:39:34.220320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-24 07:39:35.998770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-24 07:39:36.011742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 07:39:36.012452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 07:39:36.012487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 07:39:36.014111: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 07:39:36.015904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 07:39:36.016262: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 07:39:36.017756: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 07:39:36.018751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 07:39:36.022203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 07:39:36.022352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 07:39:36.022982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 07:39:36.023485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 07:39:36.028721: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz\n","2020-05-24 07:39:36.028975: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28ef2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 07:39:36.029004: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-24 07:39:36.119797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 07:39:36.120700: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28ef480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 07:39:36.120737: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-24 07:39:36.121012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 07:39:36.121588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 07:39:36.121635: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 07:39:36.121674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 07:39:36.121692: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 07:39:36.121708: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 07:39:36.121754: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 07:39:36.121766: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 07:39:36.121780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 07:39:36.121874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 07:39:36.122496: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 07:39:36.123056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 07:39:36.123120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 07:39:36.652638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 07:39:36.652699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-24 07:39:36.652708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-24 07:39:36.652970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 07:39:36.653607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 07:39:36.654188: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-24 07:39:36.654246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:188: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","speedchallenge.py:193: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400frames\n","Done prepping data\n","20399\n","20399 Training data size per Aug\n","15999 Train indices size\n","4400 Val indices size\n"," This is the range of train:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119, 6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129, 6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139, 6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169, 6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209, 6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229, 6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239, 6240, 6241, 6242, 6243, 6244, 6245, 6246, 6247, 6248, 6249, 6250, 6251, 6252, 6253, 6254, 6255, 6256, 6257, 6258, 6259, 6260, 6261, 6262, 6263, 6264, 6265, 6266, 6267, 6268, 6269, 6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289, 6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299, 6300, 6301, 6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309, 6310, 6311, 6312, 6313, 6314, 6315, 6316, 6317, 6318, 6319, 6320, 6321, 6322, 6323, 6324, 6325, 6326, 6327, 6328, 6329, 6330, 6331, 6332, 6333, 6334, 6335, 6336, 6337, 6338, 6339, 6340, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349, 6350, 6351, 6352, 6353, 6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363, 6364, 6365, 6366, 6367, 6368, 6369, 6370, 6371, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6379, 6380, 6381, 6382, 6383, 6384, 6385, 6386, 6387, 6388, 6389, 6390, 6391, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399, 6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419, 6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429, 6430, 6431, 6432, 6433, 6434, 6435, 6436, 6437, 6438, 6439, 6440, 6441, 6442, 6443, 6444, 6445, 6446, 6447, 6448, 6449, 6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459, 6460, 6461, 6462, 6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473, 6474, 6475, 6476, 6477, 6478, 6479, 6480, 6481, 6482, 6483, 6484, 6485, 6486, 6487, 6488, 6489, 6490, 6491, 6492, 6493, 6494, 6495, 6496, 6497, 6498, 6499, 6500, 6501, 6502, 6503, 6504, 6505, 6506, 6507, 6508, 6509, 6510, 6511, 6512, 6513, 6514, 6515, 6516, 6517, 6518, 6519, 6520, 6521, 6522, 6523, 6524, 6525, 6526, 6527, 6528, 6529, 6530, 6531, 6532, 6533, 6534, 6535, 6536, 6537, 6538, 6539, 6540, 6541, 6542, 6543, 6544, 6545, 6546, 6547, 6548, 6549, 6550, 6551, 6552, 6553, 6554, 6555, 6556, 6557, 6558, 6559, 6560, 6561, 6562, 6563, 6564, 6565, 6566, 6567, 6568, 6569, 6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577, 6578, 6579, 6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588, 6589, 6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599, 6600, 6601, 6602, 6603, 6604, 6605, 6606, 6607, 6608, 6609, 6610, 6611, 6612, 6613, 6614, 6615, 6616, 6617, 6618, 6619, 6620, 6621, 6622, 6623, 6624, 6625, 6626, 6627, 6628, 6629, 6630, 6631, 6632, 6633, 6634, 6635, 6636, 6637, 6638, 6639, 6640, 6641, 6642, 6643, 6644, 6645, 6646, 6647, 6648, 6649, 6650, 6651, 6652, 6653, 6654, 6655, 6656, 6657, 6658, 6659, 6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6669, 6670, 6671, 6672, 6673, 6674, 6675, 6676, 6677, 6678, 6679, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689, 6690, 6691, 6692, 6693, 6694, 6695, 6696, 6697, 6698, 6699, 6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6708, 6709, 6710, 6711, 6712, 6713, 6714, 6715, 6716, 6717, 6718, 6719, 6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729, 6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739, 6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6748, 6749, 6750, 6751, 6752, 6753, 6754, 6755, 6756, 6757, 6758, 6759, 6760, 6761, 6762, 6763, 6764, 6765, 6766, 6767, 6768, 6769, 6770, 6771, 6772, 6773, 6774, 6775, 6776, 6777, 6778, 6779, 6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6788, 6789, 6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 6804, 6805, 6806, 6807, 6808, 6809, 6810, 6811, 6812, 6813, 6814, 6815, 6816, 6817, 6818, 6819, 6820, 6821, 6822, 6823, 6824, 6825, 6826, 6827, 6828, 6829, 6830, 6831, 6832, 6833, 6834, 6835, 6836, 6837, 6838, 6839, 6840, 6841, 6842, 6843, 6844, 6845, 6846, 6847, 6848, 6849, 6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6859, 6860, 6861, 6862, 6863, 6864, 6865, 6866, 6867, 6868, 6869, 6870, 6871, 6872, 6873, 6874, 6875, 6876, 6877, 6878, 6879, 6880, 6881, 6882, 6883, 6884, 6885, 6886, 6887, 6888, 6889, 6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898, 6899, 6900, 6901, 6902, 6903, 6904, 6905, 6906, 6907, 6908, 6909, 6910, 6911, 6912, 6913, 6914, 6915, 6916, 6917, 6918, 6919, 6920, 6921, 6922, 6923, 6924, 6925, 6926, 6927, 6928, 6929, 6930, 6931, 6932, 6933, 6934, 6935, 6936, 6937, 6938, 6939, 6940, 6941, 6942, 6943, 6944, 6945, 6946, 6947, 6948, 6949, 6950, 6951, 6952, 6953, 6954, 6955, 6956, 6957, 6958, 6959, 6960, 6961, 6962, 6963, 6964, 6965, 6966, 6967, 6968, 6969, 6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979, 6980, 6981, 6982, 6983, 6984, 6985, 6986, 6987, 6988, 6989, 6990, 6991, 6992, 6993, 6994, 6995, 6996, 6997, 6998, 6999, 7000, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009, 7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019, 7020, 7021, 7022, 7023, 7024, 7025, 7026, 7027, 7028, 7029, 7030, 7031, 7032, 7033, 7034, 7035, 7036, 7037, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 7045, 7046, 7047, 7048, 7049, 7050, 7051, 7052, 7053, 7054, 7055, 7056, 7057, 7058, 7059, 7060, 7061, 7062, 7063, 7064, 7065, 7066, 7067, 7068, 7069, 7070, 7071, 7072, 7073, 7074, 7075, 7076, 7077, 7078, 7079, 7080, 7081, 7082, 7083, 7084, 7085, 7086, 7087, 7088, 7089, 7090, 7091, 7092, 7093, 7094, 7095, 7096, 7097, 7098, 7099, 7100, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7108, 7109, 7110, 7111, 7112, 7113, 7114, 7115, 7116, 7117, 7118, 7119, 7120, 7121, 7122, 7123, 7124, 7125, 7126, 7127, 7128, 7129, 7130, 7131, 7132, 7133, 7134, 7135, 7136, 7137, 7138, 7139, 7140, 7141, 7142, 7143, 7144, 7145, 7146, 7147, 7148, 7149, 7150, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7161, 7162, 7163, 7164, 7165, 7166, 7167, 7168, 7169, 7170, 7171, 7172, 7173, 7174, 7175, 7176, 7177, 7178, 7179, 7180, 7181, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189, 7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199, 7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209, 7210, 7211, 7212, 7213, 7214, 7215, 7216, 7217, 7218, 7219, 7220, 7221, 7222, 7223, 7224, 7225, 7226, 7227, 7228, 7229, 7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7239, 7240, 7241, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249, 7250, 7251, 7252, 7253, 7254, 7255, 7256, 7257, 7258, 7259, 7260, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7268, 7269, 7270, 7271, 7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281, 7282, 7283, 7284, 7285, 7286, 7287, 7288, 7289, 7290, 7291, 7292, 7293, 7294, 7295, 7296, 7297, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309, 7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318, 7319, 7320, 7321, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7329, 7330, 7331, 7332, 7333, 7334, 7335, 7336, 7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7349, 7350, 7351, 7352, 7353, 7354, 7355, 7356, 7357, 7358, 7359, 7360, 7361, 7362, 7363, 7364, 7365, 7366, 7367, 7368, 7369, 7370, 7371, 7372, 7373, 7374, 7375, 7376, 7377, 7378, 7379, 7380, 7381, 7382, 7383, 7384, 7385, 7386, 7387, 7388, 7389, 7390, 7391, 7392, 7393, 7394, 7395, 7396, 7397, 7398, 7399, 7400, 7401, 7402, 7403, 7404, 7405, 7406, 7407, 7408, 7409, 7410, 7411, 7412, 7413, 7414, 7415, 7416, 7417, 7418, 7419, 7420, 7421, 7422, 7423, 7424, 7425, 7426, 7427, 7428, 7429, 7430, 7431, 7432, 7433, 7434, 7435, 7436, 7437, 7438, 7439, 7440, 7441, 7442, 7443, 7444, 7445, 7446, 7447, 7448, 7449, 7450, 7451, 7452, 7453, 7454, 7455, 7456, 7457, 7458, 7459, 7460, 7461, 7462, 7463, 7464, 7465, 7466, 7467, 7468, 7469, 7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7479, 7480, 7481, 7482, 7483, 7484, 7485, 7486, 7487, 7488, 7489, 7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499, 7500, 7501, 7502, 7503, 7504, 7505, 7506, 7507, 7508, 7509, 7510, 7511, 7512, 7513, 7514, 7515, 7516, 7517, 7518, 7519, 7520, 7521, 7522, 7523, 7524, 7525, 7526, 7527, 7528, 7529, 7530, 7531, 7532, 7533, 7534, 7535, 7536, 7537, 7538, 7539, 7540, 7541, 7542, 7543, 7544, 7545, 7546, 7547, 7548, 7549, 7550, 7551, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559, 7560, 7561, 7562, 7563, 7564, 7565, 7566, 7567, 7568, 7569, 7570, 7571, 7572, 7573, 7574, 7575, 7576, 7577, 7578, 7579, 7580, 7581, 7582, 7583, 7584, 7585, 7586, 7587, 7588, 7589, 7590, 7591, 7592, 7593, 7594, 7595, 7596, 7597, 7598, 7599, 7600, 7601, 7602, 7603, 7604, 7605, 7606, 7607, 7608, 7609, 7610, 7611, 7612, 7613, 7614, 7615, 7616, 7617, 7618, 7619, 7620, 7621, 7622, 7623, 7624, 7625, 7626, 7627, 7628, 7629, 7630, 7631, 7632, 7633, 7634, 7635, 7636, 7637, 7638, 7639, 7640, 7641, 7642, 7643, 7644, 7645, 7646, 7647, 7648, 7649, 7650, 7651, 7652, 7653, 7654, 7655, 7656, 7657, 7658, 7659, 7660, 7661, 7662, 7663, 7664, 7665, 7666, 7667, 7668, 7669, 7670, 7671, 7672, 7673, 7674, 7675, 7676, 7677, 7678, 7679, 7680, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689, 7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699, 12100, 12101, 12102, 12103, 12104, 12105, 12106, 12107, 12108, 12109, 12110, 12111, 12112, 12113, 12114, 12115, 12116, 12117, 12118, 12119, 12120, 12121, 12122, 12123, 12124, 12125, 12126, 12127, 12128, 12129, 12130, 12131, 12132, 12133, 12134, 12135, 12136, 12137, 12138, 12139, 12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149, 12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159, 12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169, 12170, 12171, 12172, 12173, 12174, 12175, 12176, 12177, 12178, 12179, 12180, 12181, 12182, 12183, 12184, 12185, 12186, 12187, 12188, 12189, 12190, 12191, 12192, 12193, 12194, 12195, 12196, 12197, 12198, 12199, 12200, 12201, 12202, 12203, 12204, 12205, 12206, 12207, 12208, 12209, 12210, 12211, 12212, 12213, 12214, 12215, 12216, 12217, 12218, 12219, 12220, 12221, 12222, 12223, 12224, 12225, 12226, 12227, 12228, 12229, 12230, 12231, 12232, 12233, 12234, 12235, 12236, 12237, 12238, 12239, 12240, 12241, 12242, 12243, 12244, 12245, 12246, 12247, 12248, 12249, 12250, 12251, 12252, 12253, 12254, 12255, 12256, 12257, 12258, 12259, 12260, 12261, 12262, 12263, 12264, 12265, 12266, 12267, 12268, 12269, 12270, 12271, 12272, 12273, 12274, 12275, 12276, 12277, 12278, 12279, 12280, 12281, 12282, 12283, 12284, 12285, 12286, 12287, 12288, 12289, 12290, 12291, 12292, 12293, 12294, 12295, 12296, 12297, 12298, 12299, 12300, 12301, 12302, 12303, 12304, 12305, 12306, 12307, 12308, 12309, 12310, 12311, 12312, 12313, 12314, 12315, 12316, 12317, 12318, 12319, 12320, 12321, 12322, 12323, 12324, 12325, 12326, 12327, 12328, 12329, 12330, 12331, 12332, 12333, 12334, 12335, 12336, 12337, 12338, 12339, 12340, 12341, 12342, 12343, 12344, 12345, 12346, 12347, 12348, 12349, 12350, 12351, 12352, 12353, 12354, 12355, 12356, 12357, 12358, 12359, 12360, 12361, 12362, 12363, 12364, 12365, 12366, 12367, 12368, 12369, 12370, 12371, 12372, 12373, 12374, 12375, 12376, 12377, 12378, 12379, 12380, 12381, 12382, 12383, 12384, 12385, 12386, 12387, 12388, 12389, 12390, 12391, 12392, 12393, 12394, 12395, 12396, 12397, 12398, 12399, 12400, 12401, 12402, 12403, 12404, 12405, 12406, 12407, 12408, 12409, 12410, 12411, 12412, 12413, 12414, 12415, 12416, 12417, 12418, 12419, 12420, 12421, 12422, 12423, 12424, 12425, 12426, 12427, 12428, 12429, 12430, 12431, 12432, 12433, 12434, 12435, 12436, 12437, 12438, 12439, 12440, 12441, 12442, 12443, 12444, 12445, 12446, 12447, 12448, 12449, 12450, 12451, 12452, 12453, 12454, 12455, 12456, 12457, 12458, 12459, 12460, 12461, 12462, 12463, 12464, 12465, 12466, 12467, 12468, 12469, 12470, 12471, 12472, 12473, 12474, 12475, 12476, 12477, 12478, 12479, 12480, 12481, 12482, 12483, 12484, 12485, 12486, 12487, 12488, 12489, 12490, 12491, 12492, 12493, 12494, 12495, 12496, 12497, 12498, 12499, 12500, 12501, 12502, 12503, 12504, 12505, 12506, 12507, 12508, 12509, 12510, 12511, 12512, 12513, 12514, 12515, 12516, 12517, 12518, 12519, 12520, 12521, 12522, 12523, 12524, 12525, 12526, 12527, 12528, 12529, 12530, 12531, 12532, 12533, 12534, 12535, 12536, 12537, 12538, 12539, 12540, 12541, 12542, 12543, 12544, 12545, 12546, 12547, 12548, 12549, 12550, 12551, 12552, 12553, 12554, 12555, 12556, 12557, 12558, 12559, 12560, 12561, 12562, 12563, 12564, 12565, 12566, 12567, 12568, 12569, 12570, 12571, 12572, 12573, 12574, 12575, 12576, 12577, 12578, 12579, 12580, 12581, 12582, 12583, 12584, 12585, 12586, 12587, 12588, 12589, 12590, 12591, 12592, 12593, 12594, 12595, 12596, 12597, 12598, 12599, 12600, 12601, 12602, 12603, 12604, 12605, 12606, 12607, 12608, 12609, 12610, 12611, 12612, 12613, 12614, 12615, 12616, 12617, 12618, 12619, 12620, 12621, 12622, 12623, 12624, 12625, 12626, 12627, 12628, 12629, 12630, 12631, 12632, 12633, 12634, 12635, 12636, 12637, 12638, 12639, 12640, 12641, 12642, 12643, 12644, 12645, 12646, 12647, 12648, 12649, 12650, 12651, 12652, 12653, 12654, 12655, 12656, 12657, 12658, 12659, 12660, 12661, 12662, 12663, 12664, 12665, 12666, 12667, 12668, 12669, 12670, 12671, 12672, 12673, 12674, 12675, 12676, 12677, 12678, 12679, 12680, 12681, 12682, 12683, 12684, 12685, 12686, 12687, 12688, 12689, 12690, 12691, 12692, 12693, 12694, 12695, 12696, 12697, 12698, 12699, 12700, 12701, 12702, 12703, 12704, 12705, 12706, 12707, 12708, 12709, 12710, 12711, 12712, 12713, 12714, 12715, 12716, 12717, 12718, 12719, 12720, 12721, 12722, 12723, 12724, 12725, 12726, 12727, 12728, 12729, 12730, 12731, 12732, 12733, 12734, 12735, 12736, 12737, 12738, 12739, 12740, 12741, 12742, 12743, 12744, 12745, 12746, 12747, 12748, 12749, 12750, 12751, 12752, 12753, 12754, 12755, 12756, 12757, 12758, 12759, 12760, 12761, 12762, 12763, 12764, 12765, 12766, 12767, 12768, 12769, 12770, 12771, 12772, 12773, 12774, 12775, 12776, 12777, 12778, 12779, 12780, 12781, 12782, 12783, 12784, 12785, 12786, 12787, 12788, 12789, 12790, 12791, 12792, 12793, 12794, 12795, 12796, 12797, 12798, 12799, 12800, 12801, 12802, 12803, 12804, 12805, 12806, 12807, 12808, 12809, 12810, 12811, 12812, 12813, 12814, 12815, 12816, 12817, 12818, 12819, 12820, 12821, 12822, 12823, 12824, 12825, 12826, 12827, 12828, 12829, 12830, 12831, 12832, 12833, 12834, 12835, 12836, 12837, 12838, 12839, 12840, 12841, 12842, 12843, 12844, 12845, 12846, 12847, 12848, 12849, 12850, 12851, 12852, 12853, 12854, 12855, 12856, 12857, 12858, 12859, 12860, 12861, 12862, 12863, 12864, 12865, 12866, 12867, 12868, 12869, 12870, 12871, 12872, 12873, 12874, 12875, 12876, 12877, 12878, 12879, 12880, 12881, 12882, 12883, 12884, 12885, 12886, 12887, 12888, 12889, 12890, 12891, 12892, 12893, 12894, 12895, 12896, 12897, 12898, 12899, 12900, 12901, 12902, 12903, 12904, 12905, 12906, 12907, 12908, 12909, 12910, 12911, 12912, 12913, 12914, 12915, 12916, 12917, 12918, 12919, 12920, 12921, 12922, 12923, 12924, 12925, 12926, 12927, 12928, 12929, 12930, 12931, 12932, 12933, 12934, 12935, 12936, 12937, 12938, 12939, 12940, 12941, 12942, 12943, 12944, 12945, 12946, 12947, 12948, 12949, 12950, 12951, 12952, 12953, 12954, 12955, 12956, 12957, 12958, 12959, 12960, 12961, 12962, 12963, 12964, 12965, 12966, 12967, 12968, 12969, 12970, 12971, 12972, 12973, 12974, 12975, 12976, 12977, 12978, 12979, 12980, 12981, 12982, 12983, 12984, 12985, 12986, 12987, 12988, 12989, 12990, 12991, 12992, 12993, 12994, 12995, 12996, 12997, 12998, 12999, 13000, 13001, 13002, 13003, 13004, 13005, 13006, 13007, 13008, 13009, 13010, 13011, 13012, 13013, 13014, 13015, 13016, 13017, 13018, 13019, 13020, 13021, 13022, 13023, 13024, 13025, 13026, 13027, 13028, 13029, 13030, 13031, 13032, 13033, 13034, 13035, 13036, 13037, 13038, 13039, 13040, 13041, 13042, 13043, 13044, 13045, 13046, 13047, 13048, 13049, 13050, 13051, 13052, 13053, 13054, 13055, 13056, 13057, 13058, 13059, 13060, 13061, 13062, 13063, 13064, 13065, 13066, 13067, 13068, 13069, 13070, 13071, 13072, 13073, 13074, 13075, 13076, 13077, 13078, 13079, 13080, 13081, 13082, 13083, 13084, 13085, 13086, 13087, 13088, 13089, 13090, 13091, 13092, 13093, 13094, 13095, 13096, 13097, 13098, 13099, 13100, 13101, 13102, 13103, 13104, 13105, 13106, 13107, 13108, 13109, 13110, 13111, 13112, 13113, 13114, 13115, 13116, 13117, 13118, 13119, 13120, 13121, 13122, 13123, 13124, 13125, 13126, 13127, 13128, 13129, 13130, 13131, 13132, 13133, 13134, 13135, 13136, 13137, 13138, 13139, 13140, 13141, 13142, 13143, 13144, 13145, 13146, 13147, 13148, 13149, 13150, 13151, 13152, 13153, 13154, 13155, 13156, 13157, 13158, 13159, 13160, 13161, 13162, 13163, 13164, 13165, 13166, 13167, 13168, 13169, 13170, 13171, 13172, 13173, 13174, 13175, 13176, 13177, 13178, 13179, 13180, 13181, 13182, 13183, 13184, 13185, 13186, 13187, 13188, 13189, 13190, 13191, 13192, 13193, 13194, 13195, 13196, 13197, 13198, 13199, 13200, 13201, 13202, 13203, 13204, 13205, 13206, 13207, 13208, 13209, 13210, 13211, 13212, 13213, 13214, 13215, 13216, 13217, 13218, 13219, 13220, 13221, 13222, 13223, 13224, 13225, 13226, 13227, 13228, 13229, 13230, 13231, 13232, 13233, 13234, 13235, 13236, 13237, 13238, 13239, 13240, 13241, 13242, 13243, 13244, 13245, 13246, 13247, 13248, 13249, 13250, 13251, 13252, 13253, 13254, 13255, 13256, 13257, 13258, 13259, 13260, 13261, 13262, 13263, 13264, 13265, 13266, 13267, 13268, 13269, 13270, 13271, 13272, 13273, 13274, 13275, 13276, 13277, 13278, 13279, 13280, 13281, 13282, 13283, 13284, 13285, 13286, 13287, 13288, 13289, 13290, 13291, 13292, 13293, 13294, 13295, 13296, 13297, 13298, 13299, 13300, 13301, 13302, 13303, 13304, 13305, 13306, 13307, 13308, 13309, 13310, 13311, 13312, 13313, 13314, 13315, 13316, 13317, 13318, 13319, 13320, 13321, 13322, 13323, 13324, 13325, 13326, 13327, 13328, 13329, 13330, 13331, 13332, 13333, 13334, 13335, 13336, 13337, 13338, 13339, 13340, 13341, 13342, 13343, 13344, 13345, 13346, 13347, 13348, 13349, 13350, 13351, 13352, 13353, 13354, 13355, 13356, 13357, 13358, 13359, 13360, 13361, 13362, 13363, 13364, 13365, 13366, 13367, 13368, 13369, 13370, 13371, 13372, 13373, 13374, 13375, 13376, 13377, 13378, 13379, 13380, 13381, 13382, 13383, 13384, 13385, 13386, 13387, 13388, 13389, 13390, 13391, 13392, 13393, 13394, 13395, 13396, 13397, 13398, 13399, 13400, 13401, 13402, 13403, 13404, 13405, 13406, 13407, 13408, 13409, 13410, 13411, 13412, 13413, 13414, 13415, 13416, 13417, 13418, 13419, 13420, 13421, 13422, 13423, 13424, 13425, 13426, 13427, 13428, 13429, 13430, 13431, 13432, 13433, 13434, 13435, 13436, 13437, 13438, 13439, 13440, 13441, 13442, 13443, 13444, 13445, 13446, 13447, 13448, 13449, 13450, 13451, 13452, 13453, 13454, 13455, 13456, 13457, 13458, 13459, 13460, 13461, 13462, 13463, 13464, 13465, 13466, 13467, 13468, 13469, 13470, 13471, 13472, 13473, 13474, 13475, 13476, 13477, 13478, 13479, 13480, 13481, 13482, 13483, 13484, 13485, 13486, 13487, 13488, 13489, 13490, 13491, 13492, 13493, 13494, 13495, 13496, 13497, 13498, 13499, 13500, 13501, 13502, 13503, 13504, 13505, 13506, 13507, 13508, 13509, 13510, 13511, 13512, 13513, 13514, 13515, 13516, 13517, 13518, 13519, 13520, 13521, 13522, 13523, 13524, 13525, 13526, 13527, 13528, 13529, 13530, 13531, 13532, 13533, 13534, 13535, 13536, 13537, 13538, 13539, 13540, 13541, 13542, 13543, 13544, 13545, 13546, 13547, 13548, 13549, 13550, 13551, 13552, 13553, 13554, 13555, 13556, 13557, 13558, 13559, 13560, 13561, 13562, 13563, 13564, 13565, 13566, 13567, 13568, 13569, 13570, 13571, 13572, 13573, 13574, 13575, 13576, 13577, 13578, 13579, 13580, 13581, 13582, 13583, 13584, 13585, 13586, 13587, 13588, 13589, 13590, 13591, 13592, 13593, 13594, 13595, 13596, 13597, 13598, 13599, 13600, 13601, 13602, 13603, 13604, 13605, 13606, 13607, 13608, 13609, 13610, 13611, 13612, 13613, 13614, 13615, 13616, 13617, 13618, 13619, 13620, 13621, 13622, 13623, 13624, 13625, 13626, 13627, 13628, 13629, 13630, 13631, 13632, 13633, 13634, 13635, 13636, 13637, 13638, 13639, 13640, 13641, 13642, 13643, 13644, 13645, 13646, 13647, 13648, 13649, 13650, 13651, 13652, 13653, 13654, 13655, 13656, 13657, 13658, 13659, 13660, 13661, 13662, 13663, 13664, 13665, 13666, 13667, 13668, 13669, 13670, 13671, 13672, 13673, 13674, 13675, 13676, 13677, 13678, 13679, 13680, 13681, 13682, 13683, 13684, 13685, 13686, 13687, 13688, 13689, 13690, 13691, 13692, 13693, 13694, 13695, 13696, 13697, 13698, 13699, 13700, 13701, 13702, 13703, 13704, 13705, 13706, 13707, 13708, 13709, 13710, 13711, 13712, 13713, 13714, 13715, 13716, 13717, 13718, 13719, 13720, 13721, 13722, 13723, 13724, 13725, 13726, 13727, 13728, 13729, 13730, 13731, 13732, 13733, 13734, 13735, 13736, 13737, 13738, 13739, 13740, 13741, 13742, 13743, 13744, 13745, 13746, 13747, 13748, 13749, 13750, 13751, 13752, 13753, 13754, 13755, 13756, 13757, 13758, 13759, 13760, 13761, 13762, 13763, 13764, 13765, 13766, 13767, 13768, 13769, 13770, 13771, 13772, 13773, 13774, 13775, 13776, 13777, 13778, 13779, 13780, 13781, 13782, 13783, 13784, 13785, 13786, 13787, 13788, 13789, 13790, 13791, 13792, 13793, 13794, 13795, 13796, 13797, 13798, 13799, 13800, 13801, 13802, 13803, 13804, 13805, 13806, 13807, 13808, 13809, 13810, 13811, 13812, 13813, 13814, 13815, 13816, 13817, 13818, 13819, 13820, 13821, 13822, 13823, 13824, 13825, 13826, 13827, 13828, 13829, 13830, 13831, 13832, 13833, 13834, 13835, 13836, 13837, 13838, 13839, 13840, 13841, 13842, 13843, 13844, 13845, 13846, 13847, 13848, 13849, 13850, 13851, 13852, 13853, 13854, 13855, 13856, 13857, 13858, 13859, 13860, 13861, 13862, 13863, 13864, 13865, 13866, 13867, 13868, 13869, 13870, 13871, 13872, 13873, 13874, 13875, 13876, 13877, 13878, 13879, 13880, 13881, 13882, 13883, 13884, 13885, 13886, 13887, 13888, 13889, 13890, 13891, 13892, 13893, 13894, 13895, 13896, 13897, 13898, 13899, 13900, 13901, 13902, 13903, 13904, 13905, 13906, 13907, 13908, 13909, 13910, 13911, 13912, 13913, 13914, 13915, 13916, 13917, 13918, 13919, 13920, 13921, 13922, 13923, 13924, 13925, 13926, 13927, 13928, 13929, 13930, 13931, 13932, 13933, 13934, 13935, 13936, 13937, 13938, 13939, 13940, 13941, 13942, 13943, 13944, 13945, 13946, 13947, 13948, 13949, 13950, 13951, 13952, 13953, 13954, 13955, 13956, 13957, 13958, 13959, 13960, 13961, 13962, 13963, 13964, 13965, 13966, 13967, 13968, 13969, 13970, 13971, 13972, 13973, 13974, 13975, 13976, 13977, 13978, 13979, 13980, 13981, 13982, 13983, 13984, 13985, 13986, 13987, 13988, 13989, 13990, 13991, 13992, 13993, 13994, 13995, 13996, 13997, 13998, 13999, 14000, 14001, 14002, 14003, 14004, 14005, 14006, 14007, 14008, 14009, 14010, 14011, 14012, 14013, 14014, 14015, 14016, 14017, 14018, 14019, 14020, 14021, 14022, 14023, 14024, 14025, 14026, 14027, 14028, 14029, 14030, 14031, 14032, 14033, 14034, 14035, 14036, 14037, 14038, 14039, 14040, 14041, 14042, 14043, 14044, 14045, 14046, 14047, 14048, 14049, 14050, 14051, 14052, 14053, 14054, 14055, 14056, 14057, 14058, 14059, 14060, 14061, 14062, 14063, 14064, 14065, 14066, 14067, 14068, 14069, 14070, 14071, 14072, 14073, 14074, 14075, 14076, 14077, 14078, 14079, 14080, 14081, 14082, 14083, 14084, 14085, 14086, 14087, 14088, 14089, 14090, 14091, 14092, 14093, 14094, 14095, 14096, 14097, 14098, 14099, 14100, 14101, 14102, 14103, 14104, 14105, 14106, 14107, 14108, 14109, 14110, 14111, 14112, 14113, 14114, 14115, 14116, 14117, 14118, 14119, 14120, 14121, 14122, 14123, 14124, 14125, 14126, 14127, 14128, 14129, 14130, 14131, 14132, 14133, 14134, 14135, 14136, 14137, 14138, 14139, 14140, 14141, 14142, 14143, 14144, 14145, 14146, 14147, 14148, 14149, 14150, 14151, 14152, 14153, 14154, 14155, 14156, 14157, 14158, 14159, 14160, 14161, 14162, 14163, 14164, 14165, 14166, 14167, 14168, 14169, 14170, 14171, 14172, 14173, 14174, 14175, 14176, 14177, 14178, 14179, 14180, 14181, 14182, 14183, 14184, 14185, 14186, 14187, 14188, 14189, 14190, 14191, 14192, 14193, 14194, 14195, 14196, 14197, 14198, 14199, 14200, 14201, 14202, 14203, 14204, 14205, 14206, 14207, 14208, 14209, 14210, 14211, 14212, 14213, 14214, 14215, 14216, 14217, 14218, 14219, 14220, 14221, 14222, 14223, 14224, 14225, 14226, 14227, 14228, 14229, 14230, 14231, 14232, 14233, 14234, 14235, 14236, 14237, 14238, 14239, 14240, 14241, 14242, 14243, 14244, 14245, 14246, 14247, 14248, 14249, 14250, 14251, 14252, 14253, 14254, 14255, 14256, 14257, 14258, 14259, 14260, 14261, 14262, 14263, 14264, 14265, 14266, 14267, 14268, 14269, 14270, 14271, 14272, 14273, 14274, 14275, 14276, 14277, 14278, 14279, 14280, 14281, 14282, 14283, 14284, 14285, 14286, 14287, 14288, 14289, 14290, 14291, 14292, 14293, 14294, 14295, 14296, 14297, 14298, 14299, 14300, 14301, 14302, 14303, 14304, 14305, 14306, 14307, 14308, 14309, 14310, 14311, 14312, 14313, 14314, 14315, 14316, 14317, 14318, 14319, 14320, 14321, 14322, 14323, 14324, 14325, 14326, 14327, 14328, 14329, 14330, 14331, 14332, 14333, 14334, 14335, 14336, 14337, 14338, 14339, 14340, 14341, 14342, 14343, 14344, 14345, 14346, 14347, 14348, 14349, 14350, 14351, 14352, 14353, 14354, 14355, 14356, 14357, 14358, 14359, 14360, 14361, 14362, 14363, 14364, 14365, 14366, 14367, 14368, 14369, 14370, 14371, 14372, 14373, 14374, 14375, 14376, 14377, 14378, 14379, 14380, 14381, 14382, 14383, 14384, 14385, 14386, 14387, 14388, 14389, 14390, 14391, 14392, 14393, 14394, 14395, 14396, 14397, 14398, 14399, 14400, 14401, 14402, 14403, 14404, 14405, 14406, 14407, 14408, 14409, 14410, 14411, 14412, 14413, 14414, 14415, 14416, 14417, 14418, 14419, 14420, 14421, 14422, 14423, 14424, 14425, 14426, 14427, 14428, 14429, 14430, 14431, 14432, 14433, 14434, 14435, 14436, 14437, 14438, 14439, 14440, 14441, 14442, 14443, 14444, 14445, 14446, 14447, 14448, 14449, 14450, 14451, 14452, 14453, 14454, 14455, 14456, 14457, 14458, 14459, 14460, 14461, 14462, 14463, 14464, 14465, 14466, 14467, 14468, 14469, 14470, 14471, 14472, 14473, 14474, 14475, 14476, 14477, 14478, 14479, 14480, 14481, 14482, 14483, 14484, 14485, 14486, 14487, 14488, 14489, 14490, 14491, 14492, 14493, 14494, 14495, 14496, 14497, 14498, 14499, 14500, 14501, 14502, 14503, 14504, 14505, 14506, 14507, 14508, 14509, 14510, 14511, 14512, 14513, 14514, 14515, 14516, 14517, 14518, 14519, 14520, 14521, 14522, 14523, 14524, 14525, 14526, 14527, 14528, 14529, 14530, 14531, 14532, 14533, 14534, 14535, 14536, 14537, 14538, 14539, 14540, 14541, 14542, 14543, 14544, 14545, 14546, 14547, 14548, 14549, 14550, 14551, 14552, 14553, 14554, 14555, 14556, 14557, 14558, 14559, 14560, 14561, 14562, 14563, 14564, 14565, 14566, 14567, 14568, 14569, 14570, 14571, 14572, 14573, 14574, 14575, 14576, 14577, 14578, 14579, 14580, 14581, 14582, 14583, 14584, 14585, 14586, 14587, 14588, 14589, 14590, 14591, 14592, 14593, 14594, 14595, 14596, 14597, 14598, 14599, 14600, 14601, 14602, 14603, 14604, 14605, 14606, 14607, 14608, 14609, 14610, 14611, 14612, 14613, 14614, 14615, 14616, 14617, 14618, 14619, 14620, 14621, 14622, 14623, 14624, 14625, 14626, 14627, 14628, 14629, 14630, 14631, 14632, 14633, 14634, 14635, 14636, 14637, 14638, 14639, 14640, 14641, 14642, 14643, 14644, 14645, 14646, 14647, 14648, 14649, 14650, 14651, 14652, 14653, 14654, 14655, 14656, 14657, 14658, 14659, 14660, 14661, 14662, 14663, 14664, 14665, 14666, 14667, 14668, 14669, 14670, 14671, 14672, 14673, 14674, 14675, 14676, 14677, 14678, 14679, 14680, 14681, 14682, 14683, 14684, 14685, 14686, 14687, 14688, 14689, 14690, 14691, 14692, 14693, 14694, 14695, 14696, 14697, 14698, 14699, 14700, 14701, 14702, 14703, 14704, 14705, 14706, 14707, 14708, 14709, 14710, 14711, 14712, 14713, 14714, 14715, 14716, 14717, 14718, 14719, 14720, 14721, 14722, 14723, 14724, 14725, 14726, 14727, 14728, 14729, 14730, 14731, 14732, 14733, 14734, 14735, 14736, 14737, 14738, 14739, 14740, 14741, 14742, 14743, 14744, 14745, 14746, 14747, 14748, 14749, 14750, 14751, 14752, 14753, 14754, 14755, 14756, 14757, 14758, 14759, 14760, 14761, 14762, 14763, 14764, 14765, 14766, 14767, 14768, 14769, 14770, 14771, 14772, 14773, 14774, 14775, 14776, 14777, 14778, 14779, 14780, 14781, 14782, 14783, 14784, 14785, 14786, 14787, 14788, 14789, 14790, 14791, 14792, 14793, 14794, 14795, 14796, 14797, 14798, 14799, 14800, 14801, 14802, 14803, 14804, 14805, 14806, 14807, 14808, 14809, 14810, 14811, 14812, 14813, 14814, 14815, 14816, 14817, 14818, 14819, 14820, 14821, 14822, 14823, 14824, 14825, 14826, 14827, 14828, 14829, 14830, 14831, 14832, 14833, 14834, 14835, 14836, 14837, 14838, 14839, 14840, 14841, 14842, 14843, 14844, 14845, 14846, 14847, 14848, 14849, 14850, 14851, 14852, 14853, 14854, 14855, 14856, 14857, 14858, 14859, 14860, 14861, 14862, 14863, 14864, 14865, 14866, 14867, 14868, 14869, 14870, 14871, 14872, 14873, 14874, 14875, 14876, 14877, 14878, 14879, 14880, 14881, 14882, 14883, 14884, 14885, 14886, 14887, 14888, 14889, 14890, 14891, 14892, 14893, 14894, 14895, 14896, 14897, 14898, 14899, 14900, 14901, 14902, 14903, 14904, 14905, 14906, 14907, 14908, 14909, 14910, 14911, 14912, 14913, 14914, 14915, 14916, 14917, 14918, 14919, 14920, 14921, 14922, 14923, 14924, 14925, 14926, 14927, 14928, 14929, 14930, 14931, 14932, 14933, 14934, 14935, 14936, 14937, 14938, 14939, 14940, 14941, 14942, 14943, 14944, 14945, 14946, 14947, 14948, 14949, 14950, 14951, 14952, 14953, 14954, 14955, 14956, 14957, 14958, 14959, 14960, 14961, 14962, 14963, 14964, 14965, 14966, 14967, 14968, 14969, 14970, 14971, 14972, 14973, 14974, 14975, 14976, 14977, 14978, 14979, 14980, 14981, 14982, 14983, 14984, 14985, 14986, 14987, 14988, 14989, 14990, 14991, 14992, 14993, 14994, 14995, 14996, 14997, 14998, 14999, 15000, 15001, 15002, 15003, 15004, 15005, 15006, 15007, 15008, 15009, 15010, 15011, 15012, 15013, 15014, 15015, 15016, 15017, 15018, 15019, 15020, 15021, 15022, 15023, 15024, 15025, 15026, 15027, 15028, 15029, 15030, 15031, 15032, 15033, 15034, 15035, 15036, 15037, 15038, 15039, 15040, 15041, 15042, 15043, 15044, 15045, 15046, 15047, 15048, 15049, 15050, 15051, 15052, 15053, 15054, 15055, 15056, 15057, 15058, 15059, 15060, 15061, 15062, 15063, 15064, 15065, 15066, 15067, 15068, 15069, 15070, 15071, 15072, 15073, 15074, 15075, 15076, 15077, 15078, 15079, 15080, 15081, 15082, 15083, 15084, 15085, 15086, 15087, 15088, 15089, 15090, 15091, 15092, 15093, 15094, 15095, 15096, 15097, 15098, 15099, 15100, 15101, 15102, 15103, 15104, 15105, 15106, 15107, 15108, 15109, 15110, 15111, 15112, 15113, 15114, 15115, 15116, 15117, 15118, 15119, 15120, 15121, 15122, 15123, 15124, 15125, 15126, 15127, 15128, 15129, 15130, 15131, 15132, 15133, 15134, 15135, 15136, 15137, 15138, 15139, 15140, 15141, 15142, 15143, 15144, 15145, 15146, 15147, 15148, 15149, 15150, 15151, 15152, 15153, 15154, 15155, 15156, 15157, 15158, 15159, 15160, 15161, 15162, 15163, 15164, 15165, 15166, 15167, 15168, 15169, 15170, 15171, 15172, 15173, 15174, 15175, 15176, 15177, 15178, 15179, 15180, 15181, 15182, 15183, 15184, 15185, 15186, 15187, 15188, 15189, 15190, 15191, 15192, 15193, 15194, 15195, 15196, 15197, 15198, 15199, 15200, 15201, 15202, 15203, 15204, 15205, 15206, 15207, 15208, 15209, 15210, 15211, 15212, 15213, 15214, 15215, 15216, 15217, 15218, 15219, 15220, 15221, 15222, 15223, 15224, 15225, 15226, 15227, 15228, 15229, 15230, 15231, 15232, 15233, 15234, 15235, 15236, 15237, 15238, 15239, 15240, 15241, 15242, 15243, 15244, 15245, 15246, 15247, 15248, 15249, 15250, 15251, 15252, 15253, 15254, 15255, 15256, 15257, 15258, 15259, 15260, 15261, 15262, 15263, 15264, 15265, 15266, 15267, 15268, 15269, 15270, 15271, 15272, 15273, 15274, 15275, 15276, 15277, 15278, 15279, 15280, 15281, 15282, 15283, 15284, 15285, 15286, 15287, 15288, 15289, 15290, 15291, 15292, 15293, 15294, 15295, 15296, 15297, 15298, 15299, 15300, 15301, 15302, 15303, 15304, 15305, 15306, 15307, 15308, 15309, 15310, 15311, 15312, 15313, 15314, 15315, 15316, 15317, 15318, 15319, 15320, 15321, 15322, 15323, 15324, 15325, 15326, 15327, 15328, 15329, 15330, 15331, 15332, 15333, 15334, 15335, 15336, 15337, 15338, 15339, 15340, 15341, 15342, 15343, 15344, 15345, 15346, 15347, 15348, 15349, 15350, 15351, 15352, 15353, 15354, 15355, 15356, 15357, 15358, 15359, 15360, 15361, 15362, 15363, 15364, 15365, 15366, 15367, 15368, 15369, 15370, 15371, 15372, 15373, 15374, 15375, 15376, 15377, 15378, 15379, 15380, 15381, 15382, 15383, 15384, 15385, 15386, 15387, 15388, 15389, 15390, 15391, 15392, 15393, 15394, 15395, 15396, 15397, 15398, 15399, 15400, 15401, 15402, 15403, 15404, 15405, 15406, 15407, 15408, 15409, 15410, 15411, 15412, 15413, 15414, 15415, 15416, 15417, 15418, 15419, 15420, 15421, 15422, 15423, 15424, 15425, 15426, 15427, 15428, 15429, 15430, 15431, 15432, 15433, 15434, 15435, 15436, 15437, 15438, 15439, 15440, 15441, 15442, 15443, 15444, 15445, 15446, 15447, 15448, 15449, 15450, 15451, 15452, 15453, 15454, 15455, 15456, 15457, 15458, 15459, 15460, 15461, 15462, 15463, 15464, 15465, 15466, 15467, 15468, 15469, 15470, 15471, 15472, 15473, 15474, 15475, 15476, 15477, 15478, 15479, 15480, 15481, 15482, 15483, 15484, 15485, 15486, 15487, 15488, 15489, 15490, 15491, 15492, 15493, 15494, 15495, 15496, 15497, 15498, 15499, 15500, 15501, 15502, 15503, 15504, 15505, 15506, 15507, 15508, 15509, 15510, 15511, 15512, 15513, 15514, 15515, 15516, 15517, 15518, 15519, 15520, 15521, 15522, 15523, 15524, 15525, 15526, 15527, 15528, 15529, 15530, 15531, 15532, 15533, 15534, 15535, 15536, 15537, 15538, 15539, 15540, 15541, 15542, 15543, 15544, 15545, 15546, 15547, 15548, 15549, 15550, 15551, 15552, 15553, 15554, 15555, 15556, 15557, 15558, 15559, 15560, 15561, 15562, 15563, 15564, 15565, 15566, 15567, 15568, 15569, 15570, 15571, 15572, 15573, 15574, 15575, 15576, 15577, 15578, 15579, 15580, 15581, 15582, 15583, 15584, 15585, 15586, 15587, 15588, 15589, 15590, 15591, 15592, 15593, 15594, 15595, 15596, 15597, 15598, 15599, 15600, 15601, 15602, 15603, 15604, 15605, 15606, 15607, 15608, 15609, 15610, 15611, 15612, 15613, 15614, 15615, 15616, 15617, 15618, 15619, 15620, 15621, 15622, 15623, 15624, 15625, 15626, 15627, 15628, 15629, 15630, 15631, 15632, 15633, 15634, 15635, 15636, 15637, 15638, 15639, 15640, 15641, 15642, 15643, 15644, 15645, 15646, 15647, 15648, 15649, 15650, 15651, 15652, 15653, 15654, 15655, 15656, 15657, 15658, 15659, 15660, 15661, 15662, 15663, 15664, 15665, 15666, 15667, 15668, 15669, 15670, 15671, 15672, 15673, 15674, 15675, 15676, 15677, 15678, 15679, 15680, 15681, 15682, 15683, 15684, 15685, 15686, 15687, 15688, 15689, 15690, 15691, 15692, 15693, 15694, 15695, 15696, 15697, 15698, 15699, 15700, 15701, 15702, 15703, 15704, 15705, 15706, 15707, 15708, 15709, 15710, 15711, 15712, 15713, 15714, 15715, 15716, 15717, 15718, 15719, 15720, 15721, 15722, 15723, 15724, 15725, 15726, 15727, 15728, 15729, 15730, 15731, 15732, 15733, 15734, 15735, 15736, 15737, 15738, 15739, 15740, 15741, 15742, 15743, 15744, 15745, 15746, 15747, 15748, 15749, 15750, 15751, 15752, 15753, 15754, 15755, 15756, 15757, 15758, 15759, 15760, 15761, 15762, 15763, 15764, 15765, 15766, 15767, 15768, 15769, 15770, 15771, 15772, 15773, 15774, 15775, 15776, 15777, 15778, 15779, 15780, 15781, 15782, 15783, 15784, 15785, 15786, 15787, 15788, 15789, 15790, 15791, 15792, 15793, 15794, 15795, 15796, 15797, 15798, 15799, 15800, 15801, 15802, 15803, 15804, 15805, 15806, 15807, 15808, 15809, 15810, 15811, 15812, 15813, 15814, 15815, 15816, 15817, 15818, 15819, 15820, 15821, 15822, 15823, 15824, 15825, 15826, 15827, 15828, 15829, 15830, 15831, 15832, 15833, 15834, 15835, 15836, 15837, 15838, 15839, 15840, 15841, 15842, 15843, 15844, 15845, 15846, 15847, 15848, 15849, 15850, 15851, 15852, 15853, 15854, 15855, 15856, 15857, 15858, 15859, 15860, 15861, 15862, 15863, 15864, 15865, 15866, 15867, 15868, 15869, 15870, 15871, 15872, 15873, 15874, 15875, 15876, 15877, 15878, 15879, 15880, 15881, 15882, 15883, 15884, 15885, 15886, 15887, 15888, 15889, 15890, 15891, 15892, 15893, 15894, 15895, 15896, 15897, 15898, 15899, 15900, 15901, 15902, 15903, 15904, 15905, 15906, 15907, 15908, 15909, 15910, 15911, 15912, 15913, 15914, 15915, 15916, 15917, 15918, 15919, 15920, 15921, 15922, 15923, 15924, 15925, 15926, 15927, 15928, 15929, 15930, 15931, 15932, 15933, 15934, 15935, 15936, 15937, 15938, 15939, 15940, 15941, 15942, 15943, 15944, 15945, 15946, 15947, 15948, 15949, 15950, 15951, 15952, 15953, 15954, 15955, 15956, 15957, 15958, 15959, 15960, 15961, 15962, 15963, 15964, 15965, 15966, 15967, 15968, 15969, 15970, 15971, 15972, 15973, 15974, 15975, 15976, 15977, 15978, 15979, 15980, 15981, 15982, 15983, 15984, 15985, 15986, 15987, 15988, 15989, 15990, 15991, 15992, 15993, 15994, 15995, 15996, 15997, 15998, 15999, 16000, 16001, 16002, 16003, 16004, 16005, 16006, 16007, 16008, 16009, 16010, 16011, 16012, 16013, 16014, 16015, 16016, 16017, 16018, 16019, 16020, 16021, 16022, 16023, 16024, 16025, 16026, 16027, 16028, 16029, 16030, 16031, 16032, 16033, 16034, 16035, 16036, 16037, 16038, 16039, 16040, 16041, 16042, 16043, 16044, 16045, 16046, 16047, 16048, 16049, 16050, 16051, 16052, 16053, 16054, 16055, 16056, 16057, 16058, 16059, 16060, 16061, 16062, 16063, 16064, 16065, 16066, 16067, 16068, 16069, 16070, 16071, 16072, 16073, 16074, 16075, 16076, 16077, 16078, 16079, 16080, 16081, 16082, 16083, 16084, 16085, 16086, 16087, 16088, 16089, 16090, 16091, 16092, 16093, 16094, 16095, 16096, 16097, 16098, 16099, 16100, 16101, 16102, 16103, 16104, 16105, 16106, 16107, 16108, 16109, 16110, 16111, 16112, 16113, 16114, 16115, 16116, 16117, 16118, 16119, 16120, 16121, 16122, 16123, 16124, 16125, 16126, 16127, 16128, 16129, 16130, 16131, 16132, 16133, 16134, 16135, 16136, 16137, 16138, 16139, 16140, 16141, 16142, 16143, 16144, 16145, 16146, 16147, 16148, 16149, 16150, 16151, 16152, 16153, 16154, 16155, 16156, 16157, 16158, 16159, 16160, 16161, 16162, 16163, 16164, 16165, 16166, 16167, 16168, 16169, 16170, 16171, 16172, 16173, 16174, 16175, 16176, 16177, 16178, 16179, 16180, 16181, 16182, 16183, 16184, 16185, 16186, 16187, 16188, 16189, 16190, 16191, 16192, 16193, 16194, 16195, 16196, 16197, 16198, 16199, 16200, 16201, 16202, 16203, 16204, 16205, 16206, 16207, 16208, 16209, 16210, 16211, 16212, 16213, 16214, 16215, 16216, 16217, 16218, 16219, 16220, 16221, 16222, 16223, 16224, 16225, 16226, 16227, 16228, 16229, 16230, 16231, 16232, 16233, 16234, 16235, 16236, 16237, 16238, 16239, 16240, 16241, 16242, 16243, 16244, 16245, 16246, 16247, 16248, 16249, 16250, 16251, 16252, 16253, 16254, 16255, 16256, 16257, 16258, 16259, 16260, 16261, 16262, 16263, 16264, 16265, 16266, 16267, 16268, 16269, 16270, 16271, 16272, 16273, 16274, 16275, 16276, 16277, 16278, 16279, 16280, 16281, 16282, 16283, 16284, 16285, 16286, 16287, 16288, 16289, 16290, 16291, 16292, 16293, 16294, 16295, 16296, 16297, 16298, 16299, 16300, 16301, 16302, 16303, 16304, 16305, 16306, 16307, 16308, 16309, 16310, 16311, 16312, 16313, 16314, 16315, 16316, 16317, 16318, 16319, 16320, 16321, 16322, 16323, 16324, 16325, 16326, 16327, 16328, 16329, 16330, 16331, 16332, 16333, 16334, 16335, 16336, 16337, 16338, 16339, 16340, 16341, 16342, 16343, 16344, 16345, 16346, 16347, 16348, 16349, 16350, 16351, 16352, 16353, 16354, 16355, 16356, 16357, 16358, 16359, 16360, 16361, 16362, 16363, 16364, 16365, 16366, 16367, 16368, 16369, 16370, 16371, 16372, 16373, 16374, 16375, 16376, 16377, 16378, 16379, 16380, 16381, 16382, 16383, 16384, 16385, 16386, 16387, 16388, 16389, 16390, 16391, 16392, 16393, 16394, 16395, 16396, 16397, 16398, 16399, 16400, 16401, 16402, 16403, 16404, 16405, 16406, 16407, 16408, 16409, 16410, 16411, 16412, 16413, 16414, 16415, 16416, 16417, 16418, 16419, 16420, 16421, 16422, 16423, 16424, 16425, 16426, 16427, 16428, 16429, 16430, 16431, 16432, 16433, 16434, 16435, 16436, 16437, 16438, 16439, 16440, 16441, 16442, 16443, 16444, 16445, 16446, 16447, 16448, 16449, 16450, 16451, 16452, 16453, 16454, 16455, 16456, 16457, 16458, 16459, 16460, 16461, 16462, 16463, 16464, 16465, 16466, 16467, 16468, 16469, 16470, 16471, 16472, 16473, 16474, 16475, 16476, 16477, 16478, 16479, 16480, 16481, 16482, 16483, 16484, 16485, 16486, 16487, 16488, 16489, 16490, 16491, 16492, 16493, 16494, 16495, 16496, 16497, 16498, 16499, 16500, 16501, 16502, 16503, 16504, 16505, 16506, 16507, 16508, 16509, 16510, 16511, 16512, 16513, 16514, 16515, 16516, 16517, 16518, 16519, 16520, 16521, 16522, 16523, 16524, 16525, 16526, 16527, 16528, 16529, 16530, 16531, 16532, 16533, 16534, 16535, 16536, 16537, 16538, 16539, 16540, 16541, 16542, 16543, 16544, 16545, 16546, 16547, 16548, 16549, 16550, 16551, 16552, 16553, 16554, 16555, 16556, 16557, 16558, 16559, 16560, 16561, 16562, 16563, 16564, 16565, 16566, 16567, 16568, 16569, 16570, 16571, 16572, 16573, 16574, 16575, 16576, 16577, 16578, 16579, 16580, 16581, 16582, 16583, 16584, 16585, 16586, 16587, 16588, 16589, 16590, 16591, 16592, 16593, 16594, 16595, 16596, 16597, 16598, 16599, 16600, 16601, 16602, 16603, 16604, 16605, 16606, 16607, 16608, 16609, 16610, 16611, 16612, 16613, 16614, 16615, 16616, 16617, 16618, 16619, 16620, 16621, 16622, 16623, 16624, 16625, 16626, 16627, 16628, 16629, 16630, 16631, 16632, 16633, 16634, 16635, 16636, 16637, 16638, 16639, 16640, 16641, 16642, 16643, 16644, 16645, 16646, 16647, 16648, 16649, 16650, 16651, 16652, 16653, 16654, 16655, 16656, 16657, 16658, 16659, 16660, 16661, 16662, 16663, 16664, 16665, 16666, 16667, 16668, 16669, 16670, 16671, 16672, 16673, 16674, 16675, 16676, 16677, 16678, 16679, 16680, 16681, 16682, 16683, 16684, 16685, 16686, 16687, 16688, 16689, 16690, 16691, 16692, 16693, 16694, 16695, 16696, 16697, 16698, 16699, 16700, 16701, 16702, 16703, 16704, 16705, 16706, 16707, 16708, 16709, 16710, 16711, 16712, 16713, 16714, 16715, 16716, 16717, 16718, 16719, 16720, 16721, 16722, 16723, 16724, 16725, 16726, 16727, 16728, 16729, 16730, 16731, 16732, 16733, 16734, 16735, 16736, 16737, 16738, 16739, 16740, 16741, 16742, 16743, 16744, 16745, 16746, 16747, 16748, 16749, 16750, 16751, 16752, 16753, 16754, 16755, 16756, 16757, 16758, 16759, 16760, 16761, 16762, 16763, 16764, 16765, 16766, 16767, 16768, 16769, 16770, 16771, 16772, 16773, 16774, 16775, 16776, 16777, 16778, 16779, 16780, 16781, 16782, 16783, 16784, 16785, 16786, 16787, 16788, 16789, 16790, 16791, 16792, 16793, 16794, 16795, 16796, 16797, 16798, 16799, 16800, 16801, 16802, 16803, 16804, 16805, 16806, 16807, 16808, 16809, 16810, 16811, 16812, 16813, 16814, 16815, 16816, 16817, 16818, 16819, 16820, 16821, 16822, 16823, 16824, 16825, 16826, 16827, 16828, 16829, 16830, 16831, 16832, 16833, 16834, 16835, 16836, 16837, 16838, 16839, 16840, 16841, 16842, 16843, 16844, 16845, 16846, 16847, 16848, 16849, 16850, 16851, 16852, 16853, 16854, 16855, 16856, 16857, 16858, 16859, 16860, 16861, 16862, 16863, 16864, 16865, 16866, 16867, 16868, 16869, 16870, 16871, 16872, 16873, 16874, 16875, 16876, 16877, 16878, 16879, 16880, 16881, 16882, 16883, 16884, 16885, 16886, 16887, 16888, 16889, 16890, 16891, 16892, 16893, 16894, 16895, 16896, 16897, 16898, 16899, 16900, 16901, 16902, 16903, 16904, 16905, 16906, 16907, 16908, 16909, 16910, 16911, 16912, 16913, 16914, 16915, 16916, 16917, 16918, 16919, 16920, 16921, 16922, 16923, 16924, 16925, 16926, 16927, 16928, 16929, 16930, 16931, 16932, 16933, 16934, 16935, 16936, 16937, 16938, 16939, 16940, 16941, 16942, 16943, 16944, 16945, 16946, 16947, 16948, 16949, 16950, 16951, 16952, 16953, 16954, 16955, 16956, 16957, 16958, 16959, 16960, 16961, 16962, 16963, 16964, 16965, 16966, 16967, 16968, 16969, 16970, 16971, 16972, 16973, 16974, 16975, 16976, 16977, 16978, 16979, 16980, 16981, 16982, 16983, 16984, 16985, 16986, 16987, 16988, 16989, 16990, 16991, 16992, 16993, 16994, 16995, 16996, 16997, 16998, 16999, 17000, 17001, 17002, 17003, 17004, 17005, 17006, 17007, 17008, 17009, 17010, 17011, 17012, 17013, 17014, 17015, 17016, 17017, 17018, 17019, 17020, 17021, 17022, 17023, 17024, 17025, 17026, 17027, 17028, 17029, 17030, 17031, 17032, 17033, 17034, 17035, 17036, 17037, 17038, 17039, 17040, 17041, 17042, 17043, 17044, 17045, 17046, 17047, 17048, 17049, 17050, 17051, 17052, 17053, 17054, 17055, 17056, 17057, 17058, 17059, 17060, 17061, 17062, 17063, 17064, 17065, 17066, 17067, 17068, 17069, 17070, 17071, 17072, 17073, 17074, 17075, 17076, 17077, 17078, 17079, 17080, 17081, 17082, 17083, 17084, 17085, 17086, 17087, 17088, 17089, 17090, 17091, 17092, 17093, 17094, 17095, 17096, 17097, 17098, 17099, 17100, 17101, 17102, 17103, 17104, 17105, 17106, 17107, 17108, 17109, 17110, 17111, 17112, 17113, 17114, 17115, 17116, 17117, 17118, 17119, 17120, 17121, 17122, 17123, 17124, 17125, 17126, 17127, 17128, 17129, 17130, 17131, 17132, 17133, 17134, 17135, 17136, 17137, 17138, 17139, 17140, 17141, 17142, 17143, 17144, 17145, 17146, 17147, 17148, 17149, 17150, 17151, 17152, 17153, 17154, 17155, 17156, 17157, 17158, 17159, 17160, 17161, 17162, 17163, 17164, 17165, 17166, 17167, 17168, 17169, 17170, 17171, 17172, 17173, 17174, 17175, 17176, 17177, 17178, 17179, 17180, 17181, 17182, 17183, 17184, 17185, 17186, 17187, 17188, 17189, 17190, 17191, 17192, 17193, 17194, 17195, 17196, 17197, 17198, 17199, 17200, 17201, 17202, 17203, 17204, 17205, 17206, 17207, 17208, 17209, 17210, 17211, 17212, 17213, 17214, 17215, 17216, 17217, 17218, 17219, 17220, 17221, 17222, 17223, 17224, 17225, 17226, 17227, 17228, 17229, 17230, 17231, 17232, 17233, 17234, 17235, 17236, 17237, 17238, 17239, 17240, 17241, 17242, 17243, 17244, 17245, 17246, 17247, 17248, 17249, 17250, 17251, 17252, 17253, 17254, 17255, 17256, 17257, 17258, 17259, 17260, 17261, 17262, 17263, 17264, 17265, 17266, 17267, 17268, 17269, 17270, 17271, 17272, 17273, 17274, 17275, 17276, 17277, 17278, 17279, 17280, 17281, 17282, 17283, 17284, 17285, 17286, 17287, 17288, 17289, 17290, 17291, 17292, 17293, 17294, 17295, 17296, 17297, 17298, 17299, 17300, 17301, 17302, 17303, 17304, 17305, 17306, 17307, 17308, 17309, 17310, 17311, 17312, 17313, 17314, 17315, 17316, 17317, 17318, 17319, 17320, 17321, 17322, 17323, 17324, 17325, 17326, 17327, 17328, 17329, 17330, 17331, 17332, 17333, 17334, 17335, 17336, 17337, 17338, 17339, 17340, 17341, 17342, 17343, 17344, 17345, 17346, 17347, 17348, 17349, 17350, 17351, 17352, 17353, 17354, 17355, 17356, 17357, 17358, 17359, 17360, 17361, 17362, 17363, 17364, 17365, 17366, 17367, 17368, 17369, 17370, 17371, 17372, 17373, 17374, 17375, 17376, 17377, 17378, 17379, 17380, 17381, 17382, 17383, 17384, 17385, 17386, 17387, 17388, 17389, 17390, 17391, 17392, 17393, 17394, 17395, 17396, 17397, 17398, 17399, 17400, 17401, 17402, 17403, 17404, 17405, 17406, 17407, 17408, 17409, 17410, 17411, 17412, 17413, 17414, 17415, 17416, 17417, 17418, 17419, 17420, 17421, 17422, 17423, 17424, 17425, 17426, 17427, 17428, 17429, 17430, 17431, 17432, 17433, 17434, 17435, 17436, 17437, 17438, 17439, 17440, 17441, 17442, 17443, 17444, 17445, 17446, 17447, 17448, 17449, 17450, 17451, 17452, 17453, 17454, 17455, 17456, 17457, 17458, 17459, 17460, 17461, 17462, 17463, 17464, 17465, 17466, 17467, 17468, 17469, 17470, 17471, 17472, 17473, 17474, 17475, 17476, 17477, 17478, 17479, 17480, 17481, 17482, 17483, 17484, 17485, 17486, 17487, 17488, 17489, 17490, 17491, 17492, 17493, 17494, 17495, 17496, 17497, 17498, 17499, 17500, 17501, 17502, 17503, 17504, 17505, 17506, 17507, 17508, 17509, 17510, 17511, 17512, 17513, 17514, 17515, 17516, 17517, 17518, 17519, 17520, 17521, 17522, 17523, 17524, 17525, 17526, 17527, 17528, 17529, 17530, 17531, 17532, 17533, 17534, 17535, 17536, 17537, 17538, 17539, 17540, 17541, 17542, 17543, 17544, 17545, 17546, 17547, 17548, 17549, 17550, 17551, 17552, 17553, 17554, 17555, 17556, 17557, 17558, 17559, 17560, 17561, 17562, 17563, 17564, 17565, 17566, 17567, 17568, 17569, 17570, 17571, 17572, 17573, 17574, 17575, 17576, 17577, 17578, 17579, 17580, 17581, 17582, 17583, 17584, 17585, 17586, 17587, 17588, 17589, 17590, 17591, 17592, 17593, 17594, 17595, 17596, 17597, 17598, 17599, 17600, 17601, 17602, 17603, 17604, 17605, 17606, 17607, 17608, 17609, 17610, 17611, 17612, 17613, 17614, 17615, 17616, 17617, 17618, 17619, 17620, 17621, 17622, 17623, 17624, 17625, 17626, 17627, 17628, 17629, 17630, 17631, 17632, 17633, 17634, 17635, 17636, 17637, 17638, 17639, 17640, 17641, 17642, 17643, 17644, 17645, 17646, 17647, 17648, 17649, 17650, 17651, 17652, 17653, 17654, 17655, 17656, 17657, 17658, 17659, 17660, 17661, 17662, 17663, 17664, 17665, 17666, 17667, 17668, 17669, 17670, 17671, 17672, 17673, 17674, 17675, 17676, 17677, 17678, 17679, 17680, 17681, 17682, 17683, 17684, 17685, 17686, 17687, 17688, 17689, 17690, 17691, 17692, 17693, 17694, 17695, 17696, 17697, 17698, 17699, 17700, 17701, 17702, 17703, 17704, 17705, 17706, 17707, 17708, 17709, 17710, 17711, 17712, 17713, 17714, 17715, 17716, 17717, 17718, 17719, 17720, 17721, 17722, 17723, 17724, 17725, 17726, 17727, 17728, 17729, 17730, 17731, 17732, 17733, 17734, 17735, 17736, 17737, 17738, 17739, 17740, 17741, 17742, 17743, 17744, 17745, 17746, 17747, 17748, 17749, 17750, 17751, 17752, 17753, 17754, 17755, 17756, 17757, 17758, 17759, 17760, 17761, 17762, 17763, 17764, 17765, 17766, 17767, 17768, 17769, 17770, 17771, 17772, 17773, 17774, 17775, 17776, 17777, 17778, 17779, 17780, 17781, 17782, 17783, 17784, 17785, 17786, 17787, 17788, 17789, 17790, 17791, 17792, 17793, 17794, 17795, 17796, 17797, 17798, 17799, 17800, 17801, 17802, 17803, 17804, 17805, 17806, 17807, 17808, 17809, 17810, 17811, 17812, 17813, 17814, 17815, 17816, 17817, 17818, 17819, 17820, 17821, 17822, 17823, 17824, 17825, 17826, 17827, 17828, 17829, 17830, 17831, 17832, 17833, 17834, 17835, 17836, 17837, 17838, 17839, 17840, 17841, 17842, 17843, 17844, 17845, 17846, 17847, 17848, 17849, 17850, 17851, 17852, 17853, 17854, 17855, 17856, 17857, 17858, 17859, 17860, 17861, 17862, 17863, 17864, 17865, 17866, 17867, 17868, 17869, 17870, 17871, 17872, 17873, 17874, 17875, 17876, 17877, 17878, 17879, 17880, 17881, 17882, 17883, 17884, 17885, 17886, 17887, 17888, 17889, 17890, 17891, 17892, 17893, 17894, 17895, 17896, 17897, 17898, 17899, 17900, 17901, 17902, 17903, 17904, 17905, 17906, 17907, 17908, 17909, 17910, 17911, 17912, 17913, 17914, 17915, 17916, 17917, 17918, 17919, 17920, 17921, 17922, 17923, 17924, 17925, 17926, 17927, 17928, 17929, 17930, 17931, 17932, 17933, 17934, 17935, 17936, 17937, 17938, 17939, 17940, 17941, 17942, 17943, 17944, 17945, 17946, 17947, 17948, 17949, 17950, 17951, 17952, 17953, 17954, 17955, 17956, 17957, 17958, 17959, 17960, 17961, 17962, 17963, 17964, 17965, 17966, 17967, 17968, 17969, 17970, 17971, 17972, 17973, 17974, 17975, 17976, 17977, 17978, 17979, 17980, 17981, 17982, 17983, 17984, 17985, 17986, 17987, 17988, 17989, 17990, 17991, 17992, 17993, 17994, 17995, 17996, 17997, 17998, 17999, 18000, 18001, 18002, 18003, 18004, 18005, 18006, 18007, 18008, 18009, 18010, 18011, 18012, 18013, 18014, 18015, 18016, 18017, 18018, 18019, 18020, 18021, 18022, 18023, 18024, 18025, 18026, 18027, 18028, 18029, 18030, 18031, 18032, 18033, 18034, 18035, 18036, 18037, 18038, 18039, 18040, 18041, 18042, 18043, 18044, 18045, 18046, 18047, 18048, 18049, 18050, 18051, 18052, 18053, 18054, 18055, 18056, 18057, 18058, 18059, 18060, 18061, 18062, 18063, 18064, 18065, 18066, 18067, 18068, 18069, 18070, 18071, 18072, 18073, 18074, 18075, 18076, 18077, 18078, 18079, 18080, 18081, 18082, 18083, 18084, 18085, 18086, 18087, 18088, 18089, 18090, 18091, 18092, 18093, 18094, 18095, 18096, 18097, 18098, 18099, 18100, 18101, 18102, 18103, 18104, 18105, 18106, 18107, 18108, 18109, 18110, 18111, 18112, 18113, 18114, 18115, 18116, 18117, 18118, 18119, 18120, 18121, 18122, 18123, 18124, 18125, 18126, 18127, 18128, 18129, 18130, 18131, 18132, 18133, 18134, 18135, 18136, 18137, 18138, 18139, 18140, 18141, 18142, 18143, 18144, 18145, 18146, 18147, 18148, 18149, 18150, 18151, 18152, 18153, 18154, 18155, 18156, 18157, 18158, 18159, 18160, 18161, 18162, 18163, 18164, 18165, 18166, 18167, 18168, 18169, 18170, 18171, 18172, 18173, 18174, 18175, 18176, 18177, 18178, 18179, 18180, 18181, 18182, 18183, 18184, 18185, 18186, 18187, 18188, 18189, 18190, 18191, 18192, 18193, 18194, 18195, 18196, 18197, 18198, 18199, 18200, 18201, 18202, 18203, 18204, 18205, 18206, 18207, 18208, 18209, 18210, 18211, 18212, 18213, 18214, 18215, 18216, 18217, 18218, 18219, 18220, 18221, 18222, 18223, 18224, 18225, 18226, 18227, 18228, 18229, 18230, 18231, 18232, 18233, 18234, 18235, 18236, 18237, 18238, 18239, 18240, 18241, 18242, 18243, 18244, 18245, 18246, 18247, 18248, 18249, 18250, 18251, 18252, 18253, 18254, 18255, 18256, 18257, 18258, 18259, 18260, 18261, 18262, 18263, 18264, 18265, 18266, 18267, 18268, 18269, 18270, 18271, 18272, 18273, 18274, 18275, 18276, 18277, 18278, 18279, 18280, 18281, 18282, 18283, 18284, 18285, 18286, 18287, 18288, 18289, 18290, 18291, 18292, 18293, 18294, 18295, 18296, 18297, 18298, 18299, 18300, 18301, 18302, 18303, 18304, 18305, 18306, 18307, 18308, 18309, 18310, 18311, 18312, 18313, 18314, 18315, 18316, 18317, 18318, 18319, 18320, 18321, 18322, 18323, 18324, 18325, 18326, 18327, 18328, 18329, 18330, 18331, 18332, 18333, 18334, 18335, 18336, 18337, 18338, 18339, 18340, 18341, 18342, 18343, 18344, 18345, 18346, 18347, 18348, 18349, 18350, 18351, 18352, 18353, 18354, 18355, 18356, 18357, 18358, 18359, 18360, 18361, 18362, 18363, 18364, 18365, 18366, 18367, 18368, 18369, 18370, 18371, 18372, 18373, 18374, 18375, 18376, 18377, 18378, 18379, 18380, 18381, 18382, 18383, 18384, 18385, 18386, 18387, 18388, 18389, 18390, 18391, 18392, 18393, 18394, 18395, 18396, 18397, 18398, 18399, 18400, 18401, 18402, 18403, 18404, 18405, 18406, 18407, 18408, 18409, 18410, 18411, 18412, 18413, 18414, 18415, 18416, 18417, 18418, 18419, 18420, 18421, 18422, 18423, 18424, 18425, 18426, 18427, 18428, 18429, 18430, 18431, 18432, 18433, 18434, 18435, 18436, 18437, 18438, 18439, 18440, 18441, 18442, 18443, 18444, 18445, 18446, 18447, 18448, 18449, 18450, 18451, 18452, 18453, 18454, 18455, 18456, 18457, 18458, 18459, 18460, 18461, 18462, 18463, 18464, 18465, 18466, 18467, 18468, 18469, 18470, 18471, 18472, 18473, 18474, 18475, 18476, 18477, 18478, 18479, 18480, 18481, 18482, 18483, 18484, 18485, 18486, 18487, 18488, 18489, 18490, 18491, 18492, 18493, 18494, 18495, 18496, 18497, 18498, 18499, 18500, 18501, 18502, 18503, 18504, 18505, 18506, 18507, 18508, 18509, 18510, 18511, 18512, 18513, 18514, 18515, 18516, 18517, 18518, 18519, 18520, 18521, 18522, 18523, 18524, 18525, 18526, 18527, 18528, 18529, 18530, 18531, 18532, 18533, 18534, 18535, 18536, 18537, 18538, 18539, 18540, 18541, 18542, 18543, 18544, 18545, 18546, 18547, 18548, 18549, 18550, 18551, 18552, 18553, 18554, 18555, 18556, 18557, 18558, 18559, 18560, 18561, 18562, 18563, 18564, 18565, 18566, 18567, 18568, 18569, 18570, 18571, 18572, 18573, 18574, 18575, 18576, 18577, 18578, 18579, 18580, 18581, 18582, 18583, 18584, 18585, 18586, 18587, 18588, 18589, 18590, 18591, 18592, 18593, 18594, 18595, 18596, 18597, 18598, 18599, 18600, 18601, 18602, 18603, 18604, 18605, 18606, 18607, 18608, 18609, 18610, 18611, 18612, 18613, 18614, 18615, 18616, 18617, 18618, 18619, 18620, 18621, 18622, 18623, 18624, 18625, 18626, 18627, 18628, 18629, 18630, 18631, 18632, 18633, 18634, 18635, 18636, 18637, 18638, 18639, 18640, 18641, 18642, 18643, 18644, 18645, 18646, 18647, 18648, 18649, 18650, 18651, 18652, 18653, 18654, 18655, 18656, 18657, 18658, 18659, 18660, 18661, 18662, 18663, 18664, 18665, 18666, 18667, 18668, 18669, 18670, 18671, 18672, 18673, 18674, 18675, 18676, 18677, 18678, 18679, 18680, 18681, 18682, 18683, 18684, 18685, 18686, 18687, 18688, 18689, 18690, 18691, 18692, 18693, 18694, 18695, 18696, 18697, 18698, 18699, 18700, 18701, 18702, 18703, 18704, 18705, 18706, 18707, 18708, 18709, 18710, 18711, 18712, 18713, 18714, 18715, 18716, 18717, 18718, 18719, 18720, 18721, 18722, 18723, 18724, 18725, 18726, 18727, 18728, 18729, 18730, 18731, 18732, 18733, 18734, 18735, 18736, 18737, 18738, 18739, 18740, 18741, 18742, 18743, 18744, 18745, 18746, 18747, 18748, 18749, 18750, 18751, 18752, 18753, 18754, 18755, 18756, 18757, 18758, 18759, 18760, 18761, 18762, 18763, 18764, 18765, 18766, 18767, 18768, 18769, 18770, 18771, 18772, 18773, 18774, 18775, 18776, 18777, 18778, 18779, 18780, 18781, 18782, 18783, 18784, 18785, 18786, 18787, 18788, 18789, 18790, 18791, 18792, 18793, 18794, 18795, 18796, 18797, 18798, 18799, 18800, 18801, 18802, 18803, 18804, 18805, 18806, 18807, 18808, 18809, 18810, 18811, 18812, 18813, 18814, 18815, 18816, 18817, 18818, 18819, 18820, 18821, 18822, 18823, 18824, 18825, 18826, 18827, 18828, 18829, 18830, 18831, 18832, 18833, 18834, 18835, 18836, 18837, 18838, 18839, 18840, 18841, 18842, 18843, 18844, 18845, 18846, 18847, 18848, 18849, 18850, 18851, 18852, 18853, 18854, 18855, 18856, 18857, 18858, 18859, 18860, 18861, 18862, 18863, 18864, 18865, 18866, 18867, 18868, 18869, 18870, 18871, 18872, 18873, 18874, 18875, 18876, 18877, 18878, 18879, 18880, 18881, 18882, 18883, 18884, 18885, 18886, 18887, 18888, 18889, 18890, 18891, 18892, 18893, 18894, 18895, 18896, 18897, 18898, 18899, 18900, 18901, 18902, 18903, 18904, 18905, 18906, 18907, 18908, 18909, 18910, 18911, 18912, 18913, 18914, 18915, 18916, 18917, 18918, 18919, 18920, 18921, 18922, 18923, 18924, 18925, 18926, 18927, 18928, 18929, 18930, 18931, 18932, 18933, 18934, 18935, 18936, 18937, 18938, 18939, 18940, 18941, 18942, 18943, 18944, 18945, 18946, 18947, 18948, 18949, 18950, 18951, 18952, 18953, 18954, 18955, 18956, 18957, 18958, 18959, 18960, 18961, 18962, 18963, 18964, 18965, 18966, 18967, 18968, 18969, 18970, 18971, 18972, 18973, 18974, 18975, 18976, 18977, 18978, 18979, 18980, 18981, 18982, 18983, 18984, 18985, 18986, 18987, 18988, 18989, 18990, 18991, 18992, 18993, 18994, 18995, 18996, 18997, 18998, 18999, 19000, 19001, 19002, 19003, 19004, 19005, 19006, 19007, 19008, 19009, 19010, 19011, 19012, 19013, 19014, 19015, 19016, 19017, 19018, 19019, 19020, 19021, 19022, 19023, 19024, 19025, 19026, 19027, 19028, 19029, 19030, 19031, 19032, 19033, 19034, 19035, 19036, 19037, 19038, 19039, 19040, 19041, 19042, 19043, 19044, 19045, 19046, 19047, 19048, 19049, 19050, 19051, 19052, 19053, 19054, 19055, 19056, 19057, 19058, 19059, 19060, 19061, 19062, 19063, 19064, 19065, 19066, 19067, 19068, 19069, 19070, 19071, 19072, 19073, 19074, 19075, 19076, 19077, 19078, 19079, 19080, 19081, 19082, 19083, 19084, 19085, 19086, 19087, 19088, 19089, 19090, 19091, 19092, 19093, 19094, 19095, 19096, 19097, 19098, 19099, 19100, 19101, 19102, 19103, 19104, 19105, 19106, 19107, 19108, 19109, 19110, 19111, 19112, 19113, 19114, 19115, 19116, 19117, 19118, 19119, 19120, 19121, 19122, 19123, 19124, 19125, 19126, 19127, 19128, 19129, 19130, 19131, 19132, 19133, 19134, 19135, 19136, 19137, 19138, 19139, 19140, 19141, 19142, 19143, 19144, 19145, 19146, 19147, 19148, 19149, 19150, 19151, 19152, 19153, 19154, 19155, 19156, 19157, 19158, 19159, 19160, 19161, 19162, 19163, 19164, 19165, 19166, 19167, 19168, 19169, 19170, 19171, 19172, 19173, 19174, 19175, 19176, 19177, 19178, 19179, 19180, 19181, 19182, 19183, 19184, 19185, 19186, 19187, 19188, 19189, 19190, 19191, 19192, 19193, 19194, 19195, 19196, 19197, 19198, 19199, 19200, 19201, 19202, 19203, 19204, 19205, 19206, 19207, 19208, 19209, 19210, 19211, 19212, 19213, 19214, 19215, 19216, 19217, 19218, 19219, 19220, 19221, 19222, 19223, 19224, 19225, 19226, 19227, 19228, 19229, 19230, 19231, 19232, 19233, 19234, 19235, 19236, 19237, 19238, 19239, 19240, 19241, 19242, 19243, 19244, 19245, 19246, 19247, 19248, 19249, 19250, 19251, 19252, 19253, 19254, 19255, 19256, 19257, 19258, 19259, 19260, 19261, 19262, 19263, 19264, 19265, 19266, 19267, 19268, 19269, 19270, 19271, 19272, 19273, 19274, 19275, 19276, 19277, 19278, 19279, 19280, 19281, 19282, 19283, 19284, 19285, 19286, 19287, 19288, 19289, 19290, 19291, 19292, 19293, 19294, 19295, 19296, 19297, 19298, 19299, 19300, 19301, 19302, 19303, 19304, 19305, 19306, 19307, 19308, 19309, 19310, 19311, 19312, 19313, 19314, 19315, 19316, 19317, 19318, 19319, 19320, 19321, 19322, 19323, 19324, 19325, 19326, 19327, 19328, 19329, 19330, 19331, 19332, 19333, 19334, 19335, 19336, 19337, 19338, 19339, 19340, 19341, 19342, 19343, 19344, 19345, 19346, 19347, 19348, 19349, 19350, 19351, 19352, 19353, 19354, 19355, 19356, 19357, 19358, 19359, 19360, 19361, 19362, 19363, 19364, 19365, 19366, 19367, 19368, 19369, 19370, 19371, 19372, 19373, 19374, 19375, 19376, 19377, 19378, 19379, 19380, 19381, 19382, 19383, 19384, 19385, 19386, 19387, 19388, 19389, 19390, 19391, 19392, 19393, 19394, 19395, 19396, 19397, 19398, 19399, 19400, 19401, 19402, 19403, 19404, 19405, 19406, 19407, 19408, 19409, 19410, 19411, 19412, 19413, 19414, 19415, 19416, 19417, 19418, 19419, 19420, 19421, 19422, 19423, 19424, 19425, 19426, 19427, 19428, 19429, 19430, 19431, 19432, 19433, 19434, 19435, 19436, 19437, 19438, 19439, 19440, 19441, 19442, 19443, 19444, 19445, 19446, 19447, 19448, 19449, 19450, 19451, 19452, 19453, 19454, 19455, 19456, 19457, 19458, 19459, 19460, 19461, 19462, 19463, 19464, 19465, 19466, 19467, 19468, 19469, 19470, 19471, 19472, 19473, 19474, 19475, 19476, 19477, 19478, 19479, 19480, 19481, 19482, 19483, 19484, 19485, 19486, 19487, 19488, 19489, 19490, 19491, 19492, 19493, 19494, 19495, 19496, 19497, 19498, 19499, 19500, 19501, 19502, 19503, 19504, 19505, 19506, 19507, 19508, 19509, 19510, 19511, 19512, 19513, 19514, 19515, 19516, 19517, 19518, 19519, 19520, 19521, 19522, 19523, 19524, 19525, 19526, 19527, 19528, 19529, 19530, 19531, 19532, 19533, 19534, 19535, 19536, 19537, 19538, 19539, 19540, 19541, 19542, 19543, 19544, 19545, 19546, 19547, 19548, 19549, 19550, 19551, 19552, 19553, 19554, 19555, 19556, 19557, 19558, 19559, 19560, 19561, 19562, 19563, 19564, 19565, 19566, 19567, 19568, 19569, 19570, 19571, 19572, 19573, 19574, 19575, 19576, 19577, 19578, 19579, 19580, 19581, 19582, 19583, 19584, 19585, 19586, 19587, 19588, 19589, 19590, 19591, 19592, 19593, 19594, 19595, 19596, 19597, 19598, 19599, 19600, 19601, 19602, 19603, 19604, 19605, 19606, 19607, 19608, 19609, 19610, 19611, 19612, 19613, 19614, 19615, 19616, 19617, 19618, 19619, 19620, 19621, 19622, 19623, 19624, 19625, 19626, 19627, 19628, 19629, 19630, 19631, 19632, 19633, 19634, 19635, 19636, 19637, 19638, 19639, 19640, 19641, 19642, 19643, 19644, 19645, 19646, 19647, 19648, 19649, 19650, 19651, 19652, 19653, 19654, 19655, 19656, 19657, 19658, 19659, 19660, 19661, 19662, 19663, 19664, 19665, 19666, 19667, 19668, 19669, 19670, 19671, 19672, 19673, 19674, 19675, 19676, 19677, 19678, 19679, 19680, 19681, 19682, 19683, 19684, 19685, 19686, 19687, 19688, 19689, 19690, 19691, 19692, 19693, 19694, 19695, 19696, 19697, 19698, 19699, 19700, 19701, 19702, 19703, 19704, 19705, 19706, 19707, 19708, 19709, 19710, 19711, 19712, 19713, 19714, 19715, 19716, 19717, 19718, 19719, 19720, 19721, 19722, 19723, 19724, 19725, 19726, 19727, 19728, 19729, 19730, 19731, 19732, 19733, 19734, 19735, 19736, 19737, 19738, 19739, 19740, 19741, 19742, 19743, 19744, 19745, 19746, 19747, 19748, 19749, 19750, 19751, 19752, 19753, 19754, 19755, 19756, 19757, 19758, 19759, 19760, 19761, 19762, 19763, 19764, 19765, 19766, 19767, 19768, 19769, 19770, 19771, 19772, 19773, 19774, 19775, 19776, 19777, 19778, 19779, 19780, 19781, 19782, 19783, 19784, 19785, 19786, 19787, 19788, 19789, 19790, 19791, 19792, 19793, 19794, 19795, 19796, 19797, 19798, 19799, 19800, 19801, 19802, 19803, 19804, 19805, 19806, 19807, 19808, 19809, 19810, 19811, 19812, 19813, 19814, 19815, 19816, 19817, 19818, 19819, 19820, 19821, 19822, 19823, 19824, 19825, 19826, 19827, 19828, 19829, 19830, 19831, 19832, 19833, 19834, 19835, 19836, 19837, 19838, 19839, 19840, 19841, 19842, 19843, 19844, 19845, 19846, 19847, 19848, 19849, 19850, 19851, 19852, 19853, 19854, 19855, 19856, 19857, 19858, 19859, 19860, 19861, 19862, 19863, 19864, 19865, 19866, 19867, 19868, 19869, 19870, 19871, 19872, 19873, 19874, 19875, 19876, 19877, 19878, 19879, 19880, 19881, 19882, 19883, 19884, 19885, 19886, 19887, 19888, 19889, 19890, 19891, 19892, 19893, 19894, 19895, 19896, 19897, 19898, 19899, 19900, 19901, 19902, 19903, 19904, 19905, 19906, 19907, 19908, 19909, 19910, 19911, 19912, 19913, 19914, 19915, 19916, 19917, 19918, 19919, 19920, 19921, 19922, 19923, 19924, 19925, 19926, 19927, 19928, 19929, 19930, 19931, 19932, 19933, 19934, 19935, 19936, 19937, 19938, 19939, 19940, 19941, 19942, 19943, 19944, 19945, 19946, 19947, 19948, 19949, 19950, 19951, 19952, 19953, 19954, 19955, 19956, 19957, 19958, 19959, 19960, 19961, 19962, 19963, 19964, 19965, 19966, 19967, 19968, 19969, 19970, 19971, 19972, 19973, 19974, 19975, 19976, 19977, 19978, 19979, 19980, 19981, 19982, 19983, 19984, 19985, 19986, 19987, 19988, 19989, 19990, 19991, 19992, 19993, 19994, 19995, 19996, 19997, 19998, 19999, 20000, 20001, 20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012, 20013, 20014, 20015, 20016, 20017, 20018, 20019, 20020, 20021, 20022, 20023, 20024, 20025, 20026, 20027, 20028, 20029, 20030, 20031, 20032, 20033, 20034, 20035, 20036, 20037, 20038, 20039, 20040, 20041, 20042, 20043, 20044, 20045, 20046, 20047, 20048, 20049, 20050, 20051, 20052, 20053, 20054, 20055, 20056, 20057, 20058, 20059, 20060, 20061, 20062, 20063, 20064, 20065, 20066, 20067, 20068, 20069, 20070, 20071, 20072, 20073, 20074, 20075, 20076, 20077, 20078, 20079, 20080, 20081, 20082, 20083, 20084, 20085, 20086, 20087, 20088, 20089, 20090, 20091, 20092, 20093, 20094, 20095, 20096, 20097, 20098, 20099, 20100, 20101, 20102, 20103, 20104, 20105, 20106, 20107, 20108, 20109, 20110, 20111, 20112, 20113, 20114, 20115, 20116, 20117, 20118, 20119, 20120, 20121, 20122, 20123, 20124, 20125, 20126, 20127, 20128, 20129, 20130, 20131, 20132, 20133, 20134, 20135, 20136, 20137, 20138, 20139, 20140, 20141, 20142, 20143, 20144, 20145, 20146, 20147, 20148, 20149, 20150, 20151, 20152, 20153, 20154, 20155, 20156, 20157, 20158, 20159, 20160, 20161, 20162, 20163, 20164, 20165, 20166, 20167, 20168, 20169, 20170, 20171, 20172, 20173, 20174, 20175, 20176, 20177, 20178, 20179, 20180, 20181, 20182, 20183, 20184, 20185, 20186, 20187, 20188, 20189, 20190, 20191, 20192, 20193, 20194, 20195, 20196, 20197, 20198, 20199, 20200, 20201, 20202, 20203, 20204, 20205, 20206, 20207, 20208, 20209, 20210, 20211, 20212, 20213, 20214, 20215, 20216, 20217, 20218, 20219, 20220, 20221, 20222, 20223, 20224, 20225, 20226, 20227, 20228, 20229, 20230, 20231, 20232, 20233, 20234, 20235, 20236, 20237, 20238, 20239, 20240, 20241, 20242, 20243, 20244, 20245, 20246, 20247, 20248, 20249, 20250, 20251, 20252, 20253, 20254, 20255, 20256, 20257, 20258, 20259, 20260, 20261, 20262, 20263, 20264, 20265, 20266, 20267, 20268, 20269, 20270, 20271, 20272, 20273, 20274, 20275, 20276, 20277, 20278, 20279, 20280, 20281, 20282, 20283, 20284, 20285, 20286, 20287, 20288, 20289, 20290, 20291, 20292, 20293, 20294, 20295, 20296, 20297, 20298, 20299, 20300, 20301, 20302, 20303, 20304, 20305, 20306, 20307, 20308, 20309, 20310, 20311, 20312, 20313, 20314, 20315, 20316, 20317, 20318, 20319, 20320, 20321, 20322, 20323, 20324, 20325, 20326, 20327, 20328, 20329, 20330, 20331, 20332, 20333, 20334, 20335, 20336, 20337, 20338, 20339, 20340, 20341, 20342, 20343, 20344, 20345, 20346, 20347, 20348, 20349, 20350, 20351, 20352, 20353, 20354, 20355, 20356, 20357, 20358, 20359, 20360, 20361, 20362, 20363, 20364, 20365, 20366, 20367, 20368, 20369, 20370, 20371, 20372, 20373, 20374, 20375, 20376, 20377, 20378, 20379, 20380, 20381, 20382, 20383, 20384, 20385, 20386, 20387, 20388, 20389, 20390, 20391, 20392, 20393, 20394, 20395, 20396, 20397, 20398]\n"," This is the range of val:  [ 7700  7701  7702 ... 12097 12098 12099]\n","Starting training\n","shuffling\n","Epoch 1/200\n","2020-05-24 07:43:01.257853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 07:43:02.734866: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","1600/1600 [==============================] - 21s 13ms/step - loss: 167.7855 - val_loss: 4.5819\n","\n","Epoch 00001: loss improved from inf to 167.79082, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 42.2180 - val_loss: 37.4919\n","\n","Epoch 00002: loss improved from 167.79082 to 42.21834, saving model to final.h5\n","Epoch 3/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 28.2576 - val_loss: 23.6285\n","\n","Epoch 00003: loss improved from 42.21834 to 28.25722, saving model to final.h5\n","Epoch 4/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 25.0553 - val_loss: 17.8329\n","\n","Epoch 00004: loss improved from 28.25722 to 25.05499, saving model to final.h5\n","Epoch 5/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 23.6743 - val_loss: 14.3876\n","\n","Epoch 00005: loss improved from 25.05499 to 23.67479, saving model to final.h5\n","Epoch 6/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 22.3241 - val_loss: 14.6462\n","\n","Epoch 00006: loss improved from 23.67479 to 22.32469, saving model to final.h5\n","Epoch 7/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 21.6313 - val_loss: 13.7044\n","\n","Epoch 00007: loss improved from 22.32469 to 21.63195, saving model to final.h5\n","Epoch 8/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 20.9676 - val_loss: 12.7083\n","\n","Epoch 00008: loss improved from 21.63195 to 20.96762, saving model to final.h5\n","Epoch 9/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 19.8457 - val_loss: 12.1172\n","\n","Epoch 00009: loss improved from 20.96762 to 19.84592, saving model to final.h5\n","Epoch 10/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 19.5704 - val_loss: 11.2823\n","\n","Epoch 00010: loss improved from 19.84592 to 19.57091, saving model to final.h5\n","Epoch 11/200\n","1600/1600 [==============================] - 15s 9ms/step - loss: 19.2351 - val_loss: 11.0473\n","\n","Epoch 00011: loss improved from 19.57091 to 19.23532, saving model to final.h5\n","Epoch 12/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 18.5951 - val_loss: 11.6254\n","\n","Epoch 00012: loss improved from 19.23532 to 18.59452, saving model to final.h5\n","Epoch 13/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 17.8911 - val_loss: 11.2293\n","\n","Epoch 00013: loss improved from 18.59452 to 17.89178, saving model to final.h5\n","Epoch 14/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 17.4430 - val_loss: 10.0832\n","\n","Epoch 00014: loss improved from 17.89178 to 17.44127, saving model to final.h5\n","Epoch 15/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 16.9060 - val_loss: 9.9506\n","\n","Epoch 00015: loss improved from 17.44127 to 16.90600, saving model to final.h5\n","Epoch 16/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 16.7091 - val_loss: 10.6187\n","\n","Epoch 00016: loss improved from 16.90600 to 16.70887, saving model to final.h5\n","Epoch 17/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 16.7272 - val_loss: 9.4454\n","\n","Epoch 00017: loss did not improve from 16.70887\n","Epoch 18/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 16.0691 - val_loss: 9.2731\n","\n","Epoch 00018: loss improved from 16.70887 to 16.06961, saving model to final.h5\n","Epoch 19/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 15.6551 - val_loss: 9.4035\n","\n","Epoch 00019: loss improved from 16.06961 to 15.65584, saving model to final.h5\n","Epoch 20/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 15.6790 - val_loss: 8.8959\n","\n","Epoch 00020: loss did not improve from 15.65584\n","Epoch 21/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 15.3511 - val_loss: 8.6819\n","\n","Epoch 00021: loss improved from 15.65584 to 15.35076, saving model to final.h5\n","Epoch 22/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 14.9096 - val_loss: 8.1865\n","\n","Epoch 00022: loss improved from 15.35076 to 14.90924, saving model to final.h5\n","Epoch 23/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 14.5332 - val_loss: 8.7043\n","\n","Epoch 00023: loss improved from 14.90924 to 14.53340, saving model to final.h5\n","Epoch 24/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 14.3212 - val_loss: 7.8334\n","\n","Epoch 00024: loss improved from 14.53340 to 14.32118, saving model to final.h5\n","Epoch 25/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 14.4585 - val_loss: 7.5592\n","\n","Epoch 00025: loss did not improve from 14.32118\n","Epoch 26/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 13.9514 - val_loss: 9.0544\n","\n","Epoch 00026: loss improved from 14.32118 to 13.95202, saving model to final.h5\n","Epoch 27/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 13.7783 - val_loss: 7.9869\n","\n","Epoch 00027: loss improved from 13.95202 to 13.77785, saving model to final.h5\n","Epoch 28/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 13.5191 - val_loss: 7.4868\n","\n","Epoch 00028: loss improved from 13.77785 to 13.51968, saving model to final.h5\n","Epoch 29/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 13.3021 - val_loss: 8.2015\n","\n","Epoch 00029: loss improved from 13.51968 to 13.30164, saving model to final.h5\n","Epoch 30/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 13.0610 - val_loss: 7.3217\n","\n","Epoch 00030: loss improved from 13.30164 to 13.06139, saving model to final.h5\n","Epoch 31/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 13.0389 - val_loss: 7.7764\n","\n","Epoch 00031: loss improved from 13.06139 to 13.03872, saving model to final.h5\n","Epoch 32/200\n","1600/1600 [==============================] - 15s 9ms/step - loss: 12.7502 - val_loss: 7.7031\n","\n","Epoch 00032: loss improved from 13.03872 to 12.75016, saving model to final.h5\n","Epoch 33/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 12.6133 - val_loss: 7.0920\n","\n","Epoch 00033: loss improved from 12.75016 to 12.61342, saving model to final.h5\n","Epoch 34/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 12.1032 - val_loss: 7.6396\n","\n","Epoch 00034: loss improved from 12.61342 to 12.10369, saving model to final.h5\n","Epoch 35/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 12.1553 - val_loss: 7.4121\n","\n","Epoch 00035: loss did not improve from 12.10369\n","Epoch 36/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 12.2433 - val_loss: 7.6404\n","\n","Epoch 00036: loss did not improve from 12.10369\n","Epoch 37/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 12.0902 - val_loss: 7.8329\n","\n","Epoch 00037: loss improved from 12.10369 to 12.08998, saving model to final.h5\n","Epoch 38/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 11.9376 - val_loss: 7.6468\n","\n","Epoch 00038: loss improved from 12.08998 to 11.93749, saving model to final.h5\n","Epoch 39/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 11.7525 - val_loss: 7.4491\n","\n","Epoch 00039: loss improved from 11.93749 to 11.75303, saving model to final.h5\n","Epoch 40/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 11.5495 - val_loss: 6.9303\n","\n","Epoch 00040: loss improved from 11.75303 to 11.54965, saving model to final.h5\n","Epoch 41/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 11.5743 - val_loss: 7.1928\n","\n","Epoch 00041: loss did not improve from 11.54965\n","Epoch 42/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 11.3094 - val_loss: 7.0023\n","\n","Epoch 00042: loss improved from 11.54965 to 11.30953, saving model to final.h5\n","Epoch 43/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 11.1495 - val_loss: 7.2021\n","\n","Epoch 00043: loss improved from 11.30953 to 11.14822, saving model to final.h5\n","Epoch 44/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 10.9309 - val_loss: 6.8285\n","\n","Epoch 00044: loss improved from 11.14822 to 10.93098, saving model to final.h5\n","Epoch 45/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 10.8315 - val_loss: 7.4020\n","\n","Epoch 00045: loss improved from 10.93098 to 10.83148, saving model to final.h5\n","Epoch 46/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 10.7582 - val_loss: 7.1961\n","\n","Epoch 00046: loss improved from 10.83148 to 10.75762, saving model to final.h5\n","Epoch 47/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 10.8008 - val_loss: 7.0193\n","\n","Epoch 00047: loss did not improve from 10.75762\n","Epoch 48/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 10.6365 - val_loss: 7.2210\n","\n","Epoch 00048: loss improved from 10.75762 to 10.63662, saving model to final.h5\n","Epoch 49/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 10.5202 - val_loss: 7.1673\n","\n","Epoch 00049: loss improved from 10.63662 to 10.52028, saving model to final.h5\n","Epoch 50/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 10.4671 - val_loss: 7.1648\n","\n","Epoch 00050: loss improved from 10.52028 to 10.46749, saving model to final.h5\n","Epoch 51/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 10.3581 - val_loss: 7.3559\n","\n","Epoch 00051: loss improved from 10.46749 to 10.35824, saving model to final.h5\n","Epoch 52/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 10.1999 - val_loss: 7.3834\n","\n","Epoch 00052: loss improved from 10.35824 to 10.19974, saving model to final.h5\n","Epoch 53/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.9950 - val_loss: 7.0288\n","\n","Epoch 00053: loss improved from 10.19974 to 9.99513, saving model to final.h5\n","Epoch 54/200\n","1600/1600 [==============================] - 15s 9ms/step - loss: 10.2697 - val_loss: 6.8454\n","\n","Epoch 00054: loss did not improve from 9.99513\n","Epoch 55/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.9877 - val_loss: 6.9071\n","\n","Epoch 00055: loss improved from 9.99513 to 9.98689, saving model to final.h5\n","Epoch 56/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.7564 - val_loss: 6.7372\n","\n","Epoch 00056: loss improved from 9.98689 to 9.75662, saving model to final.h5\n","Epoch 57/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.7250 - val_loss: 6.9600\n","\n","Epoch 00057: loss improved from 9.75662 to 9.72525, saving model to final.h5\n","Epoch 58/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.8559 - val_loss: 6.1428\n","\n","Epoch 00058: loss did not improve from 9.72525\n","Epoch 59/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.5814 - val_loss: 6.2282\n","\n","Epoch 00059: loss improved from 9.72525 to 9.58112, saving model to final.h5\n","Epoch 60/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.5097 - val_loss: 6.3214\n","\n","Epoch 00060: loss improved from 9.58112 to 9.50951, saving model to final.h5\n","Epoch 61/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.3637 - val_loss: 5.9334\n","\n","Epoch 00061: loss improved from 9.50951 to 9.36246, saving model to final.h5\n","Epoch 62/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.2916 - val_loss: 6.4430\n","\n","Epoch 00062: loss improved from 9.36246 to 9.29113, saving model to final.h5\n","Epoch 63/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.1931 - val_loss: 6.4511\n","\n","Epoch 00063: loss improved from 9.29113 to 9.19321, saving model to final.h5\n","Epoch 64/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.2061 - val_loss: 6.2333\n","\n","Epoch 00064: loss did not improve from 9.19321\n","Epoch 65/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.0702 - val_loss: 5.8454\n","\n","Epoch 00065: loss improved from 9.19321 to 9.07044, saving model to final.h5\n","Epoch 66/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.1525 - val_loss: 6.0481\n","\n","Epoch 00066: loss did not improve from 9.07044\n","Epoch 67/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.9881 - val_loss: 6.0416\n","\n","Epoch 00067: loss improved from 9.07044 to 8.98846, saving model to final.h5\n","Epoch 68/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.7665 - val_loss: 5.8033\n","\n","Epoch 00068: loss improved from 8.98846 to 8.76674, saving model to final.h5\n","Epoch 69/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 9.0800 - val_loss: 5.7968\n","\n","Epoch 00069: loss did not improve from 8.76674\n","Epoch 70/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.8860 - val_loss: 6.3664\n","\n","Epoch 00070: loss did not improve from 8.76674\n","Epoch 71/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.8003 - val_loss: 6.3214\n","\n","Epoch 00071: loss did not improve from 8.76674\n","Epoch 72/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.5991 - val_loss: 5.8098\n","\n","Epoch 00072: loss improved from 8.76674 to 8.59934, saving model to final.h5\n","Epoch 73/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.6272 - val_loss: 5.8716\n","\n","Epoch 00073: loss did not improve from 8.59934\n","Epoch 74/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.4956 - val_loss: 5.8555\n","\n","Epoch 00074: loss improved from 8.59934 to 8.49563, saving model to final.h5\n","Epoch 75/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.5365 - val_loss: 5.5266\n","\n","Epoch 00075: loss did not improve from 8.49563\n","Epoch 76/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.5683 - val_loss: 5.7046\n","\n","Epoch 00076: loss did not improve from 8.49563\n","Epoch 77/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.4813 - val_loss: 5.2997\n","\n","Epoch 00077: loss improved from 8.49563 to 8.48116, saving model to final.h5\n","Epoch 78/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.3029 - val_loss: 5.6533\n","\n","Epoch 00078: loss improved from 8.48116 to 8.30305, saving model to final.h5\n","Epoch 79/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.4634 - val_loss: 5.2744\n","\n","Epoch 00079: loss did not improve from 8.30305\n","Epoch 80/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.3229 - val_loss: 5.2260\n","\n","Epoch 00080: loss did not improve from 8.30305\n","Epoch 81/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.2343 - val_loss: 5.3766\n","\n","Epoch 00081: loss improved from 8.30305 to 8.23448, saving model to final.h5\n","Epoch 82/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.1270 - val_loss: 5.2164\n","\n","Epoch 00082: loss improved from 8.23448 to 8.12731, saving model to final.h5\n","Epoch 83/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.0737 - val_loss: 5.8722\n","\n","Epoch 00083: loss improved from 8.12731 to 8.07350, saving model to final.h5\n","Epoch 84/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.1165 - val_loss: 5.5412\n","\n","Epoch 00084: loss did not improve from 8.07350\n","Epoch 85/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 8.0911 - val_loss: 5.7015\n","\n","Epoch 00085: loss did not improve from 8.07350\n","Epoch 86/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.8375 - val_loss: 5.3415\n","\n","Epoch 00086: loss improved from 8.07350 to 7.83746, saving model to final.h5\n","Epoch 87/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.9567 - val_loss: 5.3980\n","\n","Epoch 00087: loss did not improve from 7.83746\n","Epoch 88/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.9616 - val_loss: 5.6200\n","\n","Epoch 00088: loss did not improve from 7.83746\n","Epoch 89/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.8498 - val_loss: 5.6606\n","\n","Epoch 00089: loss did not improve from 7.83746\n","Epoch 90/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.7293 - val_loss: 5.3694\n","\n","Epoch 00090: loss improved from 7.83746 to 7.72945, saving model to final.h5\n","Epoch 91/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.9330 - val_loss: 5.8848\n","\n","Epoch 00091: loss did not improve from 7.72945\n","Epoch 92/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.8022 - val_loss: 5.6803\n","\n","Epoch 00092: loss did not improve from 7.72945\n","Epoch 93/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.8149 - val_loss: 5.5060\n","\n","Epoch 00093: loss did not improve from 7.72945\n","Epoch 94/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.5678 - val_loss: 5.2016\n","\n","Epoch 00094: loss improved from 7.72945 to 7.56816, saving model to final.h5\n","Epoch 95/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.6705 - val_loss: 5.4077\n","\n","Epoch 00095: loss did not improve from 7.56816\n","Epoch 96/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.4148 - val_loss: 5.8579\n","\n","Epoch 00096: loss improved from 7.56816 to 7.41492, saving model to final.h5\n","Epoch 97/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.5484 - val_loss: 5.4699\n","\n","Epoch 00097: loss did not improve from 7.41492\n","Epoch 98/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.5122 - val_loss: 5.6774\n","\n","Epoch 00098: loss did not improve from 7.41492\n","Epoch 99/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.5048 - val_loss: 5.4365\n","\n","Epoch 00099: loss did not improve from 7.41492\n","Epoch 100/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.3086 - val_loss: 5.2669\n","\n","Epoch 00100: loss improved from 7.41492 to 7.30832, saving model to final.h5\n","Epoch 101/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.4395 - val_loss: 5.2090\n","\n","Epoch 00101: loss did not improve from 7.30832\n","Epoch 102/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.4400 - val_loss: 5.3874\n","\n","Epoch 00102: loss did not improve from 7.30832\n","Epoch 103/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.2439 - val_loss: 5.2228\n","\n","Epoch 00103: loss improved from 7.30832 to 7.24407, saving model to final.h5\n","Epoch 104/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.3763 - val_loss: 5.1699\n","\n","Epoch 00104: loss did not improve from 7.24407\n","Epoch 105/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.2702 - val_loss: 4.9314\n","\n","Epoch 00105: loss did not improve from 7.24407\n","Epoch 106/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.1851 - val_loss: 5.2334\n","\n","Epoch 00106: loss improved from 7.24407 to 7.18431, saving model to final.h5\n","Epoch 107/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.3213 - val_loss: 4.6094\n","\n","Epoch 00107: loss did not improve from 7.18431\n","Epoch 108/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.3872 - val_loss: 5.1942\n","\n","Epoch 00108: loss did not improve from 7.18431\n","Epoch 109/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.1915 - val_loss: 5.2951\n","\n","Epoch 00109: loss did not improve from 7.18431\n","Epoch 110/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.1679 - val_loss: 5.0444\n","\n","Epoch 00110: loss improved from 7.18431 to 7.16807, saving model to final.h5\n","Epoch 111/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.0177 - val_loss: 5.0289\n","\n","Epoch 00111: loss improved from 7.16807 to 7.01758, saving model to final.h5\n","Epoch 112/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.0681 - val_loss: 5.0502\n","\n","Epoch 00112: loss did not improve from 7.01758\n","Epoch 113/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.9786 - val_loss: 4.6333\n","\n","Epoch 00113: loss improved from 7.01758 to 6.97881, saving model to final.h5\n","Epoch 114/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.9297 - val_loss: 4.9184\n","\n","Epoch 00114: loss improved from 6.97881 to 6.92957, saving model to final.h5\n","Epoch 115/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.8853 - val_loss: 4.6376\n","\n","Epoch 00115: loss improved from 6.92957 to 6.88447, saving model to final.h5\n","Epoch 116/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.7806 - val_loss: 5.0322\n","\n","Epoch 00116: loss improved from 6.88447 to 6.78040, saving model to final.h5\n","Epoch 117/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 7.0279 - val_loss: 5.0975\n","\n","Epoch 00117: loss did not improve from 6.78040\n","Epoch 118/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.7944 - val_loss: 4.9839\n","\n","Epoch 00118: loss did not improve from 6.78040\n","Epoch 119/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.9394 - val_loss: 4.7002\n","\n","Epoch 00119: loss did not improve from 6.78040\n","Epoch 120/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.8282 - val_loss: 4.5521\n","\n","Epoch 00120: loss did not improve from 6.78040\n","Epoch 121/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.8006 - val_loss: 4.6263\n","\n","Epoch 00121: loss did not improve from 6.78040\n","Epoch 122/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.6412 - val_loss: 4.5379\n","\n","Epoch 00122: loss improved from 6.78040 to 6.64070, saving model to final.h5\n","Epoch 123/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.8118 - val_loss: 4.5409\n","\n","Epoch 00123: loss did not improve from 6.64070\n","Epoch 124/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.7537 - val_loss: 4.4835\n","\n","Epoch 00124: loss did not improve from 6.64070\n","Epoch 125/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.6845 - val_loss: 4.4463\n","\n","Epoch 00125: loss did not improve from 6.64070\n","Epoch 126/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.6643 - val_loss: 4.1445\n","\n","Epoch 00126: loss did not improve from 6.64070\n","Epoch 127/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.5815 - val_loss: 4.3529\n","\n","Epoch 00127: loss improved from 6.64070 to 6.58164, saving model to final.h5\n","Epoch 128/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.7916 - val_loss: 4.6820\n","\n","Epoch 00128: loss did not improve from 6.58164\n","Epoch 129/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.5795 - val_loss: 4.6022\n","\n","Epoch 00129: loss improved from 6.58164 to 6.57946, saving model to final.h5\n","Epoch 130/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.4763 - val_loss: 4.8684\n","\n","Epoch 00130: loss improved from 6.57946 to 6.47643, saving model to final.h5\n","Epoch 131/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.5527 - val_loss: 5.0712\n","\n","Epoch 00131: loss did not improve from 6.47643\n","Epoch 132/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.5186 - val_loss: 4.6829\n","\n","Epoch 00132: loss did not improve from 6.47643\n","Epoch 133/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.4431 - val_loss: 4.9567\n","\n","Epoch 00133: loss improved from 6.47643 to 6.44331, saving model to final.h5\n","Epoch 134/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.5174 - val_loss: 4.9930\n","\n","Epoch 00134: loss did not improve from 6.44331\n","Epoch 135/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.3754 - val_loss: 5.0534\n","\n","Epoch 00135: loss improved from 6.44331 to 6.37542, saving model to final.h5\n","Epoch 136/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.4102 - val_loss: 5.0315\n","\n","Epoch 00136: loss did not improve from 6.37542\n","Epoch 137/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.5326 - val_loss: 5.2021\n","\n","Epoch 00137: loss did not improve from 6.37542\n","Epoch 138/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.4678 - val_loss: 5.2552\n","\n","Epoch 00138: loss did not improve from 6.37542\n","Epoch 139/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.3955 - val_loss: 5.3904\n","\n","Epoch 00139: loss did not improve from 6.37542\n","Epoch 140/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.3435 - val_loss: 5.1499\n","\n","Epoch 00140: loss improved from 6.37542 to 6.34292, saving model to final.h5\n","Epoch 141/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.5099 - val_loss: 5.6458\n","\n","Epoch 00141: loss did not improve from 6.34292\n","Epoch 142/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.2779 - val_loss: 5.0152\n","\n","Epoch 00142: loss improved from 6.34292 to 6.27800, saving model to final.h5\n","Epoch 143/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.3850 - val_loss: 5.1013\n","\n","Epoch 00143: loss did not improve from 6.27800\n","Epoch 144/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.5085 - val_loss: 5.3907\n","\n","Epoch 00144: loss did not improve from 6.27800\n","Epoch 145/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.2063 - val_loss: 5.4259\n","\n","Epoch 00145: loss improved from 6.27800 to 6.20636, saving model to final.h5\n","Epoch 146/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.1835 - val_loss: 5.8783\n","\n","Epoch 00146: loss improved from 6.20636 to 6.18351, saving model to final.h5\n","Epoch 147/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.1962 - val_loss: 5.3276\n","\n","Epoch 00147: loss did not improve from 6.18351\n","Epoch 148/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.3686 - val_loss: 5.2681\n","\n","Epoch 00148: loss did not improve from 6.18351\n","Epoch 149/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.1208 - val_loss: 5.9112\n","\n","Epoch 00149: loss improved from 6.18351 to 6.12088, saving model to final.h5\n","Epoch 150/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.1740 - val_loss: 5.0257\n","\n","Epoch 00150: loss did not improve from 6.12088\n","Epoch 151/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.2243 - val_loss: 5.3762\n","\n","Epoch 00151: loss did not improve from 6.12088\n","Epoch 152/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.1217 - val_loss: 5.4204\n","\n","Epoch 00152: loss did not improve from 6.12088\n","Epoch 153/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.1965 - val_loss: 5.7108\n","\n","Epoch 00153: loss did not improve from 6.12088\n","Epoch 154/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.0998 - val_loss: 5.8599\n","\n","Epoch 00154: loss improved from 6.12088 to 6.09937, saving model to final.h5\n","Epoch 155/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.1899 - val_loss: 5.3856\n","\n","Epoch 00155: loss did not improve from 6.09937\n","Epoch 156/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.9423 - val_loss: 5.2263\n","\n","Epoch 00156: loss improved from 6.09937 to 5.94226, saving model to final.h5\n","Epoch 157/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.0451 - val_loss: 5.3862\n","\n","Epoch 00157: loss did not improve from 5.94226\n","Epoch 158/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.0216 - val_loss: 5.2535\n","\n","Epoch 00158: loss did not improve from 5.94226\n","Epoch 159/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.1387 - val_loss: 5.6336\n","\n","Epoch 00159: loss did not improve from 5.94226\n","Epoch 160/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.9294 - val_loss: 5.8262\n","\n","Epoch 00160: loss improved from 5.94226 to 5.92946, saving model to final.h5\n","Epoch 161/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.0158 - val_loss: 5.6107\n","\n","Epoch 00161: loss did not improve from 5.92946\n","Epoch 162/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.1853 - val_loss: 4.9154\n","\n","Epoch 00162: loss did not improve from 5.92946\n","Epoch 163/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.9196 - val_loss: 5.1568\n","\n","Epoch 00163: loss improved from 5.92946 to 5.91943, saving model to final.h5\n","Epoch 164/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.9229 - val_loss: 5.3139\n","\n","Epoch 00164: loss did not improve from 5.91943\n","Epoch 165/200\n","1600/1600 [==============================] - 15s 9ms/step - loss: 6.0196 - val_loss: 5.0065\n","\n","Epoch 00165: loss did not improve from 5.91943\n","Epoch 166/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.9220 - val_loss: 4.9406\n","\n","Epoch 00166: loss did not improve from 5.91943\n","Epoch 167/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.9229 - val_loss: 5.2900\n","\n","Epoch 00167: loss did not improve from 5.91943\n","Epoch 168/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.8928 - val_loss: 4.9336\n","\n","Epoch 00168: loss improved from 5.91943 to 5.89298, saving model to final.h5\n","Epoch 169/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.7667 - val_loss: 5.1817\n","\n","Epoch 00169: loss improved from 5.89298 to 5.76688, saving model to final.h5\n","Epoch 170/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 6.0013 - val_loss: 5.5047\n","\n","Epoch 00170: loss did not improve from 5.76688\n","Epoch 171/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.8767 - val_loss: 5.5512\n","\n","Epoch 00171: loss did not improve from 5.76688\n","Epoch 172/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.9216 - val_loss: 5.7187\n","\n","Epoch 00172: loss did not improve from 5.76688\n","Epoch 173/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.9182 - val_loss: 5.5611\n","\n","Epoch 00173: loss did not improve from 5.76688\n","Epoch 174/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.8555 - val_loss: 5.1440\n","\n","Epoch 00174: loss did not improve from 5.76688\n","Epoch 175/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.7899 - val_loss: 5.2672\n","\n","Epoch 00175: loss did not improve from 5.76688\n","Epoch 176/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.8852 - val_loss: 5.4674\n","\n","Epoch 00176: loss did not improve from 5.76688\n","Epoch 177/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.6774 - val_loss: 5.6276\n","\n","Epoch 00177: loss improved from 5.76688 to 5.67723, saving model to final.h5\n","Epoch 178/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.7874 - val_loss: 5.4012\n","\n","Epoch 00178: loss did not improve from 5.67723\n","Epoch 179/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.8111 - val_loss: 5.5536\n","\n","Epoch 00179: loss did not improve from 5.67723\n","Epoch 180/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.6124 - val_loss: 5.2978\n","\n","Epoch 00180: loss improved from 5.67723 to 5.61256, saving model to final.h5\n","Epoch 181/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.6131 - val_loss: 5.3777\n","\n","Epoch 00181: loss did not improve from 5.61256\n","Epoch 182/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.6800 - val_loss: 5.0463\n","\n","Epoch 00182: loss did not improve from 5.61256\n","Epoch 183/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.7367 - val_loss: 4.9558\n","\n","Epoch 00183: loss did not improve from 5.61256\n","Epoch 184/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.6727 - val_loss: 5.3668\n","\n","Epoch 00184: loss did not improve from 5.61256\n","Epoch 185/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.6713 - val_loss: 5.3842\n","\n","Epoch 00185: loss did not improve from 5.61256\n","Epoch 186/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.6794 - val_loss: 5.3710\n","\n","Epoch 00186: loss did not improve from 5.61256\n","Epoch 187/200\n","1600/1600 [==============================] - 15s 9ms/step - loss: 5.8474 - val_loss: 4.5294\n","\n","Epoch 00187: loss did not improve from 5.61256\n","Epoch 188/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.6475 - val_loss: 4.8847\n","\n","Epoch 00188: loss did not improve from 5.61256\n","Epoch 189/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.5298 - val_loss: 4.7676\n","\n","Epoch 00189: loss improved from 5.61256 to 5.53001, saving model to final.h5\n","Epoch 190/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.4902 - val_loss: 5.2252\n","\n","Epoch 00190: loss improved from 5.53001 to 5.49023, saving model to final.h5\n","Epoch 191/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.5975 - val_loss: 5.0664\n","\n","Epoch 00191: loss did not improve from 5.49023\n","Epoch 192/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.6982 - val_loss: 5.0763\n","\n","Epoch 00192: loss did not improve from 5.49023\n","Epoch 193/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.5184 - val_loss: 4.9870\n","\n","Epoch 00193: loss did not improve from 5.49023\n","Epoch 194/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.6864 - val_loss: 4.5440\n","\n","Epoch 00194: loss did not improve from 5.49023\n","Epoch 195/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.4900 - val_loss: 4.6379\n","\n","Epoch 00195: loss improved from 5.49023 to 5.49011, saving model to final.h5\n","Epoch 196/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.4695 - val_loss: 4.4174\n","\n","Epoch 00196: loss improved from 5.49011 to 5.46976, saving model to final.h5\n","Epoch 197/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.4822 - val_loss: 4.7509\n","\n","Epoch 00197: loss did not improve from 5.46976\n","Epoch 198/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.5116 - val_loss: 4.8832\n","\n","Epoch 00198: loss did not improve from 5.46976\n","Epoch 199/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.5179 - val_loss: 4.8289\n","\n","Epoch 00199: loss did not improve from 5.46976\n","Epoch 200/200\n","1600/1600 [==============================] - 14s 9ms/step - loss: 5.3927 - val_loss: 4.3022\n","\n","Epoch 00200: loss improved from 5.46976 to 5.39289, saving model to final.h5\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ig109GpTokgS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"fed95ace-1696-47da-88bd-3f3f53c04af4","executionInfo":{"status":"ok","timestamp":1590311673269,"user_tz":360,"elapsed":2439817,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}}},"source":["!python speedchallenge.py train.mp4 train.txt --epoch 200 --history 2 --model final.h5 --split_start 7700 --split_end=12100 --LR 0.00001 --mode=train"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-24 08:33:24.963900: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-24 08:33:26.688258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-24 08:33:26.700959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 08:33:26.701506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 08:33:26.701537: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 08:33:26.703201: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 08:33:26.704911: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 08:33:26.705280: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 08:33:26.706817: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 08:33:26.708037: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 08:33:26.711791: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 08:33:26.711965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 08:33:26.712557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 08:33:26.713091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 08:33:26.718201: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz\n","2020-05-24 08:33:26.718479: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b492c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 08:33:26.718521: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-24 08:33:26.806522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 08:33:26.807348: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b49480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 08:33:26.807379: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-24 08:33:26.807570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 08:33:26.808185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 08:33:26.808228: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 08:33:26.808264: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 08:33:26.808279: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 08:33:26.808304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 08:33:26.808317: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 08:33:26.808348: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 08:33:26.808362: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 08:33:26.808464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 08:33:26.809033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 08:33:26.809509: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 08:33:26.809550: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 08:33:27.319097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 08:33:27.319155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-24 08:33:27.319164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-24 08:33:27.319419: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 08:33:27.320056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 08:33:27.320541: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-24 08:33:27.320576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:188: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","speedchallenge.py:193: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 2, 100, 100, 2)    0         \n","_________________________________________________________________\n","time_distributed_1 (TimeDist (None, 2, 25, 25, 32)     4128      \n","_________________________________________________________________\n","time_distributed_2 (TimeDist (None, 2, 25, 25, 32)     0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 2, 25, 25, 32)     128       \n","_________________________________________________________________\n","time_distributed_3 (TimeDist (None, 2, 25, 25, 32)     0         \n","_________________________________________________________________\n","time_distributed_4 (TimeDist (None, 2, 7, 7, 64)       131136    \n","_________________________________________________________________\n","time_distributed_5 (TimeDist (None, 2, 7, 7, 64)       0         \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 2, 7, 7, 64)       256       \n","_________________________________________________________________\n","time_distributed_6 (TimeDist (None, 2, 7, 7, 64)       0         \n","_________________________________________________________________\n","time_distributed_7 (TimeDist (None, 2, 2, 2, 128)      524416    \n","_________________________________________________________________\n","time_distributed_8 (TimeDist (None, 2, 2, 2, 128)      0         \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 2, 2, 2, 128)      512       \n","_________________________________________________________________\n","time_distributed_9 (TimeDist (None, 2, 2, 2, 128)      0         \n","_________________________________________________________________\n","time_distributed_10 (TimeDis (None, 2, 512)            0         \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 128)               328192    \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 128)               0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 128)               16512     \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 128)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 129       \n","=================================================================\n","Total params: 1,005,409\n","Trainable params: 1,004,961\n","Non-trainable params: 448\n","_________________________________________________________________\n","None\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","Found preprocessed data\n","tcmalloc: large alloc 2447884288 bytes == 0x1e6c2000 @  0x7f0add7d51e7 0x7f0adb2bb5e1 0x7f0adb31fc78 0x7f0adb31ff37 0x7f0adb3b7f28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f0add3d2b97 0x5b250a\n","tcmalloc: large alloc 2447884288 bytes == 0xb053e000 @  0x7f0add7d51e7 0x7f0adb2bb5e1 0x7f0adb31fc78 0x7f0adb31ff37 0x7f0adb3b7f28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f0add3d2b97 0x5b250a\n","Loading frame 20398\n","done loading 20399 frames\n","Done prepping data\n","tcmalloc: large alloc 1631920128 bytes == 0x14245a000 @  0x7f0add7d51e7 0x7f0adb2bb5e1 0x7f0adb31fc78 0x7f0adb31fd93 0x7f0adb3aaed6 0x7f0adb3ab338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f0add3d2b97 0x5b250a\n","tcmalloc: large alloc 1631920128 bytes == 0x1e6c2000 @  0x7f0add7d51e7 0x7f0adb2bb5e1 0x7f0adb31fc78 0x7f0adb31fd93 0x7f0adb3aaed6 0x7f0adb3ab338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f0add3d2b97 0x5b250a\n","20399\n","20398 Training data size per Aug\n","15998 Train indices size\n","4400 Val indices size\n"," This is the range of train:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119, 6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129, 6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139, 6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169, 6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209, 6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229, 6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239, 6240, 6241, 6242, 6243, 6244, 6245, 6246, 6247, 6248, 6249, 6250, 6251, 6252, 6253, 6254, 6255, 6256, 6257, 6258, 6259, 6260, 6261, 6262, 6263, 6264, 6265, 6266, 6267, 6268, 6269, 6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289, 6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299, 6300, 6301, 6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309, 6310, 6311, 6312, 6313, 6314, 6315, 6316, 6317, 6318, 6319, 6320, 6321, 6322, 6323, 6324, 6325, 6326, 6327, 6328, 6329, 6330, 6331, 6332, 6333, 6334, 6335, 6336, 6337, 6338, 6339, 6340, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349, 6350, 6351, 6352, 6353, 6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363, 6364, 6365, 6366, 6367, 6368, 6369, 6370, 6371, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6379, 6380, 6381, 6382, 6383, 6384, 6385, 6386, 6387, 6388, 6389, 6390, 6391, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399, 6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419, 6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429, 6430, 6431, 6432, 6433, 6434, 6435, 6436, 6437, 6438, 6439, 6440, 6441, 6442, 6443, 6444, 6445, 6446, 6447, 6448, 6449, 6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459, 6460, 6461, 6462, 6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473, 6474, 6475, 6476, 6477, 6478, 6479, 6480, 6481, 6482, 6483, 6484, 6485, 6486, 6487, 6488, 6489, 6490, 6491, 6492, 6493, 6494, 6495, 6496, 6497, 6498, 6499, 6500, 6501, 6502, 6503, 6504, 6505, 6506, 6507, 6508, 6509, 6510, 6511, 6512, 6513, 6514, 6515, 6516, 6517, 6518, 6519, 6520, 6521, 6522, 6523, 6524, 6525, 6526, 6527, 6528, 6529, 6530, 6531, 6532, 6533, 6534, 6535, 6536, 6537, 6538, 6539, 6540, 6541, 6542, 6543, 6544, 6545, 6546, 6547, 6548, 6549, 6550, 6551, 6552, 6553, 6554, 6555, 6556, 6557, 6558, 6559, 6560, 6561, 6562, 6563, 6564, 6565, 6566, 6567, 6568, 6569, 6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577, 6578, 6579, 6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588, 6589, 6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599, 6600, 6601, 6602, 6603, 6604, 6605, 6606, 6607, 6608, 6609, 6610, 6611, 6612, 6613, 6614, 6615, 6616, 6617, 6618, 6619, 6620, 6621, 6622, 6623, 6624, 6625, 6626, 6627, 6628, 6629, 6630, 6631, 6632, 6633, 6634, 6635, 6636, 6637, 6638, 6639, 6640, 6641, 6642, 6643, 6644, 6645, 6646, 6647, 6648, 6649, 6650, 6651, 6652, 6653, 6654, 6655, 6656, 6657, 6658, 6659, 6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6669, 6670, 6671, 6672, 6673, 6674, 6675, 6676, 6677, 6678, 6679, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689, 6690, 6691, 6692, 6693, 6694, 6695, 6696, 6697, 6698, 6699, 6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6708, 6709, 6710, 6711, 6712, 6713, 6714, 6715, 6716, 6717, 6718, 6719, 6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729, 6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739, 6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6748, 6749, 6750, 6751, 6752, 6753, 6754, 6755, 6756, 6757, 6758, 6759, 6760, 6761, 6762, 6763, 6764, 6765, 6766, 6767, 6768, 6769, 6770, 6771, 6772, 6773, 6774, 6775, 6776, 6777, 6778, 6779, 6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6788, 6789, 6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 6804, 6805, 6806, 6807, 6808, 6809, 6810, 6811, 6812, 6813, 6814, 6815, 6816, 6817, 6818, 6819, 6820, 6821, 6822, 6823, 6824, 6825, 6826, 6827, 6828, 6829, 6830, 6831, 6832, 6833, 6834, 6835, 6836, 6837, 6838, 6839, 6840, 6841, 6842, 6843, 6844, 6845, 6846, 6847, 6848, 6849, 6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6859, 6860, 6861, 6862, 6863, 6864, 6865, 6866, 6867, 6868, 6869, 6870, 6871, 6872, 6873, 6874, 6875, 6876, 6877, 6878, 6879, 6880, 6881, 6882, 6883, 6884, 6885, 6886, 6887, 6888, 6889, 6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898, 6899, 6900, 6901, 6902, 6903, 6904, 6905, 6906, 6907, 6908, 6909, 6910, 6911, 6912, 6913, 6914, 6915, 6916, 6917, 6918, 6919, 6920, 6921, 6922, 6923, 6924, 6925, 6926, 6927, 6928, 6929, 6930, 6931, 6932, 6933, 6934, 6935, 6936, 6937, 6938, 6939, 6940, 6941, 6942, 6943, 6944, 6945, 6946, 6947, 6948, 6949, 6950, 6951, 6952, 6953, 6954, 6955, 6956, 6957, 6958, 6959, 6960, 6961, 6962, 6963, 6964, 6965, 6966, 6967, 6968, 6969, 6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979, 6980, 6981, 6982, 6983, 6984, 6985, 6986, 6987, 6988, 6989, 6990, 6991, 6992, 6993, 6994, 6995, 6996, 6997, 6998, 6999, 7000, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009, 7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019, 7020, 7021, 7022, 7023, 7024, 7025, 7026, 7027, 7028, 7029, 7030, 7031, 7032, 7033, 7034, 7035, 7036, 7037, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 7045, 7046, 7047, 7048, 7049, 7050, 7051, 7052, 7053, 7054, 7055, 7056, 7057, 7058, 7059, 7060, 7061, 7062, 7063, 7064, 7065, 7066, 7067, 7068, 7069, 7070, 7071, 7072, 7073, 7074, 7075, 7076, 7077, 7078, 7079, 7080, 7081, 7082, 7083, 7084, 7085, 7086, 7087, 7088, 7089, 7090, 7091, 7092, 7093, 7094, 7095, 7096, 7097, 7098, 7099, 7100, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7108, 7109, 7110, 7111, 7112, 7113, 7114, 7115, 7116, 7117, 7118, 7119, 7120, 7121, 7122, 7123, 7124, 7125, 7126, 7127, 7128, 7129, 7130, 7131, 7132, 7133, 7134, 7135, 7136, 7137, 7138, 7139, 7140, 7141, 7142, 7143, 7144, 7145, 7146, 7147, 7148, 7149, 7150, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7161, 7162, 7163, 7164, 7165, 7166, 7167, 7168, 7169, 7170, 7171, 7172, 7173, 7174, 7175, 7176, 7177, 7178, 7179, 7180, 7181, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189, 7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199, 7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209, 7210, 7211, 7212, 7213, 7214, 7215, 7216, 7217, 7218, 7219, 7220, 7221, 7222, 7223, 7224, 7225, 7226, 7227, 7228, 7229, 7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7239, 7240, 7241, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249, 7250, 7251, 7252, 7253, 7254, 7255, 7256, 7257, 7258, 7259, 7260, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7268, 7269, 7270, 7271, 7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281, 7282, 7283, 7284, 7285, 7286, 7287, 7288, 7289, 7290, 7291, 7292, 7293, 7294, 7295, 7296, 7297, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309, 7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318, 7319, 7320, 7321, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7329, 7330, 7331, 7332, 7333, 7334, 7335, 7336, 7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7349, 7350, 7351, 7352, 7353, 7354, 7355, 7356, 7357, 7358, 7359, 7360, 7361, 7362, 7363, 7364, 7365, 7366, 7367, 7368, 7369, 7370, 7371, 7372, 7373, 7374, 7375, 7376, 7377, 7378, 7379, 7380, 7381, 7382, 7383, 7384, 7385, 7386, 7387, 7388, 7389, 7390, 7391, 7392, 7393, 7394, 7395, 7396, 7397, 7398, 7399, 7400, 7401, 7402, 7403, 7404, 7405, 7406, 7407, 7408, 7409, 7410, 7411, 7412, 7413, 7414, 7415, 7416, 7417, 7418, 7419, 7420, 7421, 7422, 7423, 7424, 7425, 7426, 7427, 7428, 7429, 7430, 7431, 7432, 7433, 7434, 7435, 7436, 7437, 7438, 7439, 7440, 7441, 7442, 7443, 7444, 7445, 7446, 7447, 7448, 7449, 7450, 7451, 7452, 7453, 7454, 7455, 7456, 7457, 7458, 7459, 7460, 7461, 7462, 7463, 7464, 7465, 7466, 7467, 7468, 7469, 7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7479, 7480, 7481, 7482, 7483, 7484, 7485, 7486, 7487, 7488, 7489, 7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499, 7500, 7501, 7502, 7503, 7504, 7505, 7506, 7507, 7508, 7509, 7510, 7511, 7512, 7513, 7514, 7515, 7516, 7517, 7518, 7519, 7520, 7521, 7522, 7523, 7524, 7525, 7526, 7527, 7528, 7529, 7530, 7531, 7532, 7533, 7534, 7535, 7536, 7537, 7538, 7539, 7540, 7541, 7542, 7543, 7544, 7545, 7546, 7547, 7548, 7549, 7550, 7551, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559, 7560, 7561, 7562, 7563, 7564, 7565, 7566, 7567, 7568, 7569, 7570, 7571, 7572, 7573, 7574, 7575, 7576, 7577, 7578, 7579, 7580, 7581, 7582, 7583, 7584, 7585, 7586, 7587, 7588, 7589, 7590, 7591, 7592, 7593, 7594, 7595, 7596, 7597, 7598, 7599, 7600, 7601, 7602, 7603, 7604, 7605, 7606, 7607, 7608, 7609, 7610, 7611, 7612, 7613, 7614, 7615, 7616, 7617, 7618, 7619, 7620, 7621, 7622, 7623, 7624, 7625, 7626, 7627, 7628, 7629, 7630, 7631, 7632, 7633, 7634, 7635, 7636, 7637, 7638, 7639, 7640, 7641, 7642, 7643, 7644, 7645, 7646, 7647, 7648, 7649, 7650, 7651, 7652, 7653, 7654, 7655, 7656, 7657, 7658, 7659, 7660, 7661, 7662, 7663, 7664, 7665, 7666, 7667, 7668, 7669, 7670, 7671, 7672, 7673, 7674, 7675, 7676, 7677, 7678, 7679, 7680, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689, 7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699, 12100, 12101, 12102, 12103, 12104, 12105, 12106, 12107, 12108, 12109, 12110, 12111, 12112, 12113, 12114, 12115, 12116, 12117, 12118, 12119, 12120, 12121, 12122, 12123, 12124, 12125, 12126, 12127, 12128, 12129, 12130, 12131, 12132, 12133, 12134, 12135, 12136, 12137, 12138, 12139, 12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149, 12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159, 12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169, 12170, 12171, 12172, 12173, 12174, 12175, 12176, 12177, 12178, 12179, 12180, 12181, 12182, 12183, 12184, 12185, 12186, 12187, 12188, 12189, 12190, 12191, 12192, 12193, 12194, 12195, 12196, 12197, 12198, 12199, 12200, 12201, 12202, 12203, 12204, 12205, 12206, 12207, 12208, 12209, 12210, 12211, 12212, 12213, 12214, 12215, 12216, 12217, 12218, 12219, 12220, 12221, 12222, 12223, 12224, 12225, 12226, 12227, 12228, 12229, 12230, 12231, 12232, 12233, 12234, 12235, 12236, 12237, 12238, 12239, 12240, 12241, 12242, 12243, 12244, 12245, 12246, 12247, 12248, 12249, 12250, 12251, 12252, 12253, 12254, 12255, 12256, 12257, 12258, 12259, 12260, 12261, 12262, 12263, 12264, 12265, 12266, 12267, 12268, 12269, 12270, 12271, 12272, 12273, 12274, 12275, 12276, 12277, 12278, 12279, 12280, 12281, 12282, 12283, 12284, 12285, 12286, 12287, 12288, 12289, 12290, 12291, 12292, 12293, 12294, 12295, 12296, 12297, 12298, 12299, 12300, 12301, 12302, 12303, 12304, 12305, 12306, 12307, 12308, 12309, 12310, 12311, 12312, 12313, 12314, 12315, 12316, 12317, 12318, 12319, 12320, 12321, 12322, 12323, 12324, 12325, 12326, 12327, 12328, 12329, 12330, 12331, 12332, 12333, 12334, 12335, 12336, 12337, 12338, 12339, 12340, 12341, 12342, 12343, 12344, 12345, 12346, 12347, 12348, 12349, 12350, 12351, 12352, 12353, 12354, 12355, 12356, 12357, 12358, 12359, 12360, 12361, 12362, 12363, 12364, 12365, 12366, 12367, 12368, 12369, 12370, 12371, 12372, 12373, 12374, 12375, 12376, 12377, 12378, 12379, 12380, 12381, 12382, 12383, 12384, 12385, 12386, 12387, 12388, 12389, 12390, 12391, 12392, 12393, 12394, 12395, 12396, 12397, 12398, 12399, 12400, 12401, 12402, 12403, 12404, 12405, 12406, 12407, 12408, 12409, 12410, 12411, 12412, 12413, 12414, 12415, 12416, 12417, 12418, 12419, 12420, 12421, 12422, 12423, 12424, 12425, 12426, 12427, 12428, 12429, 12430, 12431, 12432, 12433, 12434, 12435, 12436, 12437, 12438, 12439, 12440, 12441, 12442, 12443, 12444, 12445, 12446, 12447, 12448, 12449, 12450, 12451, 12452, 12453, 12454, 12455, 12456, 12457, 12458, 12459, 12460, 12461, 12462, 12463, 12464, 12465, 12466, 12467, 12468, 12469, 12470, 12471, 12472, 12473, 12474, 12475, 12476, 12477, 12478, 12479, 12480, 12481, 12482, 12483, 12484, 12485, 12486, 12487, 12488, 12489, 12490, 12491, 12492, 12493, 12494, 12495, 12496, 12497, 12498, 12499, 12500, 12501, 12502, 12503, 12504, 12505, 12506, 12507, 12508, 12509, 12510, 12511, 12512, 12513, 12514, 12515, 12516, 12517, 12518, 12519, 12520, 12521, 12522, 12523, 12524, 12525, 12526, 12527, 12528, 12529, 12530, 12531, 12532, 12533, 12534, 12535, 12536, 12537, 12538, 12539, 12540, 12541, 12542, 12543, 12544, 12545, 12546, 12547, 12548, 12549, 12550, 12551, 12552, 12553, 12554, 12555, 12556, 12557, 12558, 12559, 12560, 12561, 12562, 12563, 12564, 12565, 12566, 12567, 12568, 12569, 12570, 12571, 12572, 12573, 12574, 12575, 12576, 12577, 12578, 12579, 12580, 12581, 12582, 12583, 12584, 12585, 12586, 12587, 12588, 12589, 12590, 12591, 12592, 12593, 12594, 12595, 12596, 12597, 12598, 12599, 12600, 12601, 12602, 12603, 12604, 12605, 12606, 12607, 12608, 12609, 12610, 12611, 12612, 12613, 12614, 12615, 12616, 12617, 12618, 12619, 12620, 12621, 12622, 12623, 12624, 12625, 12626, 12627, 12628, 12629, 12630, 12631, 12632, 12633, 12634, 12635, 12636, 12637, 12638, 12639, 12640, 12641, 12642, 12643, 12644, 12645, 12646, 12647, 12648, 12649, 12650, 12651, 12652, 12653, 12654, 12655, 12656, 12657, 12658, 12659, 12660, 12661, 12662, 12663, 12664, 12665, 12666, 12667, 12668, 12669, 12670, 12671, 12672, 12673, 12674, 12675, 12676, 12677, 12678, 12679, 12680, 12681, 12682, 12683, 12684, 12685, 12686, 12687, 12688, 12689, 12690, 12691, 12692, 12693, 12694, 12695, 12696, 12697, 12698, 12699, 12700, 12701, 12702, 12703, 12704, 12705, 12706, 12707, 12708, 12709, 12710, 12711, 12712, 12713, 12714, 12715, 12716, 12717, 12718, 12719, 12720, 12721, 12722, 12723, 12724, 12725, 12726, 12727, 12728, 12729, 12730, 12731, 12732, 12733, 12734, 12735, 12736, 12737, 12738, 12739, 12740, 12741, 12742, 12743, 12744, 12745, 12746, 12747, 12748, 12749, 12750, 12751, 12752, 12753, 12754, 12755, 12756, 12757, 12758, 12759, 12760, 12761, 12762, 12763, 12764, 12765, 12766, 12767, 12768, 12769, 12770, 12771, 12772, 12773, 12774, 12775, 12776, 12777, 12778, 12779, 12780, 12781, 12782, 12783, 12784, 12785, 12786, 12787, 12788, 12789, 12790, 12791, 12792, 12793, 12794, 12795, 12796, 12797, 12798, 12799, 12800, 12801, 12802, 12803, 12804, 12805, 12806, 12807, 12808, 12809, 12810, 12811, 12812, 12813, 12814, 12815, 12816, 12817, 12818, 12819, 12820, 12821, 12822, 12823, 12824, 12825, 12826, 12827, 12828, 12829, 12830, 12831, 12832, 12833, 12834, 12835, 12836, 12837, 12838, 12839, 12840, 12841, 12842, 12843, 12844, 12845, 12846, 12847, 12848, 12849, 12850, 12851, 12852, 12853, 12854, 12855, 12856, 12857, 12858, 12859, 12860, 12861, 12862, 12863, 12864, 12865, 12866, 12867, 12868, 12869, 12870, 12871, 12872, 12873, 12874, 12875, 12876, 12877, 12878, 12879, 12880, 12881, 12882, 12883, 12884, 12885, 12886, 12887, 12888, 12889, 12890, 12891, 12892, 12893, 12894, 12895, 12896, 12897, 12898, 12899, 12900, 12901, 12902, 12903, 12904, 12905, 12906, 12907, 12908, 12909, 12910, 12911, 12912, 12913, 12914, 12915, 12916, 12917, 12918, 12919, 12920, 12921, 12922, 12923, 12924, 12925, 12926, 12927, 12928, 12929, 12930, 12931, 12932, 12933, 12934, 12935, 12936, 12937, 12938, 12939, 12940, 12941, 12942, 12943, 12944, 12945, 12946, 12947, 12948, 12949, 12950, 12951, 12952, 12953, 12954, 12955, 12956, 12957, 12958, 12959, 12960, 12961, 12962, 12963, 12964, 12965, 12966, 12967, 12968, 12969, 12970, 12971, 12972, 12973, 12974, 12975, 12976, 12977, 12978, 12979, 12980, 12981, 12982, 12983, 12984, 12985, 12986, 12987, 12988, 12989, 12990, 12991, 12992, 12993, 12994, 12995, 12996, 12997, 12998, 12999, 13000, 13001, 13002, 13003, 13004, 13005, 13006, 13007, 13008, 13009, 13010, 13011, 13012, 13013, 13014, 13015, 13016, 13017, 13018, 13019, 13020, 13021, 13022, 13023, 13024, 13025, 13026, 13027, 13028, 13029, 13030, 13031, 13032, 13033, 13034, 13035, 13036, 13037, 13038, 13039, 13040, 13041, 13042, 13043, 13044, 13045, 13046, 13047, 13048, 13049, 13050, 13051, 13052, 13053, 13054, 13055, 13056, 13057, 13058, 13059, 13060, 13061, 13062, 13063, 13064, 13065, 13066, 13067, 13068, 13069, 13070, 13071, 13072, 13073, 13074, 13075, 13076, 13077, 13078, 13079, 13080, 13081, 13082, 13083, 13084, 13085, 13086, 13087, 13088, 13089, 13090, 13091, 13092, 13093, 13094, 13095, 13096, 13097, 13098, 13099, 13100, 13101, 13102, 13103, 13104, 13105, 13106, 13107, 13108, 13109, 13110, 13111, 13112, 13113, 13114, 13115, 13116, 13117, 13118, 13119, 13120, 13121, 13122, 13123, 13124, 13125, 13126, 13127, 13128, 13129, 13130, 13131, 13132, 13133, 13134, 13135, 13136, 13137, 13138, 13139, 13140, 13141, 13142, 13143, 13144, 13145, 13146, 13147, 13148, 13149, 13150, 13151, 13152, 13153, 13154, 13155, 13156, 13157, 13158, 13159, 13160, 13161, 13162, 13163, 13164, 13165, 13166, 13167, 13168, 13169, 13170, 13171, 13172, 13173, 13174, 13175, 13176, 13177, 13178, 13179, 13180, 13181, 13182, 13183, 13184, 13185, 13186, 13187, 13188, 13189, 13190, 13191, 13192, 13193, 13194, 13195, 13196, 13197, 13198, 13199, 13200, 13201, 13202, 13203, 13204, 13205, 13206, 13207, 13208, 13209, 13210, 13211, 13212, 13213, 13214, 13215, 13216, 13217, 13218, 13219, 13220, 13221, 13222, 13223, 13224, 13225, 13226, 13227, 13228, 13229, 13230, 13231, 13232, 13233, 13234, 13235, 13236, 13237, 13238, 13239, 13240, 13241, 13242, 13243, 13244, 13245, 13246, 13247, 13248, 13249, 13250, 13251, 13252, 13253, 13254, 13255, 13256, 13257, 13258, 13259, 13260, 13261, 13262, 13263, 13264, 13265, 13266, 13267, 13268, 13269, 13270, 13271, 13272, 13273, 13274, 13275, 13276, 13277, 13278, 13279, 13280, 13281, 13282, 13283, 13284, 13285, 13286, 13287, 13288, 13289, 13290, 13291, 13292, 13293, 13294, 13295, 13296, 13297, 13298, 13299, 13300, 13301, 13302, 13303, 13304, 13305, 13306, 13307, 13308, 13309, 13310, 13311, 13312, 13313, 13314, 13315, 13316, 13317, 13318, 13319, 13320, 13321, 13322, 13323, 13324, 13325, 13326, 13327, 13328, 13329, 13330, 13331, 13332, 13333, 13334, 13335, 13336, 13337, 13338, 13339, 13340, 13341, 13342, 13343, 13344, 13345, 13346, 13347, 13348, 13349, 13350, 13351, 13352, 13353, 13354, 13355, 13356, 13357, 13358, 13359, 13360, 13361, 13362, 13363, 13364, 13365, 13366, 13367, 13368, 13369, 13370, 13371, 13372, 13373, 13374, 13375, 13376, 13377, 13378, 13379, 13380, 13381, 13382, 13383, 13384, 13385, 13386, 13387, 13388, 13389, 13390, 13391, 13392, 13393, 13394, 13395, 13396, 13397, 13398, 13399, 13400, 13401, 13402, 13403, 13404, 13405, 13406, 13407, 13408, 13409, 13410, 13411, 13412, 13413, 13414, 13415, 13416, 13417, 13418, 13419, 13420, 13421, 13422, 13423, 13424, 13425, 13426, 13427, 13428, 13429, 13430, 13431, 13432, 13433, 13434, 13435, 13436, 13437, 13438, 13439, 13440, 13441, 13442, 13443, 13444, 13445, 13446, 13447, 13448, 13449, 13450, 13451, 13452, 13453, 13454, 13455, 13456, 13457, 13458, 13459, 13460, 13461, 13462, 13463, 13464, 13465, 13466, 13467, 13468, 13469, 13470, 13471, 13472, 13473, 13474, 13475, 13476, 13477, 13478, 13479, 13480, 13481, 13482, 13483, 13484, 13485, 13486, 13487, 13488, 13489, 13490, 13491, 13492, 13493, 13494, 13495, 13496, 13497, 13498, 13499, 13500, 13501, 13502, 13503, 13504, 13505, 13506, 13507, 13508, 13509, 13510, 13511, 13512, 13513, 13514, 13515, 13516, 13517, 13518, 13519, 13520, 13521, 13522, 13523, 13524, 13525, 13526, 13527, 13528, 13529, 13530, 13531, 13532, 13533, 13534, 13535, 13536, 13537, 13538, 13539, 13540, 13541, 13542, 13543, 13544, 13545, 13546, 13547, 13548, 13549, 13550, 13551, 13552, 13553, 13554, 13555, 13556, 13557, 13558, 13559, 13560, 13561, 13562, 13563, 13564, 13565, 13566, 13567, 13568, 13569, 13570, 13571, 13572, 13573, 13574, 13575, 13576, 13577, 13578, 13579, 13580, 13581, 13582, 13583, 13584, 13585, 13586, 13587, 13588, 13589, 13590, 13591, 13592, 13593, 13594, 13595, 13596, 13597, 13598, 13599, 13600, 13601, 13602, 13603, 13604, 13605, 13606, 13607, 13608, 13609, 13610, 13611, 13612, 13613, 13614, 13615, 13616, 13617, 13618, 13619, 13620, 13621, 13622, 13623, 13624, 13625, 13626, 13627, 13628, 13629, 13630, 13631, 13632, 13633, 13634, 13635, 13636, 13637, 13638, 13639, 13640, 13641, 13642, 13643, 13644, 13645, 13646, 13647, 13648, 13649, 13650, 13651, 13652, 13653, 13654, 13655, 13656, 13657, 13658, 13659, 13660, 13661, 13662, 13663, 13664, 13665, 13666, 13667, 13668, 13669, 13670, 13671, 13672, 13673, 13674, 13675, 13676, 13677, 13678, 13679, 13680, 13681, 13682, 13683, 13684, 13685, 13686, 13687, 13688, 13689, 13690, 13691, 13692, 13693, 13694, 13695, 13696, 13697, 13698, 13699, 13700, 13701, 13702, 13703, 13704, 13705, 13706, 13707, 13708, 13709, 13710, 13711, 13712, 13713, 13714, 13715, 13716, 13717, 13718, 13719, 13720, 13721, 13722, 13723, 13724, 13725, 13726, 13727, 13728, 13729, 13730, 13731, 13732, 13733, 13734, 13735, 13736, 13737, 13738, 13739, 13740, 13741, 13742, 13743, 13744, 13745, 13746, 13747, 13748, 13749, 13750, 13751, 13752, 13753, 13754, 13755, 13756, 13757, 13758, 13759, 13760, 13761, 13762, 13763, 13764, 13765, 13766, 13767, 13768, 13769, 13770, 13771, 13772, 13773, 13774, 13775, 13776, 13777, 13778, 13779, 13780, 13781, 13782, 13783, 13784, 13785, 13786, 13787, 13788, 13789, 13790, 13791, 13792, 13793, 13794, 13795, 13796, 13797, 13798, 13799, 13800, 13801, 13802, 13803, 13804, 13805, 13806, 13807, 13808, 13809, 13810, 13811, 13812, 13813, 13814, 13815, 13816, 13817, 13818, 13819, 13820, 13821, 13822, 13823, 13824, 13825, 13826, 13827, 13828, 13829, 13830, 13831, 13832, 13833, 13834, 13835, 13836, 13837, 13838, 13839, 13840, 13841, 13842, 13843, 13844, 13845, 13846, 13847, 13848, 13849, 13850, 13851, 13852, 13853, 13854, 13855, 13856, 13857, 13858, 13859, 13860, 13861, 13862, 13863, 13864, 13865, 13866, 13867, 13868, 13869, 13870, 13871, 13872, 13873, 13874, 13875, 13876, 13877, 13878, 13879, 13880, 13881, 13882, 13883, 13884, 13885, 13886, 13887, 13888, 13889, 13890, 13891, 13892, 13893, 13894, 13895, 13896, 13897, 13898, 13899, 13900, 13901, 13902, 13903, 13904, 13905, 13906, 13907, 13908, 13909, 13910, 13911, 13912, 13913, 13914, 13915, 13916, 13917, 13918, 13919, 13920, 13921, 13922, 13923, 13924, 13925, 13926, 13927, 13928, 13929, 13930, 13931, 13932, 13933, 13934, 13935, 13936, 13937, 13938, 13939, 13940, 13941, 13942, 13943, 13944, 13945, 13946, 13947, 13948, 13949, 13950, 13951, 13952, 13953, 13954, 13955, 13956, 13957, 13958, 13959, 13960, 13961, 13962, 13963, 13964, 13965, 13966, 13967, 13968, 13969, 13970, 13971, 13972, 13973, 13974, 13975, 13976, 13977, 13978, 13979, 13980, 13981, 13982, 13983, 13984, 13985, 13986, 13987, 13988, 13989, 13990, 13991, 13992, 13993, 13994, 13995, 13996, 13997, 13998, 13999, 14000, 14001, 14002, 14003, 14004, 14005, 14006, 14007, 14008, 14009, 14010, 14011, 14012, 14013, 14014, 14015, 14016, 14017, 14018, 14019, 14020, 14021, 14022, 14023, 14024, 14025, 14026, 14027, 14028, 14029, 14030, 14031, 14032, 14033, 14034, 14035, 14036, 14037, 14038, 14039, 14040, 14041, 14042, 14043, 14044, 14045, 14046, 14047, 14048, 14049, 14050, 14051, 14052, 14053, 14054, 14055, 14056, 14057, 14058, 14059, 14060, 14061, 14062, 14063, 14064, 14065, 14066, 14067, 14068, 14069, 14070, 14071, 14072, 14073, 14074, 14075, 14076, 14077, 14078, 14079, 14080, 14081, 14082, 14083, 14084, 14085, 14086, 14087, 14088, 14089, 14090, 14091, 14092, 14093, 14094, 14095, 14096, 14097, 14098, 14099, 14100, 14101, 14102, 14103, 14104, 14105, 14106, 14107, 14108, 14109, 14110, 14111, 14112, 14113, 14114, 14115, 14116, 14117, 14118, 14119, 14120, 14121, 14122, 14123, 14124, 14125, 14126, 14127, 14128, 14129, 14130, 14131, 14132, 14133, 14134, 14135, 14136, 14137, 14138, 14139, 14140, 14141, 14142, 14143, 14144, 14145, 14146, 14147, 14148, 14149, 14150, 14151, 14152, 14153, 14154, 14155, 14156, 14157, 14158, 14159, 14160, 14161, 14162, 14163, 14164, 14165, 14166, 14167, 14168, 14169, 14170, 14171, 14172, 14173, 14174, 14175, 14176, 14177, 14178, 14179, 14180, 14181, 14182, 14183, 14184, 14185, 14186, 14187, 14188, 14189, 14190, 14191, 14192, 14193, 14194, 14195, 14196, 14197, 14198, 14199, 14200, 14201, 14202, 14203, 14204, 14205, 14206, 14207, 14208, 14209, 14210, 14211, 14212, 14213, 14214, 14215, 14216, 14217, 14218, 14219, 14220, 14221, 14222, 14223, 14224, 14225, 14226, 14227, 14228, 14229, 14230, 14231, 14232, 14233, 14234, 14235, 14236, 14237, 14238, 14239, 14240, 14241, 14242, 14243, 14244, 14245, 14246, 14247, 14248, 14249, 14250, 14251, 14252, 14253, 14254, 14255, 14256, 14257, 14258, 14259, 14260, 14261, 14262, 14263, 14264, 14265, 14266, 14267, 14268, 14269, 14270, 14271, 14272, 14273, 14274, 14275, 14276, 14277, 14278, 14279, 14280, 14281, 14282, 14283, 14284, 14285, 14286, 14287, 14288, 14289, 14290, 14291, 14292, 14293, 14294, 14295, 14296, 14297, 14298, 14299, 14300, 14301, 14302, 14303, 14304, 14305, 14306, 14307, 14308, 14309, 14310, 14311, 14312, 14313, 14314, 14315, 14316, 14317, 14318, 14319, 14320, 14321, 14322, 14323, 14324, 14325, 14326, 14327, 14328, 14329, 14330, 14331, 14332, 14333, 14334, 14335, 14336, 14337, 14338, 14339, 14340, 14341, 14342, 14343, 14344, 14345, 14346, 14347, 14348, 14349, 14350, 14351, 14352, 14353, 14354, 14355, 14356, 14357, 14358, 14359, 14360, 14361, 14362, 14363, 14364, 14365, 14366, 14367, 14368, 14369, 14370, 14371, 14372, 14373, 14374, 14375, 14376, 14377, 14378, 14379, 14380, 14381, 14382, 14383, 14384, 14385, 14386, 14387, 14388, 14389, 14390, 14391, 14392, 14393, 14394, 14395, 14396, 14397, 14398, 14399, 14400, 14401, 14402, 14403, 14404, 14405, 14406, 14407, 14408, 14409, 14410, 14411, 14412, 14413, 14414, 14415, 14416, 14417, 14418, 14419, 14420, 14421, 14422, 14423, 14424, 14425, 14426, 14427, 14428, 14429, 14430, 14431, 14432, 14433, 14434, 14435, 14436, 14437, 14438, 14439, 14440, 14441, 14442, 14443, 14444, 14445, 14446, 14447, 14448, 14449, 14450, 14451, 14452, 14453, 14454, 14455, 14456, 14457, 14458, 14459, 14460, 14461, 14462, 14463, 14464, 14465, 14466, 14467, 14468, 14469, 14470, 14471, 14472, 14473, 14474, 14475, 14476, 14477, 14478, 14479, 14480, 14481, 14482, 14483, 14484, 14485, 14486, 14487, 14488, 14489, 14490, 14491, 14492, 14493, 14494, 14495, 14496, 14497, 14498, 14499, 14500, 14501, 14502, 14503, 14504, 14505, 14506, 14507, 14508, 14509, 14510, 14511, 14512, 14513, 14514, 14515, 14516, 14517, 14518, 14519, 14520, 14521, 14522, 14523, 14524, 14525, 14526, 14527, 14528, 14529, 14530, 14531, 14532, 14533, 14534, 14535, 14536, 14537, 14538, 14539, 14540, 14541, 14542, 14543, 14544, 14545, 14546, 14547, 14548, 14549, 14550, 14551, 14552, 14553, 14554, 14555, 14556, 14557, 14558, 14559, 14560, 14561, 14562, 14563, 14564, 14565, 14566, 14567, 14568, 14569, 14570, 14571, 14572, 14573, 14574, 14575, 14576, 14577, 14578, 14579, 14580, 14581, 14582, 14583, 14584, 14585, 14586, 14587, 14588, 14589, 14590, 14591, 14592, 14593, 14594, 14595, 14596, 14597, 14598, 14599, 14600, 14601, 14602, 14603, 14604, 14605, 14606, 14607, 14608, 14609, 14610, 14611, 14612, 14613, 14614, 14615, 14616, 14617, 14618, 14619, 14620, 14621, 14622, 14623, 14624, 14625, 14626, 14627, 14628, 14629, 14630, 14631, 14632, 14633, 14634, 14635, 14636, 14637, 14638, 14639, 14640, 14641, 14642, 14643, 14644, 14645, 14646, 14647, 14648, 14649, 14650, 14651, 14652, 14653, 14654, 14655, 14656, 14657, 14658, 14659, 14660, 14661, 14662, 14663, 14664, 14665, 14666, 14667, 14668, 14669, 14670, 14671, 14672, 14673, 14674, 14675, 14676, 14677, 14678, 14679, 14680, 14681, 14682, 14683, 14684, 14685, 14686, 14687, 14688, 14689, 14690, 14691, 14692, 14693, 14694, 14695, 14696, 14697, 14698, 14699, 14700, 14701, 14702, 14703, 14704, 14705, 14706, 14707, 14708, 14709, 14710, 14711, 14712, 14713, 14714, 14715, 14716, 14717, 14718, 14719, 14720, 14721, 14722, 14723, 14724, 14725, 14726, 14727, 14728, 14729, 14730, 14731, 14732, 14733, 14734, 14735, 14736, 14737, 14738, 14739, 14740, 14741, 14742, 14743, 14744, 14745, 14746, 14747, 14748, 14749, 14750, 14751, 14752, 14753, 14754, 14755, 14756, 14757, 14758, 14759, 14760, 14761, 14762, 14763, 14764, 14765, 14766, 14767, 14768, 14769, 14770, 14771, 14772, 14773, 14774, 14775, 14776, 14777, 14778, 14779, 14780, 14781, 14782, 14783, 14784, 14785, 14786, 14787, 14788, 14789, 14790, 14791, 14792, 14793, 14794, 14795, 14796, 14797, 14798, 14799, 14800, 14801, 14802, 14803, 14804, 14805, 14806, 14807, 14808, 14809, 14810, 14811, 14812, 14813, 14814, 14815, 14816, 14817, 14818, 14819, 14820, 14821, 14822, 14823, 14824, 14825, 14826, 14827, 14828, 14829, 14830, 14831, 14832, 14833, 14834, 14835, 14836, 14837, 14838, 14839, 14840, 14841, 14842, 14843, 14844, 14845, 14846, 14847, 14848, 14849, 14850, 14851, 14852, 14853, 14854, 14855, 14856, 14857, 14858, 14859, 14860, 14861, 14862, 14863, 14864, 14865, 14866, 14867, 14868, 14869, 14870, 14871, 14872, 14873, 14874, 14875, 14876, 14877, 14878, 14879, 14880, 14881, 14882, 14883, 14884, 14885, 14886, 14887, 14888, 14889, 14890, 14891, 14892, 14893, 14894, 14895, 14896, 14897, 14898, 14899, 14900, 14901, 14902, 14903, 14904, 14905, 14906, 14907, 14908, 14909, 14910, 14911, 14912, 14913, 14914, 14915, 14916, 14917, 14918, 14919, 14920, 14921, 14922, 14923, 14924, 14925, 14926, 14927, 14928, 14929, 14930, 14931, 14932, 14933, 14934, 14935, 14936, 14937, 14938, 14939, 14940, 14941, 14942, 14943, 14944, 14945, 14946, 14947, 14948, 14949, 14950, 14951, 14952, 14953, 14954, 14955, 14956, 14957, 14958, 14959, 14960, 14961, 14962, 14963, 14964, 14965, 14966, 14967, 14968, 14969, 14970, 14971, 14972, 14973, 14974, 14975, 14976, 14977, 14978, 14979, 14980, 14981, 14982, 14983, 14984, 14985, 14986, 14987, 14988, 14989, 14990, 14991, 14992, 14993, 14994, 14995, 14996, 14997, 14998, 14999, 15000, 15001, 15002, 15003, 15004, 15005, 15006, 15007, 15008, 15009, 15010, 15011, 15012, 15013, 15014, 15015, 15016, 15017, 15018, 15019, 15020, 15021, 15022, 15023, 15024, 15025, 15026, 15027, 15028, 15029, 15030, 15031, 15032, 15033, 15034, 15035, 15036, 15037, 15038, 15039, 15040, 15041, 15042, 15043, 15044, 15045, 15046, 15047, 15048, 15049, 15050, 15051, 15052, 15053, 15054, 15055, 15056, 15057, 15058, 15059, 15060, 15061, 15062, 15063, 15064, 15065, 15066, 15067, 15068, 15069, 15070, 15071, 15072, 15073, 15074, 15075, 15076, 15077, 15078, 15079, 15080, 15081, 15082, 15083, 15084, 15085, 15086, 15087, 15088, 15089, 15090, 15091, 15092, 15093, 15094, 15095, 15096, 15097, 15098, 15099, 15100, 15101, 15102, 15103, 15104, 15105, 15106, 15107, 15108, 15109, 15110, 15111, 15112, 15113, 15114, 15115, 15116, 15117, 15118, 15119, 15120, 15121, 15122, 15123, 15124, 15125, 15126, 15127, 15128, 15129, 15130, 15131, 15132, 15133, 15134, 15135, 15136, 15137, 15138, 15139, 15140, 15141, 15142, 15143, 15144, 15145, 15146, 15147, 15148, 15149, 15150, 15151, 15152, 15153, 15154, 15155, 15156, 15157, 15158, 15159, 15160, 15161, 15162, 15163, 15164, 15165, 15166, 15167, 15168, 15169, 15170, 15171, 15172, 15173, 15174, 15175, 15176, 15177, 15178, 15179, 15180, 15181, 15182, 15183, 15184, 15185, 15186, 15187, 15188, 15189, 15190, 15191, 15192, 15193, 15194, 15195, 15196, 15197, 15198, 15199, 15200, 15201, 15202, 15203, 15204, 15205, 15206, 15207, 15208, 15209, 15210, 15211, 15212, 15213, 15214, 15215, 15216, 15217, 15218, 15219, 15220, 15221, 15222, 15223, 15224, 15225, 15226, 15227, 15228, 15229, 15230, 15231, 15232, 15233, 15234, 15235, 15236, 15237, 15238, 15239, 15240, 15241, 15242, 15243, 15244, 15245, 15246, 15247, 15248, 15249, 15250, 15251, 15252, 15253, 15254, 15255, 15256, 15257, 15258, 15259, 15260, 15261, 15262, 15263, 15264, 15265, 15266, 15267, 15268, 15269, 15270, 15271, 15272, 15273, 15274, 15275, 15276, 15277, 15278, 15279, 15280, 15281, 15282, 15283, 15284, 15285, 15286, 15287, 15288, 15289, 15290, 15291, 15292, 15293, 15294, 15295, 15296, 15297, 15298, 15299, 15300, 15301, 15302, 15303, 15304, 15305, 15306, 15307, 15308, 15309, 15310, 15311, 15312, 15313, 15314, 15315, 15316, 15317, 15318, 15319, 15320, 15321, 15322, 15323, 15324, 15325, 15326, 15327, 15328, 15329, 15330, 15331, 15332, 15333, 15334, 15335, 15336, 15337, 15338, 15339, 15340, 15341, 15342, 15343, 15344, 15345, 15346, 15347, 15348, 15349, 15350, 15351, 15352, 15353, 15354, 15355, 15356, 15357, 15358, 15359, 15360, 15361, 15362, 15363, 15364, 15365, 15366, 15367, 15368, 15369, 15370, 15371, 15372, 15373, 15374, 15375, 15376, 15377, 15378, 15379, 15380, 15381, 15382, 15383, 15384, 15385, 15386, 15387, 15388, 15389, 15390, 15391, 15392, 15393, 15394, 15395, 15396, 15397, 15398, 15399, 15400, 15401, 15402, 15403, 15404, 15405, 15406, 15407, 15408, 15409, 15410, 15411, 15412, 15413, 15414, 15415, 15416, 15417, 15418, 15419, 15420, 15421, 15422, 15423, 15424, 15425, 15426, 15427, 15428, 15429, 15430, 15431, 15432, 15433, 15434, 15435, 15436, 15437, 15438, 15439, 15440, 15441, 15442, 15443, 15444, 15445, 15446, 15447, 15448, 15449, 15450, 15451, 15452, 15453, 15454, 15455, 15456, 15457, 15458, 15459, 15460, 15461, 15462, 15463, 15464, 15465, 15466, 15467, 15468, 15469, 15470, 15471, 15472, 15473, 15474, 15475, 15476, 15477, 15478, 15479, 15480, 15481, 15482, 15483, 15484, 15485, 15486, 15487, 15488, 15489, 15490, 15491, 15492, 15493, 15494, 15495, 15496, 15497, 15498, 15499, 15500, 15501, 15502, 15503, 15504, 15505, 15506, 15507, 15508, 15509, 15510, 15511, 15512, 15513, 15514, 15515, 15516, 15517, 15518, 15519, 15520, 15521, 15522, 15523, 15524, 15525, 15526, 15527, 15528, 15529, 15530, 15531, 15532, 15533, 15534, 15535, 15536, 15537, 15538, 15539, 15540, 15541, 15542, 15543, 15544, 15545, 15546, 15547, 15548, 15549, 15550, 15551, 15552, 15553, 15554, 15555, 15556, 15557, 15558, 15559, 15560, 15561, 15562, 15563, 15564, 15565, 15566, 15567, 15568, 15569, 15570, 15571, 15572, 15573, 15574, 15575, 15576, 15577, 15578, 15579, 15580, 15581, 15582, 15583, 15584, 15585, 15586, 15587, 15588, 15589, 15590, 15591, 15592, 15593, 15594, 15595, 15596, 15597, 15598, 15599, 15600, 15601, 15602, 15603, 15604, 15605, 15606, 15607, 15608, 15609, 15610, 15611, 15612, 15613, 15614, 15615, 15616, 15617, 15618, 15619, 15620, 15621, 15622, 15623, 15624, 15625, 15626, 15627, 15628, 15629, 15630, 15631, 15632, 15633, 15634, 15635, 15636, 15637, 15638, 15639, 15640, 15641, 15642, 15643, 15644, 15645, 15646, 15647, 15648, 15649, 15650, 15651, 15652, 15653, 15654, 15655, 15656, 15657, 15658, 15659, 15660, 15661, 15662, 15663, 15664, 15665, 15666, 15667, 15668, 15669, 15670, 15671, 15672, 15673, 15674, 15675, 15676, 15677, 15678, 15679, 15680, 15681, 15682, 15683, 15684, 15685, 15686, 15687, 15688, 15689, 15690, 15691, 15692, 15693, 15694, 15695, 15696, 15697, 15698, 15699, 15700, 15701, 15702, 15703, 15704, 15705, 15706, 15707, 15708, 15709, 15710, 15711, 15712, 15713, 15714, 15715, 15716, 15717, 15718, 15719, 15720, 15721, 15722, 15723, 15724, 15725, 15726, 15727, 15728, 15729, 15730, 15731, 15732, 15733, 15734, 15735, 15736, 15737, 15738, 15739, 15740, 15741, 15742, 15743, 15744, 15745, 15746, 15747, 15748, 15749, 15750, 15751, 15752, 15753, 15754, 15755, 15756, 15757, 15758, 15759, 15760, 15761, 15762, 15763, 15764, 15765, 15766, 15767, 15768, 15769, 15770, 15771, 15772, 15773, 15774, 15775, 15776, 15777, 15778, 15779, 15780, 15781, 15782, 15783, 15784, 15785, 15786, 15787, 15788, 15789, 15790, 15791, 15792, 15793, 15794, 15795, 15796, 15797, 15798, 15799, 15800, 15801, 15802, 15803, 15804, 15805, 15806, 15807, 15808, 15809, 15810, 15811, 15812, 15813, 15814, 15815, 15816, 15817, 15818, 15819, 15820, 15821, 15822, 15823, 15824, 15825, 15826, 15827, 15828, 15829, 15830, 15831, 15832, 15833, 15834, 15835, 15836, 15837, 15838, 15839, 15840, 15841, 15842, 15843, 15844, 15845, 15846, 15847, 15848, 15849, 15850, 15851, 15852, 15853, 15854, 15855, 15856, 15857, 15858, 15859, 15860, 15861, 15862, 15863, 15864, 15865, 15866, 15867, 15868, 15869, 15870, 15871, 15872, 15873, 15874, 15875, 15876, 15877, 15878, 15879, 15880, 15881, 15882, 15883, 15884, 15885, 15886, 15887, 15888, 15889, 15890, 15891, 15892, 15893, 15894, 15895, 15896, 15897, 15898, 15899, 15900, 15901, 15902, 15903, 15904, 15905, 15906, 15907, 15908, 15909, 15910, 15911, 15912, 15913, 15914, 15915, 15916, 15917, 15918, 15919, 15920, 15921, 15922, 15923, 15924, 15925, 15926, 15927, 15928, 15929, 15930, 15931, 15932, 15933, 15934, 15935, 15936, 15937, 15938, 15939, 15940, 15941, 15942, 15943, 15944, 15945, 15946, 15947, 15948, 15949, 15950, 15951, 15952, 15953, 15954, 15955, 15956, 15957, 15958, 15959, 15960, 15961, 15962, 15963, 15964, 15965, 15966, 15967, 15968, 15969, 15970, 15971, 15972, 15973, 15974, 15975, 15976, 15977, 15978, 15979, 15980, 15981, 15982, 15983, 15984, 15985, 15986, 15987, 15988, 15989, 15990, 15991, 15992, 15993, 15994, 15995, 15996, 15997, 15998, 15999, 16000, 16001, 16002, 16003, 16004, 16005, 16006, 16007, 16008, 16009, 16010, 16011, 16012, 16013, 16014, 16015, 16016, 16017, 16018, 16019, 16020, 16021, 16022, 16023, 16024, 16025, 16026, 16027, 16028, 16029, 16030, 16031, 16032, 16033, 16034, 16035, 16036, 16037, 16038, 16039, 16040, 16041, 16042, 16043, 16044, 16045, 16046, 16047, 16048, 16049, 16050, 16051, 16052, 16053, 16054, 16055, 16056, 16057, 16058, 16059, 16060, 16061, 16062, 16063, 16064, 16065, 16066, 16067, 16068, 16069, 16070, 16071, 16072, 16073, 16074, 16075, 16076, 16077, 16078, 16079, 16080, 16081, 16082, 16083, 16084, 16085, 16086, 16087, 16088, 16089, 16090, 16091, 16092, 16093, 16094, 16095, 16096, 16097, 16098, 16099, 16100, 16101, 16102, 16103, 16104, 16105, 16106, 16107, 16108, 16109, 16110, 16111, 16112, 16113, 16114, 16115, 16116, 16117, 16118, 16119, 16120, 16121, 16122, 16123, 16124, 16125, 16126, 16127, 16128, 16129, 16130, 16131, 16132, 16133, 16134, 16135, 16136, 16137, 16138, 16139, 16140, 16141, 16142, 16143, 16144, 16145, 16146, 16147, 16148, 16149, 16150, 16151, 16152, 16153, 16154, 16155, 16156, 16157, 16158, 16159, 16160, 16161, 16162, 16163, 16164, 16165, 16166, 16167, 16168, 16169, 16170, 16171, 16172, 16173, 16174, 16175, 16176, 16177, 16178, 16179, 16180, 16181, 16182, 16183, 16184, 16185, 16186, 16187, 16188, 16189, 16190, 16191, 16192, 16193, 16194, 16195, 16196, 16197, 16198, 16199, 16200, 16201, 16202, 16203, 16204, 16205, 16206, 16207, 16208, 16209, 16210, 16211, 16212, 16213, 16214, 16215, 16216, 16217, 16218, 16219, 16220, 16221, 16222, 16223, 16224, 16225, 16226, 16227, 16228, 16229, 16230, 16231, 16232, 16233, 16234, 16235, 16236, 16237, 16238, 16239, 16240, 16241, 16242, 16243, 16244, 16245, 16246, 16247, 16248, 16249, 16250, 16251, 16252, 16253, 16254, 16255, 16256, 16257, 16258, 16259, 16260, 16261, 16262, 16263, 16264, 16265, 16266, 16267, 16268, 16269, 16270, 16271, 16272, 16273, 16274, 16275, 16276, 16277, 16278, 16279, 16280, 16281, 16282, 16283, 16284, 16285, 16286, 16287, 16288, 16289, 16290, 16291, 16292, 16293, 16294, 16295, 16296, 16297, 16298, 16299, 16300, 16301, 16302, 16303, 16304, 16305, 16306, 16307, 16308, 16309, 16310, 16311, 16312, 16313, 16314, 16315, 16316, 16317, 16318, 16319, 16320, 16321, 16322, 16323, 16324, 16325, 16326, 16327, 16328, 16329, 16330, 16331, 16332, 16333, 16334, 16335, 16336, 16337, 16338, 16339, 16340, 16341, 16342, 16343, 16344, 16345, 16346, 16347, 16348, 16349, 16350, 16351, 16352, 16353, 16354, 16355, 16356, 16357, 16358, 16359, 16360, 16361, 16362, 16363, 16364, 16365, 16366, 16367, 16368, 16369, 16370, 16371, 16372, 16373, 16374, 16375, 16376, 16377, 16378, 16379, 16380, 16381, 16382, 16383, 16384, 16385, 16386, 16387, 16388, 16389, 16390, 16391, 16392, 16393, 16394, 16395, 16396, 16397, 16398, 16399, 16400, 16401, 16402, 16403, 16404, 16405, 16406, 16407, 16408, 16409, 16410, 16411, 16412, 16413, 16414, 16415, 16416, 16417, 16418, 16419, 16420, 16421, 16422, 16423, 16424, 16425, 16426, 16427, 16428, 16429, 16430, 16431, 16432, 16433, 16434, 16435, 16436, 16437, 16438, 16439, 16440, 16441, 16442, 16443, 16444, 16445, 16446, 16447, 16448, 16449, 16450, 16451, 16452, 16453, 16454, 16455, 16456, 16457, 16458, 16459, 16460, 16461, 16462, 16463, 16464, 16465, 16466, 16467, 16468, 16469, 16470, 16471, 16472, 16473, 16474, 16475, 16476, 16477, 16478, 16479, 16480, 16481, 16482, 16483, 16484, 16485, 16486, 16487, 16488, 16489, 16490, 16491, 16492, 16493, 16494, 16495, 16496, 16497, 16498, 16499, 16500, 16501, 16502, 16503, 16504, 16505, 16506, 16507, 16508, 16509, 16510, 16511, 16512, 16513, 16514, 16515, 16516, 16517, 16518, 16519, 16520, 16521, 16522, 16523, 16524, 16525, 16526, 16527, 16528, 16529, 16530, 16531, 16532, 16533, 16534, 16535, 16536, 16537, 16538, 16539, 16540, 16541, 16542, 16543, 16544, 16545, 16546, 16547, 16548, 16549, 16550, 16551, 16552, 16553, 16554, 16555, 16556, 16557, 16558, 16559, 16560, 16561, 16562, 16563, 16564, 16565, 16566, 16567, 16568, 16569, 16570, 16571, 16572, 16573, 16574, 16575, 16576, 16577, 16578, 16579, 16580, 16581, 16582, 16583, 16584, 16585, 16586, 16587, 16588, 16589, 16590, 16591, 16592, 16593, 16594, 16595, 16596, 16597, 16598, 16599, 16600, 16601, 16602, 16603, 16604, 16605, 16606, 16607, 16608, 16609, 16610, 16611, 16612, 16613, 16614, 16615, 16616, 16617, 16618, 16619, 16620, 16621, 16622, 16623, 16624, 16625, 16626, 16627, 16628, 16629, 16630, 16631, 16632, 16633, 16634, 16635, 16636, 16637, 16638, 16639, 16640, 16641, 16642, 16643, 16644, 16645, 16646, 16647, 16648, 16649, 16650, 16651, 16652, 16653, 16654, 16655, 16656, 16657, 16658, 16659, 16660, 16661, 16662, 16663, 16664, 16665, 16666, 16667, 16668, 16669, 16670, 16671, 16672, 16673, 16674, 16675, 16676, 16677, 16678, 16679, 16680, 16681, 16682, 16683, 16684, 16685, 16686, 16687, 16688, 16689, 16690, 16691, 16692, 16693, 16694, 16695, 16696, 16697, 16698, 16699, 16700, 16701, 16702, 16703, 16704, 16705, 16706, 16707, 16708, 16709, 16710, 16711, 16712, 16713, 16714, 16715, 16716, 16717, 16718, 16719, 16720, 16721, 16722, 16723, 16724, 16725, 16726, 16727, 16728, 16729, 16730, 16731, 16732, 16733, 16734, 16735, 16736, 16737, 16738, 16739, 16740, 16741, 16742, 16743, 16744, 16745, 16746, 16747, 16748, 16749, 16750, 16751, 16752, 16753, 16754, 16755, 16756, 16757, 16758, 16759, 16760, 16761, 16762, 16763, 16764, 16765, 16766, 16767, 16768, 16769, 16770, 16771, 16772, 16773, 16774, 16775, 16776, 16777, 16778, 16779, 16780, 16781, 16782, 16783, 16784, 16785, 16786, 16787, 16788, 16789, 16790, 16791, 16792, 16793, 16794, 16795, 16796, 16797, 16798, 16799, 16800, 16801, 16802, 16803, 16804, 16805, 16806, 16807, 16808, 16809, 16810, 16811, 16812, 16813, 16814, 16815, 16816, 16817, 16818, 16819, 16820, 16821, 16822, 16823, 16824, 16825, 16826, 16827, 16828, 16829, 16830, 16831, 16832, 16833, 16834, 16835, 16836, 16837, 16838, 16839, 16840, 16841, 16842, 16843, 16844, 16845, 16846, 16847, 16848, 16849, 16850, 16851, 16852, 16853, 16854, 16855, 16856, 16857, 16858, 16859, 16860, 16861, 16862, 16863, 16864, 16865, 16866, 16867, 16868, 16869, 16870, 16871, 16872, 16873, 16874, 16875, 16876, 16877, 16878, 16879, 16880, 16881, 16882, 16883, 16884, 16885, 16886, 16887, 16888, 16889, 16890, 16891, 16892, 16893, 16894, 16895, 16896, 16897, 16898, 16899, 16900, 16901, 16902, 16903, 16904, 16905, 16906, 16907, 16908, 16909, 16910, 16911, 16912, 16913, 16914, 16915, 16916, 16917, 16918, 16919, 16920, 16921, 16922, 16923, 16924, 16925, 16926, 16927, 16928, 16929, 16930, 16931, 16932, 16933, 16934, 16935, 16936, 16937, 16938, 16939, 16940, 16941, 16942, 16943, 16944, 16945, 16946, 16947, 16948, 16949, 16950, 16951, 16952, 16953, 16954, 16955, 16956, 16957, 16958, 16959, 16960, 16961, 16962, 16963, 16964, 16965, 16966, 16967, 16968, 16969, 16970, 16971, 16972, 16973, 16974, 16975, 16976, 16977, 16978, 16979, 16980, 16981, 16982, 16983, 16984, 16985, 16986, 16987, 16988, 16989, 16990, 16991, 16992, 16993, 16994, 16995, 16996, 16997, 16998, 16999, 17000, 17001, 17002, 17003, 17004, 17005, 17006, 17007, 17008, 17009, 17010, 17011, 17012, 17013, 17014, 17015, 17016, 17017, 17018, 17019, 17020, 17021, 17022, 17023, 17024, 17025, 17026, 17027, 17028, 17029, 17030, 17031, 17032, 17033, 17034, 17035, 17036, 17037, 17038, 17039, 17040, 17041, 17042, 17043, 17044, 17045, 17046, 17047, 17048, 17049, 17050, 17051, 17052, 17053, 17054, 17055, 17056, 17057, 17058, 17059, 17060, 17061, 17062, 17063, 17064, 17065, 17066, 17067, 17068, 17069, 17070, 17071, 17072, 17073, 17074, 17075, 17076, 17077, 17078, 17079, 17080, 17081, 17082, 17083, 17084, 17085, 17086, 17087, 17088, 17089, 17090, 17091, 17092, 17093, 17094, 17095, 17096, 17097, 17098, 17099, 17100, 17101, 17102, 17103, 17104, 17105, 17106, 17107, 17108, 17109, 17110, 17111, 17112, 17113, 17114, 17115, 17116, 17117, 17118, 17119, 17120, 17121, 17122, 17123, 17124, 17125, 17126, 17127, 17128, 17129, 17130, 17131, 17132, 17133, 17134, 17135, 17136, 17137, 17138, 17139, 17140, 17141, 17142, 17143, 17144, 17145, 17146, 17147, 17148, 17149, 17150, 17151, 17152, 17153, 17154, 17155, 17156, 17157, 17158, 17159, 17160, 17161, 17162, 17163, 17164, 17165, 17166, 17167, 17168, 17169, 17170, 17171, 17172, 17173, 17174, 17175, 17176, 17177, 17178, 17179, 17180, 17181, 17182, 17183, 17184, 17185, 17186, 17187, 17188, 17189, 17190, 17191, 17192, 17193, 17194, 17195, 17196, 17197, 17198, 17199, 17200, 17201, 17202, 17203, 17204, 17205, 17206, 17207, 17208, 17209, 17210, 17211, 17212, 17213, 17214, 17215, 17216, 17217, 17218, 17219, 17220, 17221, 17222, 17223, 17224, 17225, 17226, 17227, 17228, 17229, 17230, 17231, 17232, 17233, 17234, 17235, 17236, 17237, 17238, 17239, 17240, 17241, 17242, 17243, 17244, 17245, 17246, 17247, 17248, 17249, 17250, 17251, 17252, 17253, 17254, 17255, 17256, 17257, 17258, 17259, 17260, 17261, 17262, 17263, 17264, 17265, 17266, 17267, 17268, 17269, 17270, 17271, 17272, 17273, 17274, 17275, 17276, 17277, 17278, 17279, 17280, 17281, 17282, 17283, 17284, 17285, 17286, 17287, 17288, 17289, 17290, 17291, 17292, 17293, 17294, 17295, 17296, 17297, 17298, 17299, 17300, 17301, 17302, 17303, 17304, 17305, 17306, 17307, 17308, 17309, 17310, 17311, 17312, 17313, 17314, 17315, 17316, 17317, 17318, 17319, 17320, 17321, 17322, 17323, 17324, 17325, 17326, 17327, 17328, 17329, 17330, 17331, 17332, 17333, 17334, 17335, 17336, 17337, 17338, 17339, 17340, 17341, 17342, 17343, 17344, 17345, 17346, 17347, 17348, 17349, 17350, 17351, 17352, 17353, 17354, 17355, 17356, 17357, 17358, 17359, 17360, 17361, 17362, 17363, 17364, 17365, 17366, 17367, 17368, 17369, 17370, 17371, 17372, 17373, 17374, 17375, 17376, 17377, 17378, 17379, 17380, 17381, 17382, 17383, 17384, 17385, 17386, 17387, 17388, 17389, 17390, 17391, 17392, 17393, 17394, 17395, 17396, 17397, 17398, 17399, 17400, 17401, 17402, 17403, 17404, 17405, 17406, 17407, 17408, 17409, 17410, 17411, 17412, 17413, 17414, 17415, 17416, 17417, 17418, 17419, 17420, 17421, 17422, 17423, 17424, 17425, 17426, 17427, 17428, 17429, 17430, 17431, 17432, 17433, 17434, 17435, 17436, 17437, 17438, 17439, 17440, 17441, 17442, 17443, 17444, 17445, 17446, 17447, 17448, 17449, 17450, 17451, 17452, 17453, 17454, 17455, 17456, 17457, 17458, 17459, 17460, 17461, 17462, 17463, 17464, 17465, 17466, 17467, 17468, 17469, 17470, 17471, 17472, 17473, 17474, 17475, 17476, 17477, 17478, 17479, 17480, 17481, 17482, 17483, 17484, 17485, 17486, 17487, 17488, 17489, 17490, 17491, 17492, 17493, 17494, 17495, 17496, 17497, 17498, 17499, 17500, 17501, 17502, 17503, 17504, 17505, 17506, 17507, 17508, 17509, 17510, 17511, 17512, 17513, 17514, 17515, 17516, 17517, 17518, 17519, 17520, 17521, 17522, 17523, 17524, 17525, 17526, 17527, 17528, 17529, 17530, 17531, 17532, 17533, 17534, 17535, 17536, 17537, 17538, 17539, 17540, 17541, 17542, 17543, 17544, 17545, 17546, 17547, 17548, 17549, 17550, 17551, 17552, 17553, 17554, 17555, 17556, 17557, 17558, 17559, 17560, 17561, 17562, 17563, 17564, 17565, 17566, 17567, 17568, 17569, 17570, 17571, 17572, 17573, 17574, 17575, 17576, 17577, 17578, 17579, 17580, 17581, 17582, 17583, 17584, 17585, 17586, 17587, 17588, 17589, 17590, 17591, 17592, 17593, 17594, 17595, 17596, 17597, 17598, 17599, 17600, 17601, 17602, 17603, 17604, 17605, 17606, 17607, 17608, 17609, 17610, 17611, 17612, 17613, 17614, 17615, 17616, 17617, 17618, 17619, 17620, 17621, 17622, 17623, 17624, 17625, 17626, 17627, 17628, 17629, 17630, 17631, 17632, 17633, 17634, 17635, 17636, 17637, 17638, 17639, 17640, 17641, 17642, 17643, 17644, 17645, 17646, 17647, 17648, 17649, 17650, 17651, 17652, 17653, 17654, 17655, 17656, 17657, 17658, 17659, 17660, 17661, 17662, 17663, 17664, 17665, 17666, 17667, 17668, 17669, 17670, 17671, 17672, 17673, 17674, 17675, 17676, 17677, 17678, 17679, 17680, 17681, 17682, 17683, 17684, 17685, 17686, 17687, 17688, 17689, 17690, 17691, 17692, 17693, 17694, 17695, 17696, 17697, 17698, 17699, 17700, 17701, 17702, 17703, 17704, 17705, 17706, 17707, 17708, 17709, 17710, 17711, 17712, 17713, 17714, 17715, 17716, 17717, 17718, 17719, 17720, 17721, 17722, 17723, 17724, 17725, 17726, 17727, 17728, 17729, 17730, 17731, 17732, 17733, 17734, 17735, 17736, 17737, 17738, 17739, 17740, 17741, 17742, 17743, 17744, 17745, 17746, 17747, 17748, 17749, 17750, 17751, 17752, 17753, 17754, 17755, 17756, 17757, 17758, 17759, 17760, 17761, 17762, 17763, 17764, 17765, 17766, 17767, 17768, 17769, 17770, 17771, 17772, 17773, 17774, 17775, 17776, 17777, 17778, 17779, 17780, 17781, 17782, 17783, 17784, 17785, 17786, 17787, 17788, 17789, 17790, 17791, 17792, 17793, 17794, 17795, 17796, 17797, 17798, 17799, 17800, 17801, 17802, 17803, 17804, 17805, 17806, 17807, 17808, 17809, 17810, 17811, 17812, 17813, 17814, 17815, 17816, 17817, 17818, 17819, 17820, 17821, 17822, 17823, 17824, 17825, 17826, 17827, 17828, 17829, 17830, 17831, 17832, 17833, 17834, 17835, 17836, 17837, 17838, 17839, 17840, 17841, 17842, 17843, 17844, 17845, 17846, 17847, 17848, 17849, 17850, 17851, 17852, 17853, 17854, 17855, 17856, 17857, 17858, 17859, 17860, 17861, 17862, 17863, 17864, 17865, 17866, 17867, 17868, 17869, 17870, 17871, 17872, 17873, 17874, 17875, 17876, 17877, 17878, 17879, 17880, 17881, 17882, 17883, 17884, 17885, 17886, 17887, 17888, 17889, 17890, 17891, 17892, 17893, 17894, 17895, 17896, 17897, 17898, 17899, 17900, 17901, 17902, 17903, 17904, 17905, 17906, 17907, 17908, 17909, 17910, 17911, 17912, 17913, 17914, 17915, 17916, 17917, 17918, 17919, 17920, 17921, 17922, 17923, 17924, 17925, 17926, 17927, 17928, 17929, 17930, 17931, 17932, 17933, 17934, 17935, 17936, 17937, 17938, 17939, 17940, 17941, 17942, 17943, 17944, 17945, 17946, 17947, 17948, 17949, 17950, 17951, 17952, 17953, 17954, 17955, 17956, 17957, 17958, 17959, 17960, 17961, 17962, 17963, 17964, 17965, 17966, 17967, 17968, 17969, 17970, 17971, 17972, 17973, 17974, 17975, 17976, 17977, 17978, 17979, 17980, 17981, 17982, 17983, 17984, 17985, 17986, 17987, 17988, 17989, 17990, 17991, 17992, 17993, 17994, 17995, 17996, 17997, 17998, 17999, 18000, 18001, 18002, 18003, 18004, 18005, 18006, 18007, 18008, 18009, 18010, 18011, 18012, 18013, 18014, 18015, 18016, 18017, 18018, 18019, 18020, 18021, 18022, 18023, 18024, 18025, 18026, 18027, 18028, 18029, 18030, 18031, 18032, 18033, 18034, 18035, 18036, 18037, 18038, 18039, 18040, 18041, 18042, 18043, 18044, 18045, 18046, 18047, 18048, 18049, 18050, 18051, 18052, 18053, 18054, 18055, 18056, 18057, 18058, 18059, 18060, 18061, 18062, 18063, 18064, 18065, 18066, 18067, 18068, 18069, 18070, 18071, 18072, 18073, 18074, 18075, 18076, 18077, 18078, 18079, 18080, 18081, 18082, 18083, 18084, 18085, 18086, 18087, 18088, 18089, 18090, 18091, 18092, 18093, 18094, 18095, 18096, 18097, 18098, 18099, 18100, 18101, 18102, 18103, 18104, 18105, 18106, 18107, 18108, 18109, 18110, 18111, 18112, 18113, 18114, 18115, 18116, 18117, 18118, 18119, 18120, 18121, 18122, 18123, 18124, 18125, 18126, 18127, 18128, 18129, 18130, 18131, 18132, 18133, 18134, 18135, 18136, 18137, 18138, 18139, 18140, 18141, 18142, 18143, 18144, 18145, 18146, 18147, 18148, 18149, 18150, 18151, 18152, 18153, 18154, 18155, 18156, 18157, 18158, 18159, 18160, 18161, 18162, 18163, 18164, 18165, 18166, 18167, 18168, 18169, 18170, 18171, 18172, 18173, 18174, 18175, 18176, 18177, 18178, 18179, 18180, 18181, 18182, 18183, 18184, 18185, 18186, 18187, 18188, 18189, 18190, 18191, 18192, 18193, 18194, 18195, 18196, 18197, 18198, 18199, 18200, 18201, 18202, 18203, 18204, 18205, 18206, 18207, 18208, 18209, 18210, 18211, 18212, 18213, 18214, 18215, 18216, 18217, 18218, 18219, 18220, 18221, 18222, 18223, 18224, 18225, 18226, 18227, 18228, 18229, 18230, 18231, 18232, 18233, 18234, 18235, 18236, 18237, 18238, 18239, 18240, 18241, 18242, 18243, 18244, 18245, 18246, 18247, 18248, 18249, 18250, 18251, 18252, 18253, 18254, 18255, 18256, 18257, 18258, 18259, 18260, 18261, 18262, 18263, 18264, 18265, 18266, 18267, 18268, 18269, 18270, 18271, 18272, 18273, 18274, 18275, 18276, 18277, 18278, 18279, 18280, 18281, 18282, 18283, 18284, 18285, 18286, 18287, 18288, 18289, 18290, 18291, 18292, 18293, 18294, 18295, 18296, 18297, 18298, 18299, 18300, 18301, 18302, 18303, 18304, 18305, 18306, 18307, 18308, 18309, 18310, 18311, 18312, 18313, 18314, 18315, 18316, 18317, 18318, 18319, 18320, 18321, 18322, 18323, 18324, 18325, 18326, 18327, 18328, 18329, 18330, 18331, 18332, 18333, 18334, 18335, 18336, 18337, 18338, 18339, 18340, 18341, 18342, 18343, 18344, 18345, 18346, 18347, 18348, 18349, 18350, 18351, 18352, 18353, 18354, 18355, 18356, 18357, 18358, 18359, 18360, 18361, 18362, 18363, 18364, 18365, 18366, 18367, 18368, 18369, 18370, 18371, 18372, 18373, 18374, 18375, 18376, 18377, 18378, 18379, 18380, 18381, 18382, 18383, 18384, 18385, 18386, 18387, 18388, 18389, 18390, 18391, 18392, 18393, 18394, 18395, 18396, 18397, 18398, 18399, 18400, 18401, 18402, 18403, 18404, 18405, 18406, 18407, 18408, 18409, 18410, 18411, 18412, 18413, 18414, 18415, 18416, 18417, 18418, 18419, 18420, 18421, 18422, 18423, 18424, 18425, 18426, 18427, 18428, 18429, 18430, 18431, 18432, 18433, 18434, 18435, 18436, 18437, 18438, 18439, 18440, 18441, 18442, 18443, 18444, 18445, 18446, 18447, 18448, 18449, 18450, 18451, 18452, 18453, 18454, 18455, 18456, 18457, 18458, 18459, 18460, 18461, 18462, 18463, 18464, 18465, 18466, 18467, 18468, 18469, 18470, 18471, 18472, 18473, 18474, 18475, 18476, 18477, 18478, 18479, 18480, 18481, 18482, 18483, 18484, 18485, 18486, 18487, 18488, 18489, 18490, 18491, 18492, 18493, 18494, 18495, 18496, 18497, 18498, 18499, 18500, 18501, 18502, 18503, 18504, 18505, 18506, 18507, 18508, 18509, 18510, 18511, 18512, 18513, 18514, 18515, 18516, 18517, 18518, 18519, 18520, 18521, 18522, 18523, 18524, 18525, 18526, 18527, 18528, 18529, 18530, 18531, 18532, 18533, 18534, 18535, 18536, 18537, 18538, 18539, 18540, 18541, 18542, 18543, 18544, 18545, 18546, 18547, 18548, 18549, 18550, 18551, 18552, 18553, 18554, 18555, 18556, 18557, 18558, 18559, 18560, 18561, 18562, 18563, 18564, 18565, 18566, 18567, 18568, 18569, 18570, 18571, 18572, 18573, 18574, 18575, 18576, 18577, 18578, 18579, 18580, 18581, 18582, 18583, 18584, 18585, 18586, 18587, 18588, 18589, 18590, 18591, 18592, 18593, 18594, 18595, 18596, 18597, 18598, 18599, 18600, 18601, 18602, 18603, 18604, 18605, 18606, 18607, 18608, 18609, 18610, 18611, 18612, 18613, 18614, 18615, 18616, 18617, 18618, 18619, 18620, 18621, 18622, 18623, 18624, 18625, 18626, 18627, 18628, 18629, 18630, 18631, 18632, 18633, 18634, 18635, 18636, 18637, 18638, 18639, 18640, 18641, 18642, 18643, 18644, 18645, 18646, 18647, 18648, 18649, 18650, 18651, 18652, 18653, 18654, 18655, 18656, 18657, 18658, 18659, 18660, 18661, 18662, 18663, 18664, 18665, 18666, 18667, 18668, 18669, 18670, 18671, 18672, 18673, 18674, 18675, 18676, 18677, 18678, 18679, 18680, 18681, 18682, 18683, 18684, 18685, 18686, 18687, 18688, 18689, 18690, 18691, 18692, 18693, 18694, 18695, 18696, 18697, 18698, 18699, 18700, 18701, 18702, 18703, 18704, 18705, 18706, 18707, 18708, 18709, 18710, 18711, 18712, 18713, 18714, 18715, 18716, 18717, 18718, 18719, 18720, 18721, 18722, 18723, 18724, 18725, 18726, 18727, 18728, 18729, 18730, 18731, 18732, 18733, 18734, 18735, 18736, 18737, 18738, 18739, 18740, 18741, 18742, 18743, 18744, 18745, 18746, 18747, 18748, 18749, 18750, 18751, 18752, 18753, 18754, 18755, 18756, 18757, 18758, 18759, 18760, 18761, 18762, 18763, 18764, 18765, 18766, 18767, 18768, 18769, 18770, 18771, 18772, 18773, 18774, 18775, 18776, 18777, 18778, 18779, 18780, 18781, 18782, 18783, 18784, 18785, 18786, 18787, 18788, 18789, 18790, 18791, 18792, 18793, 18794, 18795, 18796, 18797, 18798, 18799, 18800, 18801, 18802, 18803, 18804, 18805, 18806, 18807, 18808, 18809, 18810, 18811, 18812, 18813, 18814, 18815, 18816, 18817, 18818, 18819, 18820, 18821, 18822, 18823, 18824, 18825, 18826, 18827, 18828, 18829, 18830, 18831, 18832, 18833, 18834, 18835, 18836, 18837, 18838, 18839, 18840, 18841, 18842, 18843, 18844, 18845, 18846, 18847, 18848, 18849, 18850, 18851, 18852, 18853, 18854, 18855, 18856, 18857, 18858, 18859, 18860, 18861, 18862, 18863, 18864, 18865, 18866, 18867, 18868, 18869, 18870, 18871, 18872, 18873, 18874, 18875, 18876, 18877, 18878, 18879, 18880, 18881, 18882, 18883, 18884, 18885, 18886, 18887, 18888, 18889, 18890, 18891, 18892, 18893, 18894, 18895, 18896, 18897, 18898, 18899, 18900, 18901, 18902, 18903, 18904, 18905, 18906, 18907, 18908, 18909, 18910, 18911, 18912, 18913, 18914, 18915, 18916, 18917, 18918, 18919, 18920, 18921, 18922, 18923, 18924, 18925, 18926, 18927, 18928, 18929, 18930, 18931, 18932, 18933, 18934, 18935, 18936, 18937, 18938, 18939, 18940, 18941, 18942, 18943, 18944, 18945, 18946, 18947, 18948, 18949, 18950, 18951, 18952, 18953, 18954, 18955, 18956, 18957, 18958, 18959, 18960, 18961, 18962, 18963, 18964, 18965, 18966, 18967, 18968, 18969, 18970, 18971, 18972, 18973, 18974, 18975, 18976, 18977, 18978, 18979, 18980, 18981, 18982, 18983, 18984, 18985, 18986, 18987, 18988, 18989, 18990, 18991, 18992, 18993, 18994, 18995, 18996, 18997, 18998, 18999, 19000, 19001, 19002, 19003, 19004, 19005, 19006, 19007, 19008, 19009, 19010, 19011, 19012, 19013, 19014, 19015, 19016, 19017, 19018, 19019, 19020, 19021, 19022, 19023, 19024, 19025, 19026, 19027, 19028, 19029, 19030, 19031, 19032, 19033, 19034, 19035, 19036, 19037, 19038, 19039, 19040, 19041, 19042, 19043, 19044, 19045, 19046, 19047, 19048, 19049, 19050, 19051, 19052, 19053, 19054, 19055, 19056, 19057, 19058, 19059, 19060, 19061, 19062, 19063, 19064, 19065, 19066, 19067, 19068, 19069, 19070, 19071, 19072, 19073, 19074, 19075, 19076, 19077, 19078, 19079, 19080, 19081, 19082, 19083, 19084, 19085, 19086, 19087, 19088, 19089, 19090, 19091, 19092, 19093, 19094, 19095, 19096, 19097, 19098, 19099, 19100, 19101, 19102, 19103, 19104, 19105, 19106, 19107, 19108, 19109, 19110, 19111, 19112, 19113, 19114, 19115, 19116, 19117, 19118, 19119, 19120, 19121, 19122, 19123, 19124, 19125, 19126, 19127, 19128, 19129, 19130, 19131, 19132, 19133, 19134, 19135, 19136, 19137, 19138, 19139, 19140, 19141, 19142, 19143, 19144, 19145, 19146, 19147, 19148, 19149, 19150, 19151, 19152, 19153, 19154, 19155, 19156, 19157, 19158, 19159, 19160, 19161, 19162, 19163, 19164, 19165, 19166, 19167, 19168, 19169, 19170, 19171, 19172, 19173, 19174, 19175, 19176, 19177, 19178, 19179, 19180, 19181, 19182, 19183, 19184, 19185, 19186, 19187, 19188, 19189, 19190, 19191, 19192, 19193, 19194, 19195, 19196, 19197, 19198, 19199, 19200, 19201, 19202, 19203, 19204, 19205, 19206, 19207, 19208, 19209, 19210, 19211, 19212, 19213, 19214, 19215, 19216, 19217, 19218, 19219, 19220, 19221, 19222, 19223, 19224, 19225, 19226, 19227, 19228, 19229, 19230, 19231, 19232, 19233, 19234, 19235, 19236, 19237, 19238, 19239, 19240, 19241, 19242, 19243, 19244, 19245, 19246, 19247, 19248, 19249, 19250, 19251, 19252, 19253, 19254, 19255, 19256, 19257, 19258, 19259, 19260, 19261, 19262, 19263, 19264, 19265, 19266, 19267, 19268, 19269, 19270, 19271, 19272, 19273, 19274, 19275, 19276, 19277, 19278, 19279, 19280, 19281, 19282, 19283, 19284, 19285, 19286, 19287, 19288, 19289, 19290, 19291, 19292, 19293, 19294, 19295, 19296, 19297, 19298, 19299, 19300, 19301, 19302, 19303, 19304, 19305, 19306, 19307, 19308, 19309, 19310, 19311, 19312, 19313, 19314, 19315, 19316, 19317, 19318, 19319, 19320, 19321, 19322, 19323, 19324, 19325, 19326, 19327, 19328, 19329, 19330, 19331, 19332, 19333, 19334, 19335, 19336, 19337, 19338, 19339, 19340, 19341, 19342, 19343, 19344, 19345, 19346, 19347, 19348, 19349, 19350, 19351, 19352, 19353, 19354, 19355, 19356, 19357, 19358, 19359, 19360, 19361, 19362, 19363, 19364, 19365, 19366, 19367, 19368, 19369, 19370, 19371, 19372, 19373, 19374, 19375, 19376, 19377, 19378, 19379, 19380, 19381, 19382, 19383, 19384, 19385, 19386, 19387, 19388, 19389, 19390, 19391, 19392, 19393, 19394, 19395, 19396, 19397, 19398, 19399, 19400, 19401, 19402, 19403, 19404, 19405, 19406, 19407, 19408, 19409, 19410, 19411, 19412, 19413, 19414, 19415, 19416, 19417, 19418, 19419, 19420, 19421, 19422, 19423, 19424, 19425, 19426, 19427, 19428, 19429, 19430, 19431, 19432, 19433, 19434, 19435, 19436, 19437, 19438, 19439, 19440, 19441, 19442, 19443, 19444, 19445, 19446, 19447, 19448, 19449, 19450, 19451, 19452, 19453, 19454, 19455, 19456, 19457, 19458, 19459, 19460, 19461, 19462, 19463, 19464, 19465, 19466, 19467, 19468, 19469, 19470, 19471, 19472, 19473, 19474, 19475, 19476, 19477, 19478, 19479, 19480, 19481, 19482, 19483, 19484, 19485, 19486, 19487, 19488, 19489, 19490, 19491, 19492, 19493, 19494, 19495, 19496, 19497, 19498, 19499, 19500, 19501, 19502, 19503, 19504, 19505, 19506, 19507, 19508, 19509, 19510, 19511, 19512, 19513, 19514, 19515, 19516, 19517, 19518, 19519, 19520, 19521, 19522, 19523, 19524, 19525, 19526, 19527, 19528, 19529, 19530, 19531, 19532, 19533, 19534, 19535, 19536, 19537, 19538, 19539, 19540, 19541, 19542, 19543, 19544, 19545, 19546, 19547, 19548, 19549, 19550, 19551, 19552, 19553, 19554, 19555, 19556, 19557, 19558, 19559, 19560, 19561, 19562, 19563, 19564, 19565, 19566, 19567, 19568, 19569, 19570, 19571, 19572, 19573, 19574, 19575, 19576, 19577, 19578, 19579, 19580, 19581, 19582, 19583, 19584, 19585, 19586, 19587, 19588, 19589, 19590, 19591, 19592, 19593, 19594, 19595, 19596, 19597, 19598, 19599, 19600, 19601, 19602, 19603, 19604, 19605, 19606, 19607, 19608, 19609, 19610, 19611, 19612, 19613, 19614, 19615, 19616, 19617, 19618, 19619, 19620, 19621, 19622, 19623, 19624, 19625, 19626, 19627, 19628, 19629, 19630, 19631, 19632, 19633, 19634, 19635, 19636, 19637, 19638, 19639, 19640, 19641, 19642, 19643, 19644, 19645, 19646, 19647, 19648, 19649, 19650, 19651, 19652, 19653, 19654, 19655, 19656, 19657, 19658, 19659, 19660, 19661, 19662, 19663, 19664, 19665, 19666, 19667, 19668, 19669, 19670, 19671, 19672, 19673, 19674, 19675, 19676, 19677, 19678, 19679, 19680, 19681, 19682, 19683, 19684, 19685, 19686, 19687, 19688, 19689, 19690, 19691, 19692, 19693, 19694, 19695, 19696, 19697, 19698, 19699, 19700, 19701, 19702, 19703, 19704, 19705, 19706, 19707, 19708, 19709, 19710, 19711, 19712, 19713, 19714, 19715, 19716, 19717, 19718, 19719, 19720, 19721, 19722, 19723, 19724, 19725, 19726, 19727, 19728, 19729, 19730, 19731, 19732, 19733, 19734, 19735, 19736, 19737, 19738, 19739, 19740, 19741, 19742, 19743, 19744, 19745, 19746, 19747, 19748, 19749, 19750, 19751, 19752, 19753, 19754, 19755, 19756, 19757, 19758, 19759, 19760, 19761, 19762, 19763, 19764, 19765, 19766, 19767, 19768, 19769, 19770, 19771, 19772, 19773, 19774, 19775, 19776, 19777, 19778, 19779, 19780, 19781, 19782, 19783, 19784, 19785, 19786, 19787, 19788, 19789, 19790, 19791, 19792, 19793, 19794, 19795, 19796, 19797, 19798, 19799, 19800, 19801, 19802, 19803, 19804, 19805, 19806, 19807, 19808, 19809, 19810, 19811, 19812, 19813, 19814, 19815, 19816, 19817, 19818, 19819, 19820, 19821, 19822, 19823, 19824, 19825, 19826, 19827, 19828, 19829, 19830, 19831, 19832, 19833, 19834, 19835, 19836, 19837, 19838, 19839, 19840, 19841, 19842, 19843, 19844, 19845, 19846, 19847, 19848, 19849, 19850, 19851, 19852, 19853, 19854, 19855, 19856, 19857, 19858, 19859, 19860, 19861, 19862, 19863, 19864, 19865, 19866, 19867, 19868, 19869, 19870, 19871, 19872, 19873, 19874, 19875, 19876, 19877, 19878, 19879, 19880, 19881, 19882, 19883, 19884, 19885, 19886, 19887, 19888, 19889, 19890, 19891, 19892, 19893, 19894, 19895, 19896, 19897, 19898, 19899, 19900, 19901, 19902, 19903, 19904, 19905, 19906, 19907, 19908, 19909, 19910, 19911, 19912, 19913, 19914, 19915, 19916, 19917, 19918, 19919, 19920, 19921, 19922, 19923, 19924, 19925, 19926, 19927, 19928, 19929, 19930, 19931, 19932, 19933, 19934, 19935, 19936, 19937, 19938, 19939, 19940, 19941, 19942, 19943, 19944, 19945, 19946, 19947, 19948, 19949, 19950, 19951, 19952, 19953, 19954, 19955, 19956, 19957, 19958, 19959, 19960, 19961, 19962, 19963, 19964, 19965, 19966, 19967, 19968, 19969, 19970, 19971, 19972, 19973, 19974, 19975, 19976, 19977, 19978, 19979, 19980, 19981, 19982, 19983, 19984, 19985, 19986, 19987, 19988, 19989, 19990, 19991, 19992, 19993, 19994, 19995, 19996, 19997, 19998, 19999, 20000, 20001, 20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012, 20013, 20014, 20015, 20016, 20017, 20018, 20019, 20020, 20021, 20022, 20023, 20024, 20025, 20026, 20027, 20028, 20029, 20030, 20031, 20032, 20033, 20034, 20035, 20036, 20037, 20038, 20039, 20040, 20041, 20042, 20043, 20044, 20045, 20046, 20047, 20048, 20049, 20050, 20051, 20052, 20053, 20054, 20055, 20056, 20057, 20058, 20059, 20060, 20061, 20062, 20063, 20064, 20065, 20066, 20067, 20068, 20069, 20070, 20071, 20072, 20073, 20074, 20075, 20076, 20077, 20078, 20079, 20080, 20081, 20082, 20083, 20084, 20085, 20086, 20087, 20088, 20089, 20090, 20091, 20092, 20093, 20094, 20095, 20096, 20097, 20098, 20099, 20100, 20101, 20102, 20103, 20104, 20105, 20106, 20107, 20108, 20109, 20110, 20111, 20112, 20113, 20114, 20115, 20116, 20117, 20118, 20119, 20120, 20121, 20122, 20123, 20124, 20125, 20126, 20127, 20128, 20129, 20130, 20131, 20132, 20133, 20134, 20135, 20136, 20137, 20138, 20139, 20140, 20141, 20142, 20143, 20144, 20145, 20146, 20147, 20148, 20149, 20150, 20151, 20152, 20153, 20154, 20155, 20156, 20157, 20158, 20159, 20160, 20161, 20162, 20163, 20164, 20165, 20166, 20167, 20168, 20169, 20170, 20171, 20172, 20173, 20174, 20175, 20176, 20177, 20178, 20179, 20180, 20181, 20182, 20183, 20184, 20185, 20186, 20187, 20188, 20189, 20190, 20191, 20192, 20193, 20194, 20195, 20196, 20197, 20198, 20199, 20200, 20201, 20202, 20203, 20204, 20205, 20206, 20207, 20208, 20209, 20210, 20211, 20212, 20213, 20214, 20215, 20216, 20217, 20218, 20219, 20220, 20221, 20222, 20223, 20224, 20225, 20226, 20227, 20228, 20229, 20230, 20231, 20232, 20233, 20234, 20235, 20236, 20237, 20238, 20239, 20240, 20241, 20242, 20243, 20244, 20245, 20246, 20247, 20248, 20249, 20250, 20251, 20252, 20253, 20254, 20255, 20256, 20257, 20258, 20259, 20260, 20261, 20262, 20263, 20264, 20265, 20266, 20267, 20268, 20269, 20270, 20271, 20272, 20273, 20274, 20275, 20276, 20277, 20278, 20279, 20280, 20281, 20282, 20283, 20284, 20285, 20286, 20287, 20288, 20289, 20290, 20291, 20292, 20293, 20294, 20295, 20296, 20297, 20298, 20299, 20300, 20301, 20302, 20303, 20304, 20305, 20306, 20307, 20308, 20309, 20310, 20311, 20312, 20313, 20314, 20315, 20316, 20317, 20318, 20319, 20320, 20321, 20322, 20323, 20324, 20325, 20326, 20327, 20328, 20329, 20330, 20331, 20332, 20333, 20334, 20335, 20336, 20337, 20338, 20339, 20340, 20341, 20342, 20343, 20344, 20345, 20346, 20347, 20348, 20349, 20350, 20351, 20352, 20353, 20354, 20355, 20356, 20357, 20358, 20359, 20360, 20361, 20362, 20363, 20364, 20365, 20366, 20367, 20368, 20369, 20370, 20371, 20372, 20373, 20374, 20375, 20376, 20377, 20378, 20379, 20380, 20381, 20382, 20383, 20384, 20385, 20386, 20387, 20388, 20389, 20390, 20391, 20392, 20393, 20394, 20395, 20396, 20397]\n"," This is the range of val:  [ 7700  7701  7702 ... 12097 12098 12099]\n","Starting training\n","shuffling\n","Epoch 1/200\n","2020-05-24 08:33:40.227547: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 08:33:40.428096: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","1600/1600 [==============================] - 19s 12ms/step - loss: 139.4723 - val_loss: 25.9425\n","\n","Epoch 00001: loss improved from inf to 139.47648, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 37.3906 - val_loss: 3.1664\n","\n","Epoch 00002: loss improved from 139.47648 to 37.39217, saving model to final.h5\n","Epoch 3/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 19.4806 - val_loss: 3.1677\n","\n","Epoch 00003: loss improved from 37.39217 to 19.48069, saving model to final.h5\n","Epoch 4/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 16.6381 - val_loss: 5.5689\n","\n","Epoch 00004: loss improved from 19.48069 to 16.63791, saving model to final.h5\n","Epoch 5/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 14.9722 - val_loss: 2.2433\n","\n","Epoch 00005: loss improved from 16.63791 to 14.97214, saving model to final.h5\n","Epoch 6/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 13.7995 - val_loss: 4.1070\n","\n","Epoch 00006: loss improved from 14.97214 to 13.80052, saving model to final.h5\n","Epoch 7/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 13.0597 - val_loss: 1.8198\n","\n","Epoch 00007: loss improved from 13.80052 to 13.06098, saving model to final.h5\n","Epoch 8/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 12.3583 - val_loss: 0.8897\n","\n","Epoch 00008: loss improved from 13.06098 to 12.35839, saving model to final.h5\n","Epoch 9/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 11.9577 - val_loss: 0.5033\n","\n","Epoch 00009: loss improved from 12.35839 to 11.95793, saving model to final.h5\n","Epoch 10/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 10.9131 - val_loss: 0.8416\n","\n","Epoch 00010: loss improved from 11.95793 to 10.91344, saving model to final.h5\n","Epoch 11/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 10.5411 - val_loss: 0.4472\n","\n","Epoch 00011: loss improved from 10.91344 to 10.54167, saving model to final.h5\n","Epoch 12/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 10.3280 - val_loss: 0.4654\n","\n","Epoch 00012: loss improved from 10.54167 to 10.32784, saving model to final.h5\n","Epoch 13/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 9.9561 - val_loss: 0.8693\n","\n","Epoch 00013: loss improved from 10.32784 to 9.95693, saving model to final.h5\n","Epoch 14/200\n","1600/1600 [==============================] - 17s 11ms/step - loss: 9.3562 - val_loss: 1.8064\n","\n","Epoch 00014: loss improved from 9.95693 to 9.35529, saving model to final.h5\n","Epoch 15/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 9.1685 - val_loss: 1.1381\n","\n","Epoch 00015: loss improved from 9.35529 to 9.16737, saving model to final.h5\n","Epoch 16/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 9.0471 - val_loss: 0.4821\n","\n","Epoch 00016: loss improved from 9.16737 to 9.04740, saving model to final.h5\n","Epoch 17/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 8.7557 - val_loss: 0.6798\n","\n","Epoch 00017: loss improved from 9.04740 to 8.75552, saving model to final.h5\n","Epoch 18/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 8.6140 - val_loss: 0.6160\n","\n","Epoch 00018: loss improved from 8.75552 to 8.61426, saving model to final.h5\n","Epoch 19/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 8.3821 - val_loss: 1.2844\n","\n","Epoch 00019: loss improved from 8.61426 to 8.38169, saving model to final.h5\n","Epoch 20/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 8.1497 - val_loss: 2.0441\n","\n","Epoch 00020: loss improved from 8.38169 to 8.14910, saving model to final.h5\n","Epoch 21/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 8.1846 - val_loss: 0.7512\n","\n","Epoch 00021: loss did not improve from 8.14910\n","Epoch 22/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 7.9872 - val_loss: 1.2096\n","\n","Epoch 00022: loss improved from 8.14910 to 7.98728, saving model to final.h5\n","Epoch 23/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 7.8327 - val_loss: 2.9840\n","\n","Epoch 00023: loss improved from 7.98728 to 7.83153, saving model to final.h5\n","Epoch 24/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 7.7100 - val_loss: 0.8214\n","\n","Epoch 00024: loss improved from 7.83153 to 7.71037, saving model to final.h5\n","Epoch 25/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 7.5308 - val_loss: 0.7970\n","\n","Epoch 00025: loss improved from 7.71037 to 7.53164, saving model to final.h5\n","Epoch 26/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 7.5216 - val_loss: 1.4256\n","\n","Epoch 00026: loss improved from 7.53164 to 7.52184, saving model to final.h5\n","Epoch 27/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 7.2611 - val_loss: 0.5608\n","\n","Epoch 00027: loss improved from 7.52184 to 7.26169, saving model to final.h5\n","Epoch 28/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 7.1125 - val_loss: 0.9124\n","\n","Epoch 00028: loss improved from 7.26169 to 7.11212, saving model to final.h5\n","Epoch 29/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 7.1789 - val_loss: 0.9080\n","\n","Epoch 00029: loss did not improve from 7.11212\n","Epoch 30/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 7.0855 - val_loss: 0.7777\n","\n","Epoch 00030: loss improved from 7.11212 to 7.08609, saving model to final.h5\n","Epoch 31/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 6.9971 - val_loss: 0.9993\n","\n","Epoch 00031: loss improved from 7.08609 to 6.99734, saving model to final.h5\n","Epoch 32/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 6.7814 - val_loss: 0.9743\n","\n","Epoch 00032: loss improved from 6.99734 to 6.78171, saving model to final.h5\n","Epoch 33/200\n","1600/1600 [==============================] - 17s 11ms/step - loss: 6.6923 - val_loss: 1.7298\n","\n","Epoch 00033: loss improved from 6.78171 to 6.69229, saving model to final.h5\n","Epoch 34/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 6.7709 - val_loss: 0.5676\n","\n","Epoch 00034: loss did not improve from 6.69229\n","Epoch 35/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 6.8777 - val_loss: 0.7370\n","\n","Epoch 00035: loss did not improve from 6.69229\n","Epoch 36/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 6.4706 - val_loss: 0.6071\n","\n","Epoch 00036: loss improved from 6.69229 to 6.47035, saving model to final.h5\n","Epoch 37/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 6.7880 - val_loss: 0.5346\n","\n","Epoch 00037: loss did not improve from 6.47035\n","Epoch 38/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 6.4445 - val_loss: 0.6039\n","\n","Epoch 00038: loss improved from 6.47035 to 6.44502, saving model to final.h5\n","Epoch 39/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 6.4602 - val_loss: 0.6945\n","\n","Epoch 00039: loss did not improve from 6.44502\n","Epoch 40/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 6.4675 - val_loss: 0.7068\n","\n","Epoch 00040: loss did not improve from 6.44502\n","Epoch 41/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 6.4305 - val_loss: 0.6326\n","\n","Epoch 00041: loss improved from 6.44502 to 6.43058, saving model to final.h5\n","Epoch 42/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 6.2238 - val_loss: 0.8092\n","\n","Epoch 00042: loss improved from 6.43058 to 6.22436, saving model to final.h5\n","Epoch 43/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 6.3253 - val_loss: 1.1027\n","\n","Epoch 00043: loss did not improve from 6.22436\n","Epoch 44/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 6.3762 - val_loss: 0.8113\n","\n","Epoch 00044: loss did not improve from 6.22436\n","Epoch 45/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 6.1855 - val_loss: 0.7875\n","\n","Epoch 00045: loss improved from 6.22436 to 6.18572, saving model to final.h5\n","Epoch 46/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 6.1298 - val_loss: 0.7157\n","\n","Epoch 00046: loss improved from 6.18572 to 6.13011, saving model to final.h5\n","Epoch 47/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 6.0754 - val_loss: 1.0240\n","\n","Epoch 00047: loss improved from 6.13011 to 6.07521, saving model to final.h5\n","Epoch 48/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 6.0384 - val_loss: 0.6889\n","\n","Epoch 00048: loss improved from 6.07521 to 6.03839, saving model to final.h5\n","Epoch 49/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 5.9641 - val_loss: 0.6586\n","\n","Epoch 00049: loss improved from 6.03839 to 5.96432, saving model to final.h5\n","Epoch 50/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 6.0649 - val_loss: 0.6053\n","\n","Epoch 00050: loss did not improve from 5.96432\n","Epoch 51/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 5.8731 - val_loss: 0.5648\n","\n","Epoch 00051: loss improved from 5.96432 to 5.87366, saving model to final.h5\n","Epoch 52/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.8800 - val_loss: 1.6878\n","\n","Epoch 00052: loss did not improve from 5.87366\n","Epoch 53/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.8597 - val_loss: 0.6815\n","\n","Epoch 00053: loss improved from 5.87366 to 5.85979, saving model to final.h5\n","Epoch 54/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.7543 - val_loss: 1.0516\n","\n","Epoch 00054: loss improved from 5.85979 to 5.75413, saving model to final.h5\n","Epoch 55/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.7915 - val_loss: 1.0577\n","\n","Epoch 00055: loss did not improve from 5.75413\n","Epoch 56/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.6823 - val_loss: 0.8112\n","\n","Epoch 00056: loss improved from 5.75413 to 5.68263, saving model to final.h5\n","Epoch 57/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 5.5165 - val_loss: 0.8589\n","\n","Epoch 00057: loss improved from 5.68263 to 5.51655, saving model to final.h5\n","Epoch 58/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.7625 - val_loss: 0.9061\n","\n","Epoch 00058: loss did not improve from 5.51655\n","Epoch 59/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.6176 - val_loss: 1.0084\n","\n","Epoch 00059: loss did not improve from 5.51655\n","Epoch 60/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.6531 - val_loss: 1.8811\n","\n","Epoch 00060: loss did not improve from 5.51655\n","Epoch 61/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.6663 - val_loss: 1.0099\n","\n","Epoch 00061: loss did not improve from 5.51655\n","Epoch 62/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.5725 - val_loss: 0.6935\n","\n","Epoch 00062: loss did not improve from 5.51655\n","Epoch 63/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.5133 - val_loss: 1.4060\n","\n","Epoch 00063: loss improved from 5.51655 to 5.51348, saving model to final.h5\n","Epoch 64/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.4892 - val_loss: 2.0009\n","\n","Epoch 00064: loss improved from 5.51348 to 5.48948, saving model to final.h5\n","Epoch 65/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.3957 - val_loss: 1.3827\n","\n","Epoch 00065: loss improved from 5.48948 to 5.39589, saving model to final.h5\n","Epoch 66/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.4970 - val_loss: 0.7852\n","\n","Epoch 00066: loss did not improve from 5.39589\n","Epoch 67/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.4395 - val_loss: 0.7867\n","\n","Epoch 00067: loss did not improve from 5.39589\n","Epoch 68/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.4124 - val_loss: 0.9388\n","\n","Epoch 00068: loss did not improve from 5.39589\n","Epoch 69/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.5113 - val_loss: 0.9314\n","\n","Epoch 00069: loss did not improve from 5.39589\n","Epoch 70/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 5.4946 - val_loss: 1.3316\n","\n","Epoch 00070: loss did not improve from 5.39589\n","Epoch 71/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.2463 - val_loss: 0.9970\n","\n","Epoch 00071: loss improved from 5.39589 to 5.24658, saving model to final.h5\n","Epoch 72/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.3201 - val_loss: 1.0071\n","\n","Epoch 00072: loss did not improve from 5.24658\n","Epoch 73/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.3354 - val_loss: 0.9146\n","\n","Epoch 00073: loss did not improve from 5.24658\n","Epoch 74/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.1945 - val_loss: 1.4882\n","\n","Epoch 00074: loss improved from 5.24658 to 5.19438, saving model to final.h5\n","Epoch 75/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.2496 - val_loss: 1.2938\n","\n","Epoch 00075: loss did not improve from 5.19438\n","Epoch 76/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.2988 - val_loss: 1.3122\n","\n","Epoch 00076: loss did not improve from 5.19438\n","Epoch 77/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.0710 - val_loss: 0.8350\n","\n","Epoch 00077: loss improved from 5.19438 to 5.07108, saving model to final.h5\n","Epoch 78/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.1973 - val_loss: 1.9639\n","\n","Epoch 00078: loss did not improve from 5.07108\n","Epoch 79/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.1734 - val_loss: 1.5484\n","\n","Epoch 00079: loss did not improve from 5.07108\n","Epoch 80/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.3114 - val_loss: 1.0902\n","\n","Epoch 00080: loss did not improve from 5.07108\n","Epoch 81/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.1608 - val_loss: 1.7337\n","\n","Epoch 00081: loss did not improve from 5.07108\n","Epoch 82/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.1042 - val_loss: 1.2256\n","\n","Epoch 00082: loss did not improve from 5.07108\n","Epoch 83/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.1328 - val_loss: 2.3894\n","\n","Epoch 00083: loss did not improve from 5.07108\n","Epoch 84/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.0870 - val_loss: 0.9503\n","\n","Epoch 00084: loss did not improve from 5.07108\n","Epoch 85/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.1541 - val_loss: 2.2402\n","\n","Epoch 00085: loss did not improve from 5.07108\n","Epoch 86/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.1798 - val_loss: 0.9487\n","\n","Epoch 00086: loss did not improve from 5.07108\n","Epoch 87/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.0654 - val_loss: 2.3301\n","\n","Epoch 00087: loss improved from 5.07108 to 5.06573, saving model to final.h5\n","Epoch 88/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.0504 - val_loss: 1.2359\n","\n","Epoch 00088: loss improved from 5.06573 to 5.04982, saving model to final.h5\n","Epoch 89/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.1242 - val_loss: 2.0793\n","\n","Epoch 00089: loss did not improve from 5.04982\n","Epoch 90/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.9919 - val_loss: 1.4180\n","\n","Epoch 00090: loss improved from 5.04982 to 4.99236, saving model to final.h5\n","Epoch 91/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.0255 - val_loss: 2.0410\n","\n","Epoch 00091: loss did not improve from 4.99236\n","Epoch 92/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.9954 - val_loss: 1.9128\n","\n","Epoch 00092: loss did not improve from 4.99236\n","Epoch 93/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 5.0165 - val_loss: 1.7178\n","\n","Epoch 00093: loss did not improve from 4.99236\n","Epoch 94/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.9929 - val_loss: 1.2299\n","\n","Epoch 00094: loss improved from 4.99236 to 4.99197, saving model to final.h5\n","Epoch 95/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.8129 - val_loss: 1.2335\n","\n","Epoch 00095: loss improved from 4.99197 to 4.81329, saving model to final.h5\n","Epoch 96/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.9676 - val_loss: 1.4230\n","\n","Epoch 00096: loss did not improve from 4.81329\n","Epoch 97/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.9215 - val_loss: 2.5638\n","\n","Epoch 00097: loss did not improve from 4.81329\n","Epoch 98/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.9112 - val_loss: 1.4545\n","\n","Epoch 00098: loss did not improve from 4.81329\n","Epoch 99/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.8347 - val_loss: 2.4634\n","\n","Epoch 00099: loss did not improve from 4.81329\n","Epoch 100/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.8242 - val_loss: 1.4329\n","\n","Epoch 00100: loss did not improve from 4.81329\n","Epoch 101/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.7570 - val_loss: 1.5274\n","\n","Epoch 00101: loss improved from 4.81329 to 4.75751, saving model to final.h5\n","Epoch 102/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.8155 - val_loss: 1.2030\n","\n","Epoch 00102: loss did not improve from 4.75751\n","Epoch 103/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.6991 - val_loss: 2.0166\n","\n","Epoch 00103: loss improved from 4.75751 to 4.69938, saving model to final.h5\n","Epoch 104/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.8928 - val_loss: 1.4707\n","\n","Epoch 00104: loss did not improve from 4.69938\n","Epoch 105/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.8081 - val_loss: 0.9537\n","\n","Epoch 00105: loss did not improve from 4.69938\n","Epoch 106/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.7619 - val_loss: 1.0432\n","\n","Epoch 00106: loss did not improve from 4.69938\n","Epoch 107/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.8039 - val_loss: 1.9600\n","\n","Epoch 00107: loss did not improve from 4.69938\n","Epoch 108/200\n","1600/1600 [==============================] - 17s 11ms/step - loss: 4.7343 - val_loss: 2.2409\n","\n","Epoch 00108: loss did not improve from 4.69938\n","Epoch 109/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.7712 - val_loss: 1.3279\n","\n","Epoch 00109: loss did not improve from 4.69938\n","Epoch 110/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.7346 - val_loss: 2.5937\n","\n","Epoch 00110: loss did not improve from 4.69938\n","Epoch 111/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.8155 - val_loss: 1.8675\n","\n","Epoch 00111: loss did not improve from 4.69938\n","Epoch 112/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.7595 - val_loss: 1.8508\n","\n","Epoch 00112: loss did not improve from 4.69938\n","Epoch 113/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.7668 - val_loss: 1.8166\n","\n","Epoch 00113: loss did not improve from 4.69938\n","Epoch 114/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.7444 - val_loss: 1.5840\n","\n","Epoch 00114: loss did not improve from 4.69938\n","Epoch 115/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.6594 - val_loss: 2.1147\n","\n","Epoch 00115: loss improved from 4.69938 to 4.65931, saving model to final.h5\n","Epoch 116/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.5740 - val_loss: 0.9429\n","\n","Epoch 00116: loss improved from 4.65931 to 4.57438, saving model to final.h5\n","Epoch 117/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.6586 - val_loss: 1.2221\n","\n","Epoch 00117: loss did not improve from 4.57438\n","Epoch 118/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.7326 - val_loss: 1.5616\n","\n","Epoch 00118: loss did not improve from 4.57438\n","Epoch 119/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.5894 - val_loss: 1.0114\n","\n","Epoch 00119: loss did not improve from 4.57438\n","Epoch 120/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.6364 - val_loss: 1.1243\n","\n","Epoch 00120: loss did not improve from 4.57438\n","Epoch 121/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.6126 - val_loss: 0.8958\n","\n","Epoch 00121: loss did not improve from 4.57438\n","Epoch 122/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.6410 - val_loss: 1.3359\n","\n","Epoch 00122: loss did not improve from 4.57438\n","Epoch 123/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.5114 - val_loss: 0.8095\n","\n","Epoch 00123: loss improved from 4.57438 to 4.51018, saving model to final.h5\n","Epoch 124/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.6213 - val_loss: 1.6244\n","\n","Epoch 00124: loss did not improve from 4.51018\n","Epoch 125/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.6240 - val_loss: 2.6564\n","\n","Epoch 00125: loss did not improve from 4.51018\n","Epoch 126/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.6077 - val_loss: 2.5291\n","\n","Epoch 00126: loss did not improve from 4.51018\n","Epoch 127/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.6634 - val_loss: 1.0037\n","\n","Epoch 00127: loss did not improve from 4.51018\n","Epoch 128/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.4943 - val_loss: 1.6304\n","\n","Epoch 00128: loss improved from 4.51018 to 4.49474, saving model to final.h5\n","Epoch 129/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.4906 - val_loss: 1.2283\n","\n","Epoch 00129: loss improved from 4.49474 to 4.49094, saving model to final.h5\n","Epoch 130/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.5210 - val_loss: 1.3315\n","\n","Epoch 00130: loss did not improve from 4.49094\n","Epoch 131/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.5205 - val_loss: 1.1893\n","\n","Epoch 00131: loss did not improve from 4.49094\n","Epoch 132/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.3837 - val_loss: 1.6430\n","\n","Epoch 00132: loss improved from 4.49094 to 4.38406, saving model to final.h5\n","Epoch 133/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.3313 - val_loss: 1.3420\n","\n","Epoch 00133: loss improved from 4.38406 to 4.33142, saving model to final.h5\n","Epoch 134/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.5568 - val_loss: 0.5574\n","\n","Epoch 00134: loss did not improve from 4.33142\n","Epoch 135/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.4843 - val_loss: 1.2588\n","\n","Epoch 00135: loss did not improve from 4.33142\n","Epoch 136/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.5354 - val_loss: 1.3507\n","\n","Epoch 00136: loss did not improve from 4.33142\n","Epoch 137/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.4948 - val_loss: 1.2132\n","\n","Epoch 00137: loss did not improve from 4.33142\n","Epoch 138/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.3194 - val_loss: 0.8126\n","\n","Epoch 00138: loss improved from 4.33142 to 4.31985, saving model to final.h5\n","Epoch 139/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.3672 - val_loss: 0.9106\n","\n","Epoch 00139: loss did not improve from 4.31985\n","Epoch 140/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.4474 - val_loss: 1.0774\n","\n","Epoch 00140: loss did not improve from 4.31985\n","Epoch 141/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.5147 - val_loss: 1.9617\n","\n","Epoch 00141: loss did not improve from 4.31985\n","Epoch 142/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.4302 - val_loss: 1.2084\n","\n","Epoch 00142: loss did not improve from 4.31985\n","Epoch 143/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.4404 - val_loss: 1.3087\n","\n","Epoch 00143: loss did not improve from 4.31985\n","Epoch 144/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.4122 - val_loss: 1.6650\n","\n","Epoch 00144: loss did not improve from 4.31985\n","Epoch 145/200\n","1600/1600 [==============================] - 16s 10ms/step - loss: 4.3271 - val_loss: 1.3397\n","\n","Epoch 00145: loss did not improve from 4.31985\n","Epoch 146/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.3607 - val_loss: 1.5255\n","\n","Epoch 00146: loss did not improve from 4.31985\n","Epoch 147/200\n","1600/1600 [==============================] - 17s 10ms/step - loss: 4.4750 - val_loss: 1.7317\n","\n","Epoch 00147: loss did not improve from 4.31985\n","Epoch 148/200\n","1236/1600 [======================>.......] - ETA: 3s - loss: 4.3718Traceback (most recent call last):\n","  File \"speedchallenge.py\", line 362, in <module>\n","    help=\"percentage of train data for validation\")\n","  File \"speedchallenge.py\", line 62, in main\n","    self.train(args.video_file,args.speed_file,args.wipe,self.EPOCHS,self.BATCH_SIZE,args.augment)\n","  File \"speedchallenge.py\", line 276, in train\n","    print(len(val_indexes),'Val indices size')\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1732, in fit_generator\n","    initial_epoch=initial_epoch)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\", line 220, in fit_generator\n","    reset_metrics=False)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1514, in train_on_batch\n","    outputs = self.train_function(ins)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\", line 3792, in __call__\n","    outputs = self._graph_fn(*converted_inputs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1605, in __call__\n","    return self._call_impl(args, kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1645, in _call_impl\n","    return self._call_flat(args, self.captured_inputs, cancellation_manager)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\n","    ctx, args, cancellation_manager=cancellation_manager))\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\", line 598, in call\n","    ctx=ctx)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n","    inputs, attrs, num_outputs)\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jmo1M72b2pZG","colab_type":"text"},"source":["Added time distributed batch norm and a 2 layer for global average and pooling"]},{"cell_type":"code","metadata":{"id":"uxk-QYuh2mZV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d68cd03e-284d-48fb-b799-77f8dcef9b69","executionInfo":{"status":"ok","timestamp":1590317234861,"user_tz":360,"elapsed":4481637,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}}},"source":["!python speedchallenge.py train.mp4 train.txt --epoch 200 --history 2 --model final.h5 --split_start 7700 --split_end=12100 --LR 0.00001 --mode=train"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-24 09:32:04.784083: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow_1=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-24 09:32:06.595444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-24 09:32:06.608819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 09:32:06.609396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 09:32:06.609428: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 09:32:06.611043: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 09:32:06.612778: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 09:32:06.613118: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 09:32:06.614753: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 09:32:06.615909: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 09:32:06.619643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 09:32:06.619785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 09:32:06.620474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 09:32:06.621043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 09:32:06.626507: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz\n","2020-05-24 09:32:06.626788: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x266b2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 09:32:06.626817: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-24 09:32:06.722039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 09:32:06.722927: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x266b480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 09:32:06.722982: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-24 09:32:06.723185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 09:32:06.723769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 09:32:06.723805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 09:32:06.723837: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 09:32:06.723851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 09:32:06.723865: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 09:32:06.723878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 09:32:06.723929: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 09:32:06.723957: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 09:32:06.724039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 09:32:06.724630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 09:32:06.725173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 09:32:06.725227: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 09:32:07.248086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 09:32:07.248163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-24 09:32:07.248174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-24 09:32:07.248458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 09:32:07.249163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 09:32:07.249749: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-24 09:32:07.249787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:188: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow_1=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_1)\n","speedchallenge.py:193: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow_1=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_1)\n","speedchallenge.py:201: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow_2=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","speedchallenge.py:206: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow_2=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_2)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 2, 100, 100,  0                                            \n","__________________________________________________________________________________________________\n","time_distributed_14 (TimeDistri (None, 2, 25, 25, 32 4128        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 2, 25, 25, 32 4128        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","time_distributed_15 (TimeDistri (None, 2, 25, 25, 32 0           time_distributed_14[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_2 (TimeDistrib (None, 2, 25, 25, 32 0           time_distributed_1[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_16 (TimeDistri (None, 2, 25, 25, 32 128         time_distributed_15[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_3 (TimeDistrib (None, 2, 25, 25, 32 128         time_distributed_2[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_17 (TimeDistri (None, 2, 25, 25, 32 0           time_distributed_16[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_4 (TimeDistrib (None, 2, 25, 25, 32 0           time_distributed_3[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_18 (TimeDistri (None, 2, 7, 7, 64)  131136      time_distributed_17[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_5 (TimeDistrib (None, 2, 7, 7, 64)  131136      time_distributed_4[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_19 (TimeDistri (None, 2, 7, 7, 64)  0           time_distributed_18[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_6 (TimeDistrib (None, 2, 7, 7, 64)  0           time_distributed_5[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_20 (TimeDistri (None, 2, 7, 7, 64)  256         time_distributed_19[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_7 (TimeDistrib (None, 2, 7, 7, 64)  256         time_distributed_6[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_21 (TimeDistri (None, 2, 7, 7, 64)  0           time_distributed_20[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_8 (TimeDistrib (None, 2, 7, 7, 64)  0           time_distributed_7[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_22 (TimeDistri (None, 2, 4, 4, 64)  0           time_distributed_21[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_9 (TimeDistrib (None, 2, 2, 2, 128) 524416      time_distributed_8[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_23 (TimeDistri (None, 2, 4, 4, 64)  256         time_distributed_22[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_10 (TimeDistri (None, 2, 2, 2, 128) 0           time_distributed_9[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_24 (TimeDistri (None, 2, 4, 4, 64)  0           time_distributed_23[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_11 (TimeDistri (None, 2, 2, 2, 128) 512         time_distributed_10[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_25 (TimeDistri (None, 2, 64)        0           time_distributed_24[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_26 (TimeDistri (None, 2, 64)        0           time_distributed_24[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_12 (TimeDistri (None, 2, 2, 2, 128) 0           time_distributed_11[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_27 (TimeDistri (None, 2, 64)        0           time_distributed_25[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_28 (TimeDistri (None, 2, 64)        0           time_distributed_26[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_13 (TimeDistri (None, 2, 512)       0           time_distributed_12[0][0]        \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 2, 640)       0           time_distributed_27[0][0]        \n","                                                                 time_distributed_28[0][0]        \n","                                                                 time_distributed_13[0][0]        \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   (None, 128)          393728      concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 128)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 128)          0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 128)          16512       dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_8 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 1)            129         dropout_8[0][0]                  \n","==================================================================================================\n","Total params: 1,206,849\n","Trainable params: 1,206,081\n","Non-trainable params: 768\n","__________________________________________________________________________________________________\n","None\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","Found preprocessed data\n","tcmalloc: large alloc 2447884288 bytes == 0x1f406000 @  0x7ff287fe41e7 0x7ff285a8a5e1 0x7ff285aeec78 0x7ff285aeef37 0x7ff285b86f28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7ff287be1b97 0x5b250a\n","tcmalloc: large alloc 2447884288 bytes == 0xb1282000 @  0x7ff287fe41e7 0x7ff285a8a5e1 0x7ff285aeec78 0x7ff285aeef37 0x7ff285b86f28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7ff287be1b97 0x5b250a\n","Loading frame 20398\n","done loading 20399 frames\n","Done prepping data\n","tcmalloc: large alloc 1631920128 bytes == 0x143152000 @  0x7ff287fe41e7 0x7ff285a8a5e1 0x7ff285aeec78 0x7ff285aeed93 0x7ff285b79ed6 0x7ff285b7a338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7ff287be1b97 0x5b250a\n","tcmalloc: large alloc 1631920128 bytes == 0x1f406000 @  0x7ff287fe41e7 0x7ff285a8a5e1 0x7ff285aeec78 0x7ff285aeed93 0x7ff285b79ed6 0x7ff285b7a338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7ff287be1b97 0x5b250a\n","20399\n","20398 Training data size per Aug\n","15998 Train indices size\n","4400 Val indices size\n"," This is the range of train:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119, 6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129, 6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139, 6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169, 6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209, 6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229, 6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239, 6240, 6241, 6242, 6243, 6244, 6245, 6246, 6247, 6248, 6249, 6250, 6251, 6252, 6253, 6254, 6255, 6256, 6257, 6258, 6259, 6260, 6261, 6262, 6263, 6264, 6265, 6266, 6267, 6268, 6269, 6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289, 6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299, 6300, 6301, 6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309, 6310, 6311, 6312, 6313, 6314, 6315, 6316, 6317, 6318, 6319, 6320, 6321, 6322, 6323, 6324, 6325, 6326, 6327, 6328, 6329, 6330, 6331, 6332, 6333, 6334, 6335, 6336, 6337, 6338, 6339, 6340, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349, 6350, 6351, 6352, 6353, 6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363, 6364, 6365, 6366, 6367, 6368, 6369, 6370, 6371, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6379, 6380, 6381, 6382, 6383, 6384, 6385, 6386, 6387, 6388, 6389, 6390, 6391, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399, 6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419, 6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429, 6430, 6431, 6432, 6433, 6434, 6435, 6436, 6437, 6438, 6439, 6440, 6441, 6442, 6443, 6444, 6445, 6446, 6447, 6448, 6449, 6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459, 6460, 6461, 6462, 6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473, 6474, 6475, 6476, 6477, 6478, 6479, 6480, 6481, 6482, 6483, 6484, 6485, 6486, 6487, 6488, 6489, 6490, 6491, 6492, 6493, 6494, 6495, 6496, 6497, 6498, 6499, 6500, 6501, 6502, 6503, 6504, 6505, 6506, 6507, 6508, 6509, 6510, 6511, 6512, 6513, 6514, 6515, 6516, 6517, 6518, 6519, 6520, 6521, 6522, 6523, 6524, 6525, 6526, 6527, 6528, 6529, 6530, 6531, 6532, 6533, 6534, 6535, 6536, 6537, 6538, 6539, 6540, 6541, 6542, 6543, 6544, 6545, 6546, 6547, 6548, 6549, 6550, 6551, 6552, 6553, 6554, 6555, 6556, 6557, 6558, 6559, 6560, 6561, 6562, 6563, 6564, 6565, 6566, 6567, 6568, 6569, 6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577, 6578, 6579, 6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588, 6589, 6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599, 6600, 6601, 6602, 6603, 6604, 6605, 6606, 6607, 6608, 6609, 6610, 6611, 6612, 6613, 6614, 6615, 6616, 6617, 6618, 6619, 6620, 6621, 6622, 6623, 6624, 6625, 6626, 6627, 6628, 6629, 6630, 6631, 6632, 6633, 6634, 6635, 6636, 6637, 6638, 6639, 6640, 6641, 6642, 6643, 6644, 6645, 6646, 6647, 6648, 6649, 6650, 6651, 6652, 6653, 6654, 6655, 6656, 6657, 6658, 6659, 6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6669, 6670, 6671, 6672, 6673, 6674, 6675, 6676, 6677, 6678, 6679, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689, 6690, 6691, 6692, 6693, 6694, 6695, 6696, 6697, 6698, 6699, 6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6708, 6709, 6710, 6711, 6712, 6713, 6714, 6715, 6716, 6717, 6718, 6719, 6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729, 6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739, 6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6748, 6749, 6750, 6751, 6752, 6753, 6754, 6755, 6756, 6757, 6758, 6759, 6760, 6761, 6762, 6763, 6764, 6765, 6766, 6767, 6768, 6769, 6770, 6771, 6772, 6773, 6774, 6775, 6776, 6777, 6778, 6779, 6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6788, 6789, 6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 6804, 6805, 6806, 6807, 6808, 6809, 6810, 6811, 6812, 6813, 6814, 6815, 6816, 6817, 6818, 6819, 6820, 6821, 6822, 6823, 6824, 6825, 6826, 6827, 6828, 6829, 6830, 6831, 6832, 6833, 6834, 6835, 6836, 6837, 6838, 6839, 6840, 6841, 6842, 6843, 6844, 6845, 6846, 6847, 6848, 6849, 6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6859, 6860, 6861, 6862, 6863, 6864, 6865, 6866, 6867, 6868, 6869, 6870, 6871, 6872, 6873, 6874, 6875, 6876, 6877, 6878, 6879, 6880, 6881, 6882, 6883, 6884, 6885, 6886, 6887, 6888, 6889, 6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898, 6899, 6900, 6901, 6902, 6903, 6904, 6905, 6906, 6907, 6908, 6909, 6910, 6911, 6912, 6913, 6914, 6915, 6916, 6917, 6918, 6919, 6920, 6921, 6922, 6923, 6924, 6925, 6926, 6927, 6928, 6929, 6930, 6931, 6932, 6933, 6934, 6935, 6936, 6937, 6938, 6939, 6940, 6941, 6942, 6943, 6944, 6945, 6946, 6947, 6948, 6949, 6950, 6951, 6952, 6953, 6954, 6955, 6956, 6957, 6958, 6959, 6960, 6961, 6962, 6963, 6964, 6965, 6966, 6967, 6968, 6969, 6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979, 6980, 6981, 6982, 6983, 6984, 6985, 6986, 6987, 6988, 6989, 6990, 6991, 6992, 6993, 6994, 6995, 6996, 6997, 6998, 6999, 7000, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009, 7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019, 7020, 7021, 7022, 7023, 7024, 7025, 7026, 7027, 7028, 7029, 7030, 7031, 7032, 7033, 7034, 7035, 7036, 7037, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 7045, 7046, 7047, 7048, 7049, 7050, 7051, 7052, 7053, 7054, 7055, 7056, 7057, 7058, 7059, 7060, 7061, 7062, 7063, 7064, 7065, 7066, 7067, 7068, 7069, 7070, 7071, 7072, 7073, 7074, 7075, 7076, 7077, 7078, 7079, 7080, 7081, 7082, 7083, 7084, 7085, 7086, 7087, 7088, 7089, 7090, 7091, 7092, 7093, 7094, 7095, 7096, 7097, 7098, 7099, 7100, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7108, 7109, 7110, 7111, 7112, 7113, 7114, 7115, 7116, 7117, 7118, 7119, 7120, 7121, 7122, 7123, 7124, 7125, 7126, 7127, 7128, 7129, 7130, 7131, 7132, 7133, 7134, 7135, 7136, 7137, 7138, 7139, 7140, 7141, 7142, 7143, 7144, 7145, 7146, 7147, 7148, 7149, 7150, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7161, 7162, 7163, 7164, 7165, 7166, 7167, 7168, 7169, 7170, 7171, 7172, 7173, 7174, 7175, 7176, 7177, 7178, 7179, 7180, 7181, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189, 7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199, 7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209, 7210, 7211, 7212, 7213, 7214, 7215, 7216, 7217, 7218, 7219, 7220, 7221, 7222, 7223, 7224, 7225, 7226, 7227, 7228, 7229, 7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7239, 7240, 7241, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249, 7250, 7251, 7252, 7253, 7254, 7255, 7256, 7257, 7258, 7259, 7260, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7268, 7269, 7270, 7271, 7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281, 7282, 7283, 7284, 7285, 7286, 7287, 7288, 7289, 7290, 7291, 7292, 7293, 7294, 7295, 7296, 7297, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309, 7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318, 7319, 7320, 7321, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7329, 7330, 7331, 7332, 7333, 7334, 7335, 7336, 7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7349, 7350, 7351, 7352, 7353, 7354, 7355, 7356, 7357, 7358, 7359, 7360, 7361, 7362, 7363, 7364, 7365, 7366, 7367, 7368, 7369, 7370, 7371, 7372, 7373, 7374, 7375, 7376, 7377, 7378, 7379, 7380, 7381, 7382, 7383, 7384, 7385, 7386, 7387, 7388, 7389, 7390, 7391, 7392, 7393, 7394, 7395, 7396, 7397, 7398, 7399, 7400, 7401, 7402, 7403, 7404, 7405, 7406, 7407, 7408, 7409, 7410, 7411, 7412, 7413, 7414, 7415, 7416, 7417, 7418, 7419, 7420, 7421, 7422, 7423, 7424, 7425, 7426, 7427, 7428, 7429, 7430, 7431, 7432, 7433, 7434, 7435, 7436, 7437, 7438, 7439, 7440, 7441, 7442, 7443, 7444, 7445, 7446, 7447, 7448, 7449, 7450, 7451, 7452, 7453, 7454, 7455, 7456, 7457, 7458, 7459, 7460, 7461, 7462, 7463, 7464, 7465, 7466, 7467, 7468, 7469, 7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7479, 7480, 7481, 7482, 7483, 7484, 7485, 7486, 7487, 7488, 7489, 7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499, 7500, 7501, 7502, 7503, 7504, 7505, 7506, 7507, 7508, 7509, 7510, 7511, 7512, 7513, 7514, 7515, 7516, 7517, 7518, 7519, 7520, 7521, 7522, 7523, 7524, 7525, 7526, 7527, 7528, 7529, 7530, 7531, 7532, 7533, 7534, 7535, 7536, 7537, 7538, 7539, 7540, 7541, 7542, 7543, 7544, 7545, 7546, 7547, 7548, 7549, 7550, 7551, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559, 7560, 7561, 7562, 7563, 7564, 7565, 7566, 7567, 7568, 7569, 7570, 7571, 7572, 7573, 7574, 7575, 7576, 7577, 7578, 7579, 7580, 7581, 7582, 7583, 7584, 7585, 7586, 7587, 7588, 7589, 7590, 7591, 7592, 7593, 7594, 7595, 7596, 7597, 7598, 7599, 7600, 7601, 7602, 7603, 7604, 7605, 7606, 7607, 7608, 7609, 7610, 7611, 7612, 7613, 7614, 7615, 7616, 7617, 7618, 7619, 7620, 7621, 7622, 7623, 7624, 7625, 7626, 7627, 7628, 7629, 7630, 7631, 7632, 7633, 7634, 7635, 7636, 7637, 7638, 7639, 7640, 7641, 7642, 7643, 7644, 7645, 7646, 7647, 7648, 7649, 7650, 7651, 7652, 7653, 7654, 7655, 7656, 7657, 7658, 7659, 7660, 7661, 7662, 7663, 7664, 7665, 7666, 7667, 7668, 7669, 7670, 7671, 7672, 7673, 7674, 7675, 7676, 7677, 7678, 7679, 7680, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689, 7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699, 12100, 12101, 12102, 12103, 12104, 12105, 12106, 12107, 12108, 12109, 12110, 12111, 12112, 12113, 12114, 12115, 12116, 12117, 12118, 12119, 12120, 12121, 12122, 12123, 12124, 12125, 12126, 12127, 12128, 12129, 12130, 12131, 12132, 12133, 12134, 12135, 12136, 12137, 12138, 12139, 12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149, 12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159, 12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169, 12170, 12171, 12172, 12173, 12174, 12175, 12176, 12177, 12178, 12179, 12180, 12181, 12182, 12183, 12184, 12185, 12186, 12187, 12188, 12189, 12190, 12191, 12192, 12193, 12194, 12195, 12196, 12197, 12198, 12199, 12200, 12201, 12202, 12203, 12204, 12205, 12206, 12207, 12208, 12209, 12210, 12211, 12212, 12213, 12214, 12215, 12216, 12217, 12218, 12219, 12220, 12221, 12222, 12223, 12224, 12225, 12226, 12227, 12228, 12229, 12230, 12231, 12232, 12233, 12234, 12235, 12236, 12237, 12238, 12239, 12240, 12241, 12242, 12243, 12244, 12245, 12246, 12247, 12248, 12249, 12250, 12251, 12252, 12253, 12254, 12255, 12256, 12257, 12258, 12259, 12260, 12261, 12262, 12263, 12264, 12265, 12266, 12267, 12268, 12269, 12270, 12271, 12272, 12273, 12274, 12275, 12276, 12277, 12278, 12279, 12280, 12281, 12282, 12283, 12284, 12285, 12286, 12287, 12288, 12289, 12290, 12291, 12292, 12293, 12294, 12295, 12296, 12297, 12298, 12299, 12300, 12301, 12302, 12303, 12304, 12305, 12306, 12307, 12308, 12309, 12310, 12311, 12312, 12313, 12314, 12315, 12316, 12317, 12318, 12319, 12320, 12321, 12322, 12323, 12324, 12325, 12326, 12327, 12328, 12329, 12330, 12331, 12332, 12333, 12334, 12335, 12336, 12337, 12338, 12339, 12340, 12341, 12342, 12343, 12344, 12345, 12346, 12347, 12348, 12349, 12350, 12351, 12352, 12353, 12354, 12355, 12356, 12357, 12358, 12359, 12360, 12361, 12362, 12363, 12364, 12365, 12366, 12367, 12368, 12369, 12370, 12371, 12372, 12373, 12374, 12375, 12376, 12377, 12378, 12379, 12380, 12381, 12382, 12383, 12384, 12385, 12386, 12387, 12388, 12389, 12390, 12391, 12392, 12393, 12394, 12395, 12396, 12397, 12398, 12399, 12400, 12401, 12402, 12403, 12404, 12405, 12406, 12407, 12408, 12409, 12410, 12411, 12412, 12413, 12414, 12415, 12416, 12417, 12418, 12419, 12420, 12421, 12422, 12423, 12424, 12425, 12426, 12427, 12428, 12429, 12430, 12431, 12432, 12433, 12434, 12435, 12436, 12437, 12438, 12439, 12440, 12441, 12442, 12443, 12444, 12445, 12446, 12447, 12448, 12449, 12450, 12451, 12452, 12453, 12454, 12455, 12456, 12457, 12458, 12459, 12460, 12461, 12462, 12463, 12464, 12465, 12466, 12467, 12468, 12469, 12470, 12471, 12472, 12473, 12474, 12475, 12476, 12477, 12478, 12479, 12480, 12481, 12482, 12483, 12484, 12485, 12486, 12487, 12488, 12489, 12490, 12491, 12492, 12493, 12494, 12495, 12496, 12497, 12498, 12499, 12500, 12501, 12502, 12503, 12504, 12505, 12506, 12507, 12508, 12509, 12510, 12511, 12512, 12513, 12514, 12515, 12516, 12517, 12518, 12519, 12520, 12521, 12522, 12523, 12524, 12525, 12526, 12527, 12528, 12529, 12530, 12531, 12532, 12533, 12534, 12535, 12536, 12537, 12538, 12539, 12540, 12541, 12542, 12543, 12544, 12545, 12546, 12547, 12548, 12549, 12550, 12551, 12552, 12553, 12554, 12555, 12556, 12557, 12558, 12559, 12560, 12561, 12562, 12563, 12564, 12565, 12566, 12567, 12568, 12569, 12570, 12571, 12572, 12573, 12574, 12575, 12576, 12577, 12578, 12579, 12580, 12581, 12582, 12583, 12584, 12585, 12586, 12587, 12588, 12589, 12590, 12591, 12592, 12593, 12594, 12595, 12596, 12597, 12598, 12599, 12600, 12601, 12602, 12603, 12604, 12605, 12606, 12607, 12608, 12609, 12610, 12611, 12612, 12613, 12614, 12615, 12616, 12617, 12618, 12619, 12620, 12621, 12622, 12623, 12624, 12625, 12626, 12627, 12628, 12629, 12630, 12631, 12632, 12633, 12634, 12635, 12636, 12637, 12638, 12639, 12640, 12641, 12642, 12643, 12644, 12645, 12646, 12647, 12648, 12649, 12650, 12651, 12652, 12653, 12654, 12655, 12656, 12657, 12658, 12659, 12660, 12661, 12662, 12663, 12664, 12665, 12666, 12667, 12668, 12669, 12670, 12671, 12672, 12673, 12674, 12675, 12676, 12677, 12678, 12679, 12680, 12681, 12682, 12683, 12684, 12685, 12686, 12687, 12688, 12689, 12690, 12691, 12692, 12693, 12694, 12695, 12696, 12697, 12698, 12699, 12700, 12701, 12702, 12703, 12704, 12705, 12706, 12707, 12708, 12709, 12710, 12711, 12712, 12713, 12714, 12715, 12716, 12717, 12718, 12719, 12720, 12721, 12722, 12723, 12724, 12725, 12726, 12727, 12728, 12729, 12730, 12731, 12732, 12733, 12734, 12735, 12736, 12737, 12738, 12739, 12740, 12741, 12742, 12743, 12744, 12745, 12746, 12747, 12748, 12749, 12750, 12751, 12752, 12753, 12754, 12755, 12756, 12757, 12758, 12759, 12760, 12761, 12762, 12763, 12764, 12765, 12766, 12767, 12768, 12769, 12770, 12771, 12772, 12773, 12774, 12775, 12776, 12777, 12778, 12779, 12780, 12781, 12782, 12783, 12784, 12785, 12786, 12787, 12788, 12789, 12790, 12791, 12792, 12793, 12794, 12795, 12796, 12797, 12798, 12799, 12800, 12801, 12802, 12803, 12804, 12805, 12806, 12807, 12808, 12809, 12810, 12811, 12812, 12813, 12814, 12815, 12816, 12817, 12818, 12819, 12820, 12821, 12822, 12823, 12824, 12825, 12826, 12827, 12828, 12829, 12830, 12831, 12832, 12833, 12834, 12835, 12836, 12837, 12838, 12839, 12840, 12841, 12842, 12843, 12844, 12845, 12846, 12847, 12848, 12849, 12850, 12851, 12852, 12853, 12854, 12855, 12856, 12857, 12858, 12859, 12860, 12861, 12862, 12863, 12864, 12865, 12866, 12867, 12868, 12869, 12870, 12871, 12872, 12873, 12874, 12875, 12876, 12877, 12878, 12879, 12880, 12881, 12882, 12883, 12884, 12885, 12886, 12887, 12888, 12889, 12890, 12891, 12892, 12893, 12894, 12895, 12896, 12897, 12898, 12899, 12900, 12901, 12902, 12903, 12904, 12905, 12906, 12907, 12908, 12909, 12910, 12911, 12912, 12913, 12914, 12915, 12916, 12917, 12918, 12919, 12920, 12921, 12922, 12923, 12924, 12925, 12926, 12927, 12928, 12929, 12930, 12931, 12932, 12933, 12934, 12935, 12936, 12937, 12938, 12939, 12940, 12941, 12942, 12943, 12944, 12945, 12946, 12947, 12948, 12949, 12950, 12951, 12952, 12953, 12954, 12955, 12956, 12957, 12958, 12959, 12960, 12961, 12962, 12963, 12964, 12965, 12966, 12967, 12968, 12969, 12970, 12971, 12972, 12973, 12974, 12975, 12976, 12977, 12978, 12979, 12980, 12981, 12982, 12983, 12984, 12985, 12986, 12987, 12988, 12989, 12990, 12991, 12992, 12993, 12994, 12995, 12996, 12997, 12998, 12999, 13000, 13001, 13002, 13003, 13004, 13005, 13006, 13007, 13008, 13009, 13010, 13011, 13012, 13013, 13014, 13015, 13016, 13017, 13018, 13019, 13020, 13021, 13022, 13023, 13024, 13025, 13026, 13027, 13028, 13029, 13030, 13031, 13032, 13033, 13034, 13035, 13036, 13037, 13038, 13039, 13040, 13041, 13042, 13043, 13044, 13045, 13046, 13047, 13048, 13049, 13050, 13051, 13052, 13053, 13054, 13055, 13056, 13057, 13058, 13059, 13060, 13061, 13062, 13063, 13064, 13065, 13066, 13067, 13068, 13069, 13070, 13071, 13072, 13073, 13074, 13075, 13076, 13077, 13078, 13079, 13080, 13081, 13082, 13083, 13084, 13085, 13086, 13087, 13088, 13089, 13090, 13091, 13092, 13093, 13094, 13095, 13096, 13097, 13098, 13099, 13100, 13101, 13102, 13103, 13104, 13105, 13106, 13107, 13108, 13109, 13110, 13111, 13112, 13113, 13114, 13115, 13116, 13117, 13118, 13119, 13120, 13121, 13122, 13123, 13124, 13125, 13126, 13127, 13128, 13129, 13130, 13131, 13132, 13133, 13134, 13135, 13136, 13137, 13138, 13139, 13140, 13141, 13142, 13143, 13144, 13145, 13146, 13147, 13148, 13149, 13150, 13151, 13152, 13153, 13154, 13155, 13156, 13157, 13158, 13159, 13160, 13161, 13162, 13163, 13164, 13165, 13166, 13167, 13168, 13169, 13170, 13171, 13172, 13173, 13174, 13175, 13176, 13177, 13178, 13179, 13180, 13181, 13182, 13183, 13184, 13185, 13186, 13187, 13188, 13189, 13190, 13191, 13192, 13193, 13194, 13195, 13196, 13197, 13198, 13199, 13200, 13201, 13202, 13203, 13204, 13205, 13206, 13207, 13208, 13209, 13210, 13211, 13212, 13213, 13214, 13215, 13216, 13217, 13218, 13219, 13220, 13221, 13222, 13223, 13224, 13225, 13226, 13227, 13228, 13229, 13230, 13231, 13232, 13233, 13234, 13235, 13236, 13237, 13238, 13239, 13240, 13241, 13242, 13243, 13244, 13245, 13246, 13247, 13248, 13249, 13250, 13251, 13252, 13253, 13254, 13255, 13256, 13257, 13258, 13259, 13260, 13261, 13262, 13263, 13264, 13265, 13266, 13267, 13268, 13269, 13270, 13271, 13272, 13273, 13274, 13275, 13276, 13277, 13278, 13279, 13280, 13281, 13282, 13283, 13284, 13285, 13286, 13287, 13288, 13289, 13290, 13291, 13292, 13293, 13294, 13295, 13296, 13297, 13298, 13299, 13300, 13301, 13302, 13303, 13304, 13305, 13306, 13307, 13308, 13309, 13310, 13311, 13312, 13313, 13314, 13315, 13316, 13317, 13318, 13319, 13320, 13321, 13322, 13323, 13324, 13325, 13326, 13327, 13328, 13329, 13330, 13331, 13332, 13333, 13334, 13335, 13336, 13337, 13338, 13339, 13340, 13341, 13342, 13343, 13344, 13345, 13346, 13347, 13348, 13349, 13350, 13351, 13352, 13353, 13354, 13355, 13356, 13357, 13358, 13359, 13360, 13361, 13362, 13363, 13364, 13365, 13366, 13367, 13368, 13369, 13370, 13371, 13372, 13373, 13374, 13375, 13376, 13377, 13378, 13379, 13380, 13381, 13382, 13383, 13384, 13385, 13386, 13387, 13388, 13389, 13390, 13391, 13392, 13393, 13394, 13395, 13396, 13397, 13398, 13399, 13400, 13401, 13402, 13403, 13404, 13405, 13406, 13407, 13408, 13409, 13410, 13411, 13412, 13413, 13414, 13415, 13416, 13417, 13418, 13419, 13420, 13421, 13422, 13423, 13424, 13425, 13426, 13427, 13428, 13429, 13430, 13431, 13432, 13433, 13434, 13435, 13436, 13437, 13438, 13439, 13440, 13441, 13442, 13443, 13444, 13445, 13446, 13447, 13448, 13449, 13450, 13451, 13452, 13453, 13454, 13455, 13456, 13457, 13458, 13459, 13460, 13461, 13462, 13463, 13464, 13465, 13466, 13467, 13468, 13469, 13470, 13471, 13472, 13473, 13474, 13475, 13476, 13477, 13478, 13479, 13480, 13481, 13482, 13483, 13484, 13485, 13486, 13487, 13488, 13489, 13490, 13491, 13492, 13493, 13494, 13495, 13496, 13497, 13498, 13499, 13500, 13501, 13502, 13503, 13504, 13505, 13506, 13507, 13508, 13509, 13510, 13511, 13512, 13513, 13514, 13515, 13516, 13517, 13518, 13519, 13520, 13521, 13522, 13523, 13524, 13525, 13526, 13527, 13528, 13529, 13530, 13531, 13532, 13533, 13534, 13535, 13536, 13537, 13538, 13539, 13540, 13541, 13542, 13543, 13544, 13545, 13546, 13547, 13548, 13549, 13550, 13551, 13552, 13553, 13554, 13555, 13556, 13557, 13558, 13559, 13560, 13561, 13562, 13563, 13564, 13565, 13566, 13567, 13568, 13569, 13570, 13571, 13572, 13573, 13574, 13575, 13576, 13577, 13578, 13579, 13580, 13581, 13582, 13583, 13584, 13585, 13586, 13587, 13588, 13589, 13590, 13591, 13592, 13593, 13594, 13595, 13596, 13597, 13598, 13599, 13600, 13601, 13602, 13603, 13604, 13605, 13606, 13607, 13608, 13609, 13610, 13611, 13612, 13613, 13614, 13615, 13616, 13617, 13618, 13619, 13620, 13621, 13622, 13623, 13624, 13625, 13626, 13627, 13628, 13629, 13630, 13631, 13632, 13633, 13634, 13635, 13636, 13637, 13638, 13639, 13640, 13641, 13642, 13643, 13644, 13645, 13646, 13647, 13648, 13649, 13650, 13651, 13652, 13653, 13654, 13655, 13656, 13657, 13658, 13659, 13660, 13661, 13662, 13663, 13664, 13665, 13666, 13667, 13668, 13669, 13670, 13671, 13672, 13673, 13674, 13675, 13676, 13677, 13678, 13679, 13680, 13681, 13682, 13683, 13684, 13685, 13686, 13687, 13688, 13689, 13690, 13691, 13692, 13693, 13694, 13695, 13696, 13697, 13698, 13699, 13700, 13701, 13702, 13703, 13704, 13705, 13706, 13707, 13708, 13709, 13710, 13711, 13712, 13713, 13714, 13715, 13716, 13717, 13718, 13719, 13720, 13721, 13722, 13723, 13724, 13725, 13726, 13727, 13728, 13729, 13730, 13731, 13732, 13733, 13734, 13735, 13736, 13737, 13738, 13739, 13740, 13741, 13742, 13743, 13744, 13745, 13746, 13747, 13748, 13749, 13750, 13751, 13752, 13753, 13754, 13755, 13756, 13757, 13758, 13759, 13760, 13761, 13762, 13763, 13764, 13765, 13766, 13767, 13768, 13769, 13770, 13771, 13772, 13773, 13774, 13775, 13776, 13777, 13778, 13779, 13780, 13781, 13782, 13783, 13784, 13785, 13786, 13787, 13788, 13789, 13790, 13791, 13792, 13793, 13794, 13795, 13796, 13797, 13798, 13799, 13800, 13801, 13802, 13803, 13804, 13805, 13806, 13807, 13808, 13809, 13810, 13811, 13812, 13813, 13814, 13815, 13816, 13817, 13818, 13819, 13820, 13821, 13822, 13823, 13824, 13825, 13826, 13827, 13828, 13829, 13830, 13831, 13832, 13833, 13834, 13835, 13836, 13837, 13838, 13839, 13840, 13841, 13842, 13843, 13844, 13845, 13846, 13847, 13848, 13849, 13850, 13851, 13852, 13853, 13854, 13855, 13856, 13857, 13858, 13859, 13860, 13861, 13862, 13863, 13864, 13865, 13866, 13867, 13868, 13869, 13870, 13871, 13872, 13873, 13874, 13875, 13876, 13877, 13878, 13879, 13880, 13881, 13882, 13883, 13884, 13885, 13886, 13887, 13888, 13889, 13890, 13891, 13892, 13893, 13894, 13895, 13896, 13897, 13898, 13899, 13900, 13901, 13902, 13903, 13904, 13905, 13906, 13907, 13908, 13909, 13910, 13911, 13912, 13913, 13914, 13915, 13916, 13917, 13918, 13919, 13920, 13921, 13922, 13923, 13924, 13925, 13926, 13927, 13928, 13929, 13930, 13931, 13932, 13933, 13934, 13935, 13936, 13937, 13938, 13939, 13940, 13941, 13942, 13943, 13944, 13945, 13946, 13947, 13948, 13949, 13950, 13951, 13952, 13953, 13954, 13955, 13956, 13957, 13958, 13959, 13960, 13961, 13962, 13963, 13964, 13965, 13966, 13967, 13968, 13969, 13970, 13971, 13972, 13973, 13974, 13975, 13976, 13977, 13978, 13979, 13980, 13981, 13982, 13983, 13984, 13985, 13986, 13987, 13988, 13989, 13990, 13991, 13992, 13993, 13994, 13995, 13996, 13997, 13998, 13999, 14000, 14001, 14002, 14003, 14004, 14005, 14006, 14007, 14008, 14009, 14010, 14011, 14012, 14013, 14014, 14015, 14016, 14017, 14018, 14019, 14020, 14021, 14022, 14023, 14024, 14025, 14026, 14027, 14028, 14029, 14030, 14031, 14032, 14033, 14034, 14035, 14036, 14037, 14038, 14039, 14040, 14041, 14042, 14043, 14044, 14045, 14046, 14047, 14048, 14049, 14050, 14051, 14052, 14053, 14054, 14055, 14056, 14057, 14058, 14059, 14060, 14061, 14062, 14063, 14064, 14065, 14066, 14067, 14068, 14069, 14070, 14071, 14072, 14073, 14074, 14075, 14076, 14077, 14078, 14079, 14080, 14081, 14082, 14083, 14084, 14085, 14086, 14087, 14088, 14089, 14090, 14091, 14092, 14093, 14094, 14095, 14096, 14097, 14098, 14099, 14100, 14101, 14102, 14103, 14104, 14105, 14106, 14107, 14108, 14109, 14110, 14111, 14112, 14113, 14114, 14115, 14116, 14117, 14118, 14119, 14120, 14121, 14122, 14123, 14124, 14125, 14126, 14127, 14128, 14129, 14130, 14131, 14132, 14133, 14134, 14135, 14136, 14137, 14138, 14139, 14140, 14141, 14142, 14143, 14144, 14145, 14146, 14147, 14148, 14149, 14150, 14151, 14152, 14153, 14154, 14155, 14156, 14157, 14158, 14159, 14160, 14161, 14162, 14163, 14164, 14165, 14166, 14167, 14168, 14169, 14170, 14171, 14172, 14173, 14174, 14175, 14176, 14177, 14178, 14179, 14180, 14181, 14182, 14183, 14184, 14185, 14186, 14187, 14188, 14189, 14190, 14191, 14192, 14193, 14194, 14195, 14196, 14197, 14198, 14199, 14200, 14201, 14202, 14203, 14204, 14205, 14206, 14207, 14208, 14209, 14210, 14211, 14212, 14213, 14214, 14215, 14216, 14217, 14218, 14219, 14220, 14221, 14222, 14223, 14224, 14225, 14226, 14227, 14228, 14229, 14230, 14231, 14232, 14233, 14234, 14235, 14236, 14237, 14238, 14239, 14240, 14241, 14242, 14243, 14244, 14245, 14246, 14247, 14248, 14249, 14250, 14251, 14252, 14253, 14254, 14255, 14256, 14257, 14258, 14259, 14260, 14261, 14262, 14263, 14264, 14265, 14266, 14267, 14268, 14269, 14270, 14271, 14272, 14273, 14274, 14275, 14276, 14277, 14278, 14279, 14280, 14281, 14282, 14283, 14284, 14285, 14286, 14287, 14288, 14289, 14290, 14291, 14292, 14293, 14294, 14295, 14296, 14297, 14298, 14299, 14300, 14301, 14302, 14303, 14304, 14305, 14306, 14307, 14308, 14309, 14310, 14311, 14312, 14313, 14314, 14315, 14316, 14317, 14318, 14319, 14320, 14321, 14322, 14323, 14324, 14325, 14326, 14327, 14328, 14329, 14330, 14331, 14332, 14333, 14334, 14335, 14336, 14337, 14338, 14339, 14340, 14341, 14342, 14343, 14344, 14345, 14346, 14347, 14348, 14349, 14350, 14351, 14352, 14353, 14354, 14355, 14356, 14357, 14358, 14359, 14360, 14361, 14362, 14363, 14364, 14365, 14366, 14367, 14368, 14369, 14370, 14371, 14372, 14373, 14374, 14375, 14376, 14377, 14378, 14379, 14380, 14381, 14382, 14383, 14384, 14385, 14386, 14387, 14388, 14389, 14390, 14391, 14392, 14393, 14394, 14395, 14396, 14397, 14398, 14399, 14400, 14401, 14402, 14403, 14404, 14405, 14406, 14407, 14408, 14409, 14410, 14411, 14412, 14413, 14414, 14415, 14416, 14417, 14418, 14419, 14420, 14421, 14422, 14423, 14424, 14425, 14426, 14427, 14428, 14429, 14430, 14431, 14432, 14433, 14434, 14435, 14436, 14437, 14438, 14439, 14440, 14441, 14442, 14443, 14444, 14445, 14446, 14447, 14448, 14449, 14450, 14451, 14452, 14453, 14454, 14455, 14456, 14457, 14458, 14459, 14460, 14461, 14462, 14463, 14464, 14465, 14466, 14467, 14468, 14469, 14470, 14471, 14472, 14473, 14474, 14475, 14476, 14477, 14478, 14479, 14480, 14481, 14482, 14483, 14484, 14485, 14486, 14487, 14488, 14489, 14490, 14491, 14492, 14493, 14494, 14495, 14496, 14497, 14498, 14499, 14500, 14501, 14502, 14503, 14504, 14505, 14506, 14507, 14508, 14509, 14510, 14511, 14512, 14513, 14514, 14515, 14516, 14517, 14518, 14519, 14520, 14521, 14522, 14523, 14524, 14525, 14526, 14527, 14528, 14529, 14530, 14531, 14532, 14533, 14534, 14535, 14536, 14537, 14538, 14539, 14540, 14541, 14542, 14543, 14544, 14545, 14546, 14547, 14548, 14549, 14550, 14551, 14552, 14553, 14554, 14555, 14556, 14557, 14558, 14559, 14560, 14561, 14562, 14563, 14564, 14565, 14566, 14567, 14568, 14569, 14570, 14571, 14572, 14573, 14574, 14575, 14576, 14577, 14578, 14579, 14580, 14581, 14582, 14583, 14584, 14585, 14586, 14587, 14588, 14589, 14590, 14591, 14592, 14593, 14594, 14595, 14596, 14597, 14598, 14599, 14600, 14601, 14602, 14603, 14604, 14605, 14606, 14607, 14608, 14609, 14610, 14611, 14612, 14613, 14614, 14615, 14616, 14617, 14618, 14619, 14620, 14621, 14622, 14623, 14624, 14625, 14626, 14627, 14628, 14629, 14630, 14631, 14632, 14633, 14634, 14635, 14636, 14637, 14638, 14639, 14640, 14641, 14642, 14643, 14644, 14645, 14646, 14647, 14648, 14649, 14650, 14651, 14652, 14653, 14654, 14655, 14656, 14657, 14658, 14659, 14660, 14661, 14662, 14663, 14664, 14665, 14666, 14667, 14668, 14669, 14670, 14671, 14672, 14673, 14674, 14675, 14676, 14677, 14678, 14679, 14680, 14681, 14682, 14683, 14684, 14685, 14686, 14687, 14688, 14689, 14690, 14691, 14692, 14693, 14694, 14695, 14696, 14697, 14698, 14699, 14700, 14701, 14702, 14703, 14704, 14705, 14706, 14707, 14708, 14709, 14710, 14711, 14712, 14713, 14714, 14715, 14716, 14717, 14718, 14719, 14720, 14721, 14722, 14723, 14724, 14725, 14726, 14727, 14728, 14729, 14730, 14731, 14732, 14733, 14734, 14735, 14736, 14737, 14738, 14739, 14740, 14741, 14742, 14743, 14744, 14745, 14746, 14747, 14748, 14749, 14750, 14751, 14752, 14753, 14754, 14755, 14756, 14757, 14758, 14759, 14760, 14761, 14762, 14763, 14764, 14765, 14766, 14767, 14768, 14769, 14770, 14771, 14772, 14773, 14774, 14775, 14776, 14777, 14778, 14779, 14780, 14781, 14782, 14783, 14784, 14785, 14786, 14787, 14788, 14789, 14790, 14791, 14792, 14793, 14794, 14795, 14796, 14797, 14798, 14799, 14800, 14801, 14802, 14803, 14804, 14805, 14806, 14807, 14808, 14809, 14810, 14811, 14812, 14813, 14814, 14815, 14816, 14817, 14818, 14819, 14820, 14821, 14822, 14823, 14824, 14825, 14826, 14827, 14828, 14829, 14830, 14831, 14832, 14833, 14834, 14835, 14836, 14837, 14838, 14839, 14840, 14841, 14842, 14843, 14844, 14845, 14846, 14847, 14848, 14849, 14850, 14851, 14852, 14853, 14854, 14855, 14856, 14857, 14858, 14859, 14860, 14861, 14862, 14863, 14864, 14865, 14866, 14867, 14868, 14869, 14870, 14871, 14872, 14873, 14874, 14875, 14876, 14877, 14878, 14879, 14880, 14881, 14882, 14883, 14884, 14885, 14886, 14887, 14888, 14889, 14890, 14891, 14892, 14893, 14894, 14895, 14896, 14897, 14898, 14899, 14900, 14901, 14902, 14903, 14904, 14905, 14906, 14907, 14908, 14909, 14910, 14911, 14912, 14913, 14914, 14915, 14916, 14917, 14918, 14919, 14920, 14921, 14922, 14923, 14924, 14925, 14926, 14927, 14928, 14929, 14930, 14931, 14932, 14933, 14934, 14935, 14936, 14937, 14938, 14939, 14940, 14941, 14942, 14943, 14944, 14945, 14946, 14947, 14948, 14949, 14950, 14951, 14952, 14953, 14954, 14955, 14956, 14957, 14958, 14959, 14960, 14961, 14962, 14963, 14964, 14965, 14966, 14967, 14968, 14969, 14970, 14971, 14972, 14973, 14974, 14975, 14976, 14977, 14978, 14979, 14980, 14981, 14982, 14983, 14984, 14985, 14986, 14987, 14988, 14989, 14990, 14991, 14992, 14993, 14994, 14995, 14996, 14997, 14998, 14999, 15000, 15001, 15002, 15003, 15004, 15005, 15006, 15007, 15008, 15009, 15010, 15011, 15012, 15013, 15014, 15015, 15016, 15017, 15018, 15019, 15020, 15021, 15022, 15023, 15024, 15025, 15026, 15027, 15028, 15029, 15030, 15031, 15032, 15033, 15034, 15035, 15036, 15037, 15038, 15039, 15040, 15041, 15042, 15043, 15044, 15045, 15046, 15047, 15048, 15049, 15050, 15051, 15052, 15053, 15054, 15055, 15056, 15057, 15058, 15059, 15060, 15061, 15062, 15063, 15064, 15065, 15066, 15067, 15068, 15069, 15070, 15071, 15072, 15073, 15074, 15075, 15076, 15077, 15078, 15079, 15080, 15081, 15082, 15083, 15084, 15085, 15086, 15087, 15088, 15089, 15090, 15091, 15092, 15093, 15094, 15095, 15096, 15097, 15098, 15099, 15100, 15101, 15102, 15103, 15104, 15105, 15106, 15107, 15108, 15109, 15110, 15111, 15112, 15113, 15114, 15115, 15116, 15117, 15118, 15119, 15120, 15121, 15122, 15123, 15124, 15125, 15126, 15127, 15128, 15129, 15130, 15131, 15132, 15133, 15134, 15135, 15136, 15137, 15138, 15139, 15140, 15141, 15142, 15143, 15144, 15145, 15146, 15147, 15148, 15149, 15150, 15151, 15152, 15153, 15154, 15155, 15156, 15157, 15158, 15159, 15160, 15161, 15162, 15163, 15164, 15165, 15166, 15167, 15168, 15169, 15170, 15171, 15172, 15173, 15174, 15175, 15176, 15177, 15178, 15179, 15180, 15181, 15182, 15183, 15184, 15185, 15186, 15187, 15188, 15189, 15190, 15191, 15192, 15193, 15194, 15195, 15196, 15197, 15198, 15199, 15200, 15201, 15202, 15203, 15204, 15205, 15206, 15207, 15208, 15209, 15210, 15211, 15212, 15213, 15214, 15215, 15216, 15217, 15218, 15219, 15220, 15221, 15222, 15223, 15224, 15225, 15226, 15227, 15228, 15229, 15230, 15231, 15232, 15233, 15234, 15235, 15236, 15237, 15238, 15239, 15240, 15241, 15242, 15243, 15244, 15245, 15246, 15247, 15248, 15249, 15250, 15251, 15252, 15253, 15254, 15255, 15256, 15257, 15258, 15259, 15260, 15261, 15262, 15263, 15264, 15265, 15266, 15267, 15268, 15269, 15270, 15271, 15272, 15273, 15274, 15275, 15276, 15277, 15278, 15279, 15280, 15281, 15282, 15283, 15284, 15285, 15286, 15287, 15288, 15289, 15290, 15291, 15292, 15293, 15294, 15295, 15296, 15297, 15298, 15299, 15300, 15301, 15302, 15303, 15304, 15305, 15306, 15307, 15308, 15309, 15310, 15311, 15312, 15313, 15314, 15315, 15316, 15317, 15318, 15319, 15320, 15321, 15322, 15323, 15324, 15325, 15326, 15327, 15328, 15329, 15330, 15331, 15332, 15333, 15334, 15335, 15336, 15337, 15338, 15339, 15340, 15341, 15342, 15343, 15344, 15345, 15346, 15347, 15348, 15349, 15350, 15351, 15352, 15353, 15354, 15355, 15356, 15357, 15358, 15359, 15360, 15361, 15362, 15363, 15364, 15365, 15366, 15367, 15368, 15369, 15370, 15371, 15372, 15373, 15374, 15375, 15376, 15377, 15378, 15379, 15380, 15381, 15382, 15383, 15384, 15385, 15386, 15387, 15388, 15389, 15390, 15391, 15392, 15393, 15394, 15395, 15396, 15397, 15398, 15399, 15400, 15401, 15402, 15403, 15404, 15405, 15406, 15407, 15408, 15409, 15410, 15411, 15412, 15413, 15414, 15415, 15416, 15417, 15418, 15419, 15420, 15421, 15422, 15423, 15424, 15425, 15426, 15427, 15428, 15429, 15430, 15431, 15432, 15433, 15434, 15435, 15436, 15437, 15438, 15439, 15440, 15441, 15442, 15443, 15444, 15445, 15446, 15447, 15448, 15449, 15450, 15451, 15452, 15453, 15454, 15455, 15456, 15457, 15458, 15459, 15460, 15461, 15462, 15463, 15464, 15465, 15466, 15467, 15468, 15469, 15470, 15471, 15472, 15473, 15474, 15475, 15476, 15477, 15478, 15479, 15480, 15481, 15482, 15483, 15484, 15485, 15486, 15487, 15488, 15489, 15490, 15491, 15492, 15493, 15494, 15495, 15496, 15497, 15498, 15499, 15500, 15501, 15502, 15503, 15504, 15505, 15506, 15507, 15508, 15509, 15510, 15511, 15512, 15513, 15514, 15515, 15516, 15517, 15518, 15519, 15520, 15521, 15522, 15523, 15524, 15525, 15526, 15527, 15528, 15529, 15530, 15531, 15532, 15533, 15534, 15535, 15536, 15537, 15538, 15539, 15540, 15541, 15542, 15543, 15544, 15545, 15546, 15547, 15548, 15549, 15550, 15551, 15552, 15553, 15554, 15555, 15556, 15557, 15558, 15559, 15560, 15561, 15562, 15563, 15564, 15565, 15566, 15567, 15568, 15569, 15570, 15571, 15572, 15573, 15574, 15575, 15576, 15577, 15578, 15579, 15580, 15581, 15582, 15583, 15584, 15585, 15586, 15587, 15588, 15589, 15590, 15591, 15592, 15593, 15594, 15595, 15596, 15597, 15598, 15599, 15600, 15601, 15602, 15603, 15604, 15605, 15606, 15607, 15608, 15609, 15610, 15611, 15612, 15613, 15614, 15615, 15616, 15617, 15618, 15619, 15620, 15621, 15622, 15623, 15624, 15625, 15626, 15627, 15628, 15629, 15630, 15631, 15632, 15633, 15634, 15635, 15636, 15637, 15638, 15639, 15640, 15641, 15642, 15643, 15644, 15645, 15646, 15647, 15648, 15649, 15650, 15651, 15652, 15653, 15654, 15655, 15656, 15657, 15658, 15659, 15660, 15661, 15662, 15663, 15664, 15665, 15666, 15667, 15668, 15669, 15670, 15671, 15672, 15673, 15674, 15675, 15676, 15677, 15678, 15679, 15680, 15681, 15682, 15683, 15684, 15685, 15686, 15687, 15688, 15689, 15690, 15691, 15692, 15693, 15694, 15695, 15696, 15697, 15698, 15699, 15700, 15701, 15702, 15703, 15704, 15705, 15706, 15707, 15708, 15709, 15710, 15711, 15712, 15713, 15714, 15715, 15716, 15717, 15718, 15719, 15720, 15721, 15722, 15723, 15724, 15725, 15726, 15727, 15728, 15729, 15730, 15731, 15732, 15733, 15734, 15735, 15736, 15737, 15738, 15739, 15740, 15741, 15742, 15743, 15744, 15745, 15746, 15747, 15748, 15749, 15750, 15751, 15752, 15753, 15754, 15755, 15756, 15757, 15758, 15759, 15760, 15761, 15762, 15763, 15764, 15765, 15766, 15767, 15768, 15769, 15770, 15771, 15772, 15773, 15774, 15775, 15776, 15777, 15778, 15779, 15780, 15781, 15782, 15783, 15784, 15785, 15786, 15787, 15788, 15789, 15790, 15791, 15792, 15793, 15794, 15795, 15796, 15797, 15798, 15799, 15800, 15801, 15802, 15803, 15804, 15805, 15806, 15807, 15808, 15809, 15810, 15811, 15812, 15813, 15814, 15815, 15816, 15817, 15818, 15819, 15820, 15821, 15822, 15823, 15824, 15825, 15826, 15827, 15828, 15829, 15830, 15831, 15832, 15833, 15834, 15835, 15836, 15837, 15838, 15839, 15840, 15841, 15842, 15843, 15844, 15845, 15846, 15847, 15848, 15849, 15850, 15851, 15852, 15853, 15854, 15855, 15856, 15857, 15858, 15859, 15860, 15861, 15862, 15863, 15864, 15865, 15866, 15867, 15868, 15869, 15870, 15871, 15872, 15873, 15874, 15875, 15876, 15877, 15878, 15879, 15880, 15881, 15882, 15883, 15884, 15885, 15886, 15887, 15888, 15889, 15890, 15891, 15892, 15893, 15894, 15895, 15896, 15897, 15898, 15899, 15900, 15901, 15902, 15903, 15904, 15905, 15906, 15907, 15908, 15909, 15910, 15911, 15912, 15913, 15914, 15915, 15916, 15917, 15918, 15919, 15920, 15921, 15922, 15923, 15924, 15925, 15926, 15927, 15928, 15929, 15930, 15931, 15932, 15933, 15934, 15935, 15936, 15937, 15938, 15939, 15940, 15941, 15942, 15943, 15944, 15945, 15946, 15947, 15948, 15949, 15950, 15951, 15952, 15953, 15954, 15955, 15956, 15957, 15958, 15959, 15960, 15961, 15962, 15963, 15964, 15965, 15966, 15967, 15968, 15969, 15970, 15971, 15972, 15973, 15974, 15975, 15976, 15977, 15978, 15979, 15980, 15981, 15982, 15983, 15984, 15985, 15986, 15987, 15988, 15989, 15990, 15991, 15992, 15993, 15994, 15995, 15996, 15997, 15998, 15999, 16000, 16001, 16002, 16003, 16004, 16005, 16006, 16007, 16008, 16009, 16010, 16011, 16012, 16013, 16014, 16015, 16016, 16017, 16018, 16019, 16020, 16021, 16022, 16023, 16024, 16025, 16026, 16027, 16028, 16029, 16030, 16031, 16032, 16033, 16034, 16035, 16036, 16037, 16038, 16039, 16040, 16041, 16042, 16043, 16044, 16045, 16046, 16047, 16048, 16049, 16050, 16051, 16052, 16053, 16054, 16055, 16056, 16057, 16058, 16059, 16060, 16061, 16062, 16063, 16064, 16065, 16066, 16067, 16068, 16069, 16070, 16071, 16072, 16073, 16074, 16075, 16076, 16077, 16078, 16079, 16080, 16081, 16082, 16083, 16084, 16085, 16086, 16087, 16088, 16089, 16090, 16091, 16092, 16093, 16094, 16095, 16096, 16097, 16098, 16099, 16100, 16101, 16102, 16103, 16104, 16105, 16106, 16107, 16108, 16109, 16110, 16111, 16112, 16113, 16114, 16115, 16116, 16117, 16118, 16119, 16120, 16121, 16122, 16123, 16124, 16125, 16126, 16127, 16128, 16129, 16130, 16131, 16132, 16133, 16134, 16135, 16136, 16137, 16138, 16139, 16140, 16141, 16142, 16143, 16144, 16145, 16146, 16147, 16148, 16149, 16150, 16151, 16152, 16153, 16154, 16155, 16156, 16157, 16158, 16159, 16160, 16161, 16162, 16163, 16164, 16165, 16166, 16167, 16168, 16169, 16170, 16171, 16172, 16173, 16174, 16175, 16176, 16177, 16178, 16179, 16180, 16181, 16182, 16183, 16184, 16185, 16186, 16187, 16188, 16189, 16190, 16191, 16192, 16193, 16194, 16195, 16196, 16197, 16198, 16199, 16200, 16201, 16202, 16203, 16204, 16205, 16206, 16207, 16208, 16209, 16210, 16211, 16212, 16213, 16214, 16215, 16216, 16217, 16218, 16219, 16220, 16221, 16222, 16223, 16224, 16225, 16226, 16227, 16228, 16229, 16230, 16231, 16232, 16233, 16234, 16235, 16236, 16237, 16238, 16239, 16240, 16241, 16242, 16243, 16244, 16245, 16246, 16247, 16248, 16249, 16250, 16251, 16252, 16253, 16254, 16255, 16256, 16257, 16258, 16259, 16260, 16261, 16262, 16263, 16264, 16265, 16266, 16267, 16268, 16269, 16270, 16271, 16272, 16273, 16274, 16275, 16276, 16277, 16278, 16279, 16280, 16281, 16282, 16283, 16284, 16285, 16286, 16287, 16288, 16289, 16290, 16291, 16292, 16293, 16294, 16295, 16296, 16297, 16298, 16299, 16300, 16301, 16302, 16303, 16304, 16305, 16306, 16307, 16308, 16309, 16310, 16311, 16312, 16313, 16314, 16315, 16316, 16317, 16318, 16319, 16320, 16321, 16322, 16323, 16324, 16325, 16326, 16327, 16328, 16329, 16330, 16331, 16332, 16333, 16334, 16335, 16336, 16337, 16338, 16339, 16340, 16341, 16342, 16343, 16344, 16345, 16346, 16347, 16348, 16349, 16350, 16351, 16352, 16353, 16354, 16355, 16356, 16357, 16358, 16359, 16360, 16361, 16362, 16363, 16364, 16365, 16366, 16367, 16368, 16369, 16370, 16371, 16372, 16373, 16374, 16375, 16376, 16377, 16378, 16379, 16380, 16381, 16382, 16383, 16384, 16385, 16386, 16387, 16388, 16389, 16390, 16391, 16392, 16393, 16394, 16395, 16396, 16397, 16398, 16399, 16400, 16401, 16402, 16403, 16404, 16405, 16406, 16407, 16408, 16409, 16410, 16411, 16412, 16413, 16414, 16415, 16416, 16417, 16418, 16419, 16420, 16421, 16422, 16423, 16424, 16425, 16426, 16427, 16428, 16429, 16430, 16431, 16432, 16433, 16434, 16435, 16436, 16437, 16438, 16439, 16440, 16441, 16442, 16443, 16444, 16445, 16446, 16447, 16448, 16449, 16450, 16451, 16452, 16453, 16454, 16455, 16456, 16457, 16458, 16459, 16460, 16461, 16462, 16463, 16464, 16465, 16466, 16467, 16468, 16469, 16470, 16471, 16472, 16473, 16474, 16475, 16476, 16477, 16478, 16479, 16480, 16481, 16482, 16483, 16484, 16485, 16486, 16487, 16488, 16489, 16490, 16491, 16492, 16493, 16494, 16495, 16496, 16497, 16498, 16499, 16500, 16501, 16502, 16503, 16504, 16505, 16506, 16507, 16508, 16509, 16510, 16511, 16512, 16513, 16514, 16515, 16516, 16517, 16518, 16519, 16520, 16521, 16522, 16523, 16524, 16525, 16526, 16527, 16528, 16529, 16530, 16531, 16532, 16533, 16534, 16535, 16536, 16537, 16538, 16539, 16540, 16541, 16542, 16543, 16544, 16545, 16546, 16547, 16548, 16549, 16550, 16551, 16552, 16553, 16554, 16555, 16556, 16557, 16558, 16559, 16560, 16561, 16562, 16563, 16564, 16565, 16566, 16567, 16568, 16569, 16570, 16571, 16572, 16573, 16574, 16575, 16576, 16577, 16578, 16579, 16580, 16581, 16582, 16583, 16584, 16585, 16586, 16587, 16588, 16589, 16590, 16591, 16592, 16593, 16594, 16595, 16596, 16597, 16598, 16599, 16600, 16601, 16602, 16603, 16604, 16605, 16606, 16607, 16608, 16609, 16610, 16611, 16612, 16613, 16614, 16615, 16616, 16617, 16618, 16619, 16620, 16621, 16622, 16623, 16624, 16625, 16626, 16627, 16628, 16629, 16630, 16631, 16632, 16633, 16634, 16635, 16636, 16637, 16638, 16639, 16640, 16641, 16642, 16643, 16644, 16645, 16646, 16647, 16648, 16649, 16650, 16651, 16652, 16653, 16654, 16655, 16656, 16657, 16658, 16659, 16660, 16661, 16662, 16663, 16664, 16665, 16666, 16667, 16668, 16669, 16670, 16671, 16672, 16673, 16674, 16675, 16676, 16677, 16678, 16679, 16680, 16681, 16682, 16683, 16684, 16685, 16686, 16687, 16688, 16689, 16690, 16691, 16692, 16693, 16694, 16695, 16696, 16697, 16698, 16699, 16700, 16701, 16702, 16703, 16704, 16705, 16706, 16707, 16708, 16709, 16710, 16711, 16712, 16713, 16714, 16715, 16716, 16717, 16718, 16719, 16720, 16721, 16722, 16723, 16724, 16725, 16726, 16727, 16728, 16729, 16730, 16731, 16732, 16733, 16734, 16735, 16736, 16737, 16738, 16739, 16740, 16741, 16742, 16743, 16744, 16745, 16746, 16747, 16748, 16749, 16750, 16751, 16752, 16753, 16754, 16755, 16756, 16757, 16758, 16759, 16760, 16761, 16762, 16763, 16764, 16765, 16766, 16767, 16768, 16769, 16770, 16771, 16772, 16773, 16774, 16775, 16776, 16777, 16778, 16779, 16780, 16781, 16782, 16783, 16784, 16785, 16786, 16787, 16788, 16789, 16790, 16791, 16792, 16793, 16794, 16795, 16796, 16797, 16798, 16799, 16800, 16801, 16802, 16803, 16804, 16805, 16806, 16807, 16808, 16809, 16810, 16811, 16812, 16813, 16814, 16815, 16816, 16817, 16818, 16819, 16820, 16821, 16822, 16823, 16824, 16825, 16826, 16827, 16828, 16829, 16830, 16831, 16832, 16833, 16834, 16835, 16836, 16837, 16838, 16839, 16840, 16841, 16842, 16843, 16844, 16845, 16846, 16847, 16848, 16849, 16850, 16851, 16852, 16853, 16854, 16855, 16856, 16857, 16858, 16859, 16860, 16861, 16862, 16863, 16864, 16865, 16866, 16867, 16868, 16869, 16870, 16871, 16872, 16873, 16874, 16875, 16876, 16877, 16878, 16879, 16880, 16881, 16882, 16883, 16884, 16885, 16886, 16887, 16888, 16889, 16890, 16891, 16892, 16893, 16894, 16895, 16896, 16897, 16898, 16899, 16900, 16901, 16902, 16903, 16904, 16905, 16906, 16907, 16908, 16909, 16910, 16911, 16912, 16913, 16914, 16915, 16916, 16917, 16918, 16919, 16920, 16921, 16922, 16923, 16924, 16925, 16926, 16927, 16928, 16929, 16930, 16931, 16932, 16933, 16934, 16935, 16936, 16937, 16938, 16939, 16940, 16941, 16942, 16943, 16944, 16945, 16946, 16947, 16948, 16949, 16950, 16951, 16952, 16953, 16954, 16955, 16956, 16957, 16958, 16959, 16960, 16961, 16962, 16963, 16964, 16965, 16966, 16967, 16968, 16969, 16970, 16971, 16972, 16973, 16974, 16975, 16976, 16977, 16978, 16979, 16980, 16981, 16982, 16983, 16984, 16985, 16986, 16987, 16988, 16989, 16990, 16991, 16992, 16993, 16994, 16995, 16996, 16997, 16998, 16999, 17000, 17001, 17002, 17003, 17004, 17005, 17006, 17007, 17008, 17009, 17010, 17011, 17012, 17013, 17014, 17015, 17016, 17017, 17018, 17019, 17020, 17021, 17022, 17023, 17024, 17025, 17026, 17027, 17028, 17029, 17030, 17031, 17032, 17033, 17034, 17035, 17036, 17037, 17038, 17039, 17040, 17041, 17042, 17043, 17044, 17045, 17046, 17047, 17048, 17049, 17050, 17051, 17052, 17053, 17054, 17055, 17056, 17057, 17058, 17059, 17060, 17061, 17062, 17063, 17064, 17065, 17066, 17067, 17068, 17069, 17070, 17071, 17072, 17073, 17074, 17075, 17076, 17077, 17078, 17079, 17080, 17081, 17082, 17083, 17084, 17085, 17086, 17087, 17088, 17089, 17090, 17091, 17092, 17093, 17094, 17095, 17096, 17097, 17098, 17099, 17100, 17101, 17102, 17103, 17104, 17105, 17106, 17107, 17108, 17109, 17110, 17111, 17112, 17113, 17114, 17115, 17116, 17117, 17118, 17119, 17120, 17121, 17122, 17123, 17124, 17125, 17126, 17127, 17128, 17129, 17130, 17131, 17132, 17133, 17134, 17135, 17136, 17137, 17138, 17139, 17140, 17141, 17142, 17143, 17144, 17145, 17146, 17147, 17148, 17149, 17150, 17151, 17152, 17153, 17154, 17155, 17156, 17157, 17158, 17159, 17160, 17161, 17162, 17163, 17164, 17165, 17166, 17167, 17168, 17169, 17170, 17171, 17172, 17173, 17174, 17175, 17176, 17177, 17178, 17179, 17180, 17181, 17182, 17183, 17184, 17185, 17186, 17187, 17188, 17189, 17190, 17191, 17192, 17193, 17194, 17195, 17196, 17197, 17198, 17199, 17200, 17201, 17202, 17203, 17204, 17205, 17206, 17207, 17208, 17209, 17210, 17211, 17212, 17213, 17214, 17215, 17216, 17217, 17218, 17219, 17220, 17221, 17222, 17223, 17224, 17225, 17226, 17227, 17228, 17229, 17230, 17231, 17232, 17233, 17234, 17235, 17236, 17237, 17238, 17239, 17240, 17241, 17242, 17243, 17244, 17245, 17246, 17247, 17248, 17249, 17250, 17251, 17252, 17253, 17254, 17255, 17256, 17257, 17258, 17259, 17260, 17261, 17262, 17263, 17264, 17265, 17266, 17267, 17268, 17269, 17270, 17271, 17272, 17273, 17274, 17275, 17276, 17277, 17278, 17279, 17280, 17281, 17282, 17283, 17284, 17285, 17286, 17287, 17288, 17289, 17290, 17291, 17292, 17293, 17294, 17295, 17296, 17297, 17298, 17299, 17300, 17301, 17302, 17303, 17304, 17305, 17306, 17307, 17308, 17309, 17310, 17311, 17312, 17313, 17314, 17315, 17316, 17317, 17318, 17319, 17320, 17321, 17322, 17323, 17324, 17325, 17326, 17327, 17328, 17329, 17330, 17331, 17332, 17333, 17334, 17335, 17336, 17337, 17338, 17339, 17340, 17341, 17342, 17343, 17344, 17345, 17346, 17347, 17348, 17349, 17350, 17351, 17352, 17353, 17354, 17355, 17356, 17357, 17358, 17359, 17360, 17361, 17362, 17363, 17364, 17365, 17366, 17367, 17368, 17369, 17370, 17371, 17372, 17373, 17374, 17375, 17376, 17377, 17378, 17379, 17380, 17381, 17382, 17383, 17384, 17385, 17386, 17387, 17388, 17389, 17390, 17391, 17392, 17393, 17394, 17395, 17396, 17397, 17398, 17399, 17400, 17401, 17402, 17403, 17404, 17405, 17406, 17407, 17408, 17409, 17410, 17411, 17412, 17413, 17414, 17415, 17416, 17417, 17418, 17419, 17420, 17421, 17422, 17423, 17424, 17425, 17426, 17427, 17428, 17429, 17430, 17431, 17432, 17433, 17434, 17435, 17436, 17437, 17438, 17439, 17440, 17441, 17442, 17443, 17444, 17445, 17446, 17447, 17448, 17449, 17450, 17451, 17452, 17453, 17454, 17455, 17456, 17457, 17458, 17459, 17460, 17461, 17462, 17463, 17464, 17465, 17466, 17467, 17468, 17469, 17470, 17471, 17472, 17473, 17474, 17475, 17476, 17477, 17478, 17479, 17480, 17481, 17482, 17483, 17484, 17485, 17486, 17487, 17488, 17489, 17490, 17491, 17492, 17493, 17494, 17495, 17496, 17497, 17498, 17499, 17500, 17501, 17502, 17503, 17504, 17505, 17506, 17507, 17508, 17509, 17510, 17511, 17512, 17513, 17514, 17515, 17516, 17517, 17518, 17519, 17520, 17521, 17522, 17523, 17524, 17525, 17526, 17527, 17528, 17529, 17530, 17531, 17532, 17533, 17534, 17535, 17536, 17537, 17538, 17539, 17540, 17541, 17542, 17543, 17544, 17545, 17546, 17547, 17548, 17549, 17550, 17551, 17552, 17553, 17554, 17555, 17556, 17557, 17558, 17559, 17560, 17561, 17562, 17563, 17564, 17565, 17566, 17567, 17568, 17569, 17570, 17571, 17572, 17573, 17574, 17575, 17576, 17577, 17578, 17579, 17580, 17581, 17582, 17583, 17584, 17585, 17586, 17587, 17588, 17589, 17590, 17591, 17592, 17593, 17594, 17595, 17596, 17597, 17598, 17599, 17600, 17601, 17602, 17603, 17604, 17605, 17606, 17607, 17608, 17609, 17610, 17611, 17612, 17613, 17614, 17615, 17616, 17617, 17618, 17619, 17620, 17621, 17622, 17623, 17624, 17625, 17626, 17627, 17628, 17629, 17630, 17631, 17632, 17633, 17634, 17635, 17636, 17637, 17638, 17639, 17640, 17641, 17642, 17643, 17644, 17645, 17646, 17647, 17648, 17649, 17650, 17651, 17652, 17653, 17654, 17655, 17656, 17657, 17658, 17659, 17660, 17661, 17662, 17663, 17664, 17665, 17666, 17667, 17668, 17669, 17670, 17671, 17672, 17673, 17674, 17675, 17676, 17677, 17678, 17679, 17680, 17681, 17682, 17683, 17684, 17685, 17686, 17687, 17688, 17689, 17690, 17691, 17692, 17693, 17694, 17695, 17696, 17697, 17698, 17699, 17700, 17701, 17702, 17703, 17704, 17705, 17706, 17707, 17708, 17709, 17710, 17711, 17712, 17713, 17714, 17715, 17716, 17717, 17718, 17719, 17720, 17721, 17722, 17723, 17724, 17725, 17726, 17727, 17728, 17729, 17730, 17731, 17732, 17733, 17734, 17735, 17736, 17737, 17738, 17739, 17740, 17741, 17742, 17743, 17744, 17745, 17746, 17747, 17748, 17749, 17750, 17751, 17752, 17753, 17754, 17755, 17756, 17757, 17758, 17759, 17760, 17761, 17762, 17763, 17764, 17765, 17766, 17767, 17768, 17769, 17770, 17771, 17772, 17773, 17774, 17775, 17776, 17777, 17778, 17779, 17780, 17781, 17782, 17783, 17784, 17785, 17786, 17787, 17788, 17789, 17790, 17791, 17792, 17793, 17794, 17795, 17796, 17797, 17798, 17799, 17800, 17801, 17802, 17803, 17804, 17805, 17806, 17807, 17808, 17809, 17810, 17811, 17812, 17813, 17814, 17815, 17816, 17817, 17818, 17819, 17820, 17821, 17822, 17823, 17824, 17825, 17826, 17827, 17828, 17829, 17830, 17831, 17832, 17833, 17834, 17835, 17836, 17837, 17838, 17839, 17840, 17841, 17842, 17843, 17844, 17845, 17846, 17847, 17848, 17849, 17850, 17851, 17852, 17853, 17854, 17855, 17856, 17857, 17858, 17859, 17860, 17861, 17862, 17863, 17864, 17865, 17866, 17867, 17868, 17869, 17870, 17871, 17872, 17873, 17874, 17875, 17876, 17877, 17878, 17879, 17880, 17881, 17882, 17883, 17884, 17885, 17886, 17887, 17888, 17889, 17890, 17891, 17892, 17893, 17894, 17895, 17896, 17897, 17898, 17899, 17900, 17901, 17902, 17903, 17904, 17905, 17906, 17907, 17908, 17909, 17910, 17911, 17912, 17913, 17914, 17915, 17916, 17917, 17918, 17919, 17920, 17921, 17922, 17923, 17924, 17925, 17926, 17927, 17928, 17929, 17930, 17931, 17932, 17933, 17934, 17935, 17936, 17937, 17938, 17939, 17940, 17941, 17942, 17943, 17944, 17945, 17946, 17947, 17948, 17949, 17950, 17951, 17952, 17953, 17954, 17955, 17956, 17957, 17958, 17959, 17960, 17961, 17962, 17963, 17964, 17965, 17966, 17967, 17968, 17969, 17970, 17971, 17972, 17973, 17974, 17975, 17976, 17977, 17978, 17979, 17980, 17981, 17982, 17983, 17984, 17985, 17986, 17987, 17988, 17989, 17990, 17991, 17992, 17993, 17994, 17995, 17996, 17997, 17998, 17999, 18000, 18001, 18002, 18003, 18004, 18005, 18006, 18007, 18008, 18009, 18010, 18011, 18012, 18013, 18014, 18015, 18016, 18017, 18018, 18019, 18020, 18021, 18022, 18023, 18024, 18025, 18026, 18027, 18028, 18029, 18030, 18031, 18032, 18033, 18034, 18035, 18036, 18037, 18038, 18039, 18040, 18041, 18042, 18043, 18044, 18045, 18046, 18047, 18048, 18049, 18050, 18051, 18052, 18053, 18054, 18055, 18056, 18057, 18058, 18059, 18060, 18061, 18062, 18063, 18064, 18065, 18066, 18067, 18068, 18069, 18070, 18071, 18072, 18073, 18074, 18075, 18076, 18077, 18078, 18079, 18080, 18081, 18082, 18083, 18084, 18085, 18086, 18087, 18088, 18089, 18090, 18091, 18092, 18093, 18094, 18095, 18096, 18097, 18098, 18099, 18100, 18101, 18102, 18103, 18104, 18105, 18106, 18107, 18108, 18109, 18110, 18111, 18112, 18113, 18114, 18115, 18116, 18117, 18118, 18119, 18120, 18121, 18122, 18123, 18124, 18125, 18126, 18127, 18128, 18129, 18130, 18131, 18132, 18133, 18134, 18135, 18136, 18137, 18138, 18139, 18140, 18141, 18142, 18143, 18144, 18145, 18146, 18147, 18148, 18149, 18150, 18151, 18152, 18153, 18154, 18155, 18156, 18157, 18158, 18159, 18160, 18161, 18162, 18163, 18164, 18165, 18166, 18167, 18168, 18169, 18170, 18171, 18172, 18173, 18174, 18175, 18176, 18177, 18178, 18179, 18180, 18181, 18182, 18183, 18184, 18185, 18186, 18187, 18188, 18189, 18190, 18191, 18192, 18193, 18194, 18195, 18196, 18197, 18198, 18199, 18200, 18201, 18202, 18203, 18204, 18205, 18206, 18207, 18208, 18209, 18210, 18211, 18212, 18213, 18214, 18215, 18216, 18217, 18218, 18219, 18220, 18221, 18222, 18223, 18224, 18225, 18226, 18227, 18228, 18229, 18230, 18231, 18232, 18233, 18234, 18235, 18236, 18237, 18238, 18239, 18240, 18241, 18242, 18243, 18244, 18245, 18246, 18247, 18248, 18249, 18250, 18251, 18252, 18253, 18254, 18255, 18256, 18257, 18258, 18259, 18260, 18261, 18262, 18263, 18264, 18265, 18266, 18267, 18268, 18269, 18270, 18271, 18272, 18273, 18274, 18275, 18276, 18277, 18278, 18279, 18280, 18281, 18282, 18283, 18284, 18285, 18286, 18287, 18288, 18289, 18290, 18291, 18292, 18293, 18294, 18295, 18296, 18297, 18298, 18299, 18300, 18301, 18302, 18303, 18304, 18305, 18306, 18307, 18308, 18309, 18310, 18311, 18312, 18313, 18314, 18315, 18316, 18317, 18318, 18319, 18320, 18321, 18322, 18323, 18324, 18325, 18326, 18327, 18328, 18329, 18330, 18331, 18332, 18333, 18334, 18335, 18336, 18337, 18338, 18339, 18340, 18341, 18342, 18343, 18344, 18345, 18346, 18347, 18348, 18349, 18350, 18351, 18352, 18353, 18354, 18355, 18356, 18357, 18358, 18359, 18360, 18361, 18362, 18363, 18364, 18365, 18366, 18367, 18368, 18369, 18370, 18371, 18372, 18373, 18374, 18375, 18376, 18377, 18378, 18379, 18380, 18381, 18382, 18383, 18384, 18385, 18386, 18387, 18388, 18389, 18390, 18391, 18392, 18393, 18394, 18395, 18396, 18397, 18398, 18399, 18400, 18401, 18402, 18403, 18404, 18405, 18406, 18407, 18408, 18409, 18410, 18411, 18412, 18413, 18414, 18415, 18416, 18417, 18418, 18419, 18420, 18421, 18422, 18423, 18424, 18425, 18426, 18427, 18428, 18429, 18430, 18431, 18432, 18433, 18434, 18435, 18436, 18437, 18438, 18439, 18440, 18441, 18442, 18443, 18444, 18445, 18446, 18447, 18448, 18449, 18450, 18451, 18452, 18453, 18454, 18455, 18456, 18457, 18458, 18459, 18460, 18461, 18462, 18463, 18464, 18465, 18466, 18467, 18468, 18469, 18470, 18471, 18472, 18473, 18474, 18475, 18476, 18477, 18478, 18479, 18480, 18481, 18482, 18483, 18484, 18485, 18486, 18487, 18488, 18489, 18490, 18491, 18492, 18493, 18494, 18495, 18496, 18497, 18498, 18499, 18500, 18501, 18502, 18503, 18504, 18505, 18506, 18507, 18508, 18509, 18510, 18511, 18512, 18513, 18514, 18515, 18516, 18517, 18518, 18519, 18520, 18521, 18522, 18523, 18524, 18525, 18526, 18527, 18528, 18529, 18530, 18531, 18532, 18533, 18534, 18535, 18536, 18537, 18538, 18539, 18540, 18541, 18542, 18543, 18544, 18545, 18546, 18547, 18548, 18549, 18550, 18551, 18552, 18553, 18554, 18555, 18556, 18557, 18558, 18559, 18560, 18561, 18562, 18563, 18564, 18565, 18566, 18567, 18568, 18569, 18570, 18571, 18572, 18573, 18574, 18575, 18576, 18577, 18578, 18579, 18580, 18581, 18582, 18583, 18584, 18585, 18586, 18587, 18588, 18589, 18590, 18591, 18592, 18593, 18594, 18595, 18596, 18597, 18598, 18599, 18600, 18601, 18602, 18603, 18604, 18605, 18606, 18607, 18608, 18609, 18610, 18611, 18612, 18613, 18614, 18615, 18616, 18617, 18618, 18619, 18620, 18621, 18622, 18623, 18624, 18625, 18626, 18627, 18628, 18629, 18630, 18631, 18632, 18633, 18634, 18635, 18636, 18637, 18638, 18639, 18640, 18641, 18642, 18643, 18644, 18645, 18646, 18647, 18648, 18649, 18650, 18651, 18652, 18653, 18654, 18655, 18656, 18657, 18658, 18659, 18660, 18661, 18662, 18663, 18664, 18665, 18666, 18667, 18668, 18669, 18670, 18671, 18672, 18673, 18674, 18675, 18676, 18677, 18678, 18679, 18680, 18681, 18682, 18683, 18684, 18685, 18686, 18687, 18688, 18689, 18690, 18691, 18692, 18693, 18694, 18695, 18696, 18697, 18698, 18699, 18700, 18701, 18702, 18703, 18704, 18705, 18706, 18707, 18708, 18709, 18710, 18711, 18712, 18713, 18714, 18715, 18716, 18717, 18718, 18719, 18720, 18721, 18722, 18723, 18724, 18725, 18726, 18727, 18728, 18729, 18730, 18731, 18732, 18733, 18734, 18735, 18736, 18737, 18738, 18739, 18740, 18741, 18742, 18743, 18744, 18745, 18746, 18747, 18748, 18749, 18750, 18751, 18752, 18753, 18754, 18755, 18756, 18757, 18758, 18759, 18760, 18761, 18762, 18763, 18764, 18765, 18766, 18767, 18768, 18769, 18770, 18771, 18772, 18773, 18774, 18775, 18776, 18777, 18778, 18779, 18780, 18781, 18782, 18783, 18784, 18785, 18786, 18787, 18788, 18789, 18790, 18791, 18792, 18793, 18794, 18795, 18796, 18797, 18798, 18799, 18800, 18801, 18802, 18803, 18804, 18805, 18806, 18807, 18808, 18809, 18810, 18811, 18812, 18813, 18814, 18815, 18816, 18817, 18818, 18819, 18820, 18821, 18822, 18823, 18824, 18825, 18826, 18827, 18828, 18829, 18830, 18831, 18832, 18833, 18834, 18835, 18836, 18837, 18838, 18839, 18840, 18841, 18842, 18843, 18844, 18845, 18846, 18847, 18848, 18849, 18850, 18851, 18852, 18853, 18854, 18855, 18856, 18857, 18858, 18859, 18860, 18861, 18862, 18863, 18864, 18865, 18866, 18867, 18868, 18869, 18870, 18871, 18872, 18873, 18874, 18875, 18876, 18877, 18878, 18879, 18880, 18881, 18882, 18883, 18884, 18885, 18886, 18887, 18888, 18889, 18890, 18891, 18892, 18893, 18894, 18895, 18896, 18897, 18898, 18899, 18900, 18901, 18902, 18903, 18904, 18905, 18906, 18907, 18908, 18909, 18910, 18911, 18912, 18913, 18914, 18915, 18916, 18917, 18918, 18919, 18920, 18921, 18922, 18923, 18924, 18925, 18926, 18927, 18928, 18929, 18930, 18931, 18932, 18933, 18934, 18935, 18936, 18937, 18938, 18939, 18940, 18941, 18942, 18943, 18944, 18945, 18946, 18947, 18948, 18949, 18950, 18951, 18952, 18953, 18954, 18955, 18956, 18957, 18958, 18959, 18960, 18961, 18962, 18963, 18964, 18965, 18966, 18967, 18968, 18969, 18970, 18971, 18972, 18973, 18974, 18975, 18976, 18977, 18978, 18979, 18980, 18981, 18982, 18983, 18984, 18985, 18986, 18987, 18988, 18989, 18990, 18991, 18992, 18993, 18994, 18995, 18996, 18997, 18998, 18999, 19000, 19001, 19002, 19003, 19004, 19005, 19006, 19007, 19008, 19009, 19010, 19011, 19012, 19013, 19014, 19015, 19016, 19017, 19018, 19019, 19020, 19021, 19022, 19023, 19024, 19025, 19026, 19027, 19028, 19029, 19030, 19031, 19032, 19033, 19034, 19035, 19036, 19037, 19038, 19039, 19040, 19041, 19042, 19043, 19044, 19045, 19046, 19047, 19048, 19049, 19050, 19051, 19052, 19053, 19054, 19055, 19056, 19057, 19058, 19059, 19060, 19061, 19062, 19063, 19064, 19065, 19066, 19067, 19068, 19069, 19070, 19071, 19072, 19073, 19074, 19075, 19076, 19077, 19078, 19079, 19080, 19081, 19082, 19083, 19084, 19085, 19086, 19087, 19088, 19089, 19090, 19091, 19092, 19093, 19094, 19095, 19096, 19097, 19098, 19099, 19100, 19101, 19102, 19103, 19104, 19105, 19106, 19107, 19108, 19109, 19110, 19111, 19112, 19113, 19114, 19115, 19116, 19117, 19118, 19119, 19120, 19121, 19122, 19123, 19124, 19125, 19126, 19127, 19128, 19129, 19130, 19131, 19132, 19133, 19134, 19135, 19136, 19137, 19138, 19139, 19140, 19141, 19142, 19143, 19144, 19145, 19146, 19147, 19148, 19149, 19150, 19151, 19152, 19153, 19154, 19155, 19156, 19157, 19158, 19159, 19160, 19161, 19162, 19163, 19164, 19165, 19166, 19167, 19168, 19169, 19170, 19171, 19172, 19173, 19174, 19175, 19176, 19177, 19178, 19179, 19180, 19181, 19182, 19183, 19184, 19185, 19186, 19187, 19188, 19189, 19190, 19191, 19192, 19193, 19194, 19195, 19196, 19197, 19198, 19199, 19200, 19201, 19202, 19203, 19204, 19205, 19206, 19207, 19208, 19209, 19210, 19211, 19212, 19213, 19214, 19215, 19216, 19217, 19218, 19219, 19220, 19221, 19222, 19223, 19224, 19225, 19226, 19227, 19228, 19229, 19230, 19231, 19232, 19233, 19234, 19235, 19236, 19237, 19238, 19239, 19240, 19241, 19242, 19243, 19244, 19245, 19246, 19247, 19248, 19249, 19250, 19251, 19252, 19253, 19254, 19255, 19256, 19257, 19258, 19259, 19260, 19261, 19262, 19263, 19264, 19265, 19266, 19267, 19268, 19269, 19270, 19271, 19272, 19273, 19274, 19275, 19276, 19277, 19278, 19279, 19280, 19281, 19282, 19283, 19284, 19285, 19286, 19287, 19288, 19289, 19290, 19291, 19292, 19293, 19294, 19295, 19296, 19297, 19298, 19299, 19300, 19301, 19302, 19303, 19304, 19305, 19306, 19307, 19308, 19309, 19310, 19311, 19312, 19313, 19314, 19315, 19316, 19317, 19318, 19319, 19320, 19321, 19322, 19323, 19324, 19325, 19326, 19327, 19328, 19329, 19330, 19331, 19332, 19333, 19334, 19335, 19336, 19337, 19338, 19339, 19340, 19341, 19342, 19343, 19344, 19345, 19346, 19347, 19348, 19349, 19350, 19351, 19352, 19353, 19354, 19355, 19356, 19357, 19358, 19359, 19360, 19361, 19362, 19363, 19364, 19365, 19366, 19367, 19368, 19369, 19370, 19371, 19372, 19373, 19374, 19375, 19376, 19377, 19378, 19379, 19380, 19381, 19382, 19383, 19384, 19385, 19386, 19387, 19388, 19389, 19390, 19391, 19392, 19393, 19394, 19395, 19396, 19397, 19398, 19399, 19400, 19401, 19402, 19403, 19404, 19405, 19406, 19407, 19408, 19409, 19410, 19411, 19412, 19413, 19414, 19415, 19416, 19417, 19418, 19419, 19420, 19421, 19422, 19423, 19424, 19425, 19426, 19427, 19428, 19429, 19430, 19431, 19432, 19433, 19434, 19435, 19436, 19437, 19438, 19439, 19440, 19441, 19442, 19443, 19444, 19445, 19446, 19447, 19448, 19449, 19450, 19451, 19452, 19453, 19454, 19455, 19456, 19457, 19458, 19459, 19460, 19461, 19462, 19463, 19464, 19465, 19466, 19467, 19468, 19469, 19470, 19471, 19472, 19473, 19474, 19475, 19476, 19477, 19478, 19479, 19480, 19481, 19482, 19483, 19484, 19485, 19486, 19487, 19488, 19489, 19490, 19491, 19492, 19493, 19494, 19495, 19496, 19497, 19498, 19499, 19500, 19501, 19502, 19503, 19504, 19505, 19506, 19507, 19508, 19509, 19510, 19511, 19512, 19513, 19514, 19515, 19516, 19517, 19518, 19519, 19520, 19521, 19522, 19523, 19524, 19525, 19526, 19527, 19528, 19529, 19530, 19531, 19532, 19533, 19534, 19535, 19536, 19537, 19538, 19539, 19540, 19541, 19542, 19543, 19544, 19545, 19546, 19547, 19548, 19549, 19550, 19551, 19552, 19553, 19554, 19555, 19556, 19557, 19558, 19559, 19560, 19561, 19562, 19563, 19564, 19565, 19566, 19567, 19568, 19569, 19570, 19571, 19572, 19573, 19574, 19575, 19576, 19577, 19578, 19579, 19580, 19581, 19582, 19583, 19584, 19585, 19586, 19587, 19588, 19589, 19590, 19591, 19592, 19593, 19594, 19595, 19596, 19597, 19598, 19599, 19600, 19601, 19602, 19603, 19604, 19605, 19606, 19607, 19608, 19609, 19610, 19611, 19612, 19613, 19614, 19615, 19616, 19617, 19618, 19619, 19620, 19621, 19622, 19623, 19624, 19625, 19626, 19627, 19628, 19629, 19630, 19631, 19632, 19633, 19634, 19635, 19636, 19637, 19638, 19639, 19640, 19641, 19642, 19643, 19644, 19645, 19646, 19647, 19648, 19649, 19650, 19651, 19652, 19653, 19654, 19655, 19656, 19657, 19658, 19659, 19660, 19661, 19662, 19663, 19664, 19665, 19666, 19667, 19668, 19669, 19670, 19671, 19672, 19673, 19674, 19675, 19676, 19677, 19678, 19679, 19680, 19681, 19682, 19683, 19684, 19685, 19686, 19687, 19688, 19689, 19690, 19691, 19692, 19693, 19694, 19695, 19696, 19697, 19698, 19699, 19700, 19701, 19702, 19703, 19704, 19705, 19706, 19707, 19708, 19709, 19710, 19711, 19712, 19713, 19714, 19715, 19716, 19717, 19718, 19719, 19720, 19721, 19722, 19723, 19724, 19725, 19726, 19727, 19728, 19729, 19730, 19731, 19732, 19733, 19734, 19735, 19736, 19737, 19738, 19739, 19740, 19741, 19742, 19743, 19744, 19745, 19746, 19747, 19748, 19749, 19750, 19751, 19752, 19753, 19754, 19755, 19756, 19757, 19758, 19759, 19760, 19761, 19762, 19763, 19764, 19765, 19766, 19767, 19768, 19769, 19770, 19771, 19772, 19773, 19774, 19775, 19776, 19777, 19778, 19779, 19780, 19781, 19782, 19783, 19784, 19785, 19786, 19787, 19788, 19789, 19790, 19791, 19792, 19793, 19794, 19795, 19796, 19797, 19798, 19799, 19800, 19801, 19802, 19803, 19804, 19805, 19806, 19807, 19808, 19809, 19810, 19811, 19812, 19813, 19814, 19815, 19816, 19817, 19818, 19819, 19820, 19821, 19822, 19823, 19824, 19825, 19826, 19827, 19828, 19829, 19830, 19831, 19832, 19833, 19834, 19835, 19836, 19837, 19838, 19839, 19840, 19841, 19842, 19843, 19844, 19845, 19846, 19847, 19848, 19849, 19850, 19851, 19852, 19853, 19854, 19855, 19856, 19857, 19858, 19859, 19860, 19861, 19862, 19863, 19864, 19865, 19866, 19867, 19868, 19869, 19870, 19871, 19872, 19873, 19874, 19875, 19876, 19877, 19878, 19879, 19880, 19881, 19882, 19883, 19884, 19885, 19886, 19887, 19888, 19889, 19890, 19891, 19892, 19893, 19894, 19895, 19896, 19897, 19898, 19899, 19900, 19901, 19902, 19903, 19904, 19905, 19906, 19907, 19908, 19909, 19910, 19911, 19912, 19913, 19914, 19915, 19916, 19917, 19918, 19919, 19920, 19921, 19922, 19923, 19924, 19925, 19926, 19927, 19928, 19929, 19930, 19931, 19932, 19933, 19934, 19935, 19936, 19937, 19938, 19939, 19940, 19941, 19942, 19943, 19944, 19945, 19946, 19947, 19948, 19949, 19950, 19951, 19952, 19953, 19954, 19955, 19956, 19957, 19958, 19959, 19960, 19961, 19962, 19963, 19964, 19965, 19966, 19967, 19968, 19969, 19970, 19971, 19972, 19973, 19974, 19975, 19976, 19977, 19978, 19979, 19980, 19981, 19982, 19983, 19984, 19985, 19986, 19987, 19988, 19989, 19990, 19991, 19992, 19993, 19994, 19995, 19996, 19997, 19998, 19999, 20000, 20001, 20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012, 20013, 20014, 20015, 20016, 20017, 20018, 20019, 20020, 20021, 20022, 20023, 20024, 20025, 20026, 20027, 20028, 20029, 20030, 20031, 20032, 20033, 20034, 20035, 20036, 20037, 20038, 20039, 20040, 20041, 20042, 20043, 20044, 20045, 20046, 20047, 20048, 20049, 20050, 20051, 20052, 20053, 20054, 20055, 20056, 20057, 20058, 20059, 20060, 20061, 20062, 20063, 20064, 20065, 20066, 20067, 20068, 20069, 20070, 20071, 20072, 20073, 20074, 20075, 20076, 20077, 20078, 20079, 20080, 20081, 20082, 20083, 20084, 20085, 20086, 20087, 20088, 20089, 20090, 20091, 20092, 20093, 20094, 20095, 20096, 20097, 20098, 20099, 20100, 20101, 20102, 20103, 20104, 20105, 20106, 20107, 20108, 20109, 20110, 20111, 20112, 20113, 20114, 20115, 20116, 20117, 20118, 20119, 20120, 20121, 20122, 20123, 20124, 20125, 20126, 20127, 20128, 20129, 20130, 20131, 20132, 20133, 20134, 20135, 20136, 20137, 20138, 20139, 20140, 20141, 20142, 20143, 20144, 20145, 20146, 20147, 20148, 20149, 20150, 20151, 20152, 20153, 20154, 20155, 20156, 20157, 20158, 20159, 20160, 20161, 20162, 20163, 20164, 20165, 20166, 20167, 20168, 20169, 20170, 20171, 20172, 20173, 20174, 20175, 20176, 20177, 20178, 20179, 20180, 20181, 20182, 20183, 20184, 20185, 20186, 20187, 20188, 20189, 20190, 20191, 20192, 20193, 20194, 20195, 20196, 20197, 20198, 20199, 20200, 20201, 20202, 20203, 20204, 20205, 20206, 20207, 20208, 20209, 20210, 20211, 20212, 20213, 20214, 20215, 20216, 20217, 20218, 20219, 20220, 20221, 20222, 20223, 20224, 20225, 20226, 20227, 20228, 20229, 20230, 20231, 20232, 20233, 20234, 20235, 20236, 20237, 20238, 20239, 20240, 20241, 20242, 20243, 20244, 20245, 20246, 20247, 20248, 20249, 20250, 20251, 20252, 20253, 20254, 20255, 20256, 20257, 20258, 20259, 20260, 20261, 20262, 20263, 20264, 20265, 20266, 20267, 20268, 20269, 20270, 20271, 20272, 20273, 20274, 20275, 20276, 20277, 20278, 20279, 20280, 20281, 20282, 20283, 20284, 20285, 20286, 20287, 20288, 20289, 20290, 20291, 20292, 20293, 20294, 20295, 20296, 20297, 20298, 20299, 20300, 20301, 20302, 20303, 20304, 20305, 20306, 20307, 20308, 20309, 20310, 20311, 20312, 20313, 20314, 20315, 20316, 20317, 20318, 20319, 20320, 20321, 20322, 20323, 20324, 20325, 20326, 20327, 20328, 20329, 20330, 20331, 20332, 20333, 20334, 20335, 20336, 20337, 20338, 20339, 20340, 20341, 20342, 20343, 20344, 20345, 20346, 20347, 20348, 20349, 20350, 20351, 20352, 20353, 20354, 20355, 20356, 20357, 20358, 20359, 20360, 20361, 20362, 20363, 20364, 20365, 20366, 20367, 20368, 20369, 20370, 20371, 20372, 20373, 20374, 20375, 20376, 20377, 20378, 20379, 20380, 20381, 20382, 20383, 20384, 20385, 20386, 20387, 20388, 20389, 20390, 20391, 20392, 20393, 20394, 20395, 20396, 20397]\n"," This is the range of val:  [ 7700  7701  7702 ... 12097 12098 12099]\n","Starting training\n","shuffling\n","Epoch 1/200\n","2020-05-24 09:32:22.042902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 09:32:22.245832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","1600/1600 [==============================] - 24s 15ms/step - loss: 138.4314 - val_loss: 0.4992\n","\n","Epoch 00001: loss improved from inf to 138.42730, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 42.9154 - val_loss: 84.3592\n","\n","Epoch 00002: loss improved from 138.42730 to 42.91750, saving model to final.h5\n","Epoch 3/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 19.3351 - val_loss: 85.3171\n","\n","Epoch 00003: loss improved from 42.91750 to 19.33445, saving model to final.h5\n","Epoch 4/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 15.2748 - val_loss: 80.4547\n","\n","Epoch 00004: loss improved from 19.33445 to 15.27549, saving model to final.h5\n","Epoch 5/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 13.7350 - val_loss: 73.8023\n","\n","Epoch 00005: loss improved from 15.27549 to 13.73482, saving model to final.h5\n","Epoch 6/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 12.5392 - val_loss: 63.0380\n","\n","Epoch 00006: loss improved from 13.73482 to 12.54020, saving model to final.h5\n","Epoch 7/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 12.3245 - val_loss: 63.6658\n","\n","Epoch 00007: loss improved from 12.54020 to 12.32551, saving model to final.h5\n","Epoch 8/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 11.5666 - val_loss: 54.0655\n","\n","Epoch 00008: loss improved from 12.32551 to 11.56749, saving model to final.h5\n","Epoch 9/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 10.9298 - val_loss: 47.7825\n","\n","Epoch 00009: loss improved from 11.56749 to 10.93054, saving model to final.h5\n","Epoch 10/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 10.7513 - val_loss: 44.7794\n","\n","Epoch 00010: loss improved from 10.93054 to 10.75158, saving model to final.h5\n","Epoch 11/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 10.1050 - val_loss: 42.7975\n","\n","Epoch 00011: loss improved from 10.75158 to 10.10528, saving model to final.h5\n","Epoch 12/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 10.0117 - val_loss: 39.0179\n","\n","Epoch 00012: loss improved from 10.10528 to 10.01191, saving model to final.h5\n","Epoch 13/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 9.7431 - val_loss: 36.8587\n","\n","Epoch 00013: loss improved from 10.01191 to 9.74334, saving model to final.h5\n","Epoch 14/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 9.4687 - val_loss: 34.6095\n","\n","Epoch 00014: loss improved from 9.74334 to 9.46910, saving model to final.h5\n","Epoch 15/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 9.3156 - val_loss: 37.2563\n","\n","Epoch 00015: loss improved from 9.46910 to 9.31462, saving model to final.h5\n","Epoch 16/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 9.0662 - val_loss: 31.3192\n","\n","Epoch 00016: loss improved from 9.31462 to 9.06638, saving model to final.h5\n","Epoch 17/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 8.6368 - val_loss: 28.3717\n","\n","Epoch 00017: loss improved from 9.06638 to 8.63574, saving model to final.h5\n","Epoch 18/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 8.5582 - val_loss: 28.0968\n","\n","Epoch 00018: loss improved from 8.63574 to 8.55862, saving model to final.h5\n","Epoch 19/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 8.4708 - val_loss: 24.7988\n","\n","Epoch 00019: loss improved from 8.55862 to 8.47124, saving model to final.h5\n","Epoch 20/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 8.4573 - val_loss: 22.4870\n","\n","Epoch 00020: loss improved from 8.47124 to 8.45698, saving model to final.h5\n","Epoch 21/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 8.0601 - val_loss: 22.1882\n","\n","Epoch 00021: loss improved from 8.45698 to 8.05947, saving model to final.h5\n","Epoch 22/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 8.1271 - val_loss: 24.2319\n","\n","Epoch 00022: loss did not improve from 8.05947\n","Epoch 23/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 7.8929 - val_loss: 22.5172\n","\n","Epoch 00023: loss improved from 8.05947 to 7.89345, saving model to final.h5\n","Epoch 24/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 7.9121 - val_loss: 24.0201\n","\n","Epoch 00024: loss did not improve from 7.89345\n","Epoch 25/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 7.7104 - val_loss: 20.3918\n","\n","Epoch 00025: loss improved from 7.89345 to 7.70850, saving model to final.h5\n","Epoch 26/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 7.6207 - val_loss: 24.5541\n","\n","Epoch 00026: loss improved from 7.70850 to 7.62108, saving model to final.h5\n","Epoch 27/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 7.5474 - val_loss: 21.6092\n","\n","Epoch 00027: loss improved from 7.62108 to 7.54770, saving model to final.h5\n","Epoch 28/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 7.3592 - val_loss: 19.6914\n","\n","Epoch 00028: loss improved from 7.54770 to 7.35911, saving model to final.h5\n","Epoch 29/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 7.3868 - val_loss: 15.3946\n","\n","Epoch 00029: loss did not improve from 7.35911\n","Epoch 30/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 6.9936 - val_loss: 21.9633\n","\n","Epoch 00030: loss improved from 7.35911 to 6.99405, saving model to final.h5\n","Epoch 31/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 7.2289 - val_loss: 18.9795\n","\n","Epoch 00031: loss did not improve from 6.99405\n","Epoch 32/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 7.0536 - val_loss: 15.4244\n","\n","Epoch 00032: loss did not improve from 6.99405\n","Epoch 33/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 7.0829 - val_loss: 19.5427\n","\n","Epoch 00033: loss did not improve from 6.99405\n","Epoch 34/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 6.9754 - val_loss: 18.0059\n","\n","Epoch 00034: loss improved from 6.99405 to 6.97611, saving model to final.h5\n","Epoch 35/200\n","1600/1600 [==============================] - 22s 13ms/step - loss: 6.9119 - val_loss: 20.4205\n","\n","Epoch 00035: loss improved from 6.97611 to 6.91157, saving model to final.h5\n","Epoch 36/200\n","1600/1600 [==============================] - 22s 13ms/step - loss: 6.9109 - val_loss: 19.5554\n","\n","Epoch 00036: loss improved from 6.91157 to 6.91099, saving model to final.h5\n","Epoch 37/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 6.8224 - val_loss: 15.5488\n","\n","Epoch 00037: loss improved from 6.91099 to 6.82262, saving model to final.h5\n","Epoch 38/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 6.7264 - val_loss: 14.3866\n","\n","Epoch 00038: loss improved from 6.82262 to 6.72646, saving model to final.h5\n","Epoch 39/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 6.5703 - val_loss: 17.0021\n","\n","Epoch 00039: loss improved from 6.72646 to 6.57093, saving model to final.h5\n","Epoch 40/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 6.6278 - val_loss: 19.3283\n","\n","Epoch 00040: loss did not improve from 6.57093\n","Epoch 41/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 6.5253 - val_loss: 16.7719\n","\n","Epoch 00041: loss improved from 6.57093 to 6.52447, saving model to final.h5\n","Epoch 42/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 6.4925 - val_loss: 17.2126\n","\n","Epoch 00042: loss improved from 6.52447 to 6.49319, saving model to final.h5\n","Epoch 43/200\n","1600/1600 [==============================] - 23s 15ms/step - loss: 6.5522 - val_loss: 15.6576\n","\n","Epoch 00043: loss did not improve from 6.49319\n","Epoch 44/200\n","1600/1600 [==============================] - 23s 15ms/step - loss: 6.3999 - val_loss: 13.1089\n","\n","Epoch 00044: loss improved from 6.49319 to 6.40026, saving model to final.h5\n","Epoch 45/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 6.3040 - val_loss: 17.4928\n","\n","Epoch 00045: loss improved from 6.40026 to 6.30442, saving model to final.h5\n","Epoch 46/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 6.2969 - val_loss: 13.6313\n","\n","Epoch 00046: loss improved from 6.30442 to 6.29719, saving model to final.h5\n","Epoch 47/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 6.5239 - val_loss: 16.6281\n","\n","Epoch 00047: loss did not improve from 6.29719\n","Epoch 48/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 6.3658 - val_loss: 13.9898\n","\n","Epoch 00048: loss did not improve from 6.29719\n","Epoch 49/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 6.2000 - val_loss: 11.5379\n","\n","Epoch 00049: loss improved from 6.29719 to 6.19917, saving model to final.h5\n","Epoch 50/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 6.2211 - val_loss: 17.8566\n","\n","Epoch 00050: loss did not improve from 6.19917\n","Epoch 51/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 6.1040 - val_loss: 11.9850\n","\n","Epoch 00051: loss improved from 6.19917 to 6.10366, saving model to final.h5\n","Epoch 52/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 6.2068 - val_loss: 13.6124\n","\n","Epoch 00052: loss did not improve from 6.10366\n","Epoch 53/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 6.0344 - val_loss: 12.5559\n","\n","Epoch 00053: loss improved from 6.10366 to 6.03455, saving model to final.h5\n","Epoch 54/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.9366 - val_loss: 11.8242\n","\n","Epoch 00054: loss improved from 6.03455 to 5.93630, saving model to final.h5\n","Epoch 55/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.9416 - val_loss: 13.2802\n","\n","Epoch 00055: loss did not improve from 5.93630\n","Epoch 56/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.9104 - val_loss: 12.1738\n","\n","Epoch 00056: loss improved from 5.93630 to 5.91047, saving model to final.h5\n","Epoch 57/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.9509 - val_loss: 14.2657\n","\n","Epoch 00057: loss did not improve from 5.91047\n","Epoch 58/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.7850 - val_loss: 10.1241\n","\n","Epoch 00058: loss improved from 5.91047 to 5.78567, saving model to final.h5\n","Epoch 59/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.9739 - val_loss: 11.4697\n","\n","Epoch 00059: loss did not improve from 5.78567\n","Epoch 60/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.9656 - val_loss: 13.2609\n","\n","Epoch 00060: loss did not improve from 5.78567\n","Epoch 61/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.6795 - val_loss: 11.4628\n","\n","Epoch 00061: loss improved from 5.78567 to 5.68001, saving model to final.h5\n","Epoch 62/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.6319 - val_loss: 12.0209\n","\n","Epoch 00062: loss improved from 5.68001 to 5.63241, saving model to final.h5\n","Epoch 63/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.6902 - val_loss: 10.1598\n","\n","Epoch 00063: loss did not improve from 5.63241\n","Epoch 64/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.6124 - val_loss: 12.3362\n","\n","Epoch 00064: loss improved from 5.63241 to 5.61166, saving model to final.h5\n","Epoch 65/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.7745 - val_loss: 11.1145\n","\n","Epoch 00065: loss did not improve from 5.61166\n","Epoch 66/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.5962 - val_loss: 10.9063\n","\n","Epoch 00066: loss improved from 5.61166 to 5.59673, saving model to final.h5\n","Epoch 67/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.5617 - val_loss: 13.6025\n","\n","Epoch 00067: loss improved from 5.59673 to 5.56128, saving model to final.h5\n","Epoch 68/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.6113 - val_loss: 10.3957\n","\n","Epoch 00068: loss did not improve from 5.56128\n","Epoch 69/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.4432 - val_loss: 11.7085\n","\n","Epoch 00069: loss improved from 5.56128 to 5.44310, saving model to final.h5\n","Epoch 70/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.4191 - val_loss: 13.3809\n","\n","Epoch 00070: loss improved from 5.44310 to 5.41916, saving model to final.h5\n","Epoch 71/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.5692 - val_loss: 13.9936\n","\n","Epoch 00071: loss did not improve from 5.41916\n","Epoch 72/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.4260 - val_loss: 11.4966\n","\n","Epoch 00072: loss did not improve from 5.41916\n","Epoch 73/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.4941 - val_loss: 11.6701\n","\n","Epoch 00073: loss did not improve from 5.41916\n","Epoch 74/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.5012 - val_loss: 11.3052\n","\n","Epoch 00074: loss did not improve from 5.41916\n","Epoch 75/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.4790 - val_loss: 9.8682\n","\n","Epoch 00075: loss did not improve from 5.41916\n","Epoch 76/200\n","1600/1600 [==============================] - 23s 15ms/step - loss: 5.5180 - val_loss: 10.8365\n","\n","Epoch 00076: loss did not improve from 5.41916\n","Epoch 77/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.3820 - val_loss: 10.2083\n","\n","Epoch 00077: loss improved from 5.41916 to 5.38229, saving model to final.h5\n","Epoch 78/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.2709 - val_loss: 8.8354\n","\n","Epoch 00078: loss improved from 5.38229 to 5.27090, saving model to final.h5\n","Epoch 79/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.2560 - val_loss: 9.5201\n","\n","Epoch 00079: loss improved from 5.27090 to 5.25638, saving model to final.h5\n","Epoch 80/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.1415 - val_loss: 12.0153\n","\n","Epoch 00080: loss improved from 5.25638 to 5.14143, saving model to final.h5\n","Epoch 81/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.3063 - val_loss: 10.5043\n","\n","Epoch 00081: loss did not improve from 5.14143\n","Epoch 82/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.2326 - val_loss: 10.0398\n","\n","Epoch 00082: loss did not improve from 5.14143\n","Epoch 83/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.3659 - val_loss: 10.5085\n","\n","Epoch 00083: loss did not improve from 5.14143\n","Epoch 84/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.2471 - val_loss: 11.1387\n","\n","Epoch 00084: loss did not improve from 5.14143\n","Epoch 85/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.0428 - val_loss: 10.0132\n","\n","Epoch 00085: loss improved from 5.14143 to 5.04313, saving model to final.h5\n","Epoch 86/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.1974 - val_loss: 9.5043\n","\n","Epoch 00086: loss did not improve from 5.04313\n","Epoch 87/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.2074 - val_loss: 11.0005\n","\n","Epoch 00087: loss did not improve from 5.04313\n","Epoch 88/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.2280 - val_loss: 9.7968\n","\n","Epoch 00088: loss did not improve from 5.04313\n","Epoch 89/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.0829 - val_loss: 9.4098\n","\n","Epoch 00089: loss did not improve from 5.04313\n","Epoch 90/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.2609 - val_loss: 12.1985\n","\n","Epoch 00090: loss did not improve from 5.04313\n","Epoch 91/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.1954 - val_loss: 11.0146\n","\n","Epoch 00091: loss did not improve from 5.04313\n","Epoch 92/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.1020 - val_loss: 8.8983\n","\n","Epoch 00092: loss did not improve from 5.04313\n","Epoch 93/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.1396 - val_loss: 10.2738\n","\n","Epoch 00093: loss did not improve from 5.04313\n","Epoch 94/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.1354 - val_loss: 9.4513\n","\n","Epoch 00094: loss did not improve from 5.04313\n","Epoch 95/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.0803 - val_loss: 9.3978\n","\n","Epoch 00095: loss did not improve from 5.04313\n","Epoch 96/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.0281 - val_loss: 8.8179\n","\n","Epoch 00096: loss improved from 5.04313 to 5.02784, saving model to final.h5\n","Epoch 97/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.0890 - val_loss: 9.4052\n","\n","Epoch 00097: loss did not improve from 5.02784\n","Epoch 98/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.1376 - val_loss: 8.9537\n","\n","Epoch 00098: loss did not improve from 5.02784\n","Epoch 99/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.0194 - val_loss: 9.2384\n","\n","Epoch 00099: loss improved from 5.02784 to 5.01934, saving model to final.h5\n","Epoch 100/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.0916 - val_loss: 10.2828\n","\n","Epoch 00100: loss did not improve from 5.01934\n","Epoch 101/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.9641 - val_loss: 9.4348\n","\n","Epoch 00101: loss improved from 5.01934 to 4.96407, saving model to final.h5\n","Epoch 102/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.0204 - val_loss: 8.8057\n","\n","Epoch 00102: loss did not improve from 4.96407\n","Epoch 103/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 5.0418 - val_loss: 9.7490\n","\n","Epoch 00103: loss did not improve from 4.96407\n","Epoch 104/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.8293 - val_loss: 10.6646\n","\n","Epoch 00104: loss improved from 4.96407 to 4.82918, saving model to final.h5\n","Epoch 105/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 5.0449 - val_loss: 8.9203\n","\n","Epoch 00105: loss did not improve from 4.82918\n","Epoch 106/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.8736 - val_loss: 10.9620\n","\n","Epoch 00106: loss did not improve from 4.82918\n","Epoch 107/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.9180 - val_loss: 8.8969\n","\n","Epoch 00107: loss did not improve from 4.82918\n","Epoch 108/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.8686 - val_loss: 10.6424\n","\n","Epoch 00108: loss did not improve from 4.82918\n","Epoch 109/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.8974 - val_loss: 9.5596\n","\n","Epoch 00109: loss did not improve from 4.82918\n","Epoch 110/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.7824 - val_loss: 8.2724\n","\n","Epoch 00110: loss improved from 4.82918 to 4.78265, saving model to final.h5\n","Epoch 111/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.8403 - val_loss: 9.7870\n","\n","Epoch 00111: loss did not improve from 4.78265\n","Epoch 112/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.8442 - val_loss: 9.1365\n","\n","Epoch 00112: loss did not improve from 4.78265\n","Epoch 113/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.9141 - val_loss: 7.9226\n","\n","Epoch 00113: loss did not improve from 4.78265\n","Epoch 114/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.8217 - val_loss: 8.5660\n","\n","Epoch 00114: loss did not improve from 4.78265\n","Epoch 115/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.8265 - val_loss: 8.7585\n","\n","Epoch 00115: loss did not improve from 4.78265\n","Epoch 116/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 4.8461 - val_loss: 9.0932\n","\n","Epoch 00116: loss did not improve from 4.78265\n","Epoch 117/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.6661 - val_loss: 8.4338\n","\n","Epoch 00117: loss improved from 4.78265 to 4.66599, saving model to final.h5\n","Epoch 118/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.8884 - val_loss: 9.1237\n","\n","Epoch 00118: loss did not improve from 4.66599\n","Epoch 119/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.7774 - val_loss: 8.0521\n","\n","Epoch 00119: loss did not improve from 4.66599\n","Epoch 120/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.8343 - val_loss: 8.3553\n","\n","Epoch 00120: loss did not improve from 4.66599\n","Epoch 121/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.6547 - val_loss: 8.7895\n","\n","Epoch 00121: loss improved from 4.66599 to 4.65435, saving model to final.h5\n","Epoch 122/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.6731 - val_loss: 8.6475\n","\n","Epoch 00122: loss did not improve from 4.65435\n","Epoch 123/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.7739 - val_loss: 8.0302\n","\n","Epoch 00123: loss did not improve from 4.65435\n","Epoch 124/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.8065 - val_loss: 9.3059\n","\n","Epoch 00124: loss did not improve from 4.65435\n","Epoch 125/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.6354 - val_loss: 8.8186\n","\n","Epoch 00125: loss improved from 4.65435 to 4.63557, saving model to final.h5\n","Epoch 126/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.6063 - val_loss: 8.2760\n","\n","Epoch 00126: loss improved from 4.63557 to 4.60636, saving model to final.h5\n","Epoch 127/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.6150 - val_loss: 8.2605\n","\n","Epoch 00127: loss did not improve from 4.60636\n","Epoch 128/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.6136 - val_loss: 8.6176\n","\n","Epoch 00128: loss did not improve from 4.60636\n","Epoch 129/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.4941 - val_loss: 9.4526\n","\n","Epoch 00129: loss improved from 4.60636 to 4.49421, saving model to final.h5\n","Epoch 130/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.6024 - val_loss: 8.1281\n","\n","Epoch 00130: loss did not improve from 4.49421\n","Epoch 131/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.5308 - val_loss: 8.7034\n","\n","Epoch 00131: loss did not improve from 4.49421\n","Epoch 132/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.6680 - val_loss: 9.5350\n","\n","Epoch 00132: loss did not improve from 4.49421\n","Epoch 133/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.6894 - val_loss: 7.9450\n","\n","Epoch 00133: loss did not improve from 4.49421\n","Epoch 134/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.6285 - val_loss: 8.4706\n","\n","Epoch 00134: loss did not improve from 4.49421\n","Epoch 135/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.5531 - val_loss: 8.3713\n","\n","Epoch 00135: loss did not improve from 4.49421\n","Epoch 136/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.4599 - val_loss: 5.9521\n","\n","Epoch 00136: loss improved from 4.49421 to 4.45933, saving model to final.h5\n","Epoch 137/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.5569 - val_loss: 7.0169\n","\n","Epoch 00137: loss did not improve from 4.45933\n","Epoch 138/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.6123 - val_loss: 7.5959\n","\n","Epoch 00138: loss did not improve from 4.45933\n","Epoch 139/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.5824 - val_loss: 6.6279\n","\n","Epoch 00139: loss did not improve from 4.45933\n","Epoch 140/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.5229 - val_loss: 6.7936\n","\n","Epoch 00140: loss did not improve from 4.45933\n","Epoch 141/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.5169 - val_loss: 6.8474\n","\n","Epoch 00141: loss did not improve from 4.45933\n","Epoch 142/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.4724 - val_loss: 6.8826\n","\n","Epoch 00142: loss did not improve from 4.45933\n","Epoch 143/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.4148 - val_loss: 7.9818\n","\n","Epoch 00143: loss improved from 4.45933 to 4.41422, saving model to final.h5\n","Epoch 144/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.5410 - val_loss: 8.6736\n","\n","Epoch 00144: loss did not improve from 4.41422\n","Epoch 145/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.4992 - val_loss: 7.4262\n","\n","Epoch 00145: loss did not improve from 4.41422\n","Epoch 146/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.5148 - val_loss: 6.9531\n","\n","Epoch 00146: loss did not improve from 4.41422\n","Epoch 147/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.3740 - val_loss: 7.5974\n","\n","Epoch 00147: loss improved from 4.41422 to 4.37450, saving model to final.h5\n","Epoch 148/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.4836 - val_loss: 7.1396\n","\n","Epoch 00148: loss did not improve from 4.37450\n","Epoch 149/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.4451 - val_loss: 7.2404\n","\n","Epoch 00149: loss did not improve from 4.37450\n","Epoch 150/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.3388 - val_loss: 7.8537\n","\n","Epoch 00150: loss improved from 4.37450 to 4.33905, saving model to final.h5\n","Epoch 151/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.3767 - val_loss: 6.1188\n","\n","Epoch 00151: loss did not improve from 4.33905\n","Epoch 152/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.4331 - val_loss: 5.3799\n","\n","Epoch 00152: loss did not improve from 4.33905\n","Epoch 153/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.3315 - val_loss: 5.9710\n","\n","Epoch 00153: loss improved from 4.33905 to 4.33145, saving model to final.h5\n","Epoch 154/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.4130 - val_loss: 6.6690\n","\n","Epoch 00154: loss did not improve from 4.33145\n","Epoch 155/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.4024 - val_loss: 6.6585\n","\n","Epoch 00155: loss did not improve from 4.33145\n","Epoch 156/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 4.3122 - val_loss: 7.2122\n","\n","Epoch 00156: loss improved from 4.33145 to 4.31255, saving model to final.h5\n","Epoch 157/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 4.3639 - val_loss: 6.5637\n","\n","Epoch 00157: loss did not improve from 4.31255\n","Epoch 158/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 4.3657 - val_loss: 6.8344\n","\n","Epoch 00158: loss did not improve from 4.31255\n","Epoch 159/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 4.3623 - val_loss: 6.0024\n","\n","Epoch 00159: loss did not improve from 4.31255\n","Epoch 160/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 4.3012 - val_loss: 8.0257\n","\n","Epoch 00160: loss improved from 4.31255 to 4.30146, saving model to final.h5\n","Epoch 161/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 4.3773 - val_loss: 8.1840\n","\n","Epoch 00161: loss did not improve from 4.30146\n","Epoch 162/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.3693 - val_loss: 8.7698\n","\n","Epoch 00162: loss did not improve from 4.30146\n","Epoch 163/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 4.1932 - val_loss: 5.6868\n","\n","Epoch 00163: loss improved from 4.30146 to 4.19356, saving model to final.h5\n","Epoch 164/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.2610 - val_loss: 5.7441\n","\n","Epoch 00164: loss did not improve from 4.19356\n","Epoch 165/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.3827 - val_loss: 6.8641\n","\n","Epoch 00165: loss did not improve from 4.19356\n","Epoch 166/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.2522 - val_loss: 6.9109\n","\n","Epoch 00166: loss did not improve from 4.19356\n","Epoch 167/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.1768 - val_loss: 6.9791\n","\n","Epoch 00167: loss improved from 4.19356 to 4.17680, saving model to final.h5\n","Epoch 168/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.1928 - val_loss: 7.6113\n","\n","Epoch 00168: loss did not improve from 4.17680\n","Epoch 169/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.3092 - val_loss: 6.2630\n","\n","Epoch 00169: loss did not improve from 4.17680\n","Epoch 170/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.2367 - val_loss: 7.8841\n","\n","Epoch 00170: loss did not improve from 4.17680\n","Epoch 171/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.2461 - val_loss: 5.9265\n","\n","Epoch 00171: loss did not improve from 4.17680\n","Epoch 172/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 4.2138 - val_loss: 7.6142\n","\n","Epoch 00172: loss did not improve from 4.17680\n","Epoch 173/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.1902 - val_loss: 6.5287\n","\n","Epoch 00173: loss did not improve from 4.17680\n","Epoch 174/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.1241 - val_loss: 7.8393\n","\n","Epoch 00174: loss improved from 4.17680 to 4.12375, saving model to final.h5\n","Epoch 175/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.2070 - val_loss: 6.1509\n","\n","Epoch 00175: loss did not improve from 4.12375\n","Epoch 176/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.1637 - val_loss: 7.4805\n","\n","Epoch 00176: loss did not improve from 4.12375\n","Epoch 177/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.1145 - val_loss: 6.9848\n","\n","Epoch 00177: loss improved from 4.12375 to 4.11475, saving model to final.h5\n","Epoch 178/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.0633 - val_loss: 7.9620\n","\n","Epoch 00178: loss improved from 4.11475 to 4.06351, saving model to final.h5\n","Epoch 179/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.2779 - val_loss: 6.9178\n","\n","Epoch 00179: loss did not improve from 4.06351\n","Epoch 180/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.2051 - val_loss: 8.2762\n","\n","Epoch 00180: loss did not improve from 4.06351\n","Epoch 181/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.2618 - val_loss: 4.6601\n","\n","Epoch 00181: loss did not improve from 4.06351\n","Epoch 182/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.2459 - val_loss: 5.5752\n","\n","Epoch 00182: loss did not improve from 4.06351\n","Epoch 183/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.1654 - val_loss: 6.5213\n","\n","Epoch 00183: loss did not improve from 4.06351\n","Epoch 184/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.0753 - val_loss: 6.0305\n","\n","Epoch 00184: loss did not improve from 4.06351\n","Epoch 185/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.1753 - val_loss: 7.2402\n","\n","Epoch 00185: loss did not improve from 4.06351\n","Epoch 186/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.1940 - val_loss: 5.5287\n","\n","Epoch 00186: loss did not improve from 4.06351\n","Epoch 187/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.1133 - val_loss: 6.3563\n","\n","Epoch 00187: loss did not improve from 4.06351\n","Epoch 188/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 4.0926 - val_loss: 6.2896\n","\n","Epoch 00188: loss did not improve from 4.06351\n","Epoch 189/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 4.1392 - val_loss: 5.0569\n","\n","Epoch 00189: loss did not improve from 4.06351\n","Epoch 190/200\n","1600/1600 [==============================] - 23s 14ms/step - loss: 4.0442 - val_loss: 5.0062\n","\n","Epoch 00190: loss improved from 4.06351 to 4.04439, saving model to final.h5\n","Epoch 191/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.0875 - val_loss: 7.0034\n","\n","Epoch 00191: loss did not improve from 4.04439\n","Epoch 192/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.0951 - val_loss: 4.9767\n","\n","Epoch 00192: loss did not improve from 4.04439\n","Epoch 193/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.0835 - val_loss: 5.8757\n","\n","Epoch 00193: loss did not improve from 4.04439\n","Epoch 194/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.0604 - val_loss: 6.3801\n","\n","Epoch 00194: loss did not improve from 4.04439\n","Epoch 195/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.0985 - val_loss: 6.4536\n","\n","Epoch 00195: loss did not improve from 4.04439\n","Epoch 196/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.0288 - val_loss: 5.5925\n","\n","Epoch 00196: loss improved from 4.04439 to 4.02895, saving model to final.h5\n","Epoch 197/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.0664 - val_loss: 7.8416\n","\n","Epoch 00197: loss did not improve from 4.02895\n","Epoch 198/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.1068 - val_loss: 6.7276\n","\n","Epoch 00198: loss did not improve from 4.02895\n","Epoch 199/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.1125 - val_loss: 4.6329\n","\n","Epoch 00199: loss did not improve from 4.02895\n","Epoch 200/200\n","1600/1600 [==============================] - 22s 14ms/step - loss: 4.0384 - val_loss: 4.1302\n","\n","Epoch 00200: loss did not improve from 4.02895\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fYfxGYdOuNav","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"01abd731-bd6d-4e34-edb1-4fa1aded4b7f","executionInfo":{"status":"ok","timestamp":1590343869437,"user_tz":360,"elapsed":2352191,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}}},"source":["!python speedchallenge.py train.mp4 train.txt --epoch 200 --history 2 --model final.h5 --split=0.2 --LR 0.00001 --mode=train "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-05-24 17:31:29.207812: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow_1=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","2020-05-24 17:31:32.174193: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-05-24 17:31:32.228635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 17:31:32.229589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 17:31:32.229631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 17:31:32.481504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 17:31:32.618782: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 17:31:32.654857: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 17:31:32.911974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 17:31:32.959392: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 17:31:33.472373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 17:31:33.472646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 17:31:33.473640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 17:31:33.474476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 17:31:33.501819: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz\n","2020-05-24 17:31:33.502185: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ca92c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 17:31:33.502224: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-05-24 17:31:33.643234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 17:31:33.644405: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ca9480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-05-24 17:31:33.644446: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-05-24 17:31:33.645799: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 17:31:33.646723: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-05-24 17:31:33.646819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 17:31:33.646915: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 17:31:33.646960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-05-24 17:31:33.646991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-05-24 17:31:33.647026: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-05-24 17:31:33.647053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-05-24 17:31:33.647081: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-05-24 17:31:33.647182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 17:31:33.648277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 17:31:33.649131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-05-24 17:31:33.654607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-05-24 17:31:40.071982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-05-24 17:31:40.072043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-05-24 17:31:40.072055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-05-24 17:31:40.077629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 17:31:40.078711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-05-24 17:31:40.079568: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-05-24 17:31:40.079623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:188: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow_1=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_1)\n","speedchallenge.py:193: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow_1=TimeDistributed(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_1)\n","speedchallenge.py:201: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow_2=TimeDistributed(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","speedchallenge.py:206: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow_2=TimeDistributed(Convolution2D(64, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_2)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            (None, 2, 100, 100,  0                                            \n","__________________________________________________________________________________________________\n","time_distributed_14 (TimeDistri (None, 2, 25, 25, 32 4128        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 2, 25, 25, 32 4128        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","time_distributed_15 (TimeDistri (None, 2, 25, 25, 32 0           time_distributed_14[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_2 (TimeDistrib (None, 2, 25, 25, 32 0           time_distributed_1[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_16 (TimeDistri (None, 2, 25, 25, 32 128         time_distributed_15[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_3 (TimeDistrib (None, 2, 25, 25, 32 128         time_distributed_2[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_17 (TimeDistri (None, 2, 25, 25, 32 0           time_distributed_16[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_4 (TimeDistrib (None, 2, 25, 25, 32 0           time_distributed_3[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_18 (TimeDistri (None, 2, 7, 7, 64)  131136      time_distributed_17[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_5 (TimeDistrib (None, 2, 7, 7, 64)  131136      time_distributed_4[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_19 (TimeDistri (None, 2, 7, 7, 64)  0           time_distributed_18[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_6 (TimeDistrib (None, 2, 7, 7, 64)  0           time_distributed_5[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_20 (TimeDistri (None, 2, 7, 7, 64)  256         time_distributed_19[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_7 (TimeDistrib (None, 2, 7, 7, 64)  256         time_distributed_6[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_21 (TimeDistri (None, 2, 7, 7, 64)  0           time_distributed_20[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_8 (TimeDistrib (None, 2, 7, 7, 64)  0           time_distributed_7[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_22 (TimeDistri (None, 2, 4, 4, 64)  0           time_distributed_21[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_9 (TimeDistrib (None, 2, 2, 2, 128) 524416      time_distributed_8[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_23 (TimeDistri (None, 2, 4, 4, 64)  256         time_distributed_22[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_10 (TimeDistri (None, 2, 2, 2, 128) 0           time_distributed_9[0][0]         \n","__________________________________________________________________________________________________\n","time_distributed_24 (TimeDistri (None, 2, 4, 4, 64)  0           time_distributed_23[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_11 (TimeDistri (None, 2, 2, 2, 128) 512         time_distributed_10[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_25 (TimeDistri (None, 2, 64)        0           time_distributed_24[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_26 (TimeDistri (None, 2, 64)        0           time_distributed_24[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_12 (TimeDistri (None, 2, 2, 2, 128) 0           time_distributed_11[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_27 (TimeDistri (None, 2, 64)        0           time_distributed_25[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_28 (TimeDistri (None, 2, 64)        0           time_distributed_26[0][0]        \n","__________________________________________________________________________________________________\n","time_distributed_13 (TimeDistri (None, 2, 512)       0           time_distributed_12[0][0]        \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 2, 640)       0           time_distributed_27[0][0]        \n","                                                                 time_distributed_28[0][0]        \n","                                                                 time_distributed_13[0][0]        \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   (None, 128)          393728      concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 128)          0           lstm_1[0][0]                     \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 128)          0           activation_6[0][0]               \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 128)          16512       dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","dropout_8 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 1)            129         dropout_8[0][0]                  \n","==================================================================================================\n","Total params: 1,206,849\n","Trainable params: 1,206,081\n","Non-trainable params: 768\n","__________________________________________________________________________________________________\n","None\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400frames\n","Done prepping data\n","20399\n","20398 Training data size per Aug\n","16318 Train indices size\n","4080 Val indices size\n"," This is the range of train:  [    0     1     2 ... 16315 16316 16317]\n"," This is the range of val:  [16318 16319 16320 ... 20395 20396 20397]\n","Starting training\n","shuffling\n","Epoch 1/200\n","2020-05-24 17:35:21.110822: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-05-24 17:35:22.583902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","1632/1632 [==============================] - 37s 23ms/step - loss: 149.7509 - val_loss: 22.7042\n","\n","Epoch 00001: loss improved from inf to 149.74902, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 48.3765 - val_loss: 0.5694\n","\n","Epoch 00002: loss improved from 149.74902 to 48.37748, saving model to final.h5\n","Epoch 3/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 30.4341 - val_loss: 0.7313\n","\n","Epoch 00003: loss improved from 48.37748 to 30.43544, saving model to final.h5\n","Epoch 4/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 26.1610 - val_loss: 1.3971\n","\n","Epoch 00004: loss improved from 30.43544 to 26.15918, saving model to final.h5\n","Epoch 5/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 24.2958 - val_loss: 6.1540\n","\n","Epoch 00005: loss improved from 26.15918 to 24.29651, saving model to final.h5\n","Epoch 6/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 23.2441 - val_loss: 11.3893\n","\n","Epoch 00006: loss improved from 24.29651 to 23.24428, saving model to final.h5\n","Epoch 7/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 21.9260 - val_loss: 18.9560\n","\n","Epoch 00007: loss improved from 23.24428 to 21.92704, saving model to final.h5\n","Epoch 8/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 20.8928 - val_loss: 27.0424\n","\n","Epoch 00008: loss improved from 21.92704 to 20.89405, saving model to final.h5\n","Epoch 9/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 20.1520 - val_loss: 25.8067\n","\n","Epoch 00009: loss improved from 20.89405 to 20.15067, saving model to final.h5\n","Epoch 10/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 18.7501 - val_loss: 29.9113\n","\n","Epoch 00010: loss improved from 20.15067 to 18.75037, saving model to final.h5\n","Epoch 11/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 18.5249 - val_loss: 34.4014\n","\n","Epoch 00011: loss improved from 18.75037 to 18.52559, saving model to final.h5\n","Epoch 12/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 18.5142 - val_loss: 30.2361\n","\n","Epoch 00012: loss improved from 18.52559 to 18.51322, saving model to final.h5\n","Epoch 13/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 17.7206 - val_loss: 26.0765\n","\n","Epoch 00013: loss improved from 18.51322 to 17.72155, saving model to final.h5\n","Epoch 14/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 17.2097 - val_loss: 33.4709\n","\n","Epoch 00014: loss improved from 17.72155 to 17.21034, saving model to final.h5\n","Epoch 15/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 16.5841 - val_loss: 34.0274\n","\n","Epoch 00015: loss improved from 17.21034 to 16.58454, saving model to final.h5\n","Epoch 16/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 15.9394 - val_loss: 33.6212\n","\n","Epoch 00016: loss improved from 16.58454 to 15.94054, saving model to final.h5\n","Epoch 17/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 15.6064 - val_loss: 31.5579\n","\n","Epoch 00017: loss improved from 15.94054 to 15.60718, saving model to final.h5\n","Epoch 18/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 15.2443 - val_loss: 20.7596\n","\n","Epoch 00018: loss improved from 15.60718 to 15.24427, saving model to final.h5\n","Epoch 19/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 14.7945 - val_loss: 18.7892\n","\n","Epoch 00019: loss improved from 15.24427 to 14.79570, saving model to final.h5\n","Epoch 20/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 14.4604 - val_loss: 24.0184\n","\n","Epoch 00020: loss improved from 14.79570 to 14.46097, saving model to final.h5\n","Epoch 21/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 14.2090 - val_loss: 23.4794\n","\n","Epoch 00021: loss improved from 14.46097 to 14.20980, saving model to final.h5\n","Epoch 22/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 13.8889 - val_loss: 21.0006\n","\n","Epoch 00022: loss improved from 14.20980 to 13.88877, saving model to final.h5\n","Epoch 23/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 13.5425 - val_loss: 19.2006\n","\n","Epoch 00023: loss improved from 13.88877 to 13.54299, saving model to final.h5\n","Epoch 24/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 13.5064 - val_loss: 20.5739\n","\n","Epoch 00024: loss improved from 13.54299 to 13.50783, saving model to final.h5\n","Epoch 25/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 12.8570 - val_loss: 20.5330\n","\n","Epoch 00025: loss improved from 13.50783 to 12.85727, saving model to final.h5\n","Epoch 26/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 12.5625 - val_loss: 15.1060\n","\n","Epoch 00026: loss improved from 12.85727 to 12.56362, saving model to final.h5\n","Epoch 27/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 12.2522 - val_loss: 19.6568\n","\n","Epoch 00027: loss improved from 12.56362 to 12.25262, saving model to final.h5\n","Epoch 28/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 12.5706 - val_loss: 15.0875\n","\n","Epoch 00028: loss did not improve from 12.25262\n","Epoch 29/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 12.3023 - val_loss: 17.6981\n","\n","Epoch 00029: loss did not improve from 12.25262\n","Epoch 30/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 11.7709 - val_loss: 19.1359\n","\n","Epoch 00030: loss improved from 12.25262 to 11.77211, saving model to final.h5\n","Epoch 31/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 11.8102 - val_loss: 19.7322\n","\n","Epoch 00031: loss did not improve from 11.77211\n","Epoch 32/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 11.5815 - val_loss: 14.4420\n","\n","Epoch 00032: loss improved from 11.77211 to 11.58180, saving model to final.h5\n","Epoch 33/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 11.3430 - val_loss: 10.3478\n","\n","Epoch 00033: loss improved from 11.58180 to 11.34349, saving model to final.h5\n","Epoch 34/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 10.9154 - val_loss: 15.4173\n","\n","Epoch 00034: loss improved from 11.34349 to 10.91543, saving model to final.h5\n","Epoch 35/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 10.9228 - val_loss: 8.7145\n","\n","Epoch 00035: loss did not improve from 10.91543\n","Epoch 36/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 10.7180 - val_loss: 11.3880\n","\n","Epoch 00036: loss improved from 10.91543 to 10.71665, saving model to final.h5\n","Epoch 37/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 10.9287 - val_loss: 12.3239\n","\n","Epoch 00037: loss did not improve from 10.71665\n","Epoch 38/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 10.6412 - val_loss: 11.3610\n","\n","Epoch 00038: loss improved from 10.71665 to 10.64192, saving model to final.h5\n","Epoch 39/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 10.3071 - val_loss: 10.0315\n","\n","Epoch 00039: loss improved from 10.64192 to 10.30781, saving model to final.h5\n","Epoch 40/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 10.1183 - val_loss: 11.1407\n","\n","Epoch 00040: loss improved from 10.30781 to 10.11905, saving model to final.h5\n","Epoch 41/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 10.0325 - val_loss: 14.3059\n","\n","Epoch 00041: loss improved from 10.11905 to 10.03175, saving model to final.h5\n","Epoch 42/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 10.0766 - val_loss: 14.0349\n","\n","Epoch 00042: loss did not improve from 10.03175\n","Epoch 43/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 9.8909 - val_loss: 11.4279\n","\n","Epoch 00043: loss improved from 10.03175 to 9.89123, saving model to final.h5\n","Epoch 44/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 9.8848 - val_loss: 9.3998\n","\n","Epoch 00044: loss improved from 9.89123 to 9.88365, saving model to final.h5\n","Epoch 45/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 9.7013 - val_loss: 9.0191\n","\n","Epoch 00045: loss improved from 9.88365 to 9.70142, saving model to final.h5\n","Epoch 46/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 9.5146 - val_loss: 14.0005\n","\n","Epoch 00046: loss improved from 9.70142 to 9.51452, saving model to final.h5\n","Epoch 47/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 9.3828 - val_loss: 15.2520\n","\n","Epoch 00047: loss improved from 9.51452 to 9.37904, saving model to final.h5\n","Epoch 48/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 9.5873 - val_loss: 12.3058\n","\n","Epoch 00048: loss did not improve from 9.37904\n","Epoch 49/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 9.2830 - val_loss: 13.1595\n","\n","Epoch 00049: loss improved from 9.37904 to 9.28332, saving model to final.h5\n","Epoch 50/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 9.2034 - val_loss: 10.9974\n","\n","Epoch 00050: loss improved from 9.28332 to 9.20262, saving model to final.h5\n","Epoch 51/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 8.9577 - val_loss: 11.3595\n","\n","Epoch 00051: loss improved from 9.20262 to 8.95841, saving model to final.h5\n","Epoch 52/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 9.0065 - val_loss: 6.8912\n","\n","Epoch 00052: loss did not improve from 8.95841\n","Epoch 53/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 8.8310 - val_loss: 7.3050\n","\n","Epoch 00053: loss improved from 8.95841 to 8.83182, saving model to final.h5\n","Epoch 54/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 8.9323 - val_loss: 7.5341\n","\n","Epoch 00054: loss did not improve from 8.83182\n","Epoch 55/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 8.6592 - val_loss: 6.5293\n","\n","Epoch 00055: loss improved from 8.83182 to 8.65779, saving model to final.h5\n","Epoch 56/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 8.6647 - val_loss: 8.5318\n","\n","Epoch 00056: loss did not improve from 8.65779\n","Epoch 57/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 8.5630 - val_loss: 8.4181\n","\n","Epoch 00057: loss improved from 8.65779 to 8.56331, saving model to final.h5\n","Epoch 58/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 8.7099 - val_loss: 6.2843\n","\n","Epoch 00058: loss did not improve from 8.56331\n","Epoch 59/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 8.5234 - val_loss: 7.1434\n","\n","Epoch 00059: loss improved from 8.56331 to 8.52305, saving model to final.h5\n","Epoch 60/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 8.2963 - val_loss: 5.7823\n","\n","Epoch 00060: loss improved from 8.52305 to 8.29663, saving model to final.h5\n","Epoch 61/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 8.0795 - val_loss: 4.8168\n","\n","Epoch 00061: loss improved from 8.29663 to 8.07983, saving model to final.h5\n","Epoch 62/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 8.3913 - val_loss: 4.4087\n","\n","Epoch 00062: loss did not improve from 8.07983\n","Epoch 63/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 8.2451 - val_loss: 4.6716\n","\n","Epoch 00063: loss did not improve from 8.07983\n","Epoch 64/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 8.0741 - val_loss: 5.5525\n","\n","Epoch 00064: loss improved from 8.07983 to 8.07440, saving model to final.h5\n","Epoch 65/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 7.9709 - val_loss: 4.5635\n","\n","Epoch 00065: loss improved from 8.07440 to 7.97112, saving model to final.h5\n","Epoch 66/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 7.9103 - val_loss: 4.3457\n","\n","Epoch 00066: loss improved from 7.97112 to 7.91061, saving model to final.h5\n","Epoch 67/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 7.8530 - val_loss: 3.8901\n","\n","Epoch 00067: loss improved from 7.91061 to 7.85372, saving model to final.h5\n","Epoch 68/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 7.7998 - val_loss: 5.0165\n","\n","Epoch 00068: loss improved from 7.85372 to 7.80050, saving model to final.h5\n","Epoch 69/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 7.7267 - val_loss: 4.9957\n","\n","Epoch 00069: loss improved from 7.80050 to 7.72717, saving model to final.h5\n","Epoch 70/200\n","1632/1632 [==============================] - 30s 18ms/step - loss: 7.6774 - val_loss: 3.9578\n","\n","Epoch 00070: loss improved from 7.72717 to 7.67772, saving model to final.h5\n","Epoch 71/200\n","1632/1632 [==============================] - 29s 18ms/step - loss: 7.6043 - val_loss: 4.7159\n","\n","Epoch 00071: loss improved from 7.67772 to 7.60440, saving model to final.h5\n","Epoch 72/200\n","1205/1632 [=====================>........] - ETA: 7s - loss: 7.5801Traceback (most recent call last):\n","  File \"speedchallenge.py\", line 387, in <module>\n","    net.main(args)\n","  File \"speedchallenge.py\", line 61, in main\n","    self.train(args.video_file,args.speed_file,args.wipe,self.EPOCHS,self.BATCH_SIZE,args.augment)\n","  File \"speedchallenge.py\", line 301, in train\n","    callbacks=[checkpoint])\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1732, in fit_generator\n","    initial_epoch=initial_epoch)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\", line 220, in fit_generator\n","    reset_metrics=False)\n","  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1514, in train_on_batch\n","    outputs = self.train_function(ins)\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\", line 3800, in __call__\n","    [x._numpy() for x in outputs],  # pylint: disable=protected-access\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\", line 3800, in <listcomp>\n","    [x._numpy() for x in outputs],  # pylint: disable=protected-access\n","  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 927, in _numpy\n","    return self._numpy_internal()\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6GaBpGlqbFv_","colab_type":"code","colab":{}},"source":["! rm -rf *_optflow*"],"execution_count":0,"outputs":[]}]}