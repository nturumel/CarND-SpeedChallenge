{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPQY3KRQDFwO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.layers import Conv2D, MaxPool2D, CuDNNGRU, GlobalMaxPool2D, Reshape, GRU, \\\n",
    "concatenate, Input, TimeDistributed , Dense, BatchNormalization, SpatialDropout2D, SpatialDropout1D, Dropout, GlobalAvgPool2D, Flatten\n",
    "from keras import Model\n",
    "from keras.applications import Xception\n",
    "import keras.backend as k\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Cropping2D\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout,ConvLSTM2D,TimeDistributed\n",
    "from keras.layers import Conv2D, MaxPool2D, CuDNNGRU, GlobalMaxPool2D, Reshape, GRU, \\\n",
    "concatenate, Input, TimeDistributed, Dense, BatchNormalization, SpatialDropout2D, SpatialDropout1D, Dropout, GlobalAvgPool2D, Flatten\n",
    "from keras.layers.core import Dense,Activation,Flatten,Lambda\n",
    "from keras.layers import Lambda\n",
    "from math import ceil\n",
    "from keras import optimizers\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import CuDNNGRU\n",
    "from keras.layers import ELU\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YryN0zfRVY7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/train\\\\train1.hdf5', './data/train\\\\train2.hdf5', './data/train\\\\train3.hdf5', './data/train\\\\train4.hdf5']\n",
      "['./data/train\\\\op1.hdf5', './data/train\\\\op2.hdf5', './data/train\\\\op3.hdf5', './data/train\\\\op4.hdf5']\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "\n",
    "hdf5_paths =  glob.glob(r\"./data/train/train*.hdf5\")\n",
    "op_hdf5_paths =  glob.glob(r\"./data/train/op*.hdf5\")\n",
    "augFactor=4\n",
    "\n",
    "hdf5_paths.sort()\n",
    "op_hdf5_paths.sort()\n",
    "\n",
    "print(hdf5_paths)\n",
    "print(op_hdf5_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gAa-iDR-DFwU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean intensity of optical flow: -0.20283675\n",
      "mean intensity of optical flow: 0.0\n",
      "mean intensity of optical flow: 0.20495185\n",
      "(20400, 240, 320, 2) opFlows size\n",
      "mean intensity of optical flow: -0.20461942\n",
      "mean intensity of optical flow: 0.0\n",
      "mean intensity of optical flow: -0.15664284\n",
      "(20400, 240, 320, 2) opFlows size\n",
      "mean intensity of optical flow: -0.0767796\n",
      "mean intensity of optical flow: 0.0\n",
      "mean intensity of optical flow: 0.010146371\n",
      "(20400, 240, 320, 2) opFlows size\n",
      "mean intensity of optical flow: -0.061669122\n",
      "mean intensity of optical flow: 0.0\n",
      "mean intensity of optical flow: -0.0012286025\n",
      "(20400, 240, 320, 2) opFlows size\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Unable to open object (object 'speeds' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8f96db820c84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mop_hdf5_paths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mdata_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"speeds\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mean intensity of optical flow:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"frames\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5601\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"frames\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5601\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\TensorFlow-GPU\\lib\\site-packages\\h5py\\_hl\\group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    262\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m             \u001b[0moid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[0motype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Unable to open object (object 'speeds' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "for path in op_hdf5_paths:\n",
    "      with h5py.File(path, \"r\") as g:\n",
    "        print(\"mean intensity of optical flow:\" , g[\"op_flow\"][1].mean())\n",
    "        print(\"mean intensity of optical flow:\" , g[\"op_flow\"][0].mean())\n",
    "        print(\"mean intensity of optical flow:\" , g[\"op_flow\"][20339].mean())\n",
    "        print((g[\"op_flow\"]).shape,'opFlows size')\n",
    "    \n",
    "for path in op_hdf5_paths:\n",
    "      with h5py.File(path, \"r\") as f:\n",
    "        data_size = len(f[\"speeds\"])\n",
    "        print(\"mean intensity of optical flow:\", f[\"frames\"][5601].mean())\n",
    "        plt.imshow(f[\"frames\"][5601]/255)\n",
    "        train_size=f[\"speeds\"].shape[0]\n",
    "        print((f[\"frames\"]).shape,'frame size')\n",
    "  \n",
    "    \n",
    "data_size*=augFactor \n",
    "print(train_size,'number of speed data samples')\n",
    "print(data_size,'real Training Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ydKeDXmODFwX"
   },
   "source": [
    "## Developing a time history model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcCu6t6TDFwY"
   },
   "outputs": [],
   "source": [
    "from statistics import median as median\n",
    "\n",
    "time_history = 3\n",
    "# All we need to do is store the indices\n",
    "FrameIndices=[]\n",
    "SpeedIndices=[]\n",
    "train_size=int(data_size/augFactor)\n",
    "print(train_size)\n",
    "for j in range(0,train_size-time_history+1):\n",
    "    tempFrame=[]\n",
    "    for i in range(0,time_history):\n",
    "        tempFrame.append(i+j)\n",
    "    SpeedIndices.append(median(tempFrame))\n",
    "    FrameIndices.append(tempFrame)\n",
    "\n",
    "train_size=len(FrameIndices)\n",
    "print(train_size,'Revised Train Size')\n",
    "\n",
    "\n",
    "train_size=len(SpeedIndices)\n",
    "print(train_size,'Revised Train Size Again')\n",
    "\n",
    "print(FrameIndices[-1],'last Frame slice')\n",
    "print(SpeedIndices[-1], 'last Speed slice')\n",
    "\n",
    "print()\n",
    "print('Checking if within indices:')\n",
    "for path in hdf5_paths:\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        plt.imshow(f[\"frames\"][FrameIndices[-1][-1]]/255)\n",
    "        print(f[\"speeds\"][SpeedIndices[-1]],'Last Speed Indice Value')\n",
    "        \n",
    "for path in op_hdf5_paths:\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "           print(\"mean intensity of optical flow:\" , f[\"op_flow\"][FrameIndices[-1][-1]].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "znkqNYo7DFwe"
   },
   "source": [
    "## Generator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oC8QJ4m2DFwe"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, hdf5_paths,op_hdf5_paths,FrameIndices,SpeedIndices,train_size,indexes = None, validation_mode = False):\n",
    "        \n",
    "        self.hdf5_paths = hdf5_paths\n",
    "        self.op_hdf5_paths=op_hdf5_paths\n",
    "        self.FrameIndices=FrameIndices\n",
    "        self.SpeedIndices=SpeedIndices\n",
    "        \n",
    "        \n",
    "        if indexes is None:\n",
    "            with indexes is None:\n",
    "                 with h5py.File(self.hdf5_path, \"r\") as f:\n",
    "                        self.indexes=np.arrange(train_size)\n",
    "        else:\n",
    "            self.indexes=indexes\n",
    "        self.batch_size=batch_size\n",
    "        self.validation_mode=validation_mode\n",
    "        if self.validation_mode==False:\n",
    "            print(\"shuffling\")\n",
    "            self.on_epoch_end()\n",
    "       \n",
    "        \n",
    "            \n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int(np.ceil(len(self.indexes)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        # Generate indexes of the batch\n",
    "        indexes=self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # Generate the data\n",
    "        return self.__data_generation(indexes)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        # updates indexes after each epoch\n",
    "        if self.validation_mode == False:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generation(self,indexes):\n",
    "        # Generates data containing batch samples\n",
    "        indexes=list(indexes)\n",
    "        indexes.sort()## We are making the Generator Class to read the video and the speed file\n",
    "        frames=[]\n",
    "        speeds=[]\n",
    "        op_flow=[]\n",
    "        \n",
    "        for paths in self.hdf5_paths:\n",
    "            with h5py.File(paths, \"r\") as f:\n",
    "                #print(\"[\")\n",
    "                for index in indexes:\n",
    "                    #print(index,\",\",end=\"\")\n",
    "                    frames.append(f[\"frames\"][self.FrameIndices[index]])\n",
    "                    speeds.append(f[\"speeds\"][self.SpeedIndices[index]])\n",
    "                #print()\n",
    "                #print(\"]\")   \n",
    "        \n",
    "        for paths in self.op_hdf5_paths:\n",
    "            with h5py.File(paths, \"r\") as f:\n",
    "                for index in indexes:\n",
    "                    op_flow.append(f[\"op_flow\"][self.FrameIndices[index]])\n",
    "                        \n",
    "                        \n",
    "        frames=np.array(frames)\n",
    "        op_flow=np.array(op_flow)\n",
    "        speeds=np.array(speeds)\n",
    "        return [[frames,op_flow],speeds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SrlAwVwlDFwh"
   },
   "outputs": [],
   "source": [
    "def build_model_history_combined_2(history_size):\n",
    "    \n",
    "    frame_inp=Input(shape=(history_size,240,320,3))\n",
    "    op_flow_inp=Input(shape=(history_size,240,320,2))\n",
    "    filter_size = (3,3)\n",
    "   \n",
    "    \n",
    "    frame=TimeDistributed(Lambda(lambda x: (x / 255.0) - 0.5))(frame_inp)\n",
    "    op_flow=TimeDistributed(Lambda(lambda x: (x / 255.0) - 0.5))(op_flow_inp)\n",
    "    \n",
    "    op_flow = TimeDistributed(BatchNormalization())(op_flow_inp)\n",
    "    op_flow = TimeDistributed(Dropout(.3))(op_flow)\n",
    "    op_flow = TimeDistributed(Conv2D(4, filter_size, activation = \"relu\", data_format = \"channels_last\"))(op_flow)\n",
    "    op_flow = TimeDistributed(MaxPool2D())(op_flow)\n",
    "    op_flow = TimeDistributed(Conv2D(8, filter_size, activation = \"relu\", data_format = \"channels_last\"))(op_flow)\n",
    "    op_flow = TimeDistributed(MaxPool2D())(op_flow)\n",
    "    op_flow = TimeDistributed(Conv2D(32, filter_size, activation = \"relu\", data_format = \"channels_last\"))(op_flow)\n",
    "    op_flow = TimeDistributed(MaxPool2D())(op_flow)\n",
    "    op_flow = TimeDistributed(Conv2D(64, filter_size, activation = \"relu\", data_format = \"channels_last\"))(op_flow)\n",
    "    op_flow = TimeDistributed(Dropout(.3))(op_flow)\n",
    "    op_flow = TimeDistributed(MaxPool2D())(op_flow)\n",
    "    op_flow = TimeDistributed(Conv2D(128, filter_size, activation = \"relu\", data_format = \"channels_last\"))(op_flow)\n",
    "    op_flow = TimeDistributed(MaxPool2D())(op_flow)\n",
    "    op_flow_max = TimeDistributed(GlobalMaxPool2D())(op_flow)\n",
    "    op_flow_avg = TimeDistributed(GlobalAvgPool2D())(op_flow)\n",
    "    \n",
    "    frame = TimeDistributed(BatchNormalization())(frame_inp)\n",
    "    frame = TimeDistributed(Dropout(.3))(frame)\n",
    "    frame = TimeDistributed(Conv2D(4, filter_size, activation = \"relu\", data_format = \"channels_last\"))(frame)\n",
    "    frame = TimeDistributed(MaxPool2D())(frame)\n",
    "    frame = TimeDistributed(Conv2D(8, filter_size, activation = \"relu\", data_format = \"channels_last\"))(frame)\n",
    "    frame = TimeDistributed(MaxPool2D())(frame)\n",
    "    frame = TimeDistributed(Conv2D(32, filter_size, activation = \"relu\", data_format = \"channels_last\"))(frame)\n",
    "    frame = TimeDistributed(MaxPool2D())(frame)\n",
    "    frame = TimeDistributed(Conv2D(64, filter_size, activation = \"relu\", data_format = \"channels_last\"))(frame)\n",
    "    frame = TimeDistributed(Dropout(.3))(frame)\n",
    "    frame = TimeDistributed(MaxPool2D())(frame)\n",
    "    frame = TimeDistributed(Conv2D(128, filter_size, activation = \"relu\", data_format = \"channels_last\"))(frame)\n",
    "    frame = TimeDistributed(MaxPool2D())(frame)\n",
    "    frame_max = TimeDistributed(GlobalMaxPool2D())(frame)\n",
    "    frame_avg = TimeDistributed(GlobalAvgPool2D())(frame)\n",
    "    \n",
    "    conc=concatenate([op_flow_max,op_flow_avg,frame_max,frame_avg],axis=1)\n",
    "    \n",
    "    conc = SpatialDropout1D(.2)(conc)\n",
    "    conc = GRU(256)(conc)\n",
    "    conc = Dense(100, activation = \"relu\")(conc)\n",
    "    conc = Dropout(.2)(conc)\n",
    "    conc = Dense(50, activation = \"relu\")(conc)\n",
    "    conc = Dropout(.1)(conc)\n",
    "    result = Dense(1, activation='linear')(conc)\n",
    "    \n",
    "    model = Model(inputs=[frame_inp, op_flow_inp], outputs=[result])\n",
    "    #print(model.summary())\n",
    "    model.compile(loss=\"mse\", optimizer='adam')\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDXXZPoJDFwk"
   },
   "outputs": [],
   "source": [
    "model=build_model_history_combined_2(time_history)\n",
    "train_indexes, val_indexes=train_test_split(np.arange(int(train_size)), shuffle = True, test_size = .2)\n",
    "\n",
    "print(train_size,'Training data size per Aug')\n",
    "print(train_indexes,'Train indices')\n",
    "print(val_indexes,'Val indices')\n",
    "\n",
    "maxIndexT=max(train_indexes)\n",
    "maxIndexV=max(val_indexes)\n",
    "print(maxIndexT,maxIndexV)\n",
    "\n",
    "minIndexT=min(train_indexes)\n",
    "minIndexV=min(val_indexes)\n",
    "print(minIndexT,minIndexV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ayYOBV0kDFwn"
   },
   "outputs": [],
   "source": [
    "train_generator=DataGenerator(5,hdf5_paths,op_hdf5_paths,FrameIndices,SpeedIndices,train_size,indexes=train_indexes,validation_mode=False)\n",
    "valid_generator=DataGenerator(5,hdf5_paths,op_hdf5_paths,FrameIndices,SpeedIndices,train_size,indexes=val_indexes,validation_mode=True)\n",
    "model.fit_generator(train_generator, validation_data=valid_generator, epochs = 5,callbacks=[EarlyStopping(patience=3), ModelCheckpoint(filepath=\"cnn_time_model_combined_mid.h5\", save_weights_only=False)])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "TrainTimeModelCombined-Collab.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
