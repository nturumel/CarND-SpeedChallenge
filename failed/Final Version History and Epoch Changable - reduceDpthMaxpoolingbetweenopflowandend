{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Final Version History and Epoch Changable - reduceDpthMaxpoolingbetweenopflowandend","provenance":[{"file_id":"1F-9b4NcMrrIpkD7dw9B6otpY6kOF-wtm","timestamp":1591850023622},{"file_id":"1OgGroPiTbVQlN8gwBsSiKFLee7NbFeew","timestamp":1591570048675}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyOU4MISfZoV0XMLdS3W2JPl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hQVmyxa4wy6p","colab_type":"code","colab":{}},"source":["# !rm -r *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v-T0qS65Af76","colab_type":"code","colab":{}},"source":["!rm -r *optflow*\n","!rm -r *Process*\n","!rm *.h5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L6dxH5l5Ag1V","colab_type":"text"},"source":["# **Welcome to the final version of training and testing**"]},{"cell_type":"code","metadata":{"id":"3aHPAT3RnoX4","colab_type":"code","outputId":"2c31e347-2dc7-4fea-9460-0ba9a55f326a","executionInfo":{"status":"ok","timestamp":1591807271781,"user_tz":360,"elapsed":8431,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["nepoch=350\n","nhistory=2\n","LR=0.001\n","! echo {nepoch}\n","! echo {nhistory}\n","! echo {LR}"],"execution_count":3,"outputs":[{"output_type":"stream","text":["350\n","2\n","0.001\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c0MqFMe7z4EQ","colab_type":"text"},"source":["## **Training with augmentation normal image**"]},{"cell_type":"code","metadata":{"id":"iR9_vLugCtWb","colab_type":"code","outputId":"abe307e9-b245-4433-f937-e40e281e0849","executionInfo":{"status":"ok","timestamp":1591818886991,"user_tz":360,"elapsed":11623639,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speedchallenge.py --mode=train --epoch {nepoch} --history {nhistory}  --split_start=7700 --split_end=12100 --wipe --model final.h5  --LR {LR} --augment train.mp4 train.txt"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-06-10 16:40:52.979517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:217: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(32, (8, 8), return_sequences=True, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(32, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=True,activation=\"relu\", dropout=0.5))(flow_inp)\n","2020-06-10 16:40:55.144310: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-06-10 16:40:55.161699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 16:40:55.162810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 16:40:55.162874: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 16:40:55.165715: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 16:40:55.168736: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 16:40:55.169315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 16:40:55.171747: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 16:40:55.173146: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 16:40:55.177535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 16:40:55.177673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 16:40:55.178630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 16:40:55.179504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 16:40:55.185497: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-06-10 16:40:55.185832: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18592c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 16:40:55.185878: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-06-10 16:40:55.279225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 16:40:55.280414: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1859480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 16:40:55.280464: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-06-10 16:40:55.280657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 16:40:55.281547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 16:40:55.281598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 16:40:55.281646: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 16:40:55.281678: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 16:40:55.281725: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 16:40:55.281753: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 16:40:55.281788: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 16:40:55.281816: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 16:40:55.281888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 16:40:55.282843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 16:40:55.283701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 16:40:55.283751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 16:40:55.831177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-10 16:40:55.831241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-06-10 16:40:55.831254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-06-10 16:40:55.831481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 16:40:55.832551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 16:40:55.833409: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-06-10 16:40:55.833455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:221: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(128, (8, 8), return_sequences=False, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(128, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=False,activation=\"relu\", dropout=0.5))(flow)\n","speedchallenge.py:234: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","speedchallenge.py:241: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 100, 100, 2)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 25, 25, 32)   4128        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 25, 25, 32)   0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 25, 25, 32)   128         activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 25, 25, 32)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 2, 100, 100,  0                                            \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 32)   0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv_lst_m2d_1 (ConvLSTM2D)     (None, 2, 25, 25, 32 286848      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 6, 6, 128)    262272      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 2, 25, 25, 32 128         conv_lst_m2d_1[0][0]             \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 6, 6, 128)    0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv_lst_m2d_2 (ConvLSTM2D)     (None, 7, 7, 128)    5243392     batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 6, 6, 128)    512         activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 7, 7, 128)    512         conv_lst_m2d_2[0][0]             \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 6, 6, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 6272)         0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 128)    0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          1605888     flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 5, 5, 256)    33024       max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 256)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 6400)         0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 6656)         0           activation_1[0][0]               \n","                                                                 flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 6656)         0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 6656)         0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 128)          852096      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 128)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1)            129         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 8,289,057\n","Trainable params: 8,288,417\n","Non-trainable params: 640\n","__________________________________________________________________________________________________\n","None\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400 frames\n","Done prepping data\n","20398 Training data size per Aug\n","15998 Train indices size\n","4400 Val indices size\n"," This is the range of train:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119, 6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129, 6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139, 6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169, 6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209, 6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229, 6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239, 6240, 6241, 6242, 6243, 6244, 6245, 6246, 6247, 6248, 6249, 6250, 6251, 6252, 6253, 6254, 6255, 6256, 6257, 6258, 6259, 6260, 6261, 6262, 6263, 6264, 6265, 6266, 6267, 6268, 6269, 6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289, 6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299, 6300, 6301, 6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309, 6310, 6311, 6312, 6313, 6314, 6315, 6316, 6317, 6318, 6319, 6320, 6321, 6322, 6323, 6324, 6325, 6326, 6327, 6328, 6329, 6330, 6331, 6332, 6333, 6334, 6335, 6336, 6337, 6338, 6339, 6340, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349, 6350, 6351, 6352, 6353, 6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363, 6364, 6365, 6366, 6367, 6368, 6369, 6370, 6371, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6379, 6380, 6381, 6382, 6383, 6384, 6385, 6386, 6387, 6388, 6389, 6390, 6391, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399, 6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419, 6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429, 6430, 6431, 6432, 6433, 6434, 6435, 6436, 6437, 6438, 6439, 6440, 6441, 6442, 6443, 6444, 6445, 6446, 6447, 6448, 6449, 6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459, 6460, 6461, 6462, 6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473, 6474, 6475, 6476, 6477, 6478, 6479, 6480, 6481, 6482, 6483, 6484, 6485, 6486, 6487, 6488, 6489, 6490, 6491, 6492, 6493, 6494, 6495, 6496, 6497, 6498, 6499, 6500, 6501, 6502, 6503, 6504, 6505, 6506, 6507, 6508, 6509, 6510, 6511, 6512, 6513, 6514, 6515, 6516, 6517, 6518, 6519, 6520, 6521, 6522, 6523, 6524, 6525, 6526, 6527, 6528, 6529, 6530, 6531, 6532, 6533, 6534, 6535, 6536, 6537, 6538, 6539, 6540, 6541, 6542, 6543, 6544, 6545, 6546, 6547, 6548, 6549, 6550, 6551, 6552, 6553, 6554, 6555, 6556, 6557, 6558, 6559, 6560, 6561, 6562, 6563, 6564, 6565, 6566, 6567, 6568, 6569, 6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577, 6578, 6579, 6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588, 6589, 6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599, 6600, 6601, 6602, 6603, 6604, 6605, 6606, 6607, 6608, 6609, 6610, 6611, 6612, 6613, 6614, 6615, 6616, 6617, 6618, 6619, 6620, 6621, 6622, 6623, 6624, 6625, 6626, 6627, 6628, 6629, 6630, 6631, 6632, 6633, 6634, 6635, 6636, 6637, 6638, 6639, 6640, 6641, 6642, 6643, 6644, 6645, 6646, 6647, 6648, 6649, 6650, 6651, 6652, 6653, 6654, 6655, 6656, 6657, 6658, 6659, 6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6669, 6670, 6671, 6672, 6673, 6674, 6675, 6676, 6677, 6678, 6679, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689, 6690, 6691, 6692, 6693, 6694, 6695, 6696, 6697, 6698, 6699, 6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6708, 6709, 6710, 6711, 6712, 6713, 6714, 6715, 6716, 6717, 6718, 6719, 6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729, 6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739, 6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6748, 6749, 6750, 6751, 6752, 6753, 6754, 6755, 6756, 6757, 6758, 6759, 6760, 6761, 6762, 6763, 6764, 6765, 6766, 6767, 6768, 6769, 6770, 6771, 6772, 6773, 6774, 6775, 6776, 6777, 6778, 6779, 6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6788, 6789, 6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 6804, 6805, 6806, 6807, 6808, 6809, 6810, 6811, 6812, 6813, 6814, 6815, 6816, 6817, 6818, 6819, 6820, 6821, 6822, 6823, 6824, 6825, 6826, 6827, 6828, 6829, 6830, 6831, 6832, 6833, 6834, 6835, 6836, 6837, 6838, 6839, 6840, 6841, 6842, 6843, 6844, 6845, 6846, 6847, 6848, 6849, 6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6859, 6860, 6861, 6862, 6863, 6864, 6865, 6866, 6867, 6868, 6869, 6870, 6871, 6872, 6873, 6874, 6875, 6876, 6877, 6878, 6879, 6880, 6881, 6882, 6883, 6884, 6885, 6886, 6887, 6888, 6889, 6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898, 6899, 6900, 6901, 6902, 6903, 6904, 6905, 6906, 6907, 6908, 6909, 6910, 6911, 6912, 6913, 6914, 6915, 6916, 6917, 6918, 6919, 6920, 6921, 6922, 6923, 6924, 6925, 6926, 6927, 6928, 6929, 6930, 6931, 6932, 6933, 6934, 6935, 6936, 6937, 6938, 6939, 6940, 6941, 6942, 6943, 6944, 6945, 6946, 6947, 6948, 6949, 6950, 6951, 6952, 6953, 6954, 6955, 6956, 6957, 6958, 6959, 6960, 6961, 6962, 6963, 6964, 6965, 6966, 6967, 6968, 6969, 6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979, 6980, 6981, 6982, 6983, 6984, 6985, 6986, 6987, 6988, 6989, 6990, 6991, 6992, 6993, 6994, 6995, 6996, 6997, 6998, 6999, 7000, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009, 7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019, 7020, 7021, 7022, 7023, 7024, 7025, 7026, 7027, 7028, 7029, 7030, 7031, 7032, 7033, 7034, 7035, 7036, 7037, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 7045, 7046, 7047, 7048, 7049, 7050, 7051, 7052, 7053, 7054, 7055, 7056, 7057, 7058, 7059, 7060, 7061, 7062, 7063, 7064, 7065, 7066, 7067, 7068, 7069, 7070, 7071, 7072, 7073, 7074, 7075, 7076, 7077, 7078, 7079, 7080, 7081, 7082, 7083, 7084, 7085, 7086, 7087, 7088, 7089, 7090, 7091, 7092, 7093, 7094, 7095, 7096, 7097, 7098, 7099, 7100, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7108, 7109, 7110, 7111, 7112, 7113, 7114, 7115, 7116, 7117, 7118, 7119, 7120, 7121, 7122, 7123, 7124, 7125, 7126, 7127, 7128, 7129, 7130, 7131, 7132, 7133, 7134, 7135, 7136, 7137, 7138, 7139, 7140, 7141, 7142, 7143, 7144, 7145, 7146, 7147, 7148, 7149, 7150, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7161, 7162, 7163, 7164, 7165, 7166, 7167, 7168, 7169, 7170, 7171, 7172, 7173, 7174, 7175, 7176, 7177, 7178, 7179, 7180, 7181, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189, 7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199, 7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209, 7210, 7211, 7212, 7213, 7214, 7215, 7216, 7217, 7218, 7219, 7220, 7221, 7222, 7223, 7224, 7225, 7226, 7227, 7228, 7229, 7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7239, 7240, 7241, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249, 7250, 7251, 7252, 7253, 7254, 7255, 7256, 7257, 7258, 7259, 7260, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7268, 7269, 7270, 7271, 7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281, 7282, 7283, 7284, 7285, 7286, 7287, 7288, 7289, 7290, 7291, 7292, 7293, 7294, 7295, 7296, 7297, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309, 7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318, 7319, 7320, 7321, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7329, 7330, 7331, 7332, 7333, 7334, 7335, 7336, 7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7349, 7350, 7351, 7352, 7353, 7354, 7355, 7356, 7357, 7358, 7359, 7360, 7361, 7362, 7363, 7364, 7365, 7366, 7367, 7368, 7369, 7370, 7371, 7372, 7373, 7374, 7375, 7376, 7377, 7378, 7379, 7380, 7381, 7382, 7383, 7384, 7385, 7386, 7387, 7388, 7389, 7390, 7391, 7392, 7393, 7394, 7395, 7396, 7397, 7398, 7399, 7400, 7401, 7402, 7403, 7404, 7405, 7406, 7407, 7408, 7409, 7410, 7411, 7412, 7413, 7414, 7415, 7416, 7417, 7418, 7419, 7420, 7421, 7422, 7423, 7424, 7425, 7426, 7427, 7428, 7429, 7430, 7431, 7432, 7433, 7434, 7435, 7436, 7437, 7438, 7439, 7440, 7441, 7442, 7443, 7444, 7445, 7446, 7447, 7448, 7449, 7450, 7451, 7452, 7453, 7454, 7455, 7456, 7457, 7458, 7459, 7460, 7461, 7462, 7463, 7464, 7465, 7466, 7467, 7468, 7469, 7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7479, 7480, 7481, 7482, 7483, 7484, 7485, 7486, 7487, 7488, 7489, 7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499, 7500, 7501, 7502, 7503, 7504, 7505, 7506, 7507, 7508, 7509, 7510, 7511, 7512, 7513, 7514, 7515, 7516, 7517, 7518, 7519, 7520, 7521, 7522, 7523, 7524, 7525, 7526, 7527, 7528, 7529, 7530, 7531, 7532, 7533, 7534, 7535, 7536, 7537, 7538, 7539, 7540, 7541, 7542, 7543, 7544, 7545, 7546, 7547, 7548, 7549, 7550, 7551, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559, 7560, 7561, 7562, 7563, 7564, 7565, 7566, 7567, 7568, 7569, 7570, 7571, 7572, 7573, 7574, 7575, 7576, 7577, 7578, 7579, 7580, 7581, 7582, 7583, 7584, 7585, 7586, 7587, 7588, 7589, 7590, 7591, 7592, 7593, 7594, 7595, 7596, 7597, 7598, 7599, 7600, 7601, 7602, 7603, 7604, 7605, 7606, 7607, 7608, 7609, 7610, 7611, 7612, 7613, 7614, 7615, 7616, 7617, 7618, 7619, 7620, 7621, 7622, 7623, 7624, 7625, 7626, 7627, 7628, 7629, 7630, 7631, 7632, 7633, 7634, 7635, 7636, 7637, 7638, 7639, 7640, 7641, 7642, 7643, 7644, 7645, 7646, 7647, 7648, 7649, 7650, 7651, 7652, 7653, 7654, 7655, 7656, 7657, 7658, 7659, 7660, 7661, 7662, 7663, 7664, 7665, 7666, 7667, 7668, 7669, 7670, 7671, 7672, 7673, 7674, 7675, 7676, 7677, 7678, 7679, 7680, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689, 7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699, 12100, 12101, 12102, 12103, 12104, 12105, 12106, 12107, 12108, 12109, 12110, 12111, 12112, 12113, 12114, 12115, 12116, 12117, 12118, 12119, 12120, 12121, 12122, 12123, 12124, 12125, 12126, 12127, 12128, 12129, 12130, 12131, 12132, 12133, 12134, 12135, 12136, 12137, 12138, 12139, 12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149, 12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159, 12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169, 12170, 12171, 12172, 12173, 12174, 12175, 12176, 12177, 12178, 12179, 12180, 12181, 12182, 12183, 12184, 12185, 12186, 12187, 12188, 12189, 12190, 12191, 12192, 12193, 12194, 12195, 12196, 12197, 12198, 12199, 12200, 12201, 12202, 12203, 12204, 12205, 12206, 12207, 12208, 12209, 12210, 12211, 12212, 12213, 12214, 12215, 12216, 12217, 12218, 12219, 12220, 12221, 12222, 12223, 12224, 12225, 12226, 12227, 12228, 12229, 12230, 12231, 12232, 12233, 12234, 12235, 12236, 12237, 12238, 12239, 12240, 12241, 12242, 12243, 12244, 12245, 12246, 12247, 12248, 12249, 12250, 12251, 12252, 12253, 12254, 12255, 12256, 12257, 12258, 12259, 12260, 12261, 12262, 12263, 12264, 12265, 12266, 12267, 12268, 12269, 12270, 12271, 12272, 12273, 12274, 12275, 12276, 12277, 12278, 12279, 12280, 12281, 12282, 12283, 12284, 12285, 12286, 12287, 12288, 12289, 12290, 12291, 12292, 12293, 12294, 12295, 12296, 12297, 12298, 12299, 12300, 12301, 12302, 12303, 12304, 12305, 12306, 12307, 12308, 12309, 12310, 12311, 12312, 12313, 12314, 12315, 12316, 12317, 12318, 12319, 12320, 12321, 12322, 12323, 12324, 12325, 12326, 12327, 12328, 12329, 12330, 12331, 12332, 12333, 12334, 12335, 12336, 12337, 12338, 12339, 12340, 12341, 12342, 12343, 12344, 12345, 12346, 12347, 12348, 12349, 12350, 12351, 12352, 12353, 12354, 12355, 12356, 12357, 12358, 12359, 12360, 12361, 12362, 12363, 12364, 12365, 12366, 12367, 12368, 12369, 12370, 12371, 12372, 12373, 12374, 12375, 12376, 12377, 12378, 12379, 12380, 12381, 12382, 12383, 12384, 12385, 12386, 12387, 12388, 12389, 12390, 12391, 12392, 12393, 12394, 12395, 12396, 12397, 12398, 12399, 12400, 12401, 12402, 12403, 12404, 12405, 12406, 12407, 12408, 12409, 12410, 12411, 12412, 12413, 12414, 12415, 12416, 12417, 12418, 12419, 12420, 12421, 12422, 12423, 12424, 12425, 12426, 12427, 12428, 12429, 12430, 12431, 12432, 12433, 12434, 12435, 12436, 12437, 12438, 12439, 12440, 12441, 12442, 12443, 12444, 12445, 12446, 12447, 12448, 12449, 12450, 12451, 12452, 12453, 12454, 12455, 12456, 12457, 12458, 12459, 12460, 12461, 12462, 12463, 12464, 12465, 12466, 12467, 12468, 12469, 12470, 12471, 12472, 12473, 12474, 12475, 12476, 12477, 12478, 12479, 12480, 12481, 12482, 12483, 12484, 12485, 12486, 12487, 12488, 12489, 12490, 12491, 12492, 12493, 12494, 12495, 12496, 12497, 12498, 12499, 12500, 12501, 12502, 12503, 12504, 12505, 12506, 12507, 12508, 12509, 12510, 12511, 12512, 12513, 12514, 12515, 12516, 12517, 12518, 12519, 12520, 12521, 12522, 12523, 12524, 12525, 12526, 12527, 12528, 12529, 12530, 12531, 12532, 12533, 12534, 12535, 12536, 12537, 12538, 12539, 12540, 12541, 12542, 12543, 12544, 12545, 12546, 12547, 12548, 12549, 12550, 12551, 12552, 12553, 12554, 12555, 12556, 12557, 12558, 12559, 12560, 12561, 12562, 12563, 12564, 12565, 12566, 12567, 12568, 12569, 12570, 12571, 12572, 12573, 12574, 12575, 12576, 12577, 12578, 12579, 12580, 12581, 12582, 12583, 12584, 12585, 12586, 12587, 12588, 12589, 12590, 12591, 12592, 12593, 12594, 12595, 12596, 12597, 12598, 12599, 12600, 12601, 12602, 12603, 12604, 12605, 12606, 12607, 12608, 12609, 12610, 12611, 12612, 12613, 12614, 12615, 12616, 12617, 12618, 12619, 12620, 12621, 12622, 12623, 12624, 12625, 12626, 12627, 12628, 12629, 12630, 12631, 12632, 12633, 12634, 12635, 12636, 12637, 12638, 12639, 12640, 12641, 12642, 12643, 12644, 12645, 12646, 12647, 12648, 12649, 12650, 12651, 12652, 12653, 12654, 12655, 12656, 12657, 12658, 12659, 12660, 12661, 12662, 12663, 12664, 12665, 12666, 12667, 12668, 12669, 12670, 12671, 12672, 12673, 12674, 12675, 12676, 12677, 12678, 12679, 12680, 12681, 12682, 12683, 12684, 12685, 12686, 12687, 12688, 12689, 12690, 12691, 12692, 12693, 12694, 12695, 12696, 12697, 12698, 12699, 12700, 12701, 12702, 12703, 12704, 12705, 12706, 12707, 12708, 12709, 12710, 12711, 12712, 12713, 12714, 12715, 12716, 12717, 12718, 12719, 12720, 12721, 12722, 12723, 12724, 12725, 12726, 12727, 12728, 12729, 12730, 12731, 12732, 12733, 12734, 12735, 12736, 12737, 12738, 12739, 12740, 12741, 12742, 12743, 12744, 12745, 12746, 12747, 12748, 12749, 12750, 12751, 12752, 12753, 12754, 12755, 12756, 12757, 12758, 12759, 12760, 12761, 12762, 12763, 12764, 12765, 12766, 12767, 12768, 12769, 12770, 12771, 12772, 12773, 12774, 12775, 12776, 12777, 12778, 12779, 12780, 12781, 12782, 12783, 12784, 12785, 12786, 12787, 12788, 12789, 12790, 12791, 12792, 12793, 12794, 12795, 12796, 12797, 12798, 12799, 12800, 12801, 12802, 12803, 12804, 12805, 12806, 12807, 12808, 12809, 12810, 12811, 12812, 12813, 12814, 12815, 12816, 12817, 12818, 12819, 12820, 12821, 12822, 12823, 12824, 12825, 12826, 12827, 12828, 12829, 12830, 12831, 12832, 12833, 12834, 12835, 12836, 12837, 12838, 12839, 12840, 12841, 12842, 12843, 12844, 12845, 12846, 12847, 12848, 12849, 12850, 12851, 12852, 12853, 12854, 12855, 12856, 12857, 12858, 12859, 12860, 12861, 12862, 12863, 12864, 12865, 12866, 12867, 12868, 12869, 12870, 12871, 12872, 12873, 12874, 12875, 12876, 12877, 12878, 12879, 12880, 12881, 12882, 12883, 12884, 12885, 12886, 12887, 12888, 12889, 12890, 12891, 12892, 12893, 12894, 12895, 12896, 12897, 12898, 12899, 12900, 12901, 12902, 12903, 12904, 12905, 12906, 12907, 12908, 12909, 12910, 12911, 12912, 12913, 12914, 12915, 12916, 12917, 12918, 12919, 12920, 12921, 12922, 12923, 12924, 12925, 12926, 12927, 12928, 12929, 12930, 12931, 12932, 12933, 12934, 12935, 12936, 12937, 12938, 12939, 12940, 12941, 12942, 12943, 12944, 12945, 12946, 12947, 12948, 12949, 12950, 12951, 12952, 12953, 12954, 12955, 12956, 12957, 12958, 12959, 12960, 12961, 12962, 12963, 12964, 12965, 12966, 12967, 12968, 12969, 12970, 12971, 12972, 12973, 12974, 12975, 12976, 12977, 12978, 12979, 12980, 12981, 12982, 12983, 12984, 12985, 12986, 12987, 12988, 12989, 12990, 12991, 12992, 12993, 12994, 12995, 12996, 12997, 12998, 12999, 13000, 13001, 13002, 13003, 13004, 13005, 13006, 13007, 13008, 13009, 13010, 13011, 13012, 13013, 13014, 13015, 13016, 13017, 13018, 13019, 13020, 13021, 13022, 13023, 13024, 13025, 13026, 13027, 13028, 13029, 13030, 13031, 13032, 13033, 13034, 13035, 13036, 13037, 13038, 13039, 13040, 13041, 13042, 13043, 13044, 13045, 13046, 13047, 13048, 13049, 13050, 13051, 13052, 13053, 13054, 13055, 13056, 13057, 13058, 13059, 13060, 13061, 13062, 13063, 13064, 13065, 13066, 13067, 13068, 13069, 13070, 13071, 13072, 13073, 13074, 13075, 13076, 13077, 13078, 13079, 13080, 13081, 13082, 13083, 13084, 13085, 13086, 13087, 13088, 13089, 13090, 13091, 13092, 13093, 13094, 13095, 13096, 13097, 13098, 13099, 13100, 13101, 13102, 13103, 13104, 13105, 13106, 13107, 13108, 13109, 13110, 13111, 13112, 13113, 13114, 13115, 13116, 13117, 13118, 13119, 13120, 13121, 13122, 13123, 13124, 13125, 13126, 13127, 13128, 13129, 13130, 13131, 13132, 13133, 13134, 13135, 13136, 13137, 13138, 13139, 13140, 13141, 13142, 13143, 13144, 13145, 13146, 13147, 13148, 13149, 13150, 13151, 13152, 13153, 13154, 13155, 13156, 13157, 13158, 13159, 13160, 13161, 13162, 13163, 13164, 13165, 13166, 13167, 13168, 13169, 13170, 13171, 13172, 13173, 13174, 13175, 13176, 13177, 13178, 13179, 13180, 13181, 13182, 13183, 13184, 13185, 13186, 13187, 13188, 13189, 13190, 13191, 13192, 13193, 13194, 13195, 13196, 13197, 13198, 13199, 13200, 13201, 13202, 13203, 13204, 13205, 13206, 13207, 13208, 13209, 13210, 13211, 13212, 13213, 13214, 13215, 13216, 13217, 13218, 13219, 13220, 13221, 13222, 13223, 13224, 13225, 13226, 13227, 13228, 13229, 13230, 13231, 13232, 13233, 13234, 13235, 13236, 13237, 13238, 13239, 13240, 13241, 13242, 13243, 13244, 13245, 13246, 13247, 13248, 13249, 13250, 13251, 13252, 13253, 13254, 13255, 13256, 13257, 13258, 13259, 13260, 13261, 13262, 13263, 13264, 13265, 13266, 13267, 13268, 13269, 13270, 13271, 13272, 13273, 13274, 13275, 13276, 13277, 13278, 13279, 13280, 13281, 13282, 13283, 13284, 13285, 13286, 13287, 13288, 13289, 13290, 13291, 13292, 13293, 13294, 13295, 13296, 13297, 13298, 13299, 13300, 13301, 13302, 13303, 13304, 13305, 13306, 13307, 13308, 13309, 13310, 13311, 13312, 13313, 13314, 13315, 13316, 13317, 13318, 13319, 13320, 13321, 13322, 13323, 13324, 13325, 13326, 13327, 13328, 13329, 13330, 13331, 13332, 13333, 13334, 13335, 13336, 13337, 13338, 13339, 13340, 13341, 13342, 13343, 13344, 13345, 13346, 13347, 13348, 13349, 13350, 13351, 13352, 13353, 13354, 13355, 13356, 13357, 13358, 13359, 13360, 13361, 13362, 13363, 13364, 13365, 13366, 13367, 13368, 13369, 13370, 13371, 13372, 13373, 13374, 13375, 13376, 13377, 13378, 13379, 13380, 13381, 13382, 13383, 13384, 13385, 13386, 13387, 13388, 13389, 13390, 13391, 13392, 13393, 13394, 13395, 13396, 13397, 13398, 13399, 13400, 13401, 13402, 13403, 13404, 13405, 13406, 13407, 13408, 13409, 13410, 13411, 13412, 13413, 13414, 13415, 13416, 13417, 13418, 13419, 13420, 13421, 13422, 13423, 13424, 13425, 13426, 13427, 13428, 13429, 13430, 13431, 13432, 13433, 13434, 13435, 13436, 13437, 13438, 13439, 13440, 13441, 13442, 13443, 13444, 13445, 13446, 13447, 13448, 13449, 13450, 13451, 13452, 13453, 13454, 13455, 13456, 13457, 13458, 13459, 13460, 13461, 13462, 13463, 13464, 13465, 13466, 13467, 13468, 13469, 13470, 13471, 13472, 13473, 13474, 13475, 13476, 13477, 13478, 13479, 13480, 13481, 13482, 13483, 13484, 13485, 13486, 13487, 13488, 13489, 13490, 13491, 13492, 13493, 13494, 13495, 13496, 13497, 13498, 13499, 13500, 13501, 13502, 13503, 13504, 13505, 13506, 13507, 13508, 13509, 13510, 13511, 13512, 13513, 13514, 13515, 13516, 13517, 13518, 13519, 13520, 13521, 13522, 13523, 13524, 13525, 13526, 13527, 13528, 13529, 13530, 13531, 13532, 13533, 13534, 13535, 13536, 13537, 13538, 13539, 13540, 13541, 13542, 13543, 13544, 13545, 13546, 13547, 13548, 13549, 13550, 13551, 13552, 13553, 13554, 13555, 13556, 13557, 13558, 13559, 13560, 13561, 13562, 13563, 13564, 13565, 13566, 13567, 13568, 13569, 13570, 13571, 13572, 13573, 13574, 13575, 13576, 13577, 13578, 13579, 13580, 13581, 13582, 13583, 13584, 13585, 13586, 13587, 13588, 13589, 13590, 13591, 13592, 13593, 13594, 13595, 13596, 13597, 13598, 13599, 13600, 13601, 13602, 13603, 13604, 13605, 13606, 13607, 13608, 13609, 13610, 13611, 13612, 13613, 13614, 13615, 13616, 13617, 13618, 13619, 13620, 13621, 13622, 13623, 13624, 13625, 13626, 13627, 13628, 13629, 13630, 13631, 13632, 13633, 13634, 13635, 13636, 13637, 13638, 13639, 13640, 13641, 13642, 13643, 13644, 13645, 13646, 13647, 13648, 13649, 13650, 13651, 13652, 13653, 13654, 13655, 13656, 13657, 13658, 13659, 13660, 13661, 13662, 13663, 13664, 13665, 13666, 13667, 13668, 13669, 13670, 13671, 13672, 13673, 13674, 13675, 13676, 13677, 13678, 13679, 13680, 13681, 13682, 13683, 13684, 13685, 13686, 13687, 13688, 13689, 13690, 13691, 13692, 13693, 13694, 13695, 13696, 13697, 13698, 13699, 13700, 13701, 13702, 13703, 13704, 13705, 13706, 13707, 13708, 13709, 13710, 13711, 13712, 13713, 13714, 13715, 13716, 13717, 13718, 13719, 13720, 13721, 13722, 13723, 13724, 13725, 13726, 13727, 13728, 13729, 13730, 13731, 13732, 13733, 13734, 13735, 13736, 13737, 13738, 13739, 13740, 13741, 13742, 13743, 13744, 13745, 13746, 13747, 13748, 13749, 13750, 13751, 13752, 13753, 13754, 13755, 13756, 13757, 13758, 13759, 13760, 13761, 13762, 13763, 13764, 13765, 13766, 13767, 13768, 13769, 13770, 13771, 13772, 13773, 13774, 13775, 13776, 13777, 13778, 13779, 13780, 13781, 13782, 13783, 13784, 13785, 13786, 13787, 13788, 13789, 13790, 13791, 13792, 13793, 13794, 13795, 13796, 13797, 13798, 13799, 13800, 13801, 13802, 13803, 13804, 13805, 13806, 13807, 13808, 13809, 13810, 13811, 13812, 13813, 13814, 13815, 13816, 13817, 13818, 13819, 13820, 13821, 13822, 13823, 13824, 13825, 13826, 13827, 13828, 13829, 13830, 13831, 13832, 13833, 13834, 13835, 13836, 13837, 13838, 13839, 13840, 13841, 13842, 13843, 13844, 13845, 13846, 13847, 13848, 13849, 13850, 13851, 13852, 13853, 13854, 13855, 13856, 13857, 13858, 13859, 13860, 13861, 13862, 13863, 13864, 13865, 13866, 13867, 13868, 13869, 13870, 13871, 13872, 13873, 13874, 13875, 13876, 13877, 13878, 13879, 13880, 13881, 13882, 13883, 13884, 13885, 13886, 13887, 13888, 13889, 13890, 13891, 13892, 13893, 13894, 13895, 13896, 13897, 13898, 13899, 13900, 13901, 13902, 13903, 13904, 13905, 13906, 13907, 13908, 13909, 13910, 13911, 13912, 13913, 13914, 13915, 13916, 13917, 13918, 13919, 13920, 13921, 13922, 13923, 13924, 13925, 13926, 13927, 13928, 13929, 13930, 13931, 13932, 13933, 13934, 13935, 13936, 13937, 13938, 13939, 13940, 13941, 13942, 13943, 13944, 13945, 13946, 13947, 13948, 13949, 13950, 13951, 13952, 13953, 13954, 13955, 13956, 13957, 13958, 13959, 13960, 13961, 13962, 13963, 13964, 13965, 13966, 13967, 13968, 13969, 13970, 13971, 13972, 13973, 13974, 13975, 13976, 13977, 13978, 13979, 13980, 13981, 13982, 13983, 13984, 13985, 13986, 13987, 13988, 13989, 13990, 13991, 13992, 13993, 13994, 13995, 13996, 13997, 13998, 13999, 14000, 14001, 14002, 14003, 14004, 14005, 14006, 14007, 14008, 14009, 14010, 14011, 14012, 14013, 14014, 14015, 14016, 14017, 14018, 14019, 14020, 14021, 14022, 14023, 14024, 14025, 14026, 14027, 14028, 14029, 14030, 14031, 14032, 14033, 14034, 14035, 14036, 14037, 14038, 14039, 14040, 14041, 14042, 14043, 14044, 14045, 14046, 14047, 14048, 14049, 14050, 14051, 14052, 14053, 14054, 14055, 14056, 14057, 14058, 14059, 14060, 14061, 14062, 14063, 14064, 14065, 14066, 14067, 14068, 14069, 14070, 14071, 14072, 14073, 14074, 14075, 14076, 14077, 14078, 14079, 14080, 14081, 14082, 14083, 14084, 14085, 14086, 14087, 14088, 14089, 14090, 14091, 14092, 14093, 14094, 14095, 14096, 14097, 14098, 14099, 14100, 14101, 14102, 14103, 14104, 14105, 14106, 14107, 14108, 14109, 14110, 14111, 14112, 14113, 14114, 14115, 14116, 14117, 14118, 14119, 14120, 14121, 14122, 14123, 14124, 14125, 14126, 14127, 14128, 14129, 14130, 14131, 14132, 14133, 14134, 14135, 14136, 14137, 14138, 14139, 14140, 14141, 14142, 14143, 14144, 14145, 14146, 14147, 14148, 14149, 14150, 14151, 14152, 14153, 14154, 14155, 14156, 14157, 14158, 14159, 14160, 14161, 14162, 14163, 14164, 14165, 14166, 14167, 14168, 14169, 14170, 14171, 14172, 14173, 14174, 14175, 14176, 14177, 14178, 14179, 14180, 14181, 14182, 14183, 14184, 14185, 14186, 14187, 14188, 14189, 14190, 14191, 14192, 14193, 14194, 14195, 14196, 14197, 14198, 14199, 14200, 14201, 14202, 14203, 14204, 14205, 14206, 14207, 14208, 14209, 14210, 14211, 14212, 14213, 14214, 14215, 14216, 14217, 14218, 14219, 14220, 14221, 14222, 14223, 14224, 14225, 14226, 14227, 14228, 14229, 14230, 14231, 14232, 14233, 14234, 14235, 14236, 14237, 14238, 14239, 14240, 14241, 14242, 14243, 14244, 14245, 14246, 14247, 14248, 14249, 14250, 14251, 14252, 14253, 14254, 14255, 14256, 14257, 14258, 14259, 14260, 14261, 14262, 14263, 14264, 14265, 14266, 14267, 14268, 14269, 14270, 14271, 14272, 14273, 14274, 14275, 14276, 14277, 14278, 14279, 14280, 14281, 14282, 14283, 14284, 14285, 14286, 14287, 14288, 14289, 14290, 14291, 14292, 14293, 14294, 14295, 14296, 14297, 14298, 14299, 14300, 14301, 14302, 14303, 14304, 14305, 14306, 14307, 14308, 14309, 14310, 14311, 14312, 14313, 14314, 14315, 14316, 14317, 14318, 14319, 14320, 14321, 14322, 14323, 14324, 14325, 14326, 14327, 14328, 14329, 14330, 14331, 14332, 14333, 14334, 14335, 14336, 14337, 14338, 14339, 14340, 14341, 14342, 14343, 14344, 14345, 14346, 14347, 14348, 14349, 14350, 14351, 14352, 14353, 14354, 14355, 14356, 14357, 14358, 14359, 14360, 14361, 14362, 14363, 14364, 14365, 14366, 14367, 14368, 14369, 14370, 14371, 14372, 14373, 14374, 14375, 14376, 14377, 14378, 14379, 14380, 14381, 14382, 14383, 14384, 14385, 14386, 14387, 14388, 14389, 14390, 14391, 14392, 14393, 14394, 14395, 14396, 14397, 14398, 14399, 14400, 14401, 14402, 14403, 14404, 14405, 14406, 14407, 14408, 14409, 14410, 14411, 14412, 14413, 14414, 14415, 14416, 14417, 14418, 14419, 14420, 14421, 14422, 14423, 14424, 14425, 14426, 14427, 14428, 14429, 14430, 14431, 14432, 14433, 14434, 14435, 14436, 14437, 14438, 14439, 14440, 14441, 14442, 14443, 14444, 14445, 14446, 14447, 14448, 14449, 14450, 14451, 14452, 14453, 14454, 14455, 14456, 14457, 14458, 14459, 14460, 14461, 14462, 14463, 14464, 14465, 14466, 14467, 14468, 14469, 14470, 14471, 14472, 14473, 14474, 14475, 14476, 14477, 14478, 14479, 14480, 14481, 14482, 14483, 14484, 14485, 14486, 14487, 14488, 14489, 14490, 14491, 14492, 14493, 14494, 14495, 14496, 14497, 14498, 14499, 14500, 14501, 14502, 14503, 14504, 14505, 14506, 14507, 14508, 14509, 14510, 14511, 14512, 14513, 14514, 14515, 14516, 14517, 14518, 14519, 14520, 14521, 14522, 14523, 14524, 14525, 14526, 14527, 14528, 14529, 14530, 14531, 14532, 14533, 14534, 14535, 14536, 14537, 14538, 14539, 14540, 14541, 14542, 14543, 14544, 14545, 14546, 14547, 14548, 14549, 14550, 14551, 14552, 14553, 14554, 14555, 14556, 14557, 14558, 14559, 14560, 14561, 14562, 14563, 14564, 14565, 14566, 14567, 14568, 14569, 14570, 14571, 14572, 14573, 14574, 14575, 14576, 14577, 14578, 14579, 14580, 14581, 14582, 14583, 14584, 14585, 14586, 14587, 14588, 14589, 14590, 14591, 14592, 14593, 14594, 14595, 14596, 14597, 14598, 14599, 14600, 14601, 14602, 14603, 14604, 14605, 14606, 14607, 14608, 14609, 14610, 14611, 14612, 14613, 14614, 14615, 14616, 14617, 14618, 14619, 14620, 14621, 14622, 14623, 14624, 14625, 14626, 14627, 14628, 14629, 14630, 14631, 14632, 14633, 14634, 14635, 14636, 14637, 14638, 14639, 14640, 14641, 14642, 14643, 14644, 14645, 14646, 14647, 14648, 14649, 14650, 14651, 14652, 14653, 14654, 14655, 14656, 14657, 14658, 14659, 14660, 14661, 14662, 14663, 14664, 14665, 14666, 14667, 14668, 14669, 14670, 14671, 14672, 14673, 14674, 14675, 14676, 14677, 14678, 14679, 14680, 14681, 14682, 14683, 14684, 14685, 14686, 14687, 14688, 14689, 14690, 14691, 14692, 14693, 14694, 14695, 14696, 14697, 14698, 14699, 14700, 14701, 14702, 14703, 14704, 14705, 14706, 14707, 14708, 14709, 14710, 14711, 14712, 14713, 14714, 14715, 14716, 14717, 14718, 14719, 14720, 14721, 14722, 14723, 14724, 14725, 14726, 14727, 14728, 14729, 14730, 14731, 14732, 14733, 14734, 14735, 14736, 14737, 14738, 14739, 14740, 14741, 14742, 14743, 14744, 14745, 14746, 14747, 14748, 14749, 14750, 14751, 14752, 14753, 14754, 14755, 14756, 14757, 14758, 14759, 14760, 14761, 14762, 14763, 14764, 14765, 14766, 14767, 14768, 14769, 14770, 14771, 14772, 14773, 14774, 14775, 14776, 14777, 14778, 14779, 14780, 14781, 14782, 14783, 14784, 14785, 14786, 14787, 14788, 14789, 14790, 14791, 14792, 14793, 14794, 14795, 14796, 14797, 14798, 14799, 14800, 14801, 14802, 14803, 14804, 14805, 14806, 14807, 14808, 14809, 14810, 14811, 14812, 14813, 14814, 14815, 14816, 14817, 14818, 14819, 14820, 14821, 14822, 14823, 14824, 14825, 14826, 14827, 14828, 14829, 14830, 14831, 14832, 14833, 14834, 14835, 14836, 14837, 14838, 14839, 14840, 14841, 14842, 14843, 14844, 14845, 14846, 14847, 14848, 14849, 14850, 14851, 14852, 14853, 14854, 14855, 14856, 14857, 14858, 14859, 14860, 14861, 14862, 14863, 14864, 14865, 14866, 14867, 14868, 14869, 14870, 14871, 14872, 14873, 14874, 14875, 14876, 14877, 14878, 14879, 14880, 14881, 14882, 14883, 14884, 14885, 14886, 14887, 14888, 14889, 14890, 14891, 14892, 14893, 14894, 14895, 14896, 14897, 14898, 14899, 14900, 14901, 14902, 14903, 14904, 14905, 14906, 14907, 14908, 14909, 14910, 14911, 14912, 14913, 14914, 14915, 14916, 14917, 14918, 14919, 14920, 14921, 14922, 14923, 14924, 14925, 14926, 14927, 14928, 14929, 14930, 14931, 14932, 14933, 14934, 14935, 14936, 14937, 14938, 14939, 14940, 14941, 14942, 14943, 14944, 14945, 14946, 14947, 14948, 14949, 14950, 14951, 14952, 14953, 14954, 14955, 14956, 14957, 14958, 14959, 14960, 14961, 14962, 14963, 14964, 14965, 14966, 14967, 14968, 14969, 14970, 14971, 14972, 14973, 14974, 14975, 14976, 14977, 14978, 14979, 14980, 14981, 14982, 14983, 14984, 14985, 14986, 14987, 14988, 14989, 14990, 14991, 14992, 14993, 14994, 14995, 14996, 14997, 14998, 14999, 15000, 15001, 15002, 15003, 15004, 15005, 15006, 15007, 15008, 15009, 15010, 15011, 15012, 15013, 15014, 15015, 15016, 15017, 15018, 15019, 15020, 15021, 15022, 15023, 15024, 15025, 15026, 15027, 15028, 15029, 15030, 15031, 15032, 15033, 15034, 15035, 15036, 15037, 15038, 15039, 15040, 15041, 15042, 15043, 15044, 15045, 15046, 15047, 15048, 15049, 15050, 15051, 15052, 15053, 15054, 15055, 15056, 15057, 15058, 15059, 15060, 15061, 15062, 15063, 15064, 15065, 15066, 15067, 15068, 15069, 15070, 15071, 15072, 15073, 15074, 15075, 15076, 15077, 15078, 15079, 15080, 15081, 15082, 15083, 15084, 15085, 15086, 15087, 15088, 15089, 15090, 15091, 15092, 15093, 15094, 15095, 15096, 15097, 15098, 15099, 15100, 15101, 15102, 15103, 15104, 15105, 15106, 15107, 15108, 15109, 15110, 15111, 15112, 15113, 15114, 15115, 15116, 15117, 15118, 15119, 15120, 15121, 15122, 15123, 15124, 15125, 15126, 15127, 15128, 15129, 15130, 15131, 15132, 15133, 15134, 15135, 15136, 15137, 15138, 15139, 15140, 15141, 15142, 15143, 15144, 15145, 15146, 15147, 15148, 15149, 15150, 15151, 15152, 15153, 15154, 15155, 15156, 15157, 15158, 15159, 15160, 15161, 15162, 15163, 15164, 15165, 15166, 15167, 15168, 15169, 15170, 15171, 15172, 15173, 15174, 15175, 15176, 15177, 15178, 15179, 15180, 15181, 15182, 15183, 15184, 15185, 15186, 15187, 15188, 15189, 15190, 15191, 15192, 15193, 15194, 15195, 15196, 15197, 15198, 15199, 15200, 15201, 15202, 15203, 15204, 15205, 15206, 15207, 15208, 15209, 15210, 15211, 15212, 15213, 15214, 15215, 15216, 15217, 15218, 15219, 15220, 15221, 15222, 15223, 15224, 15225, 15226, 15227, 15228, 15229, 15230, 15231, 15232, 15233, 15234, 15235, 15236, 15237, 15238, 15239, 15240, 15241, 15242, 15243, 15244, 15245, 15246, 15247, 15248, 15249, 15250, 15251, 15252, 15253, 15254, 15255, 15256, 15257, 15258, 15259, 15260, 15261, 15262, 15263, 15264, 15265, 15266, 15267, 15268, 15269, 15270, 15271, 15272, 15273, 15274, 15275, 15276, 15277, 15278, 15279, 15280, 15281, 15282, 15283, 15284, 15285, 15286, 15287, 15288, 15289, 15290, 15291, 15292, 15293, 15294, 15295, 15296, 15297, 15298, 15299, 15300, 15301, 15302, 15303, 15304, 15305, 15306, 15307, 15308, 15309, 15310, 15311, 15312, 15313, 15314, 15315, 15316, 15317, 15318, 15319, 15320, 15321, 15322, 15323, 15324, 15325, 15326, 15327, 15328, 15329, 15330, 15331, 15332, 15333, 15334, 15335, 15336, 15337, 15338, 15339, 15340, 15341, 15342, 15343, 15344, 15345, 15346, 15347, 15348, 15349, 15350, 15351, 15352, 15353, 15354, 15355, 15356, 15357, 15358, 15359, 15360, 15361, 15362, 15363, 15364, 15365, 15366, 15367, 15368, 15369, 15370, 15371, 15372, 15373, 15374, 15375, 15376, 15377, 15378, 15379, 15380, 15381, 15382, 15383, 15384, 15385, 15386, 15387, 15388, 15389, 15390, 15391, 15392, 15393, 15394, 15395, 15396, 15397, 15398, 15399, 15400, 15401, 15402, 15403, 15404, 15405, 15406, 15407, 15408, 15409, 15410, 15411, 15412, 15413, 15414, 15415, 15416, 15417, 15418, 15419, 15420, 15421, 15422, 15423, 15424, 15425, 15426, 15427, 15428, 15429, 15430, 15431, 15432, 15433, 15434, 15435, 15436, 15437, 15438, 15439, 15440, 15441, 15442, 15443, 15444, 15445, 15446, 15447, 15448, 15449, 15450, 15451, 15452, 15453, 15454, 15455, 15456, 15457, 15458, 15459, 15460, 15461, 15462, 15463, 15464, 15465, 15466, 15467, 15468, 15469, 15470, 15471, 15472, 15473, 15474, 15475, 15476, 15477, 15478, 15479, 15480, 15481, 15482, 15483, 15484, 15485, 15486, 15487, 15488, 15489, 15490, 15491, 15492, 15493, 15494, 15495, 15496, 15497, 15498, 15499, 15500, 15501, 15502, 15503, 15504, 15505, 15506, 15507, 15508, 15509, 15510, 15511, 15512, 15513, 15514, 15515, 15516, 15517, 15518, 15519, 15520, 15521, 15522, 15523, 15524, 15525, 15526, 15527, 15528, 15529, 15530, 15531, 15532, 15533, 15534, 15535, 15536, 15537, 15538, 15539, 15540, 15541, 15542, 15543, 15544, 15545, 15546, 15547, 15548, 15549, 15550, 15551, 15552, 15553, 15554, 15555, 15556, 15557, 15558, 15559, 15560, 15561, 15562, 15563, 15564, 15565, 15566, 15567, 15568, 15569, 15570, 15571, 15572, 15573, 15574, 15575, 15576, 15577, 15578, 15579, 15580, 15581, 15582, 15583, 15584, 15585, 15586, 15587, 15588, 15589, 15590, 15591, 15592, 15593, 15594, 15595, 15596, 15597, 15598, 15599, 15600, 15601, 15602, 15603, 15604, 15605, 15606, 15607, 15608, 15609, 15610, 15611, 15612, 15613, 15614, 15615, 15616, 15617, 15618, 15619, 15620, 15621, 15622, 15623, 15624, 15625, 15626, 15627, 15628, 15629, 15630, 15631, 15632, 15633, 15634, 15635, 15636, 15637, 15638, 15639, 15640, 15641, 15642, 15643, 15644, 15645, 15646, 15647, 15648, 15649, 15650, 15651, 15652, 15653, 15654, 15655, 15656, 15657, 15658, 15659, 15660, 15661, 15662, 15663, 15664, 15665, 15666, 15667, 15668, 15669, 15670, 15671, 15672, 15673, 15674, 15675, 15676, 15677, 15678, 15679, 15680, 15681, 15682, 15683, 15684, 15685, 15686, 15687, 15688, 15689, 15690, 15691, 15692, 15693, 15694, 15695, 15696, 15697, 15698, 15699, 15700, 15701, 15702, 15703, 15704, 15705, 15706, 15707, 15708, 15709, 15710, 15711, 15712, 15713, 15714, 15715, 15716, 15717, 15718, 15719, 15720, 15721, 15722, 15723, 15724, 15725, 15726, 15727, 15728, 15729, 15730, 15731, 15732, 15733, 15734, 15735, 15736, 15737, 15738, 15739, 15740, 15741, 15742, 15743, 15744, 15745, 15746, 15747, 15748, 15749, 15750, 15751, 15752, 15753, 15754, 15755, 15756, 15757, 15758, 15759, 15760, 15761, 15762, 15763, 15764, 15765, 15766, 15767, 15768, 15769, 15770, 15771, 15772, 15773, 15774, 15775, 15776, 15777, 15778, 15779, 15780, 15781, 15782, 15783, 15784, 15785, 15786, 15787, 15788, 15789, 15790, 15791, 15792, 15793, 15794, 15795, 15796, 15797, 15798, 15799, 15800, 15801, 15802, 15803, 15804, 15805, 15806, 15807, 15808, 15809, 15810, 15811, 15812, 15813, 15814, 15815, 15816, 15817, 15818, 15819, 15820, 15821, 15822, 15823, 15824, 15825, 15826, 15827, 15828, 15829, 15830, 15831, 15832, 15833, 15834, 15835, 15836, 15837, 15838, 15839, 15840, 15841, 15842, 15843, 15844, 15845, 15846, 15847, 15848, 15849, 15850, 15851, 15852, 15853, 15854, 15855, 15856, 15857, 15858, 15859, 15860, 15861, 15862, 15863, 15864, 15865, 15866, 15867, 15868, 15869, 15870, 15871, 15872, 15873, 15874, 15875, 15876, 15877, 15878, 15879, 15880, 15881, 15882, 15883, 15884, 15885, 15886, 15887, 15888, 15889, 15890, 15891, 15892, 15893, 15894, 15895, 15896, 15897, 15898, 15899, 15900, 15901, 15902, 15903, 15904, 15905, 15906, 15907, 15908, 15909, 15910, 15911, 15912, 15913, 15914, 15915, 15916, 15917, 15918, 15919, 15920, 15921, 15922, 15923, 15924, 15925, 15926, 15927, 15928, 15929, 15930, 15931, 15932, 15933, 15934, 15935, 15936, 15937, 15938, 15939, 15940, 15941, 15942, 15943, 15944, 15945, 15946, 15947, 15948, 15949, 15950, 15951, 15952, 15953, 15954, 15955, 15956, 15957, 15958, 15959, 15960, 15961, 15962, 15963, 15964, 15965, 15966, 15967, 15968, 15969, 15970, 15971, 15972, 15973, 15974, 15975, 15976, 15977, 15978, 15979, 15980, 15981, 15982, 15983, 15984, 15985, 15986, 15987, 15988, 15989, 15990, 15991, 15992, 15993, 15994, 15995, 15996, 15997, 15998, 15999, 16000, 16001, 16002, 16003, 16004, 16005, 16006, 16007, 16008, 16009, 16010, 16011, 16012, 16013, 16014, 16015, 16016, 16017, 16018, 16019, 16020, 16021, 16022, 16023, 16024, 16025, 16026, 16027, 16028, 16029, 16030, 16031, 16032, 16033, 16034, 16035, 16036, 16037, 16038, 16039, 16040, 16041, 16042, 16043, 16044, 16045, 16046, 16047, 16048, 16049, 16050, 16051, 16052, 16053, 16054, 16055, 16056, 16057, 16058, 16059, 16060, 16061, 16062, 16063, 16064, 16065, 16066, 16067, 16068, 16069, 16070, 16071, 16072, 16073, 16074, 16075, 16076, 16077, 16078, 16079, 16080, 16081, 16082, 16083, 16084, 16085, 16086, 16087, 16088, 16089, 16090, 16091, 16092, 16093, 16094, 16095, 16096, 16097, 16098, 16099, 16100, 16101, 16102, 16103, 16104, 16105, 16106, 16107, 16108, 16109, 16110, 16111, 16112, 16113, 16114, 16115, 16116, 16117, 16118, 16119, 16120, 16121, 16122, 16123, 16124, 16125, 16126, 16127, 16128, 16129, 16130, 16131, 16132, 16133, 16134, 16135, 16136, 16137, 16138, 16139, 16140, 16141, 16142, 16143, 16144, 16145, 16146, 16147, 16148, 16149, 16150, 16151, 16152, 16153, 16154, 16155, 16156, 16157, 16158, 16159, 16160, 16161, 16162, 16163, 16164, 16165, 16166, 16167, 16168, 16169, 16170, 16171, 16172, 16173, 16174, 16175, 16176, 16177, 16178, 16179, 16180, 16181, 16182, 16183, 16184, 16185, 16186, 16187, 16188, 16189, 16190, 16191, 16192, 16193, 16194, 16195, 16196, 16197, 16198, 16199, 16200, 16201, 16202, 16203, 16204, 16205, 16206, 16207, 16208, 16209, 16210, 16211, 16212, 16213, 16214, 16215, 16216, 16217, 16218, 16219, 16220, 16221, 16222, 16223, 16224, 16225, 16226, 16227, 16228, 16229, 16230, 16231, 16232, 16233, 16234, 16235, 16236, 16237, 16238, 16239, 16240, 16241, 16242, 16243, 16244, 16245, 16246, 16247, 16248, 16249, 16250, 16251, 16252, 16253, 16254, 16255, 16256, 16257, 16258, 16259, 16260, 16261, 16262, 16263, 16264, 16265, 16266, 16267, 16268, 16269, 16270, 16271, 16272, 16273, 16274, 16275, 16276, 16277, 16278, 16279, 16280, 16281, 16282, 16283, 16284, 16285, 16286, 16287, 16288, 16289, 16290, 16291, 16292, 16293, 16294, 16295, 16296, 16297, 16298, 16299, 16300, 16301, 16302, 16303, 16304, 16305, 16306, 16307, 16308, 16309, 16310, 16311, 16312, 16313, 16314, 16315, 16316, 16317, 16318, 16319, 16320, 16321, 16322, 16323, 16324, 16325, 16326, 16327, 16328, 16329, 16330, 16331, 16332, 16333, 16334, 16335, 16336, 16337, 16338, 16339, 16340, 16341, 16342, 16343, 16344, 16345, 16346, 16347, 16348, 16349, 16350, 16351, 16352, 16353, 16354, 16355, 16356, 16357, 16358, 16359, 16360, 16361, 16362, 16363, 16364, 16365, 16366, 16367, 16368, 16369, 16370, 16371, 16372, 16373, 16374, 16375, 16376, 16377, 16378, 16379, 16380, 16381, 16382, 16383, 16384, 16385, 16386, 16387, 16388, 16389, 16390, 16391, 16392, 16393, 16394, 16395, 16396, 16397, 16398, 16399, 16400, 16401, 16402, 16403, 16404, 16405, 16406, 16407, 16408, 16409, 16410, 16411, 16412, 16413, 16414, 16415, 16416, 16417, 16418, 16419, 16420, 16421, 16422, 16423, 16424, 16425, 16426, 16427, 16428, 16429, 16430, 16431, 16432, 16433, 16434, 16435, 16436, 16437, 16438, 16439, 16440, 16441, 16442, 16443, 16444, 16445, 16446, 16447, 16448, 16449, 16450, 16451, 16452, 16453, 16454, 16455, 16456, 16457, 16458, 16459, 16460, 16461, 16462, 16463, 16464, 16465, 16466, 16467, 16468, 16469, 16470, 16471, 16472, 16473, 16474, 16475, 16476, 16477, 16478, 16479, 16480, 16481, 16482, 16483, 16484, 16485, 16486, 16487, 16488, 16489, 16490, 16491, 16492, 16493, 16494, 16495, 16496, 16497, 16498, 16499, 16500, 16501, 16502, 16503, 16504, 16505, 16506, 16507, 16508, 16509, 16510, 16511, 16512, 16513, 16514, 16515, 16516, 16517, 16518, 16519, 16520, 16521, 16522, 16523, 16524, 16525, 16526, 16527, 16528, 16529, 16530, 16531, 16532, 16533, 16534, 16535, 16536, 16537, 16538, 16539, 16540, 16541, 16542, 16543, 16544, 16545, 16546, 16547, 16548, 16549, 16550, 16551, 16552, 16553, 16554, 16555, 16556, 16557, 16558, 16559, 16560, 16561, 16562, 16563, 16564, 16565, 16566, 16567, 16568, 16569, 16570, 16571, 16572, 16573, 16574, 16575, 16576, 16577, 16578, 16579, 16580, 16581, 16582, 16583, 16584, 16585, 16586, 16587, 16588, 16589, 16590, 16591, 16592, 16593, 16594, 16595, 16596, 16597, 16598, 16599, 16600, 16601, 16602, 16603, 16604, 16605, 16606, 16607, 16608, 16609, 16610, 16611, 16612, 16613, 16614, 16615, 16616, 16617, 16618, 16619, 16620, 16621, 16622, 16623, 16624, 16625, 16626, 16627, 16628, 16629, 16630, 16631, 16632, 16633, 16634, 16635, 16636, 16637, 16638, 16639, 16640, 16641, 16642, 16643, 16644, 16645, 16646, 16647, 16648, 16649, 16650, 16651, 16652, 16653, 16654, 16655, 16656, 16657, 16658, 16659, 16660, 16661, 16662, 16663, 16664, 16665, 16666, 16667, 16668, 16669, 16670, 16671, 16672, 16673, 16674, 16675, 16676, 16677, 16678, 16679, 16680, 16681, 16682, 16683, 16684, 16685, 16686, 16687, 16688, 16689, 16690, 16691, 16692, 16693, 16694, 16695, 16696, 16697, 16698, 16699, 16700, 16701, 16702, 16703, 16704, 16705, 16706, 16707, 16708, 16709, 16710, 16711, 16712, 16713, 16714, 16715, 16716, 16717, 16718, 16719, 16720, 16721, 16722, 16723, 16724, 16725, 16726, 16727, 16728, 16729, 16730, 16731, 16732, 16733, 16734, 16735, 16736, 16737, 16738, 16739, 16740, 16741, 16742, 16743, 16744, 16745, 16746, 16747, 16748, 16749, 16750, 16751, 16752, 16753, 16754, 16755, 16756, 16757, 16758, 16759, 16760, 16761, 16762, 16763, 16764, 16765, 16766, 16767, 16768, 16769, 16770, 16771, 16772, 16773, 16774, 16775, 16776, 16777, 16778, 16779, 16780, 16781, 16782, 16783, 16784, 16785, 16786, 16787, 16788, 16789, 16790, 16791, 16792, 16793, 16794, 16795, 16796, 16797, 16798, 16799, 16800, 16801, 16802, 16803, 16804, 16805, 16806, 16807, 16808, 16809, 16810, 16811, 16812, 16813, 16814, 16815, 16816, 16817, 16818, 16819, 16820, 16821, 16822, 16823, 16824, 16825, 16826, 16827, 16828, 16829, 16830, 16831, 16832, 16833, 16834, 16835, 16836, 16837, 16838, 16839, 16840, 16841, 16842, 16843, 16844, 16845, 16846, 16847, 16848, 16849, 16850, 16851, 16852, 16853, 16854, 16855, 16856, 16857, 16858, 16859, 16860, 16861, 16862, 16863, 16864, 16865, 16866, 16867, 16868, 16869, 16870, 16871, 16872, 16873, 16874, 16875, 16876, 16877, 16878, 16879, 16880, 16881, 16882, 16883, 16884, 16885, 16886, 16887, 16888, 16889, 16890, 16891, 16892, 16893, 16894, 16895, 16896, 16897, 16898, 16899, 16900, 16901, 16902, 16903, 16904, 16905, 16906, 16907, 16908, 16909, 16910, 16911, 16912, 16913, 16914, 16915, 16916, 16917, 16918, 16919, 16920, 16921, 16922, 16923, 16924, 16925, 16926, 16927, 16928, 16929, 16930, 16931, 16932, 16933, 16934, 16935, 16936, 16937, 16938, 16939, 16940, 16941, 16942, 16943, 16944, 16945, 16946, 16947, 16948, 16949, 16950, 16951, 16952, 16953, 16954, 16955, 16956, 16957, 16958, 16959, 16960, 16961, 16962, 16963, 16964, 16965, 16966, 16967, 16968, 16969, 16970, 16971, 16972, 16973, 16974, 16975, 16976, 16977, 16978, 16979, 16980, 16981, 16982, 16983, 16984, 16985, 16986, 16987, 16988, 16989, 16990, 16991, 16992, 16993, 16994, 16995, 16996, 16997, 16998, 16999, 17000, 17001, 17002, 17003, 17004, 17005, 17006, 17007, 17008, 17009, 17010, 17011, 17012, 17013, 17014, 17015, 17016, 17017, 17018, 17019, 17020, 17021, 17022, 17023, 17024, 17025, 17026, 17027, 17028, 17029, 17030, 17031, 17032, 17033, 17034, 17035, 17036, 17037, 17038, 17039, 17040, 17041, 17042, 17043, 17044, 17045, 17046, 17047, 17048, 17049, 17050, 17051, 17052, 17053, 17054, 17055, 17056, 17057, 17058, 17059, 17060, 17061, 17062, 17063, 17064, 17065, 17066, 17067, 17068, 17069, 17070, 17071, 17072, 17073, 17074, 17075, 17076, 17077, 17078, 17079, 17080, 17081, 17082, 17083, 17084, 17085, 17086, 17087, 17088, 17089, 17090, 17091, 17092, 17093, 17094, 17095, 17096, 17097, 17098, 17099, 17100, 17101, 17102, 17103, 17104, 17105, 17106, 17107, 17108, 17109, 17110, 17111, 17112, 17113, 17114, 17115, 17116, 17117, 17118, 17119, 17120, 17121, 17122, 17123, 17124, 17125, 17126, 17127, 17128, 17129, 17130, 17131, 17132, 17133, 17134, 17135, 17136, 17137, 17138, 17139, 17140, 17141, 17142, 17143, 17144, 17145, 17146, 17147, 17148, 17149, 17150, 17151, 17152, 17153, 17154, 17155, 17156, 17157, 17158, 17159, 17160, 17161, 17162, 17163, 17164, 17165, 17166, 17167, 17168, 17169, 17170, 17171, 17172, 17173, 17174, 17175, 17176, 17177, 17178, 17179, 17180, 17181, 17182, 17183, 17184, 17185, 17186, 17187, 17188, 17189, 17190, 17191, 17192, 17193, 17194, 17195, 17196, 17197, 17198, 17199, 17200, 17201, 17202, 17203, 17204, 17205, 17206, 17207, 17208, 17209, 17210, 17211, 17212, 17213, 17214, 17215, 17216, 17217, 17218, 17219, 17220, 17221, 17222, 17223, 17224, 17225, 17226, 17227, 17228, 17229, 17230, 17231, 17232, 17233, 17234, 17235, 17236, 17237, 17238, 17239, 17240, 17241, 17242, 17243, 17244, 17245, 17246, 17247, 17248, 17249, 17250, 17251, 17252, 17253, 17254, 17255, 17256, 17257, 17258, 17259, 17260, 17261, 17262, 17263, 17264, 17265, 17266, 17267, 17268, 17269, 17270, 17271, 17272, 17273, 17274, 17275, 17276, 17277, 17278, 17279, 17280, 17281, 17282, 17283, 17284, 17285, 17286, 17287, 17288, 17289, 17290, 17291, 17292, 17293, 17294, 17295, 17296, 17297, 17298, 17299, 17300, 17301, 17302, 17303, 17304, 17305, 17306, 17307, 17308, 17309, 17310, 17311, 17312, 17313, 17314, 17315, 17316, 17317, 17318, 17319, 17320, 17321, 17322, 17323, 17324, 17325, 17326, 17327, 17328, 17329, 17330, 17331, 17332, 17333, 17334, 17335, 17336, 17337, 17338, 17339, 17340, 17341, 17342, 17343, 17344, 17345, 17346, 17347, 17348, 17349, 17350, 17351, 17352, 17353, 17354, 17355, 17356, 17357, 17358, 17359, 17360, 17361, 17362, 17363, 17364, 17365, 17366, 17367, 17368, 17369, 17370, 17371, 17372, 17373, 17374, 17375, 17376, 17377, 17378, 17379, 17380, 17381, 17382, 17383, 17384, 17385, 17386, 17387, 17388, 17389, 17390, 17391, 17392, 17393, 17394, 17395, 17396, 17397, 17398, 17399, 17400, 17401, 17402, 17403, 17404, 17405, 17406, 17407, 17408, 17409, 17410, 17411, 17412, 17413, 17414, 17415, 17416, 17417, 17418, 17419, 17420, 17421, 17422, 17423, 17424, 17425, 17426, 17427, 17428, 17429, 17430, 17431, 17432, 17433, 17434, 17435, 17436, 17437, 17438, 17439, 17440, 17441, 17442, 17443, 17444, 17445, 17446, 17447, 17448, 17449, 17450, 17451, 17452, 17453, 17454, 17455, 17456, 17457, 17458, 17459, 17460, 17461, 17462, 17463, 17464, 17465, 17466, 17467, 17468, 17469, 17470, 17471, 17472, 17473, 17474, 17475, 17476, 17477, 17478, 17479, 17480, 17481, 17482, 17483, 17484, 17485, 17486, 17487, 17488, 17489, 17490, 17491, 17492, 17493, 17494, 17495, 17496, 17497, 17498, 17499, 17500, 17501, 17502, 17503, 17504, 17505, 17506, 17507, 17508, 17509, 17510, 17511, 17512, 17513, 17514, 17515, 17516, 17517, 17518, 17519, 17520, 17521, 17522, 17523, 17524, 17525, 17526, 17527, 17528, 17529, 17530, 17531, 17532, 17533, 17534, 17535, 17536, 17537, 17538, 17539, 17540, 17541, 17542, 17543, 17544, 17545, 17546, 17547, 17548, 17549, 17550, 17551, 17552, 17553, 17554, 17555, 17556, 17557, 17558, 17559, 17560, 17561, 17562, 17563, 17564, 17565, 17566, 17567, 17568, 17569, 17570, 17571, 17572, 17573, 17574, 17575, 17576, 17577, 17578, 17579, 17580, 17581, 17582, 17583, 17584, 17585, 17586, 17587, 17588, 17589, 17590, 17591, 17592, 17593, 17594, 17595, 17596, 17597, 17598, 17599, 17600, 17601, 17602, 17603, 17604, 17605, 17606, 17607, 17608, 17609, 17610, 17611, 17612, 17613, 17614, 17615, 17616, 17617, 17618, 17619, 17620, 17621, 17622, 17623, 17624, 17625, 17626, 17627, 17628, 17629, 17630, 17631, 17632, 17633, 17634, 17635, 17636, 17637, 17638, 17639, 17640, 17641, 17642, 17643, 17644, 17645, 17646, 17647, 17648, 17649, 17650, 17651, 17652, 17653, 17654, 17655, 17656, 17657, 17658, 17659, 17660, 17661, 17662, 17663, 17664, 17665, 17666, 17667, 17668, 17669, 17670, 17671, 17672, 17673, 17674, 17675, 17676, 17677, 17678, 17679, 17680, 17681, 17682, 17683, 17684, 17685, 17686, 17687, 17688, 17689, 17690, 17691, 17692, 17693, 17694, 17695, 17696, 17697, 17698, 17699, 17700, 17701, 17702, 17703, 17704, 17705, 17706, 17707, 17708, 17709, 17710, 17711, 17712, 17713, 17714, 17715, 17716, 17717, 17718, 17719, 17720, 17721, 17722, 17723, 17724, 17725, 17726, 17727, 17728, 17729, 17730, 17731, 17732, 17733, 17734, 17735, 17736, 17737, 17738, 17739, 17740, 17741, 17742, 17743, 17744, 17745, 17746, 17747, 17748, 17749, 17750, 17751, 17752, 17753, 17754, 17755, 17756, 17757, 17758, 17759, 17760, 17761, 17762, 17763, 17764, 17765, 17766, 17767, 17768, 17769, 17770, 17771, 17772, 17773, 17774, 17775, 17776, 17777, 17778, 17779, 17780, 17781, 17782, 17783, 17784, 17785, 17786, 17787, 17788, 17789, 17790, 17791, 17792, 17793, 17794, 17795, 17796, 17797, 17798, 17799, 17800, 17801, 17802, 17803, 17804, 17805, 17806, 17807, 17808, 17809, 17810, 17811, 17812, 17813, 17814, 17815, 17816, 17817, 17818, 17819, 17820, 17821, 17822, 17823, 17824, 17825, 17826, 17827, 17828, 17829, 17830, 17831, 17832, 17833, 17834, 17835, 17836, 17837, 17838, 17839, 17840, 17841, 17842, 17843, 17844, 17845, 17846, 17847, 17848, 17849, 17850, 17851, 17852, 17853, 17854, 17855, 17856, 17857, 17858, 17859, 17860, 17861, 17862, 17863, 17864, 17865, 17866, 17867, 17868, 17869, 17870, 17871, 17872, 17873, 17874, 17875, 17876, 17877, 17878, 17879, 17880, 17881, 17882, 17883, 17884, 17885, 17886, 17887, 17888, 17889, 17890, 17891, 17892, 17893, 17894, 17895, 17896, 17897, 17898, 17899, 17900, 17901, 17902, 17903, 17904, 17905, 17906, 17907, 17908, 17909, 17910, 17911, 17912, 17913, 17914, 17915, 17916, 17917, 17918, 17919, 17920, 17921, 17922, 17923, 17924, 17925, 17926, 17927, 17928, 17929, 17930, 17931, 17932, 17933, 17934, 17935, 17936, 17937, 17938, 17939, 17940, 17941, 17942, 17943, 17944, 17945, 17946, 17947, 17948, 17949, 17950, 17951, 17952, 17953, 17954, 17955, 17956, 17957, 17958, 17959, 17960, 17961, 17962, 17963, 17964, 17965, 17966, 17967, 17968, 17969, 17970, 17971, 17972, 17973, 17974, 17975, 17976, 17977, 17978, 17979, 17980, 17981, 17982, 17983, 17984, 17985, 17986, 17987, 17988, 17989, 17990, 17991, 17992, 17993, 17994, 17995, 17996, 17997, 17998, 17999, 18000, 18001, 18002, 18003, 18004, 18005, 18006, 18007, 18008, 18009, 18010, 18011, 18012, 18013, 18014, 18015, 18016, 18017, 18018, 18019, 18020, 18021, 18022, 18023, 18024, 18025, 18026, 18027, 18028, 18029, 18030, 18031, 18032, 18033, 18034, 18035, 18036, 18037, 18038, 18039, 18040, 18041, 18042, 18043, 18044, 18045, 18046, 18047, 18048, 18049, 18050, 18051, 18052, 18053, 18054, 18055, 18056, 18057, 18058, 18059, 18060, 18061, 18062, 18063, 18064, 18065, 18066, 18067, 18068, 18069, 18070, 18071, 18072, 18073, 18074, 18075, 18076, 18077, 18078, 18079, 18080, 18081, 18082, 18083, 18084, 18085, 18086, 18087, 18088, 18089, 18090, 18091, 18092, 18093, 18094, 18095, 18096, 18097, 18098, 18099, 18100, 18101, 18102, 18103, 18104, 18105, 18106, 18107, 18108, 18109, 18110, 18111, 18112, 18113, 18114, 18115, 18116, 18117, 18118, 18119, 18120, 18121, 18122, 18123, 18124, 18125, 18126, 18127, 18128, 18129, 18130, 18131, 18132, 18133, 18134, 18135, 18136, 18137, 18138, 18139, 18140, 18141, 18142, 18143, 18144, 18145, 18146, 18147, 18148, 18149, 18150, 18151, 18152, 18153, 18154, 18155, 18156, 18157, 18158, 18159, 18160, 18161, 18162, 18163, 18164, 18165, 18166, 18167, 18168, 18169, 18170, 18171, 18172, 18173, 18174, 18175, 18176, 18177, 18178, 18179, 18180, 18181, 18182, 18183, 18184, 18185, 18186, 18187, 18188, 18189, 18190, 18191, 18192, 18193, 18194, 18195, 18196, 18197, 18198, 18199, 18200, 18201, 18202, 18203, 18204, 18205, 18206, 18207, 18208, 18209, 18210, 18211, 18212, 18213, 18214, 18215, 18216, 18217, 18218, 18219, 18220, 18221, 18222, 18223, 18224, 18225, 18226, 18227, 18228, 18229, 18230, 18231, 18232, 18233, 18234, 18235, 18236, 18237, 18238, 18239, 18240, 18241, 18242, 18243, 18244, 18245, 18246, 18247, 18248, 18249, 18250, 18251, 18252, 18253, 18254, 18255, 18256, 18257, 18258, 18259, 18260, 18261, 18262, 18263, 18264, 18265, 18266, 18267, 18268, 18269, 18270, 18271, 18272, 18273, 18274, 18275, 18276, 18277, 18278, 18279, 18280, 18281, 18282, 18283, 18284, 18285, 18286, 18287, 18288, 18289, 18290, 18291, 18292, 18293, 18294, 18295, 18296, 18297, 18298, 18299, 18300, 18301, 18302, 18303, 18304, 18305, 18306, 18307, 18308, 18309, 18310, 18311, 18312, 18313, 18314, 18315, 18316, 18317, 18318, 18319, 18320, 18321, 18322, 18323, 18324, 18325, 18326, 18327, 18328, 18329, 18330, 18331, 18332, 18333, 18334, 18335, 18336, 18337, 18338, 18339, 18340, 18341, 18342, 18343, 18344, 18345, 18346, 18347, 18348, 18349, 18350, 18351, 18352, 18353, 18354, 18355, 18356, 18357, 18358, 18359, 18360, 18361, 18362, 18363, 18364, 18365, 18366, 18367, 18368, 18369, 18370, 18371, 18372, 18373, 18374, 18375, 18376, 18377, 18378, 18379, 18380, 18381, 18382, 18383, 18384, 18385, 18386, 18387, 18388, 18389, 18390, 18391, 18392, 18393, 18394, 18395, 18396, 18397, 18398, 18399, 18400, 18401, 18402, 18403, 18404, 18405, 18406, 18407, 18408, 18409, 18410, 18411, 18412, 18413, 18414, 18415, 18416, 18417, 18418, 18419, 18420, 18421, 18422, 18423, 18424, 18425, 18426, 18427, 18428, 18429, 18430, 18431, 18432, 18433, 18434, 18435, 18436, 18437, 18438, 18439, 18440, 18441, 18442, 18443, 18444, 18445, 18446, 18447, 18448, 18449, 18450, 18451, 18452, 18453, 18454, 18455, 18456, 18457, 18458, 18459, 18460, 18461, 18462, 18463, 18464, 18465, 18466, 18467, 18468, 18469, 18470, 18471, 18472, 18473, 18474, 18475, 18476, 18477, 18478, 18479, 18480, 18481, 18482, 18483, 18484, 18485, 18486, 18487, 18488, 18489, 18490, 18491, 18492, 18493, 18494, 18495, 18496, 18497, 18498, 18499, 18500, 18501, 18502, 18503, 18504, 18505, 18506, 18507, 18508, 18509, 18510, 18511, 18512, 18513, 18514, 18515, 18516, 18517, 18518, 18519, 18520, 18521, 18522, 18523, 18524, 18525, 18526, 18527, 18528, 18529, 18530, 18531, 18532, 18533, 18534, 18535, 18536, 18537, 18538, 18539, 18540, 18541, 18542, 18543, 18544, 18545, 18546, 18547, 18548, 18549, 18550, 18551, 18552, 18553, 18554, 18555, 18556, 18557, 18558, 18559, 18560, 18561, 18562, 18563, 18564, 18565, 18566, 18567, 18568, 18569, 18570, 18571, 18572, 18573, 18574, 18575, 18576, 18577, 18578, 18579, 18580, 18581, 18582, 18583, 18584, 18585, 18586, 18587, 18588, 18589, 18590, 18591, 18592, 18593, 18594, 18595, 18596, 18597, 18598, 18599, 18600, 18601, 18602, 18603, 18604, 18605, 18606, 18607, 18608, 18609, 18610, 18611, 18612, 18613, 18614, 18615, 18616, 18617, 18618, 18619, 18620, 18621, 18622, 18623, 18624, 18625, 18626, 18627, 18628, 18629, 18630, 18631, 18632, 18633, 18634, 18635, 18636, 18637, 18638, 18639, 18640, 18641, 18642, 18643, 18644, 18645, 18646, 18647, 18648, 18649, 18650, 18651, 18652, 18653, 18654, 18655, 18656, 18657, 18658, 18659, 18660, 18661, 18662, 18663, 18664, 18665, 18666, 18667, 18668, 18669, 18670, 18671, 18672, 18673, 18674, 18675, 18676, 18677, 18678, 18679, 18680, 18681, 18682, 18683, 18684, 18685, 18686, 18687, 18688, 18689, 18690, 18691, 18692, 18693, 18694, 18695, 18696, 18697, 18698, 18699, 18700, 18701, 18702, 18703, 18704, 18705, 18706, 18707, 18708, 18709, 18710, 18711, 18712, 18713, 18714, 18715, 18716, 18717, 18718, 18719, 18720, 18721, 18722, 18723, 18724, 18725, 18726, 18727, 18728, 18729, 18730, 18731, 18732, 18733, 18734, 18735, 18736, 18737, 18738, 18739, 18740, 18741, 18742, 18743, 18744, 18745, 18746, 18747, 18748, 18749, 18750, 18751, 18752, 18753, 18754, 18755, 18756, 18757, 18758, 18759, 18760, 18761, 18762, 18763, 18764, 18765, 18766, 18767, 18768, 18769, 18770, 18771, 18772, 18773, 18774, 18775, 18776, 18777, 18778, 18779, 18780, 18781, 18782, 18783, 18784, 18785, 18786, 18787, 18788, 18789, 18790, 18791, 18792, 18793, 18794, 18795, 18796, 18797, 18798, 18799, 18800, 18801, 18802, 18803, 18804, 18805, 18806, 18807, 18808, 18809, 18810, 18811, 18812, 18813, 18814, 18815, 18816, 18817, 18818, 18819, 18820, 18821, 18822, 18823, 18824, 18825, 18826, 18827, 18828, 18829, 18830, 18831, 18832, 18833, 18834, 18835, 18836, 18837, 18838, 18839, 18840, 18841, 18842, 18843, 18844, 18845, 18846, 18847, 18848, 18849, 18850, 18851, 18852, 18853, 18854, 18855, 18856, 18857, 18858, 18859, 18860, 18861, 18862, 18863, 18864, 18865, 18866, 18867, 18868, 18869, 18870, 18871, 18872, 18873, 18874, 18875, 18876, 18877, 18878, 18879, 18880, 18881, 18882, 18883, 18884, 18885, 18886, 18887, 18888, 18889, 18890, 18891, 18892, 18893, 18894, 18895, 18896, 18897, 18898, 18899, 18900, 18901, 18902, 18903, 18904, 18905, 18906, 18907, 18908, 18909, 18910, 18911, 18912, 18913, 18914, 18915, 18916, 18917, 18918, 18919, 18920, 18921, 18922, 18923, 18924, 18925, 18926, 18927, 18928, 18929, 18930, 18931, 18932, 18933, 18934, 18935, 18936, 18937, 18938, 18939, 18940, 18941, 18942, 18943, 18944, 18945, 18946, 18947, 18948, 18949, 18950, 18951, 18952, 18953, 18954, 18955, 18956, 18957, 18958, 18959, 18960, 18961, 18962, 18963, 18964, 18965, 18966, 18967, 18968, 18969, 18970, 18971, 18972, 18973, 18974, 18975, 18976, 18977, 18978, 18979, 18980, 18981, 18982, 18983, 18984, 18985, 18986, 18987, 18988, 18989, 18990, 18991, 18992, 18993, 18994, 18995, 18996, 18997, 18998, 18999, 19000, 19001, 19002, 19003, 19004, 19005, 19006, 19007, 19008, 19009, 19010, 19011, 19012, 19013, 19014, 19015, 19016, 19017, 19018, 19019, 19020, 19021, 19022, 19023, 19024, 19025, 19026, 19027, 19028, 19029, 19030, 19031, 19032, 19033, 19034, 19035, 19036, 19037, 19038, 19039, 19040, 19041, 19042, 19043, 19044, 19045, 19046, 19047, 19048, 19049, 19050, 19051, 19052, 19053, 19054, 19055, 19056, 19057, 19058, 19059, 19060, 19061, 19062, 19063, 19064, 19065, 19066, 19067, 19068, 19069, 19070, 19071, 19072, 19073, 19074, 19075, 19076, 19077, 19078, 19079, 19080, 19081, 19082, 19083, 19084, 19085, 19086, 19087, 19088, 19089, 19090, 19091, 19092, 19093, 19094, 19095, 19096, 19097, 19098, 19099, 19100, 19101, 19102, 19103, 19104, 19105, 19106, 19107, 19108, 19109, 19110, 19111, 19112, 19113, 19114, 19115, 19116, 19117, 19118, 19119, 19120, 19121, 19122, 19123, 19124, 19125, 19126, 19127, 19128, 19129, 19130, 19131, 19132, 19133, 19134, 19135, 19136, 19137, 19138, 19139, 19140, 19141, 19142, 19143, 19144, 19145, 19146, 19147, 19148, 19149, 19150, 19151, 19152, 19153, 19154, 19155, 19156, 19157, 19158, 19159, 19160, 19161, 19162, 19163, 19164, 19165, 19166, 19167, 19168, 19169, 19170, 19171, 19172, 19173, 19174, 19175, 19176, 19177, 19178, 19179, 19180, 19181, 19182, 19183, 19184, 19185, 19186, 19187, 19188, 19189, 19190, 19191, 19192, 19193, 19194, 19195, 19196, 19197, 19198, 19199, 19200, 19201, 19202, 19203, 19204, 19205, 19206, 19207, 19208, 19209, 19210, 19211, 19212, 19213, 19214, 19215, 19216, 19217, 19218, 19219, 19220, 19221, 19222, 19223, 19224, 19225, 19226, 19227, 19228, 19229, 19230, 19231, 19232, 19233, 19234, 19235, 19236, 19237, 19238, 19239, 19240, 19241, 19242, 19243, 19244, 19245, 19246, 19247, 19248, 19249, 19250, 19251, 19252, 19253, 19254, 19255, 19256, 19257, 19258, 19259, 19260, 19261, 19262, 19263, 19264, 19265, 19266, 19267, 19268, 19269, 19270, 19271, 19272, 19273, 19274, 19275, 19276, 19277, 19278, 19279, 19280, 19281, 19282, 19283, 19284, 19285, 19286, 19287, 19288, 19289, 19290, 19291, 19292, 19293, 19294, 19295, 19296, 19297, 19298, 19299, 19300, 19301, 19302, 19303, 19304, 19305, 19306, 19307, 19308, 19309, 19310, 19311, 19312, 19313, 19314, 19315, 19316, 19317, 19318, 19319, 19320, 19321, 19322, 19323, 19324, 19325, 19326, 19327, 19328, 19329, 19330, 19331, 19332, 19333, 19334, 19335, 19336, 19337, 19338, 19339, 19340, 19341, 19342, 19343, 19344, 19345, 19346, 19347, 19348, 19349, 19350, 19351, 19352, 19353, 19354, 19355, 19356, 19357, 19358, 19359, 19360, 19361, 19362, 19363, 19364, 19365, 19366, 19367, 19368, 19369, 19370, 19371, 19372, 19373, 19374, 19375, 19376, 19377, 19378, 19379, 19380, 19381, 19382, 19383, 19384, 19385, 19386, 19387, 19388, 19389, 19390, 19391, 19392, 19393, 19394, 19395, 19396, 19397, 19398, 19399, 19400, 19401, 19402, 19403, 19404, 19405, 19406, 19407, 19408, 19409, 19410, 19411, 19412, 19413, 19414, 19415, 19416, 19417, 19418, 19419, 19420, 19421, 19422, 19423, 19424, 19425, 19426, 19427, 19428, 19429, 19430, 19431, 19432, 19433, 19434, 19435, 19436, 19437, 19438, 19439, 19440, 19441, 19442, 19443, 19444, 19445, 19446, 19447, 19448, 19449, 19450, 19451, 19452, 19453, 19454, 19455, 19456, 19457, 19458, 19459, 19460, 19461, 19462, 19463, 19464, 19465, 19466, 19467, 19468, 19469, 19470, 19471, 19472, 19473, 19474, 19475, 19476, 19477, 19478, 19479, 19480, 19481, 19482, 19483, 19484, 19485, 19486, 19487, 19488, 19489, 19490, 19491, 19492, 19493, 19494, 19495, 19496, 19497, 19498, 19499, 19500, 19501, 19502, 19503, 19504, 19505, 19506, 19507, 19508, 19509, 19510, 19511, 19512, 19513, 19514, 19515, 19516, 19517, 19518, 19519, 19520, 19521, 19522, 19523, 19524, 19525, 19526, 19527, 19528, 19529, 19530, 19531, 19532, 19533, 19534, 19535, 19536, 19537, 19538, 19539, 19540, 19541, 19542, 19543, 19544, 19545, 19546, 19547, 19548, 19549, 19550, 19551, 19552, 19553, 19554, 19555, 19556, 19557, 19558, 19559, 19560, 19561, 19562, 19563, 19564, 19565, 19566, 19567, 19568, 19569, 19570, 19571, 19572, 19573, 19574, 19575, 19576, 19577, 19578, 19579, 19580, 19581, 19582, 19583, 19584, 19585, 19586, 19587, 19588, 19589, 19590, 19591, 19592, 19593, 19594, 19595, 19596, 19597, 19598, 19599, 19600, 19601, 19602, 19603, 19604, 19605, 19606, 19607, 19608, 19609, 19610, 19611, 19612, 19613, 19614, 19615, 19616, 19617, 19618, 19619, 19620, 19621, 19622, 19623, 19624, 19625, 19626, 19627, 19628, 19629, 19630, 19631, 19632, 19633, 19634, 19635, 19636, 19637, 19638, 19639, 19640, 19641, 19642, 19643, 19644, 19645, 19646, 19647, 19648, 19649, 19650, 19651, 19652, 19653, 19654, 19655, 19656, 19657, 19658, 19659, 19660, 19661, 19662, 19663, 19664, 19665, 19666, 19667, 19668, 19669, 19670, 19671, 19672, 19673, 19674, 19675, 19676, 19677, 19678, 19679, 19680, 19681, 19682, 19683, 19684, 19685, 19686, 19687, 19688, 19689, 19690, 19691, 19692, 19693, 19694, 19695, 19696, 19697, 19698, 19699, 19700, 19701, 19702, 19703, 19704, 19705, 19706, 19707, 19708, 19709, 19710, 19711, 19712, 19713, 19714, 19715, 19716, 19717, 19718, 19719, 19720, 19721, 19722, 19723, 19724, 19725, 19726, 19727, 19728, 19729, 19730, 19731, 19732, 19733, 19734, 19735, 19736, 19737, 19738, 19739, 19740, 19741, 19742, 19743, 19744, 19745, 19746, 19747, 19748, 19749, 19750, 19751, 19752, 19753, 19754, 19755, 19756, 19757, 19758, 19759, 19760, 19761, 19762, 19763, 19764, 19765, 19766, 19767, 19768, 19769, 19770, 19771, 19772, 19773, 19774, 19775, 19776, 19777, 19778, 19779, 19780, 19781, 19782, 19783, 19784, 19785, 19786, 19787, 19788, 19789, 19790, 19791, 19792, 19793, 19794, 19795, 19796, 19797, 19798, 19799, 19800, 19801, 19802, 19803, 19804, 19805, 19806, 19807, 19808, 19809, 19810, 19811, 19812, 19813, 19814, 19815, 19816, 19817, 19818, 19819, 19820, 19821, 19822, 19823, 19824, 19825, 19826, 19827, 19828, 19829, 19830, 19831, 19832, 19833, 19834, 19835, 19836, 19837, 19838, 19839, 19840, 19841, 19842, 19843, 19844, 19845, 19846, 19847, 19848, 19849, 19850, 19851, 19852, 19853, 19854, 19855, 19856, 19857, 19858, 19859, 19860, 19861, 19862, 19863, 19864, 19865, 19866, 19867, 19868, 19869, 19870, 19871, 19872, 19873, 19874, 19875, 19876, 19877, 19878, 19879, 19880, 19881, 19882, 19883, 19884, 19885, 19886, 19887, 19888, 19889, 19890, 19891, 19892, 19893, 19894, 19895, 19896, 19897, 19898, 19899, 19900, 19901, 19902, 19903, 19904, 19905, 19906, 19907, 19908, 19909, 19910, 19911, 19912, 19913, 19914, 19915, 19916, 19917, 19918, 19919, 19920, 19921, 19922, 19923, 19924, 19925, 19926, 19927, 19928, 19929, 19930, 19931, 19932, 19933, 19934, 19935, 19936, 19937, 19938, 19939, 19940, 19941, 19942, 19943, 19944, 19945, 19946, 19947, 19948, 19949, 19950, 19951, 19952, 19953, 19954, 19955, 19956, 19957, 19958, 19959, 19960, 19961, 19962, 19963, 19964, 19965, 19966, 19967, 19968, 19969, 19970, 19971, 19972, 19973, 19974, 19975, 19976, 19977, 19978, 19979, 19980, 19981, 19982, 19983, 19984, 19985, 19986, 19987, 19988, 19989, 19990, 19991, 19992, 19993, 19994, 19995, 19996, 19997, 19998, 19999, 20000, 20001, 20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012, 20013, 20014, 20015, 20016, 20017, 20018, 20019, 20020, 20021, 20022, 20023, 20024, 20025, 20026, 20027, 20028, 20029, 20030, 20031, 20032, 20033, 20034, 20035, 20036, 20037, 20038, 20039, 20040, 20041, 20042, 20043, 20044, 20045, 20046, 20047, 20048, 20049, 20050, 20051, 20052, 20053, 20054, 20055, 20056, 20057, 20058, 20059, 20060, 20061, 20062, 20063, 20064, 20065, 20066, 20067, 20068, 20069, 20070, 20071, 20072, 20073, 20074, 20075, 20076, 20077, 20078, 20079, 20080, 20081, 20082, 20083, 20084, 20085, 20086, 20087, 20088, 20089, 20090, 20091, 20092, 20093, 20094, 20095, 20096, 20097, 20098, 20099, 20100, 20101, 20102, 20103, 20104, 20105, 20106, 20107, 20108, 20109, 20110, 20111, 20112, 20113, 20114, 20115, 20116, 20117, 20118, 20119, 20120, 20121, 20122, 20123, 20124, 20125, 20126, 20127, 20128, 20129, 20130, 20131, 20132, 20133, 20134, 20135, 20136, 20137, 20138, 20139, 20140, 20141, 20142, 20143, 20144, 20145, 20146, 20147, 20148, 20149, 20150, 20151, 20152, 20153, 20154, 20155, 20156, 20157, 20158, 20159, 20160, 20161, 20162, 20163, 20164, 20165, 20166, 20167, 20168, 20169, 20170, 20171, 20172, 20173, 20174, 20175, 20176, 20177, 20178, 20179, 20180, 20181, 20182, 20183, 20184, 20185, 20186, 20187, 20188, 20189, 20190, 20191, 20192, 20193, 20194, 20195, 20196, 20197, 20198, 20199, 20200, 20201, 20202, 20203, 20204, 20205, 20206, 20207, 20208, 20209, 20210, 20211, 20212, 20213, 20214, 20215, 20216, 20217, 20218, 20219, 20220, 20221, 20222, 20223, 20224, 20225, 20226, 20227, 20228, 20229, 20230, 20231, 20232, 20233, 20234, 20235, 20236, 20237, 20238, 20239, 20240, 20241, 20242, 20243, 20244, 20245, 20246, 20247, 20248, 20249, 20250, 20251, 20252, 20253, 20254, 20255, 20256, 20257, 20258, 20259, 20260, 20261, 20262, 20263, 20264, 20265, 20266, 20267, 20268, 20269, 20270, 20271, 20272, 20273, 20274, 20275, 20276, 20277, 20278, 20279, 20280, 20281, 20282, 20283, 20284, 20285, 20286, 20287, 20288, 20289, 20290, 20291, 20292, 20293, 20294, 20295, 20296, 20297, 20298, 20299, 20300, 20301, 20302, 20303, 20304, 20305, 20306, 20307, 20308, 20309, 20310, 20311, 20312, 20313, 20314, 20315, 20316, 20317, 20318, 20319, 20320, 20321, 20322, 20323, 20324, 20325, 20326, 20327, 20328, 20329, 20330, 20331, 20332, 20333, 20334, 20335, 20336, 20337, 20338, 20339, 20340, 20341, 20342, 20343, 20344, 20345, 20346, 20347, 20348, 20349, 20350, 20351, 20352, 20353, 20354, 20355, 20356, 20357, 20358, 20359, 20360, 20361, 20362, 20363, 20364, 20365, 20366, 20367, 20368, 20369, 20370, 20371, 20372, 20373, 20374, 20375, 20376, 20377, 20378, 20379, 20380, 20381, 20382, 20383, 20384, 20385, 20386, 20387, 20388, 20389, 20390, 20391, 20392, 20393, 20394, 20395, 20396, 20397]\n"," This is the range of val:  [ 7700  7701  7702 ... 12097 12098 12099]\n","Starting training\n","shuffling\n","Epoch 1/350\n","2020-06-10 16:51:44.878793: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 16:51:45.101846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","500/500 [==============================] - 35s 71ms/step - loss: 35.1327 - val_loss: 13.5074\n","\n","Epoch 00001: loss improved from inf to 35.12882, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/350\n","500/500 [==============================] - 32s 63ms/step - loss: 26.5933 - val_loss: 24.1517\n","\n","Epoch 00002: loss improved from 35.12882 to 26.59335, saving model to final.h5\n","Epoch 3/350\n","500/500 [==============================] - 32s 63ms/step - loss: 22.2755 - val_loss: 27.7062\n","\n","Epoch 00003: loss improved from 26.59335 to 22.27609, saving model to final.h5\n","Epoch 4/350\n","500/500 [==============================] - 32s 63ms/step - loss: 20.0970 - val_loss: 8.8617\n","\n","Epoch 00004: loss improved from 22.27609 to 20.09665, saving model to final.h5\n","Epoch 5/350\n","500/500 [==============================] - 32s 64ms/step - loss: 18.4734 - val_loss: 23.2631\n","\n","Epoch 00005: loss improved from 20.09665 to 18.47214, saving model to final.h5\n","Epoch 6/350\n","500/500 [==============================] - 31s 62ms/step - loss: 17.7025 - val_loss: 10.5047\n","\n","Epoch 00006: loss improved from 18.47214 to 17.70264, saving model to final.h5\n","Epoch 7/350\n","500/500 [==============================] - 31s 62ms/step - loss: 16.7687 - val_loss: 10.4685\n","\n","Epoch 00007: loss improved from 17.70264 to 16.76846, saving model to final.h5\n","Epoch 8/350\n","500/500 [==============================] - 31s 62ms/step - loss: 16.0161 - val_loss: 7.4206\n","\n","Epoch 00008: loss improved from 16.76846 to 16.01601, saving model to final.h5\n","Epoch 9/350\n","500/500 [==============================] - 31s 62ms/step - loss: 15.6480 - val_loss: 7.0335\n","\n","Epoch 00009: loss improved from 16.01601 to 15.64845, saving model to final.h5\n","Epoch 10/350\n","500/500 [==============================] - 31s 62ms/step - loss: 15.1068 - val_loss: 10.7101\n","\n","Epoch 00010: loss improved from 15.64845 to 15.10787, saving model to final.h5\n","Epoch 11/350\n","500/500 [==============================] - 31s 62ms/step - loss: 14.2281 - val_loss: 9.4088\n","\n","Epoch 00011: loss improved from 15.10787 to 14.22794, saving model to final.h5\n","Epoch 12/350\n","500/500 [==============================] - 31s 62ms/step - loss: 14.2009 - val_loss: 8.1381\n","\n","Epoch 00012: loss improved from 14.22794 to 14.20080, saving model to final.h5\n","Epoch 13/350\n","500/500 [==============================] - 31s 62ms/step - loss: 13.7514 - val_loss: 15.7034\n","\n","Epoch 00013: loss improved from 14.20080 to 13.75203, saving model to final.h5\n","Epoch 14/350\n","500/500 [==============================] - 31s 62ms/step - loss: 13.1614 - val_loss: 9.3204\n","\n","Epoch 00014: loss improved from 13.75203 to 13.16140, saving model to final.h5\n","Epoch 15/350\n","500/500 [==============================] - 31s 62ms/step - loss: 13.2332 - val_loss: 11.2617\n","\n","Epoch 00015: loss did not improve from 13.16140\n","Epoch 16/350\n","500/500 [==============================] - 31s 62ms/step - loss: 12.4125 - val_loss: 17.3222\n","\n","Epoch 00016: loss improved from 13.16140 to 12.41297, saving model to final.h5\n","Epoch 17/350\n","500/500 [==============================] - 31s 62ms/step - loss: 12.5390 - val_loss: 15.4310\n","\n","Epoch 00017: loss did not improve from 12.41297\n","Epoch 18/350\n","500/500 [==============================] - 31s 63ms/step - loss: 12.2719 - val_loss: 15.2323\n","\n","Epoch 00018: loss improved from 12.41297 to 12.27251, saving model to final.h5\n","Epoch 19/350\n","500/500 [==============================] - 31s 63ms/step - loss: 11.9370 - val_loss: 13.7906\n","\n","Epoch 00019: loss improved from 12.27251 to 11.93662, saving model to final.h5\n","Epoch 20/350\n","500/500 [==============================] - 31s 63ms/step - loss: 11.8893 - val_loss: 7.2604\n","\n","Epoch 00020: loss improved from 11.93662 to 11.88826, saving model to final.h5\n","Epoch 21/350\n","500/500 [==============================] - 31s 62ms/step - loss: 11.3029 - val_loss: 10.7438\n","\n","Epoch 00021: loss improved from 11.88826 to 11.30288, saving model to final.h5\n","Epoch 22/350\n","500/500 [==============================] - 31s 62ms/step - loss: 11.1738 - val_loss: 8.4164\n","\n","Epoch 00022: loss improved from 11.30288 to 11.17426, saving model to final.h5\n","Epoch 23/350\n","500/500 [==============================] - 31s 62ms/step - loss: 11.1449 - val_loss: 11.0716\n","\n","Epoch 00023: loss improved from 11.17426 to 11.14487, saving model to final.h5\n","Epoch 24/350\n","500/500 [==============================] - 31s 62ms/step - loss: 10.6754 - val_loss: 11.1217\n","\n","Epoch 00024: loss improved from 11.14487 to 10.67567, saving model to final.h5\n","Epoch 25/350\n","500/500 [==============================] - 31s 63ms/step - loss: 10.5153 - val_loss: 14.2867\n","\n","Epoch 00025: loss improved from 10.67567 to 10.51426, saving model to final.h5\n","Epoch 26/350\n","500/500 [==============================] - 31s 62ms/step - loss: 10.2505 - val_loss: 11.3826\n","\n","Epoch 00026: loss improved from 10.51426 to 10.24996, saving model to final.h5\n","Epoch 27/350\n","500/500 [==============================] - 32s 63ms/step - loss: 10.3758 - val_loss: 9.3163\n","\n","Epoch 00027: loss did not improve from 10.24996\n","Epoch 28/350\n","500/500 [==============================] - 32s 63ms/step - loss: 10.2177 - val_loss: 12.8108\n","\n","Epoch 00028: loss improved from 10.24996 to 10.21800, saving model to final.h5\n","Epoch 29/350\n","500/500 [==============================] - 31s 63ms/step - loss: 9.9610 - val_loss: 16.4374\n","\n","Epoch 00029: loss improved from 10.21800 to 9.96103, saving model to final.h5\n","Epoch 30/350\n","500/500 [==============================] - 31s 63ms/step - loss: 9.6965 - val_loss: 24.2500\n","\n","Epoch 00030: loss improved from 9.96103 to 9.69642, saving model to final.h5\n","Epoch 31/350\n","500/500 [==============================] - 31s 63ms/step - loss: 9.9209 - val_loss: 30.8719\n","\n","Epoch 00031: loss did not improve from 9.69642\n","Epoch 32/350\n","500/500 [==============================] - 31s 63ms/step - loss: 9.5048 - val_loss: 9.7094\n","\n","Epoch 00032: loss improved from 9.69642 to 9.50474, saving model to final.h5\n","Epoch 33/350\n","500/500 [==============================] - 32s 63ms/step - loss: 9.3334 - val_loss: 15.5731\n","\n","Epoch 00033: loss improved from 9.50474 to 9.33339, saving model to final.h5\n","Epoch 34/350\n","500/500 [==============================] - 32s 63ms/step - loss: 9.5275 - val_loss: 18.5851\n","\n","Epoch 00034: loss did not improve from 9.33339\n","Epoch 35/350\n","500/500 [==============================] - 32s 63ms/step - loss: 9.4006 - val_loss: 21.3097\n","\n","Epoch 00035: loss did not improve from 9.33339\n","Epoch 36/350\n","500/500 [==============================] - 32s 63ms/step - loss: 9.1887 - val_loss: 19.1281\n","\n","Epoch 00036: loss improved from 9.33339 to 9.18894, saving model to final.h5\n","Epoch 37/350\n","500/500 [==============================] - 31s 63ms/step - loss: 8.9355 - val_loss: 18.4979\n","\n","Epoch 00037: loss improved from 9.18894 to 8.93530, saving model to final.h5\n","Epoch 38/350\n","500/500 [==============================] - 31s 63ms/step - loss: 9.2050 - val_loss: 39.3306\n","\n","Epoch 00038: loss did not improve from 8.93530\n","Epoch 39/350\n","500/500 [==============================] - 31s 63ms/step - loss: 8.8086 - val_loss: 16.9556\n","\n","Epoch 00039: loss improved from 8.93530 to 8.80852, saving model to final.h5\n","Epoch 40/350\n","500/500 [==============================] - 31s 63ms/step - loss: 8.8050 - val_loss: 13.5630\n","\n","Epoch 00040: loss improved from 8.80852 to 8.80534, saving model to final.h5\n","Epoch 41/350\n","500/500 [==============================] - 31s 63ms/step - loss: 9.2048 - val_loss: 9.1355\n","\n","Epoch 00041: loss did not improve from 8.80534\n","Epoch 42/350\n","500/500 [==============================] - 31s 63ms/step - loss: 8.7224 - val_loss: 20.4425\n","\n","Epoch 00042: loss improved from 8.80534 to 8.72258, saving model to final.h5\n","Epoch 43/350\n","500/500 [==============================] - 32s 63ms/step - loss: 8.4081 - val_loss: 38.9766\n","\n","Epoch 00043: loss improved from 8.72258 to 8.40802, saving model to final.h5\n","Epoch 44/350\n","500/500 [==============================] - 32s 63ms/step - loss: 8.6194 - val_loss: 33.5461\n","\n","Epoch 00044: loss did not improve from 8.40802\n","Epoch 45/350\n","500/500 [==============================] - 32s 63ms/step - loss: 8.7317 - val_loss: 30.6461\n","\n","Epoch 00045: loss did not improve from 8.40802\n","Epoch 46/350\n","500/500 [==============================] - 32s 63ms/step - loss: 8.4092 - val_loss: 33.6807\n","\n","Epoch 00046: loss did not improve from 8.40802\n","Epoch 47/350\n","500/500 [==============================] - 31s 63ms/step - loss: 8.4064 - val_loss: 30.0420\n","\n","Epoch 00047: loss improved from 8.40802 to 8.40628, saving model to final.h5\n","Epoch 48/350\n","500/500 [==============================] - 32s 63ms/step - loss: 8.4332 - val_loss: 36.2361\n","\n","Epoch 00048: loss did not improve from 8.40628\n","Epoch 49/350\n","500/500 [==============================] - 32s 63ms/step - loss: 8.2813 - val_loss: 32.9429\n","\n","Epoch 00049: loss improved from 8.40628 to 8.28136, saving model to final.h5\n","Epoch 50/350\n","500/500 [==============================] - 32s 63ms/step - loss: 8.2306 - val_loss: 34.5833\n","\n","Epoch 00050: loss improved from 8.28136 to 8.23023, saving model to final.h5\n","Epoch 51/350\n","500/500 [==============================] - 32s 63ms/step - loss: 8.1021 - val_loss: 14.0971\n","\n","Epoch 00051: loss improved from 8.23023 to 8.10220, saving model to final.h5\n","Epoch 52/350\n","500/500 [==============================] - 32s 63ms/step - loss: 8.1153 - val_loss: 20.1497\n","\n","Epoch 00052: loss did not improve from 8.10220\n","Epoch 53/350\n","500/500 [==============================] - 32s 63ms/step - loss: 7.7974 - val_loss: 26.2166\n","\n","Epoch 00053: loss improved from 8.10220 to 7.79721, saving model to final.h5\n","Epoch 54/350\n","500/500 [==============================] - 32s 63ms/step - loss: 7.7941 - val_loss: 15.7727\n","\n","Epoch 00054: loss improved from 7.79721 to 7.79381, saving model to final.h5\n","Epoch 55/350\n","500/500 [==============================] - 32s 64ms/step - loss: 7.9278 - val_loss: 35.9614\n","\n","Epoch 00055: loss did not improve from 7.79381\n","Epoch 56/350\n","500/500 [==============================] - 31s 63ms/step - loss: 7.8619 - val_loss: 26.6980\n","\n","Epoch 00056: loss did not improve from 7.79381\n","Epoch 57/350\n","500/500 [==============================] - 32s 63ms/step - loss: 7.9089 - val_loss: 25.1848\n","\n","Epoch 00057: loss did not improve from 7.79381\n","Epoch 58/350\n","500/500 [==============================] - 32s 63ms/step - loss: 7.8903 - val_loss: 14.9072\n","\n","Epoch 00058: loss did not improve from 7.79381\n","Epoch 59/350\n","500/500 [==============================] - 31s 63ms/step - loss: 7.6453 - val_loss: 39.5271\n","\n","Epoch 00059: loss improved from 7.79381 to 7.64530, saving model to final.h5\n","Epoch 60/350\n","500/500 [==============================] - 31s 63ms/step - loss: 7.6069 - val_loss: 38.5625\n","\n","Epoch 00060: loss improved from 7.64530 to 7.60712, saving model to final.h5\n","Epoch 61/350\n","500/500 [==============================] - 31s 63ms/step - loss: 7.4415 - val_loss: 23.0923\n","\n","Epoch 00061: loss improved from 7.60712 to 7.44151, saving model to final.h5\n","Epoch 62/350\n","500/500 [==============================] - 31s 62ms/step - loss: 7.4565 - val_loss: 32.5704\n","\n","Epoch 00062: loss did not improve from 7.44151\n","Epoch 63/350\n","500/500 [==============================] - 31s 63ms/step - loss: 7.5435 - val_loss: 30.1929\n","\n","Epoch 00063: loss did not improve from 7.44151\n","Epoch 64/350\n","500/500 [==============================] - 31s 63ms/step - loss: 7.4193 - val_loss: 56.2999\n","\n","Epoch 00064: loss improved from 7.44151 to 7.41915, saving model to final.h5\n","Epoch 65/350\n","500/500 [==============================] - 31s 62ms/step - loss: 7.2550 - val_loss: 20.6403\n","\n","Epoch 00065: loss improved from 7.41915 to 7.25515, saving model to final.h5\n","Epoch 66/350\n","500/500 [==============================] - 31s 62ms/step - loss: 7.4348 - val_loss: 17.7792\n","\n","Epoch 00066: loss did not improve from 7.25515\n","Epoch 67/350\n","500/500 [==============================] - 31s 62ms/step - loss: 7.1807 - val_loss: 28.9462\n","\n","Epoch 00067: loss improved from 7.25515 to 7.18071, saving model to final.h5\n","Epoch 68/350\n","500/500 [==============================] - 31s 62ms/step - loss: 7.1271 - val_loss: 14.7170\n","\n","Epoch 00068: loss improved from 7.18071 to 7.12667, saving model to final.h5\n","Epoch 69/350\n","500/500 [==============================] - 31s 62ms/step - loss: 7.0041 - val_loss: 11.6528\n","\n","Epoch 00069: loss improved from 7.12667 to 7.00398, saving model to final.h5\n","Epoch 70/350\n","500/500 [==============================] - 31s 62ms/step - loss: 7.0886 - val_loss: 14.7600\n","\n","Epoch 00070: loss did not improve from 7.00398\n","Epoch 71/350\n","500/500 [==============================] - 31s 62ms/step - loss: 7.0065 - val_loss: 19.5093\n","\n","Epoch 00071: loss did not improve from 7.00398\n","Epoch 72/350\n","500/500 [==============================] - 31s 62ms/step - loss: 7.1078 - val_loss: 17.7820\n","\n","Epoch 00072: loss did not improve from 7.00398\n","Epoch 73/350\n","500/500 [==============================] - 31s 62ms/step - loss: 6.9388 - val_loss: 19.3664\n","\n","Epoch 00073: loss improved from 7.00398 to 6.93883, saving model to final.h5\n","Epoch 74/350\n","500/500 [==============================] - 31s 63ms/step - loss: 6.9815 - val_loss: 35.6175\n","\n","Epoch 00074: loss did not improve from 6.93883\n","Epoch 75/350\n","500/500 [==============================] - 32s 64ms/step - loss: 6.8603 - val_loss: 15.5405\n","\n","Epoch 00075: loss improved from 6.93883 to 6.86016, saving model to final.h5\n","Epoch 76/350\n","500/500 [==============================] - 31s 62ms/step - loss: 6.9740 - val_loss: 29.5984\n","\n","Epoch 00076: loss did not improve from 6.86016\n","Epoch 77/350\n","500/500 [==============================] - 31s 62ms/step - loss: 6.8893 - val_loss: 24.2409\n","\n","Epoch 00077: loss did not improve from 6.86016\n","Epoch 78/350\n","500/500 [==============================] - 31s 62ms/step - loss: 6.8592 - val_loss: 22.8281\n","\n","Epoch 00078: loss improved from 6.86016 to 6.85885, saving model to final.h5\n","Epoch 79/350\n","500/500 [==============================] - 31s 62ms/step - loss: 6.8479 - val_loss: 16.3389\n","\n","Epoch 00079: loss improved from 6.85885 to 6.84775, saving model to final.h5\n","Epoch 80/350\n","500/500 [==============================] - 31s 62ms/step - loss: 6.8188 - val_loss: 32.6934\n","\n","Epoch 00080: loss improved from 6.84775 to 6.81873, saving model to final.h5\n","Epoch 81/350\n","500/500 [==============================] - 31s 63ms/step - loss: 6.7275 - val_loss: 22.9885\n","\n","Epoch 00081: loss improved from 6.81873 to 6.72780, saving model to final.h5\n","Epoch 82/350\n","500/500 [==============================] - 31s 63ms/step - loss: 6.8351 - val_loss: 22.6194\n","\n","Epoch 00082: loss did not improve from 6.72780\n","Epoch 83/350\n","500/500 [==============================] - 31s 62ms/step - loss: 6.5801 - val_loss: 16.4885\n","\n","Epoch 00083: loss improved from 6.72780 to 6.58020, saving model to final.h5\n","Epoch 84/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.7379 - val_loss: 32.8803\n","\n","Epoch 00084: loss did not improve from 6.58020\n","Epoch 85/350\n","500/500 [==============================] - 31s 63ms/step - loss: 6.6554 - val_loss: 21.1692\n","\n","Epoch 00085: loss did not improve from 6.58020\n","Epoch 86/350\n","500/500 [==============================] - 31s 63ms/step - loss: 6.5677 - val_loss: 15.6906\n","\n","Epoch 00086: loss improved from 6.58020 to 6.56781, saving model to final.h5\n","Epoch 87/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.6472 - val_loss: 19.3620\n","\n","Epoch 00087: loss did not improve from 6.56781\n","Epoch 88/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.5233 - val_loss: 19.7482\n","\n","Epoch 00088: loss improved from 6.56781 to 6.52300, saving model to final.h5\n","Epoch 89/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.5895 - val_loss: 16.8536\n","\n","Epoch 00089: loss did not improve from 6.52300\n","Epoch 90/350\n","500/500 [==============================] - 31s 63ms/step - loss: 6.4915 - val_loss: 11.1053\n","\n","Epoch 00090: loss improved from 6.52300 to 6.49164, saving model to final.h5\n","Epoch 91/350\n","500/500 [==============================] - 31s 63ms/step - loss: 6.2916 - val_loss: 13.2760\n","\n","Epoch 00091: loss improved from 6.49164 to 6.29084, saving model to final.h5\n","Epoch 92/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.4402 - val_loss: 9.5412\n","\n","Epoch 00092: loss did not improve from 6.29084\n","Epoch 93/350\n","500/500 [==============================] - 31s 63ms/step - loss: 6.4808 - val_loss: 15.1652\n","\n","Epoch 00093: loss did not improve from 6.29084\n","Epoch 94/350\n","500/500 [==============================] - 32s 64ms/step - loss: 6.5639 - val_loss: 12.6380\n","\n","Epoch 00094: loss did not improve from 6.29084\n","Epoch 95/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.2509 - val_loss: 18.4331\n","\n","Epoch 00095: loss improved from 6.29084 to 6.25099, saving model to final.h5\n","Epoch 96/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.4072 - val_loss: 20.1054\n","\n","Epoch 00096: loss did not improve from 6.25099\n","Epoch 97/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.2808 - val_loss: 17.5821\n","\n","Epoch 00097: loss did not improve from 6.25099\n","Epoch 98/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.2665 - val_loss: 19.4407\n","\n","Epoch 00098: loss did not improve from 6.25099\n","Epoch 99/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.3387 - val_loss: 6.8409\n","\n","Epoch 00099: loss did not improve from 6.25099\n","Epoch 100/350\n","500/500 [==============================] - 32s 64ms/step - loss: 6.3324 - val_loss: 6.5644\n","\n","Epoch 00100: loss did not improve from 6.25099\n","Epoch 101/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.2394 - val_loss: 20.3634\n","\n","Epoch 00101: loss improved from 6.25099 to 6.23940, saving model to final.h5\n","Epoch 102/350\n","500/500 [==============================] - 32s 64ms/step - loss: 6.2998 - val_loss: 11.2017\n","\n","Epoch 00102: loss did not improve from 6.23940\n","Epoch 103/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.2095 - val_loss: 13.1758\n","\n","Epoch 00103: loss improved from 6.23940 to 6.20939, saving model to final.h5\n","Epoch 104/350\n","500/500 [==============================] - 32s 64ms/step - loss: 6.1254 - val_loss: 11.7555\n","\n","Epoch 00104: loss improved from 6.20939 to 6.12575, saving model to final.h5\n","Epoch 105/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.1679 - val_loss: 14.7018\n","\n","Epoch 00105: loss did not improve from 6.12575\n","Epoch 106/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.1552 - val_loss: 13.9269\n","\n","Epoch 00106: loss did not improve from 6.12575\n","Epoch 107/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.1368 - val_loss: 7.1708\n","\n","Epoch 00107: loss did not improve from 6.12575\n","Epoch 108/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.0698 - val_loss: 30.3694\n","\n","Epoch 00108: loss improved from 6.12575 to 6.06977, saving model to final.h5\n","Epoch 109/350\n","500/500 [==============================] - 32s 64ms/step - loss: 6.1937 - val_loss: 24.4033\n","\n","Epoch 00109: loss did not improve from 6.06977\n","Epoch 110/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.2109 - val_loss: 12.6100\n","\n","Epoch 00110: loss did not improve from 6.06977\n","Epoch 111/350\n","500/500 [==============================] - 32s 64ms/step - loss: 6.1521 - val_loss: 20.2241\n","\n","Epoch 00111: loss did not improve from 6.06977\n","Epoch 112/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.0335 - val_loss: 13.3097\n","\n","Epoch 00112: loss improved from 6.06977 to 6.03361, saving model to final.h5\n","Epoch 113/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.1942 - val_loss: 12.0435\n","\n","Epoch 00113: loss did not improve from 6.03361\n","Epoch 114/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.8021 - val_loss: 8.3643\n","\n","Epoch 00114: loss improved from 6.03361 to 5.80190, saving model to final.h5\n","Epoch 115/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.0616 - val_loss: 13.7029\n","\n","Epoch 00115: loss did not improve from 5.80190\n","Epoch 116/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.9459 - val_loss: 7.8578\n","\n","Epoch 00116: loss did not improve from 5.80190\n","Epoch 117/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.0296 - val_loss: 7.9510\n","\n","Epoch 00117: loss did not improve from 5.80190\n","Epoch 118/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.8550 - val_loss: 10.1991\n","\n","Epoch 00118: loss did not improve from 5.80190\n","Epoch 119/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.9101 - val_loss: 14.8253\n","\n","Epoch 00119: loss did not improve from 5.80190\n","Epoch 120/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.8696 - val_loss: 13.8800\n","\n","Epoch 00120: loss did not improve from 5.80190\n","Epoch 121/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.7176 - val_loss: 10.2003\n","\n","Epoch 00121: loss improved from 5.80190 to 5.71769, saving model to final.h5\n","Epoch 122/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.8810 - val_loss: 5.8135\n","\n","Epoch 00122: loss did not improve from 5.71769\n","Epoch 123/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.9187 - val_loss: 9.3705\n","\n","Epoch 00123: loss did not improve from 5.71769\n","Epoch 124/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.8134 - val_loss: 10.0163\n","\n","Epoch 00124: loss did not improve from 5.71769\n","Epoch 125/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.8255 - val_loss: 10.1596\n","\n","Epoch 00125: loss did not improve from 5.71769\n","Epoch 126/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.7339 - val_loss: 6.0572\n","\n","Epoch 00126: loss did not improve from 5.71769\n","Epoch 127/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.7745 - val_loss: 6.7174\n","\n","Epoch 00127: loss did not improve from 5.71769\n","Epoch 128/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.6968 - val_loss: 9.3222\n","\n","Epoch 00128: loss improved from 5.71769 to 5.69676, saving model to final.h5\n","Epoch 129/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.7035 - val_loss: 4.4801\n","\n","Epoch 00129: loss did not improve from 5.69676\n","Epoch 130/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.6378 - val_loss: 5.4331\n","\n","Epoch 00130: loss improved from 5.69676 to 5.63754, saving model to final.h5\n","Epoch 131/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.8277 - val_loss: 6.2604\n","\n","Epoch 00131: loss did not improve from 5.63754\n","Epoch 132/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.7785 - val_loss: 6.6245\n","\n","Epoch 00132: loss did not improve from 5.63754\n","Epoch 133/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.7362 - val_loss: 6.7618\n","\n","Epoch 00133: loss did not improve from 5.63754\n","Epoch 134/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.5974 - val_loss: 11.1984\n","\n","Epoch 00134: loss improved from 5.63754 to 5.59738, saving model to final.h5\n","Epoch 135/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.7555 - val_loss: 4.2574\n","\n","Epoch 00135: loss did not improve from 5.59738\n","Epoch 136/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.7826 - val_loss: 4.0432\n","\n","Epoch 00136: loss did not improve from 5.59738\n","Epoch 137/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.6516 - val_loss: 5.1014\n","\n","Epoch 00137: loss did not improve from 5.59738\n","Epoch 138/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.6186 - val_loss: 4.2833\n","\n","Epoch 00138: loss did not improve from 5.59738\n","Epoch 139/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.7364 - val_loss: 4.6258\n","\n","Epoch 00139: loss did not improve from 5.59738\n","Epoch 140/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.6879 - val_loss: 5.3779\n","\n","Epoch 00140: loss did not improve from 5.59738\n","Epoch 141/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.7318 - val_loss: 5.2517\n","\n","Epoch 00141: loss did not improve from 5.59738\n","Epoch 142/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.6568 - val_loss: 6.2181\n","\n","Epoch 00142: loss did not improve from 5.59738\n","Epoch 143/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.6019 - val_loss: 5.4613\n","\n","Epoch 00143: loss did not improve from 5.59738\n","Epoch 144/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.4941 - val_loss: 6.2323\n","\n","Epoch 00144: loss improved from 5.59738 to 5.49440, saving model to final.h5\n","Epoch 145/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.6088 - val_loss: 6.7581\n","\n","Epoch 00145: loss did not improve from 5.49440\n","Epoch 146/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.7112 - val_loss: 5.6058\n","\n","Epoch 00146: loss did not improve from 5.49440\n","Epoch 147/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.6894 - val_loss: 2.5563\n","\n","Epoch 00147: loss did not improve from 5.49440\n","Epoch 148/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.5614 - val_loss: 2.3357\n","\n","Epoch 00148: loss did not improve from 5.49440\n","Epoch 149/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.4676 - val_loss: 2.3610\n","\n","Epoch 00149: loss improved from 5.49440 to 5.46783, saving model to final.h5\n","Epoch 150/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.6599 - val_loss: 4.8837\n","\n","Epoch 00150: loss did not improve from 5.46783\n","Epoch 151/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.5543 - val_loss: 4.5343\n","\n","Epoch 00151: loss did not improve from 5.46783\n","Epoch 152/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.5995 - val_loss: 5.0350\n","\n","Epoch 00152: loss did not improve from 5.46783\n","Epoch 153/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.5672 - val_loss: 4.2577\n","\n","Epoch 00153: loss did not improve from 5.46783\n","Epoch 154/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.3836 - val_loss: 5.0331\n","\n","Epoch 00154: loss improved from 5.46783 to 5.38342, saving model to final.h5\n","Epoch 155/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.5429 - val_loss: 7.1919\n","\n","Epoch 00155: loss did not improve from 5.38342\n","Epoch 156/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.5952 - val_loss: 6.4333\n","\n","Epoch 00156: loss did not improve from 5.38342\n","Epoch 157/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.5519 - val_loss: 5.7949\n","\n","Epoch 00157: loss did not improve from 5.38342\n","Epoch 158/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.5658 - val_loss: 8.0350\n","\n","Epoch 00158: loss did not improve from 5.38342\n","Epoch 159/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.3651 - val_loss: 2.9278\n","\n","Epoch 00159: loss improved from 5.38342 to 5.36519, saving model to final.h5\n","Epoch 160/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.5225 - val_loss: 6.3962\n","\n","Epoch 00160: loss did not improve from 5.36519\n","Epoch 161/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.4501 - val_loss: 5.8041\n","\n","Epoch 00161: loss did not improve from 5.36519\n","Epoch 162/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.5898 - val_loss: 6.1691\n","\n","Epoch 00162: loss did not improve from 5.36519\n","Epoch 163/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.4037 - val_loss: 5.2379\n","\n","Epoch 00163: loss did not improve from 5.36519\n","Epoch 164/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.6135 - val_loss: 5.8619\n","\n","Epoch 00164: loss did not improve from 5.36519\n","Epoch 165/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.5627 - val_loss: 5.0262\n","\n","Epoch 00165: loss did not improve from 5.36519\n","Epoch 166/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.4862 - val_loss: 5.5478\n","\n","Epoch 00166: loss did not improve from 5.36519\n","Epoch 167/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.4662 - val_loss: 6.1397\n","\n","Epoch 00167: loss did not improve from 5.36519\n","Epoch 168/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.3437 - val_loss: 6.4753\n","\n","Epoch 00168: loss improved from 5.36519 to 5.34326, saving model to final.h5\n","Epoch 169/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.3538 - val_loss: 4.2909\n","\n","Epoch 00169: loss did not improve from 5.34326\n","Epoch 170/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.2813 - val_loss: 8.1909\n","\n","Epoch 00170: loss improved from 5.34326 to 5.28118, saving model to final.h5\n","Epoch 171/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.2840 - val_loss: 3.4958\n","\n","Epoch 00171: loss did not improve from 5.28118\n","Epoch 172/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.2618 - val_loss: 3.2243\n","\n","Epoch 00172: loss improved from 5.28118 to 5.26204, saving model to final.h5\n","Epoch 173/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.5063 - val_loss: 5.0825\n","\n","Epoch 00173: loss did not improve from 5.26204\n","Epoch 174/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.2435 - val_loss: 5.1297\n","\n","Epoch 00174: loss improved from 5.26204 to 5.24383, saving model to final.h5\n","Epoch 175/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.2882 - val_loss: 6.3147\n","\n","Epoch 00175: loss did not improve from 5.24383\n","Epoch 176/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.2546 - val_loss: 4.6807\n","\n","Epoch 00176: loss did not improve from 5.24383\n","Epoch 177/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.2534 - val_loss: 3.0964\n","\n","Epoch 00177: loss did not improve from 5.24383\n","Epoch 178/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.3753 - val_loss: 6.5286\n","\n","Epoch 00178: loss did not improve from 5.24383\n","Epoch 179/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.2903 - val_loss: 6.2098\n","\n","Epoch 00179: loss did not improve from 5.24383\n","Epoch 180/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.3053 - val_loss: 7.3507\n","\n","Epoch 00180: loss did not improve from 5.24383\n","Epoch 181/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.3484 - val_loss: 7.0936\n","\n","Epoch 00181: loss did not improve from 5.24383\n","Epoch 182/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.4004 - val_loss: 3.1520\n","\n","Epoch 00182: loss did not improve from 5.24383\n","Epoch 183/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.2548 - val_loss: 2.2201\n","\n","Epoch 00183: loss did not improve from 5.24383\n","Epoch 184/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.2508 - val_loss: 5.8220\n","\n","Epoch 00184: loss did not improve from 5.24383\n","Epoch 185/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.3764 - val_loss: 7.0031\n","\n","Epoch 00185: loss did not improve from 5.24383\n","Epoch 186/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.2978 - val_loss: 7.5786\n","\n","Epoch 00186: loss did not improve from 5.24383\n","Epoch 187/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.2306 - val_loss: 4.7345\n","\n","Epoch 00187: loss improved from 5.24383 to 5.23049, saving model to final.h5\n","Epoch 188/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.1848 - val_loss: 2.9324\n","\n","Epoch 00188: loss improved from 5.23049 to 5.18484, saving model to final.h5\n","Epoch 189/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.3731 - val_loss: 5.1584\n","\n","Epoch 00189: loss did not improve from 5.18484\n","Epoch 190/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.1599 - val_loss: 9.6322\n","\n","Epoch 00190: loss improved from 5.18484 to 5.16000, saving model to final.h5\n","Epoch 191/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.0959 - val_loss: 3.6435\n","\n","Epoch 00191: loss improved from 5.16000 to 5.09600, saving model to final.h5\n","Epoch 192/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.1863 - val_loss: 3.4153\n","\n","Epoch 00192: loss did not improve from 5.09600\n","Epoch 193/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.2417 - val_loss: 5.7282\n","\n","Epoch 00193: loss did not improve from 5.09600\n","Epoch 194/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.1138 - val_loss: 3.5937\n","\n","Epoch 00194: loss did not improve from 5.09600\n","Epoch 195/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.1280 - val_loss: 2.3869\n","\n","Epoch 00195: loss did not improve from 5.09600\n","Epoch 196/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.0966 - val_loss: 4.5313\n","\n","Epoch 00196: loss did not improve from 5.09600\n","Epoch 197/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.2446 - val_loss: 4.0195\n","\n","Epoch 00197: loss did not improve from 5.09600\n","Epoch 198/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.1434 - val_loss: 4.6054\n","\n","Epoch 00198: loss did not improve from 5.09600\n","Epoch 199/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.3145 - val_loss: 4.3266\n","\n","Epoch 00199: loss did not improve from 5.09600\n","Epoch 200/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.1832 - val_loss: 3.0960\n","\n","Epoch 00200: loss did not improve from 5.09600\n","Epoch 201/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.0549 - val_loss: 2.8338\n","\n","Epoch 00201: loss improved from 5.09600 to 5.05477, saving model to final.h5\n","Epoch 202/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.1994 - val_loss: 2.7068\n","\n","Epoch 00202: loss did not improve from 5.05477\n","Epoch 203/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.1244 - val_loss: 7.2363\n","\n","Epoch 00203: loss did not improve from 5.05477\n","Epoch 204/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.1327 - val_loss: 7.2642\n","\n","Epoch 00204: loss did not improve from 5.05477\n","Epoch 205/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.1141 - val_loss: 4.8628\n","\n","Epoch 00205: loss did not improve from 5.05477\n","Epoch 206/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.1378 - val_loss: 4.2567\n","\n","Epoch 00206: loss did not improve from 5.05477\n","Epoch 207/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.0612 - val_loss: 2.6443\n","\n","Epoch 00207: loss did not improve from 5.05477\n","Epoch 208/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.0472 - val_loss: 2.7599\n","\n","Epoch 00208: loss improved from 5.05477 to 5.04735, saving model to final.h5\n","Epoch 209/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.0242 - val_loss: 1.8615\n","\n","Epoch 00209: loss improved from 5.04735 to 5.02429, saving model to final.h5\n","Epoch 210/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.0509 - val_loss: 2.1408\n","\n","Epoch 00210: loss did not improve from 5.02429\n","Epoch 211/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.0353 - val_loss: 3.1469\n","\n","Epoch 00211: loss did not improve from 5.02429\n","Epoch 212/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.0674 - val_loss: 3.7523\n","\n","Epoch 00212: loss did not improve from 5.02429\n","Epoch 213/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.9646 - val_loss: 1.9933\n","\n","Epoch 00213: loss improved from 5.02429 to 4.96428, saving model to final.h5\n","Epoch 214/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.9255 - val_loss: 1.4291\n","\n","Epoch 00214: loss improved from 4.96428 to 4.92537, saving model to final.h5\n","Epoch 215/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.9715 - val_loss: 2.2235\n","\n","Epoch 00215: loss did not improve from 4.92537\n","Epoch 216/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.9539 - val_loss: 2.0409\n","\n","Epoch 00216: loss did not improve from 4.92537\n","Epoch 217/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.1665 - val_loss: 2.3153\n","\n","Epoch 00217: loss did not improve from 4.92537\n","Epoch 218/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.9786 - val_loss: 3.3968\n","\n","Epoch 00218: loss did not improve from 4.92537\n","Epoch 219/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.0173 - val_loss: 2.0492\n","\n","Epoch 00219: loss did not improve from 4.92537\n","Epoch 220/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.2620 - val_loss: 2.1783\n","\n","Epoch 00220: loss did not improve from 4.92537\n","Epoch 221/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.0785 - val_loss: 2.2741\n","\n","Epoch 00221: loss did not improve from 4.92537\n","Epoch 222/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.8924 - val_loss: 1.7239\n","\n","Epoch 00222: loss improved from 4.92537 to 4.89253, saving model to final.h5\n","Epoch 223/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.0168 - val_loss: 2.5420\n","\n","Epoch 00223: loss did not improve from 4.89253\n","Epoch 224/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.9437 - val_loss: 1.9795\n","\n","Epoch 00224: loss did not improve from 4.89253\n","Epoch 225/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.9239 - val_loss: 1.5209\n","\n","Epoch 00225: loss did not improve from 4.89253\n","Epoch 226/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.9668 - val_loss: 1.7024\n","\n","Epoch 00226: loss did not improve from 4.89253\n","Epoch 227/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.0260 - val_loss: 1.8526\n","\n","Epoch 00227: loss did not improve from 4.89253\n","Epoch 228/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.9325 - val_loss: 1.8818\n","\n","Epoch 00228: loss did not improve from 4.89253\n","Epoch 229/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.9801 - val_loss: 2.0203\n","\n","Epoch 00229: loss did not improve from 4.89253\n","Epoch 230/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.8485 - val_loss: 2.4812\n","\n","Epoch 00230: loss improved from 4.89253 to 4.84826, saving model to final.h5\n","Epoch 231/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.9735 - val_loss: 1.2553\n","\n","Epoch 00231: loss did not improve from 4.84826\n","Epoch 232/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.9927 - val_loss: 1.9349\n","\n","Epoch 00232: loss did not improve from 4.84826\n","Epoch 233/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.9090 - val_loss: 1.7717\n","\n","Epoch 00233: loss did not improve from 4.84826\n","Epoch 234/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.9456 - val_loss: 1.4225\n","\n","Epoch 00234: loss did not improve from 4.84826\n","Epoch 235/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.8532 - val_loss: 5.4599\n","\n","Epoch 00235: loss did not improve from 4.84826\n","Epoch 236/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.8787 - val_loss: 3.1655\n","\n","Epoch 00236: loss did not improve from 4.84826\n","Epoch 237/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.9641 - val_loss: 4.1708\n","\n","Epoch 00237: loss did not improve from 4.84826\n","Epoch 238/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.9405 - val_loss: 2.6914\n","\n","Epoch 00238: loss did not improve from 4.84826\n","Epoch 239/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.8535 - val_loss: 2.0326\n","\n","Epoch 00239: loss did not improve from 4.84826\n","Epoch 240/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7433 - val_loss: 1.6315\n","\n","Epoch 00240: loss improved from 4.84826 to 4.74332, saving model to final.h5\n","Epoch 241/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.9361 - val_loss: 1.2450\n","\n","Epoch 00241: loss did not improve from 4.74332\n","Epoch 242/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7713 - val_loss: 1.5001\n","\n","Epoch 00242: loss did not improve from 4.74332\n","Epoch 243/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7676 - val_loss: 2.7861\n","\n","Epoch 00243: loss did not improve from 4.74332\n","Epoch 244/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7538 - val_loss: 1.7879\n","\n","Epoch 00244: loss did not improve from 4.74332\n","Epoch 245/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.8747 - val_loss: 2.8113\n","\n","Epoch 00245: loss did not improve from 4.74332\n","Epoch 246/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.8302 - val_loss: 1.3532\n","\n","Epoch 00246: loss did not improve from 4.74332\n","Epoch 247/350\n","500/500 [==============================] - 31s 61ms/step - loss: 4.7948 - val_loss: 1.0171\n","\n","Epoch 00247: loss did not improve from 4.74332\n","Epoch 248/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7595 - val_loss: 1.5224\n","\n","Epoch 00248: loss did not improve from 4.74332\n","Epoch 249/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.6385 - val_loss: 1.5295\n","\n","Epoch 00249: loss improved from 4.74332 to 4.63830, saving model to final.h5\n","Epoch 250/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.8332 - val_loss: 1.2823\n","\n","Epoch 00250: loss did not improve from 4.63830\n","Epoch 251/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7761 - val_loss: 1.3475\n","\n","Epoch 00251: loss did not improve from 4.63830\n","Epoch 252/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.6279 - val_loss: 1.9774\n","\n","Epoch 00252: loss improved from 4.63830 to 4.62785, saving model to final.h5\n","Epoch 253/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7091 - val_loss: 2.0815\n","\n","Epoch 00253: loss did not improve from 4.62785\n","Epoch 254/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.6253 - val_loss: 0.8963\n","\n","Epoch 00254: loss improved from 4.62785 to 4.62538, saving model to final.h5\n","Epoch 255/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7289 - val_loss: 1.0937\n","\n","Epoch 00255: loss did not improve from 4.62538\n","Epoch 256/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7341 - val_loss: 1.5378\n","\n","Epoch 00256: loss did not improve from 4.62538\n","Epoch 257/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.8863 - val_loss: 1.4544\n","\n","Epoch 00257: loss did not improve from 4.62538\n","Epoch 258/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.5877 - val_loss: 1.3108\n","\n","Epoch 00258: loss improved from 4.62538 to 4.58759, saving model to final.h5\n","Epoch 259/350\n","500/500 [==============================] - 31s 61ms/step - loss: 4.7719 - val_loss: 1.3117\n","\n","Epoch 00259: loss did not improve from 4.58759\n","Epoch 260/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7103 - val_loss: 1.0687\n","\n","Epoch 00260: loss did not improve from 4.58759\n","Epoch 261/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7514 - val_loss: 1.3516\n","\n","Epoch 00261: loss did not improve from 4.58759\n","Epoch 262/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.6930 - val_loss: 1.5356\n","\n","Epoch 00262: loss did not improve from 4.58759\n","Epoch 263/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.6653 - val_loss: 2.0500\n","\n","Epoch 00263: loss did not improve from 4.58759\n","Epoch 264/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.8098 - val_loss: 2.0675\n","\n","Epoch 00264: loss did not improve from 4.58759\n","Epoch 265/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7293 - val_loss: 2.3321\n","\n","Epoch 00265: loss did not improve from 4.58759\n","Epoch 266/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7755 - val_loss: 2.6281\n","\n","Epoch 00266: loss did not improve from 4.58759\n","Epoch 267/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.6095 - val_loss: 2.7245\n","\n","Epoch 00267: loss did not improve from 4.58759\n","Epoch 268/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.7186 - val_loss: 2.0801\n","\n","Epoch 00268: loss did not improve from 4.58759\n","Epoch 269/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.6891 - val_loss: 1.8651\n","\n","Epoch 00269: loss did not improve from 4.58759\n","Epoch 270/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.6045 - val_loss: 1.9721\n","\n","Epoch 00270: loss did not improve from 4.58759\n","Epoch 271/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.7467 - val_loss: 2.8536\n","\n","Epoch 00271: loss did not improve from 4.58759\n","Epoch 272/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.6333 - val_loss: 2.0624\n","\n","Epoch 00272: loss did not improve from 4.58759\n","Epoch 273/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.6298 - val_loss: 2.0423\n","\n","Epoch 00273: loss did not improve from 4.58759\n","Epoch 274/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.6012 - val_loss: 2.1141\n","\n","Epoch 00274: loss did not improve from 4.58759\n","Epoch 275/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.6203 - val_loss: 2.9104\n","\n","Epoch 00275: loss did not improve from 4.58759\n","Epoch 276/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.5920 - val_loss: 1.2158\n","\n","Epoch 00276: loss did not improve from 4.58759\n","Epoch 277/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.6355 - val_loss: 1.7437\n","\n","Epoch 00277: loss did not improve from 4.58759\n","Epoch 278/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.6607 - val_loss: 1.4260\n","\n","Epoch 00278: loss did not improve from 4.58759\n","Epoch 279/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.6469 - val_loss: 1.8305\n","\n","Epoch 00279: loss did not improve from 4.58759\n","Epoch 280/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.5108 - val_loss: 1.4316\n","\n","Epoch 00280: loss improved from 4.58759 to 4.51089, saving model to final.h5\n","Epoch 281/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.4954 - val_loss: 2.3737\n","\n","Epoch 00281: loss improved from 4.51089 to 4.49526, saving model to final.h5\n","Epoch 282/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.6388 - val_loss: 1.9460\n","\n","Epoch 00282: loss did not improve from 4.49526\n","Epoch 283/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.5226 - val_loss: 1.5264\n","\n","Epoch 00283: loss did not improve from 4.49526\n","Epoch 284/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.5152 - val_loss: 1.3944\n","\n","Epoch 00284: loss did not improve from 4.49526\n","Epoch 285/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.5140 - val_loss: 1.8224\n","\n","Epoch 00285: loss did not improve from 4.49526\n","Epoch 286/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.5798 - val_loss: 1.4961\n","\n","Epoch 00286: loss did not improve from 4.49526\n","Epoch 287/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.5056 - val_loss: 1.8053\n","\n","Epoch 00287: loss did not improve from 4.49526\n","Epoch 288/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.6660 - val_loss: 2.8996\n","\n","Epoch 00288: loss did not improve from 4.49526\n","Epoch 289/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.4766 - val_loss: 1.8825\n","\n","Epoch 00289: loss improved from 4.49526 to 4.47667, saving model to final.h5\n","Epoch 290/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.4638 - val_loss: 2.6532\n","\n","Epoch 00290: loss improved from 4.47667 to 4.46395, saving model to final.h5\n","Epoch 291/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.4643 - val_loss: 2.8153\n","\n","Epoch 00291: loss did not improve from 4.46395\n","Epoch 292/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.6728 - val_loss: 2.5240\n","\n","Epoch 00292: loss did not improve from 4.46395\n","Epoch 293/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.4195 - val_loss: 1.5652\n","\n","Epoch 00293: loss improved from 4.46395 to 4.41957, saving model to final.h5\n","Epoch 294/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.4184 - val_loss: 2.4243\n","\n","Epoch 00294: loss improved from 4.41957 to 4.41815, saving model to final.h5\n","Epoch 295/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.5213 - val_loss: 1.6373\n","\n","Epoch 00295: loss did not improve from 4.41815\n","Epoch 296/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.4837 - val_loss: 2.2189\n","\n","Epoch 00296: loss did not improve from 4.41815\n","Epoch 297/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.4160 - val_loss: 2.9007\n","\n","Epoch 00297: loss improved from 4.41815 to 4.41578, saving model to final.h5\n","Epoch 298/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.4780 - val_loss: 2.1365\n","\n","Epoch 00298: loss did not improve from 4.41578\n","Epoch 299/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.4096 - val_loss: 3.2636\n","\n","Epoch 00299: loss improved from 4.41578 to 4.40983, saving model to final.h5\n","Epoch 300/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3016 - val_loss: 2.2021\n","\n","Epoch 00300: loss improved from 4.40983 to 4.30187, saving model to final.h5\n","Epoch 301/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3362 - val_loss: 1.3463\n","\n","Epoch 00301: loss did not improve from 4.30187\n","Epoch 302/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.4805 - val_loss: 1.9705\n","\n","Epoch 00302: loss did not improve from 4.30187\n","Epoch 303/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.3730 - val_loss: 1.3212\n","\n","Epoch 00303: loss did not improve from 4.30187\n","Epoch 304/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3840 - val_loss: 1.0575\n","\n","Epoch 00304: loss did not improve from 4.30187\n","Epoch 305/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3202 - val_loss: 0.6217\n","\n","Epoch 00305: loss did not improve from 4.30187\n","Epoch 306/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3252 - val_loss: 0.8795\n","\n","Epoch 00306: loss did not improve from 4.30187\n","Epoch 307/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.4342 - val_loss: 1.3447\n","\n","Epoch 00307: loss did not improve from 4.30187\n","Epoch 308/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3139 - val_loss: 1.5497\n","\n","Epoch 00308: loss did not improve from 4.30187\n","Epoch 309/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.4595 - val_loss: 1.5586\n","\n","Epoch 00309: loss did not improve from 4.30187\n","Epoch 310/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.4123 - val_loss: 2.2302\n","\n","Epoch 00310: loss did not improve from 4.30187\n","Epoch 311/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2863 - val_loss: 0.8259\n","\n","Epoch 00311: loss improved from 4.30187 to 4.28612, saving model to final.h5\n","Epoch 312/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3031 - val_loss: 0.7866\n","\n","Epoch 00312: loss did not improve from 4.28612\n","Epoch 313/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2934 - val_loss: 0.7668\n","\n","Epoch 00313: loss did not improve from 4.28612\n","Epoch 314/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.3288 - val_loss: 0.9900\n","\n","Epoch 00314: loss did not improve from 4.28612\n","Epoch 315/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.3088 - val_loss: 1.9687\n","\n","Epoch 00315: loss did not improve from 4.28612\n","Epoch 316/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2670 - val_loss: 1.0457\n","\n","Epoch 00316: loss improved from 4.28612 to 4.26714, saving model to final.h5\n","Epoch 317/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.2100 - val_loss: 1.4207\n","\n","Epoch 00317: loss improved from 4.26714 to 4.21021, saving model to final.h5\n","Epoch 318/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2566 - val_loss: 1.0385\n","\n","Epoch 00318: loss did not improve from 4.21021\n","Epoch 319/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.3352 - val_loss: 0.9090\n","\n","Epoch 00319: loss did not improve from 4.21021\n","Epoch 320/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2612 - val_loss: 1.4033\n","\n","Epoch 00320: loss did not improve from 4.21021\n","Epoch 321/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.3114 - val_loss: 1.5317\n","\n","Epoch 00321: loss did not improve from 4.21021\n","Epoch 322/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.3159 - val_loss: 1.4319\n","\n","Epoch 00322: loss did not improve from 4.21021\n","Epoch 323/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.3677 - val_loss: 2.6215\n","\n","Epoch 00323: loss did not improve from 4.21021\n","Epoch 324/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2924 - val_loss: 1.1389\n","\n","Epoch 00324: loss did not improve from 4.21021\n","Epoch 325/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2239 - val_loss: 0.9941\n","\n","Epoch 00325: loss did not improve from 4.21021\n","Epoch 326/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.2693 - val_loss: 1.1758\n","\n","Epoch 00326: loss did not improve from 4.21021\n","Epoch 327/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3365 - val_loss: 1.2906\n","\n","Epoch 00327: loss did not improve from 4.21021\n","Epoch 328/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.1634 - val_loss: 1.1998\n","\n","Epoch 00328: loss improved from 4.21021 to 4.16360, saving model to final.h5\n","Epoch 329/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2420 - val_loss: 0.7931\n","\n","Epoch 00329: loss did not improve from 4.16360\n","Epoch 330/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2184 - val_loss: 0.8637\n","\n","Epoch 00330: loss did not improve from 4.16360\n","Epoch 331/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2486 - val_loss: 0.7406\n","\n","Epoch 00331: loss did not improve from 4.16360\n","Epoch 332/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2371 - val_loss: 0.9027\n","\n","Epoch 00332: loss did not improve from 4.16360\n","Epoch 333/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3191 - val_loss: 1.4287\n","\n","Epoch 00333: loss did not improve from 4.16360\n","Epoch 334/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2186 - val_loss: 0.6472\n","\n","Epoch 00334: loss did not improve from 4.16360\n","Epoch 335/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3538 - val_loss: 1.0636\n","\n","Epoch 00335: loss did not improve from 4.16360\n","Epoch 336/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2994 - val_loss: 0.6062\n","\n","Epoch 00336: loss did not improve from 4.16360\n","Epoch 337/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2016 - val_loss: 0.8608\n","\n","Epoch 00337: loss did not improve from 4.16360\n","Epoch 338/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2799 - val_loss: 0.9248\n","\n","Epoch 00338: loss did not improve from 4.16360\n","Epoch 339/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2520 - val_loss: 2.0595\n","\n","Epoch 00339: loss did not improve from 4.16360\n","Epoch 340/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2841 - val_loss: 2.3748\n","\n","Epoch 00340: loss did not improve from 4.16360\n","Epoch 341/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2028 - val_loss: 2.8265\n","\n","Epoch 00341: loss did not improve from 4.16360\n","Epoch 342/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2081 - val_loss: 1.1788\n","\n","Epoch 00342: loss did not improve from 4.16360\n","Epoch 343/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2133 - val_loss: 1.8325\n","\n","Epoch 00343: loss did not improve from 4.16360\n","Epoch 344/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2117 - val_loss: 1.3513\n","\n","Epoch 00344: loss did not improve from 4.16360\n","Epoch 345/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.1233 - val_loss: 1.4985\n","\n","Epoch 00345: loss improved from 4.16360 to 4.12340, saving model to final.h5\n","Epoch 346/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2639 - val_loss: 0.7537\n","\n","Epoch 00346: loss did not improve from 4.12340\n","Epoch 347/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3244 - val_loss: 1.0312\n","\n","Epoch 00347: loss did not improve from 4.12340\n","Epoch 348/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3452 - val_loss: 0.9382\n","\n","Epoch 00348: loss did not improve from 4.12340\n","Epoch 349/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.1745 - val_loss: 1.7322\n","\n","Epoch 00349: loss did not improve from 4.12340\n","Epoch 350/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3179 - val_loss: 1.5371\n","\n","Epoch 00350: loss did not improve from 4.12340\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HGpYvXqcz033","outputId":"504d006d-5783-4150-bc89-8e25efa79419","executionInfo":{"status":"ok","timestamp":1591818985903,"user_tz":360,"elapsed":11722549,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speedchallenge.py --mode=test  --history {nhistory}  --model final.h5  train_Flipped_for_testing.mp4 train_Flipped_for_testing.txt  "],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-06-10 19:54:28.578915: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:217: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(32, (8, 8), return_sequences=True, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(32, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=True,activation=\"relu\", dropout=0.5))(flow_inp)\n","2020-06-10 19:54:30.763770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-06-10 19:54:30.779618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:54:30.780678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 19:54:30.780746: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 19:54:30.782974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 19:54:30.785444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 19:54:30.785831: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 19:54:30.787790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 19:54:30.789047: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 19:54:30.792960: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 19:54:30.793076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:54:30.794014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:54:30.794940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 19:54:30.800974: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-06-10 19:54:30.801314: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x139f2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 19:54:30.801351: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-06-10 19:54:30.892633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:54:30.893776: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x139f480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 19:54:30.893817: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-06-10 19:54:30.894046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:54:30.894964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 19:54:30.895023: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 19:54:30.895070: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 19:54:30.895105: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 19:54:30.895164: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 19:54:30.895214: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 19:54:30.895244: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 19:54:30.895278: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 19:54:30.895350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:54:30.896236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:54:30.897051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 19:54:30.897100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 19:54:31.464026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-10 19:54:31.464086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-06-10 19:54:31.464097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-06-10 19:54:31.464317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:54:31.465446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:54:31.466312: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-06-10 19:54:31.466361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:221: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(128, (8, 8), return_sequences=False, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(128, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=False,activation=\"relu\", dropout=0.5))(flow)\n","speedchallenge.py:234: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","speedchallenge.py:241: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 100, 100, 2)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 25, 25, 32)   4128        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 25, 25, 32)   0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 25, 25, 32)   128         activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 25, 25, 32)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 2, 100, 100,  0                                            \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 32)   0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv_lst_m2d_1 (ConvLSTM2D)     (None, 2, 25, 25, 32 286848      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 6, 6, 128)    262272      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 2, 25, 25, 32 128         conv_lst_m2d_1[0][0]             \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 6, 6, 128)    0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv_lst_m2d_2 (ConvLSTM2D)     (None, 7, 7, 128)    5243392     batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 6, 6, 128)    512         activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 7, 7, 128)    512         conv_lst_m2d_2[0][0]             \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 6, 6, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 6272)         0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 128)    0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          1605888     flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 5, 5, 256)    33024       max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 256)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 6400)         0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 6656)         0           activation_1[0][0]               \n","                                                                 flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 6656)         0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 6656)         0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 128)          852096      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 128)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1)            129         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 8,289,057\n","Trainable params: 8,288,417\n","Non-trainable params: 640\n","__________________________________________________________________________________________________\n","None\n","Prepping data\n","Decoding speed data\n","loaded 4399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 4398 frames\n","done processing 4400 frames\n","Done prepping data\n","loading weights\n","Starting testing\n","Number of frames to be evaluated:  4398\n","2020-06-10 19:55:22.258470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 19:55:22.479104: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","The mean square error is:  70.2775269171892\n","Done testing\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B2v8v80hg3Hh","colab_type":"code","outputId":"ebb0f45a-8cd2-4626-8262-5c95978b9d14","executionInfo":{"status":"ok","timestamp":1591819416879,"user_tz":360,"elapsed":12153522,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speedchallenge.py --mode=test  --history {nhistory}  --model final.h5  train_Flipped_for_training.mp4 train_Flipped_for_training.txt"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-06-10 19:56:07.529427: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:217: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(32, (8, 8), return_sequences=True, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(32, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=True,activation=\"relu\", dropout=0.5))(flow_inp)\n","2020-06-10 19:56:09.658011: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-06-10 19:56:09.673367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:56:09.674331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 19:56:09.674373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 19:56:09.676306: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 19:56:09.678414: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 19:56:09.678787: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 19:56:09.680808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 19:56:09.682169: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 19:56:09.686244: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 19:56:09.686360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:56:09.687329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:56:09.688234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 19:56:09.694355: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-06-10 19:56:09.694675: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x12ed2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 19:56:09.694710: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-06-10 19:56:09.787409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:56:09.788578: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x12ed480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 19:56:09.788621: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-06-10 19:56:09.788933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:56:09.789870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 19:56:09.789935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 19:56:09.789999: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 19:56:09.790048: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 19:56:09.790091: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 19:56:09.790153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 19:56:09.790205: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 19:56:09.790252: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 19:56:09.790346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:56:09.791243: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:56:09.792085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 19:56:09.792154: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 19:56:10.350384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-10 19:56:10.350452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-06-10 19:56:10.350471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-06-10 19:56:10.350727: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:56:10.351738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 19:56:10.352622: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-06-10 19:56:10.352681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:221: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(128, (8, 8), return_sequences=False, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(128, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=False,activation=\"relu\", dropout=0.5))(flow)\n","speedchallenge.py:234: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","speedchallenge.py:241: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 100, 100, 2)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 25, 25, 32)   4128        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 25, 25, 32)   0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 25, 25, 32)   128         activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 25, 25, 32)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 2, 100, 100,  0                                            \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 32)   0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv_lst_m2d_1 (ConvLSTM2D)     (None, 2, 25, 25, 32 286848      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 6, 6, 128)    262272      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 2, 25, 25, 32 128         conv_lst_m2d_1[0][0]             \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 6, 6, 128)    0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv_lst_m2d_2 (ConvLSTM2D)     (None, 7, 7, 128)    5243392     batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 6, 6, 128)    512         activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 7, 7, 128)    512         conv_lst_m2d_2[0][0]             \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 6, 6, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 6272)         0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 128)    0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          1605888     flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 5, 5, 256)    33024       max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 256)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 6400)         0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 6656)         0           activation_1[0][0]               \n","                                                                 flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 6656)         0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 6656)         0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 128)          852096      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 128)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1)            129         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 8,289,057\n","Trainable params: 8,288,417\n","Non-trainable params: 640\n","__________________________________________________________________________________________________\n","None\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400 frames\n","Done prepping data\n","loading weights\n","Starting testing\n","Number of frames to be evaluated:  20398\n","2020-06-10 20:00:03.288374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 20:00:03.524103: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","The mean square error is:  37.81736554548414\n","Done testing\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5_frZ-nAMRLN","outputId":"cf161b32-d7aa-4831-c181-68b900325e6a","executionInfo":{"status":"ok","timestamp":1591819846637,"user_tz":360,"elapsed":12583279,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python speedchallenge.py --mode=test  --history {nhistory}  --model final.h5  train.mp4 train.txt"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-06-10 20:03:18.198374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:217: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(32, (8, 8), return_sequences=True, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(32, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=True,activation=\"relu\", dropout=0.5))(flow_inp)\n","2020-06-10 20:03:20.329018: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-06-10 20:03:20.343974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:03:20.345010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 20:03:20.345065: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 20:03:20.346934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 20:03:20.349290: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 20:03:20.349662: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 20:03:20.351569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 20:03:20.352826: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 20:03:20.357160: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 20:03:20.357298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:03:20.358223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:03:20.359128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 20:03:20.365070: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-06-10 20:03:20.365546: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x22f52c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 20:03:20.365583: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-06-10 20:03:20.461295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:03:20.462535: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x22f5480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 20:03:20.462572: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-06-10 20:03:20.462751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:03:20.463653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 20:03:20.463704: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 20:03:20.463752: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 20:03:20.463782: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 20:03:20.463814: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 20:03:20.463841: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 20:03:20.463867: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 20:03:20.463893: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 20:03:20.463985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:03:20.464904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:03:20.465740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 20:03:20.465791: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 20:03:21.021714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-10 20:03:21.021777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-06-10 20:03:21.021790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-06-10 20:03:21.022028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:03:21.023165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:03:21.024252: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-06-10 20:03:21.024304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:221: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(128, (8, 8), return_sequences=False, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(128, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=False,activation=\"relu\", dropout=0.5))(flow)\n","speedchallenge.py:234: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","speedchallenge.py:241: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 100, 100, 2)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 25, 25, 32)   4128        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 25, 25, 32)   0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 25, 25, 32)   128         activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 25, 25, 32)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 2, 100, 100,  0                                            \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 32)   0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv_lst_m2d_1 (ConvLSTM2D)     (None, 2, 25, 25, 32 286848      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 6, 6, 128)    262272      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 2, 25, 25, 32 128         conv_lst_m2d_1[0][0]             \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 6, 6, 128)    0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv_lst_m2d_2 (ConvLSTM2D)     (None, 7, 7, 128)    5243392     batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 6, 6, 128)    512         activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 7, 7, 128)    512         conv_lst_m2d_2[0][0]             \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 6, 6, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 6272)         0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 128)    0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          1605888     flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 5, 5, 256)    33024       max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 256)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 6400)         0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 6656)         0           activation_1[0][0]               \n","                                                                 flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 6656)         0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 6656)         0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 128)          852096      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 128)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1)            129         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 8,289,057\n","Trainable params: 8,288,417\n","Non-trainable params: 640\n","__________________________________________________________________________________________________\n","None\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400 frames\n","Done prepping data\n","loading weights\n","Starting testing\n","Number of frames to be evaluated:  20398\n","2020-06-10 20:07:16.801173: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 20:07:17.042358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","The mean square error is:  11.669817083302403\n","Done testing\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2bbUlS7X0Ma_","colab_type":"text"},"source":["## **Training with Augmentation and Flipped image**"]},{"cell_type":"code","metadata":{"id":"wi1mUxsUY7lp","colab_type":"code","outputId":"77100c42-bc68-43b5-8bbb-f1cc6c010382","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1591831551564,"user_tz":360,"elapsed":24288204,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}}},"source":["!python speedchallenge.py --mode=train --epoch {nepoch} --history {nhistory} --split_start=7700 --split_end=12100 --resume --model final.h5  --LR {LR} --augment --wipe train_Flipped_for_training.mp4 train_Flipped_for_training.txt"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-06-10 20:10:28.047064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:217: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(32, (8, 8), return_sequences=True, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(32, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=True,activation=\"relu\", dropout=0.5))(flow_inp)\n","2020-06-10 20:10:30.169956: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-06-10 20:10:30.185338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:10:30.186267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 20:10:30.186321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 20:10:30.188209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 20:10:30.190206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 20:10:30.190564: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 20:10:30.192359: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 20:10:30.193535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 20:10:30.197997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 20:10:30.198158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:10:30.199252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:10:30.200198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 20:10:30.206311: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-06-10 20:10:30.206633: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x19812c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 20:10:30.206675: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-06-10 20:10:30.299724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:10:30.300994: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1981480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 20:10:30.301028: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-06-10 20:10:30.301278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:10:30.302216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 20:10:30.302276: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 20:10:30.302320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 20:10:30.302351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 20:10:30.302378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 20:10:30.302419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 20:10:30.302441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 20:10:30.302463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 20:10:30.302531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:10:30.303446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:10:30.304318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 20:10:30.304368: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 20:10:30.866475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-10 20:10:30.866549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-06-10 20:10:30.866586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-06-10 20:10:30.866830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:10:30.867834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 20:10:30.868744: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-06-10 20:10:30.868797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:221: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(128, (8, 8), return_sequences=False, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(128, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=False,activation=\"relu\", dropout=0.5))(flow)\n","speedchallenge.py:234: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","speedchallenge.py:241: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 100, 100, 2)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 25, 25, 32)   4128        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 25, 25, 32)   0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 25, 25, 32)   128         activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 25, 25, 32)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 2, 100, 100,  0                                            \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 32)   0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv_lst_m2d_1 (ConvLSTM2D)     (None, 2, 25, 25, 32 286848      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 6, 6, 128)    262272      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 2, 25, 25, 32 128         conv_lst_m2d_1[0][0]             \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 6, 6, 128)    0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv_lst_m2d_2 (ConvLSTM2D)     (None, 7, 7, 128)    5243392     batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 6, 6, 128)    512         activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 7, 7, 128)    512         conv_lst_m2d_2[0][0]             \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 6, 6, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 6272)         0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 128)    0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          1605888     flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 5, 5, 256)    33024       max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 256)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 6400)         0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 6656)         0           activation_1[0][0]               \n","                                                                 flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 6656)         0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 6656)         0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 128)          852096      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 128)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1)            129         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 8,289,057\n","Trainable params: 8,288,417\n","Non-trainable params: 640\n","__________________________________________________________________________________________________\n","None\n","loading weights\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400 frames\n","Done prepping data\n","20398 Training data size per Aug\n","15998 Train indices size\n","4400 Val indices size\n"," This is the range of train:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119, 6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129, 6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139, 6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169, 6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209, 6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229, 6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239, 6240, 6241, 6242, 6243, 6244, 6245, 6246, 6247, 6248, 6249, 6250, 6251, 6252, 6253, 6254, 6255, 6256, 6257, 6258, 6259, 6260, 6261, 6262, 6263, 6264, 6265, 6266, 6267, 6268, 6269, 6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289, 6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299, 6300, 6301, 6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309, 6310, 6311, 6312, 6313, 6314, 6315, 6316, 6317, 6318, 6319, 6320, 6321, 6322, 6323, 6324, 6325, 6326, 6327, 6328, 6329, 6330, 6331, 6332, 6333, 6334, 6335, 6336, 6337, 6338, 6339, 6340, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349, 6350, 6351, 6352, 6353, 6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363, 6364, 6365, 6366, 6367, 6368, 6369, 6370, 6371, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6379, 6380, 6381, 6382, 6383, 6384, 6385, 6386, 6387, 6388, 6389, 6390, 6391, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399, 6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419, 6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429, 6430, 6431, 6432, 6433, 6434, 6435, 6436, 6437, 6438, 6439, 6440, 6441, 6442, 6443, 6444, 6445, 6446, 6447, 6448, 6449, 6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459, 6460, 6461, 6462, 6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473, 6474, 6475, 6476, 6477, 6478, 6479, 6480, 6481, 6482, 6483, 6484, 6485, 6486, 6487, 6488, 6489, 6490, 6491, 6492, 6493, 6494, 6495, 6496, 6497, 6498, 6499, 6500, 6501, 6502, 6503, 6504, 6505, 6506, 6507, 6508, 6509, 6510, 6511, 6512, 6513, 6514, 6515, 6516, 6517, 6518, 6519, 6520, 6521, 6522, 6523, 6524, 6525, 6526, 6527, 6528, 6529, 6530, 6531, 6532, 6533, 6534, 6535, 6536, 6537, 6538, 6539, 6540, 6541, 6542, 6543, 6544, 6545, 6546, 6547, 6548, 6549, 6550, 6551, 6552, 6553, 6554, 6555, 6556, 6557, 6558, 6559, 6560, 6561, 6562, 6563, 6564, 6565, 6566, 6567, 6568, 6569, 6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577, 6578, 6579, 6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588, 6589, 6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599, 6600, 6601, 6602, 6603, 6604, 6605, 6606, 6607, 6608, 6609, 6610, 6611, 6612, 6613, 6614, 6615, 6616, 6617, 6618, 6619, 6620, 6621, 6622, 6623, 6624, 6625, 6626, 6627, 6628, 6629, 6630, 6631, 6632, 6633, 6634, 6635, 6636, 6637, 6638, 6639, 6640, 6641, 6642, 6643, 6644, 6645, 6646, 6647, 6648, 6649, 6650, 6651, 6652, 6653, 6654, 6655, 6656, 6657, 6658, 6659, 6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6669, 6670, 6671, 6672, 6673, 6674, 6675, 6676, 6677, 6678, 6679, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689, 6690, 6691, 6692, 6693, 6694, 6695, 6696, 6697, 6698, 6699, 6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6708, 6709, 6710, 6711, 6712, 6713, 6714, 6715, 6716, 6717, 6718, 6719, 6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729, 6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739, 6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6748, 6749, 6750, 6751, 6752, 6753, 6754, 6755, 6756, 6757, 6758, 6759, 6760, 6761, 6762, 6763, 6764, 6765, 6766, 6767, 6768, 6769, 6770, 6771, 6772, 6773, 6774, 6775, 6776, 6777, 6778, 6779, 6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6788, 6789, 6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 6804, 6805, 6806, 6807, 6808, 6809, 6810, 6811, 6812, 6813, 6814, 6815, 6816, 6817, 6818, 6819, 6820, 6821, 6822, 6823, 6824, 6825, 6826, 6827, 6828, 6829, 6830, 6831, 6832, 6833, 6834, 6835, 6836, 6837, 6838, 6839, 6840, 6841, 6842, 6843, 6844, 6845, 6846, 6847, 6848, 6849, 6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6859, 6860, 6861, 6862, 6863, 6864, 6865, 6866, 6867, 6868, 6869, 6870, 6871, 6872, 6873, 6874, 6875, 6876, 6877, 6878, 6879, 6880, 6881, 6882, 6883, 6884, 6885, 6886, 6887, 6888, 6889, 6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898, 6899, 6900, 6901, 6902, 6903, 6904, 6905, 6906, 6907, 6908, 6909, 6910, 6911, 6912, 6913, 6914, 6915, 6916, 6917, 6918, 6919, 6920, 6921, 6922, 6923, 6924, 6925, 6926, 6927, 6928, 6929, 6930, 6931, 6932, 6933, 6934, 6935, 6936, 6937, 6938, 6939, 6940, 6941, 6942, 6943, 6944, 6945, 6946, 6947, 6948, 6949, 6950, 6951, 6952, 6953, 6954, 6955, 6956, 6957, 6958, 6959, 6960, 6961, 6962, 6963, 6964, 6965, 6966, 6967, 6968, 6969, 6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979, 6980, 6981, 6982, 6983, 6984, 6985, 6986, 6987, 6988, 6989, 6990, 6991, 6992, 6993, 6994, 6995, 6996, 6997, 6998, 6999, 7000, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009, 7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019, 7020, 7021, 7022, 7023, 7024, 7025, 7026, 7027, 7028, 7029, 7030, 7031, 7032, 7033, 7034, 7035, 7036, 7037, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 7045, 7046, 7047, 7048, 7049, 7050, 7051, 7052, 7053, 7054, 7055, 7056, 7057, 7058, 7059, 7060, 7061, 7062, 7063, 7064, 7065, 7066, 7067, 7068, 7069, 7070, 7071, 7072, 7073, 7074, 7075, 7076, 7077, 7078, 7079, 7080, 7081, 7082, 7083, 7084, 7085, 7086, 7087, 7088, 7089, 7090, 7091, 7092, 7093, 7094, 7095, 7096, 7097, 7098, 7099, 7100, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7108, 7109, 7110, 7111, 7112, 7113, 7114, 7115, 7116, 7117, 7118, 7119, 7120, 7121, 7122, 7123, 7124, 7125, 7126, 7127, 7128, 7129, 7130, 7131, 7132, 7133, 7134, 7135, 7136, 7137, 7138, 7139, 7140, 7141, 7142, 7143, 7144, 7145, 7146, 7147, 7148, 7149, 7150, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7161, 7162, 7163, 7164, 7165, 7166, 7167, 7168, 7169, 7170, 7171, 7172, 7173, 7174, 7175, 7176, 7177, 7178, 7179, 7180, 7181, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189, 7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199, 7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209, 7210, 7211, 7212, 7213, 7214, 7215, 7216, 7217, 7218, 7219, 7220, 7221, 7222, 7223, 7224, 7225, 7226, 7227, 7228, 7229, 7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7239, 7240, 7241, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249, 7250, 7251, 7252, 7253, 7254, 7255, 7256, 7257, 7258, 7259, 7260, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7268, 7269, 7270, 7271, 7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281, 7282, 7283, 7284, 7285, 7286, 7287, 7288, 7289, 7290, 7291, 7292, 7293, 7294, 7295, 7296, 7297, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309, 7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318, 7319, 7320, 7321, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7329, 7330, 7331, 7332, 7333, 7334, 7335, 7336, 7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7349, 7350, 7351, 7352, 7353, 7354, 7355, 7356, 7357, 7358, 7359, 7360, 7361, 7362, 7363, 7364, 7365, 7366, 7367, 7368, 7369, 7370, 7371, 7372, 7373, 7374, 7375, 7376, 7377, 7378, 7379, 7380, 7381, 7382, 7383, 7384, 7385, 7386, 7387, 7388, 7389, 7390, 7391, 7392, 7393, 7394, 7395, 7396, 7397, 7398, 7399, 7400, 7401, 7402, 7403, 7404, 7405, 7406, 7407, 7408, 7409, 7410, 7411, 7412, 7413, 7414, 7415, 7416, 7417, 7418, 7419, 7420, 7421, 7422, 7423, 7424, 7425, 7426, 7427, 7428, 7429, 7430, 7431, 7432, 7433, 7434, 7435, 7436, 7437, 7438, 7439, 7440, 7441, 7442, 7443, 7444, 7445, 7446, 7447, 7448, 7449, 7450, 7451, 7452, 7453, 7454, 7455, 7456, 7457, 7458, 7459, 7460, 7461, 7462, 7463, 7464, 7465, 7466, 7467, 7468, 7469, 7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7479, 7480, 7481, 7482, 7483, 7484, 7485, 7486, 7487, 7488, 7489, 7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499, 7500, 7501, 7502, 7503, 7504, 7505, 7506, 7507, 7508, 7509, 7510, 7511, 7512, 7513, 7514, 7515, 7516, 7517, 7518, 7519, 7520, 7521, 7522, 7523, 7524, 7525, 7526, 7527, 7528, 7529, 7530, 7531, 7532, 7533, 7534, 7535, 7536, 7537, 7538, 7539, 7540, 7541, 7542, 7543, 7544, 7545, 7546, 7547, 7548, 7549, 7550, 7551, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559, 7560, 7561, 7562, 7563, 7564, 7565, 7566, 7567, 7568, 7569, 7570, 7571, 7572, 7573, 7574, 7575, 7576, 7577, 7578, 7579, 7580, 7581, 7582, 7583, 7584, 7585, 7586, 7587, 7588, 7589, 7590, 7591, 7592, 7593, 7594, 7595, 7596, 7597, 7598, 7599, 7600, 7601, 7602, 7603, 7604, 7605, 7606, 7607, 7608, 7609, 7610, 7611, 7612, 7613, 7614, 7615, 7616, 7617, 7618, 7619, 7620, 7621, 7622, 7623, 7624, 7625, 7626, 7627, 7628, 7629, 7630, 7631, 7632, 7633, 7634, 7635, 7636, 7637, 7638, 7639, 7640, 7641, 7642, 7643, 7644, 7645, 7646, 7647, 7648, 7649, 7650, 7651, 7652, 7653, 7654, 7655, 7656, 7657, 7658, 7659, 7660, 7661, 7662, 7663, 7664, 7665, 7666, 7667, 7668, 7669, 7670, 7671, 7672, 7673, 7674, 7675, 7676, 7677, 7678, 7679, 7680, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689, 7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699, 12100, 12101, 12102, 12103, 12104, 12105, 12106, 12107, 12108, 12109, 12110, 12111, 12112, 12113, 12114, 12115, 12116, 12117, 12118, 12119, 12120, 12121, 12122, 12123, 12124, 12125, 12126, 12127, 12128, 12129, 12130, 12131, 12132, 12133, 12134, 12135, 12136, 12137, 12138, 12139, 12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149, 12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159, 12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169, 12170, 12171, 12172, 12173, 12174, 12175, 12176, 12177, 12178, 12179, 12180, 12181, 12182, 12183, 12184, 12185, 12186, 12187, 12188, 12189, 12190, 12191, 12192, 12193, 12194, 12195, 12196, 12197, 12198, 12199, 12200, 12201, 12202, 12203, 12204, 12205, 12206, 12207, 12208, 12209, 12210, 12211, 12212, 12213, 12214, 12215, 12216, 12217, 12218, 12219, 12220, 12221, 12222, 12223, 12224, 12225, 12226, 12227, 12228, 12229, 12230, 12231, 12232, 12233, 12234, 12235, 12236, 12237, 12238, 12239, 12240, 12241, 12242, 12243, 12244, 12245, 12246, 12247, 12248, 12249, 12250, 12251, 12252, 12253, 12254, 12255, 12256, 12257, 12258, 12259, 12260, 12261, 12262, 12263, 12264, 12265, 12266, 12267, 12268, 12269, 12270, 12271, 12272, 12273, 12274, 12275, 12276, 12277, 12278, 12279, 12280, 12281, 12282, 12283, 12284, 12285, 12286, 12287, 12288, 12289, 12290, 12291, 12292, 12293, 12294, 12295, 12296, 12297, 12298, 12299, 12300, 12301, 12302, 12303, 12304, 12305, 12306, 12307, 12308, 12309, 12310, 12311, 12312, 12313, 12314, 12315, 12316, 12317, 12318, 12319, 12320, 12321, 12322, 12323, 12324, 12325, 12326, 12327, 12328, 12329, 12330, 12331, 12332, 12333, 12334, 12335, 12336, 12337, 12338, 12339, 12340, 12341, 12342, 12343, 12344, 12345, 12346, 12347, 12348, 12349, 12350, 12351, 12352, 12353, 12354, 12355, 12356, 12357, 12358, 12359, 12360, 12361, 12362, 12363, 12364, 12365, 12366, 12367, 12368, 12369, 12370, 12371, 12372, 12373, 12374, 12375, 12376, 12377, 12378, 12379, 12380, 12381, 12382, 12383, 12384, 12385, 12386, 12387, 12388, 12389, 12390, 12391, 12392, 12393, 12394, 12395, 12396, 12397, 12398, 12399, 12400, 12401, 12402, 12403, 12404, 12405, 12406, 12407, 12408, 12409, 12410, 12411, 12412, 12413, 12414, 12415, 12416, 12417, 12418, 12419, 12420, 12421, 12422, 12423, 12424, 12425, 12426, 12427, 12428, 12429, 12430, 12431, 12432, 12433, 12434, 12435, 12436, 12437, 12438, 12439, 12440, 12441, 12442, 12443, 12444, 12445, 12446, 12447, 12448, 12449, 12450, 12451, 12452, 12453, 12454, 12455, 12456, 12457, 12458, 12459, 12460, 12461, 12462, 12463, 12464, 12465, 12466, 12467, 12468, 12469, 12470, 12471, 12472, 12473, 12474, 12475, 12476, 12477, 12478, 12479, 12480, 12481, 12482, 12483, 12484, 12485, 12486, 12487, 12488, 12489, 12490, 12491, 12492, 12493, 12494, 12495, 12496, 12497, 12498, 12499, 12500, 12501, 12502, 12503, 12504, 12505, 12506, 12507, 12508, 12509, 12510, 12511, 12512, 12513, 12514, 12515, 12516, 12517, 12518, 12519, 12520, 12521, 12522, 12523, 12524, 12525, 12526, 12527, 12528, 12529, 12530, 12531, 12532, 12533, 12534, 12535, 12536, 12537, 12538, 12539, 12540, 12541, 12542, 12543, 12544, 12545, 12546, 12547, 12548, 12549, 12550, 12551, 12552, 12553, 12554, 12555, 12556, 12557, 12558, 12559, 12560, 12561, 12562, 12563, 12564, 12565, 12566, 12567, 12568, 12569, 12570, 12571, 12572, 12573, 12574, 12575, 12576, 12577, 12578, 12579, 12580, 12581, 12582, 12583, 12584, 12585, 12586, 12587, 12588, 12589, 12590, 12591, 12592, 12593, 12594, 12595, 12596, 12597, 12598, 12599, 12600, 12601, 12602, 12603, 12604, 12605, 12606, 12607, 12608, 12609, 12610, 12611, 12612, 12613, 12614, 12615, 12616, 12617, 12618, 12619, 12620, 12621, 12622, 12623, 12624, 12625, 12626, 12627, 12628, 12629, 12630, 12631, 12632, 12633, 12634, 12635, 12636, 12637, 12638, 12639, 12640, 12641, 12642, 12643, 12644, 12645, 12646, 12647, 12648, 12649, 12650, 12651, 12652, 12653, 12654, 12655, 12656, 12657, 12658, 12659, 12660, 12661, 12662, 12663, 12664, 12665, 12666, 12667, 12668, 12669, 12670, 12671, 12672, 12673, 12674, 12675, 12676, 12677, 12678, 12679, 12680, 12681, 12682, 12683, 12684, 12685, 12686, 12687, 12688, 12689, 12690, 12691, 12692, 12693, 12694, 12695, 12696, 12697, 12698, 12699, 12700, 12701, 12702, 12703, 12704, 12705, 12706, 12707, 12708, 12709, 12710, 12711, 12712, 12713, 12714, 12715, 12716, 12717, 12718, 12719, 12720, 12721, 12722, 12723, 12724, 12725, 12726, 12727, 12728, 12729, 12730, 12731, 12732, 12733, 12734, 12735, 12736, 12737, 12738, 12739, 12740, 12741, 12742, 12743, 12744, 12745, 12746, 12747, 12748, 12749, 12750, 12751, 12752, 12753, 12754, 12755, 12756, 12757, 12758, 12759, 12760, 12761, 12762, 12763, 12764, 12765, 12766, 12767, 12768, 12769, 12770, 12771, 12772, 12773, 12774, 12775, 12776, 12777, 12778, 12779, 12780, 12781, 12782, 12783, 12784, 12785, 12786, 12787, 12788, 12789, 12790, 12791, 12792, 12793, 12794, 12795, 12796, 12797, 12798, 12799, 12800, 12801, 12802, 12803, 12804, 12805, 12806, 12807, 12808, 12809, 12810, 12811, 12812, 12813, 12814, 12815, 12816, 12817, 12818, 12819, 12820, 12821, 12822, 12823, 12824, 12825, 12826, 12827, 12828, 12829, 12830, 12831, 12832, 12833, 12834, 12835, 12836, 12837, 12838, 12839, 12840, 12841, 12842, 12843, 12844, 12845, 12846, 12847, 12848, 12849, 12850, 12851, 12852, 12853, 12854, 12855, 12856, 12857, 12858, 12859, 12860, 12861, 12862, 12863, 12864, 12865, 12866, 12867, 12868, 12869, 12870, 12871, 12872, 12873, 12874, 12875, 12876, 12877, 12878, 12879, 12880, 12881, 12882, 12883, 12884, 12885, 12886, 12887, 12888, 12889, 12890, 12891, 12892, 12893, 12894, 12895, 12896, 12897, 12898, 12899, 12900, 12901, 12902, 12903, 12904, 12905, 12906, 12907, 12908, 12909, 12910, 12911, 12912, 12913, 12914, 12915, 12916, 12917, 12918, 12919, 12920, 12921, 12922, 12923, 12924, 12925, 12926, 12927, 12928, 12929, 12930, 12931, 12932, 12933, 12934, 12935, 12936, 12937, 12938, 12939, 12940, 12941, 12942, 12943, 12944, 12945, 12946, 12947, 12948, 12949, 12950, 12951, 12952, 12953, 12954, 12955, 12956, 12957, 12958, 12959, 12960, 12961, 12962, 12963, 12964, 12965, 12966, 12967, 12968, 12969, 12970, 12971, 12972, 12973, 12974, 12975, 12976, 12977, 12978, 12979, 12980, 12981, 12982, 12983, 12984, 12985, 12986, 12987, 12988, 12989, 12990, 12991, 12992, 12993, 12994, 12995, 12996, 12997, 12998, 12999, 13000, 13001, 13002, 13003, 13004, 13005, 13006, 13007, 13008, 13009, 13010, 13011, 13012, 13013, 13014, 13015, 13016, 13017, 13018, 13019, 13020, 13021, 13022, 13023, 13024, 13025, 13026, 13027, 13028, 13029, 13030, 13031, 13032, 13033, 13034, 13035, 13036, 13037, 13038, 13039, 13040, 13041, 13042, 13043, 13044, 13045, 13046, 13047, 13048, 13049, 13050, 13051, 13052, 13053, 13054, 13055, 13056, 13057, 13058, 13059, 13060, 13061, 13062, 13063, 13064, 13065, 13066, 13067, 13068, 13069, 13070, 13071, 13072, 13073, 13074, 13075, 13076, 13077, 13078, 13079, 13080, 13081, 13082, 13083, 13084, 13085, 13086, 13087, 13088, 13089, 13090, 13091, 13092, 13093, 13094, 13095, 13096, 13097, 13098, 13099, 13100, 13101, 13102, 13103, 13104, 13105, 13106, 13107, 13108, 13109, 13110, 13111, 13112, 13113, 13114, 13115, 13116, 13117, 13118, 13119, 13120, 13121, 13122, 13123, 13124, 13125, 13126, 13127, 13128, 13129, 13130, 13131, 13132, 13133, 13134, 13135, 13136, 13137, 13138, 13139, 13140, 13141, 13142, 13143, 13144, 13145, 13146, 13147, 13148, 13149, 13150, 13151, 13152, 13153, 13154, 13155, 13156, 13157, 13158, 13159, 13160, 13161, 13162, 13163, 13164, 13165, 13166, 13167, 13168, 13169, 13170, 13171, 13172, 13173, 13174, 13175, 13176, 13177, 13178, 13179, 13180, 13181, 13182, 13183, 13184, 13185, 13186, 13187, 13188, 13189, 13190, 13191, 13192, 13193, 13194, 13195, 13196, 13197, 13198, 13199, 13200, 13201, 13202, 13203, 13204, 13205, 13206, 13207, 13208, 13209, 13210, 13211, 13212, 13213, 13214, 13215, 13216, 13217, 13218, 13219, 13220, 13221, 13222, 13223, 13224, 13225, 13226, 13227, 13228, 13229, 13230, 13231, 13232, 13233, 13234, 13235, 13236, 13237, 13238, 13239, 13240, 13241, 13242, 13243, 13244, 13245, 13246, 13247, 13248, 13249, 13250, 13251, 13252, 13253, 13254, 13255, 13256, 13257, 13258, 13259, 13260, 13261, 13262, 13263, 13264, 13265, 13266, 13267, 13268, 13269, 13270, 13271, 13272, 13273, 13274, 13275, 13276, 13277, 13278, 13279, 13280, 13281, 13282, 13283, 13284, 13285, 13286, 13287, 13288, 13289, 13290, 13291, 13292, 13293, 13294, 13295, 13296, 13297, 13298, 13299, 13300, 13301, 13302, 13303, 13304, 13305, 13306, 13307, 13308, 13309, 13310, 13311, 13312, 13313, 13314, 13315, 13316, 13317, 13318, 13319, 13320, 13321, 13322, 13323, 13324, 13325, 13326, 13327, 13328, 13329, 13330, 13331, 13332, 13333, 13334, 13335, 13336, 13337, 13338, 13339, 13340, 13341, 13342, 13343, 13344, 13345, 13346, 13347, 13348, 13349, 13350, 13351, 13352, 13353, 13354, 13355, 13356, 13357, 13358, 13359, 13360, 13361, 13362, 13363, 13364, 13365, 13366, 13367, 13368, 13369, 13370, 13371, 13372, 13373, 13374, 13375, 13376, 13377, 13378, 13379, 13380, 13381, 13382, 13383, 13384, 13385, 13386, 13387, 13388, 13389, 13390, 13391, 13392, 13393, 13394, 13395, 13396, 13397, 13398, 13399, 13400, 13401, 13402, 13403, 13404, 13405, 13406, 13407, 13408, 13409, 13410, 13411, 13412, 13413, 13414, 13415, 13416, 13417, 13418, 13419, 13420, 13421, 13422, 13423, 13424, 13425, 13426, 13427, 13428, 13429, 13430, 13431, 13432, 13433, 13434, 13435, 13436, 13437, 13438, 13439, 13440, 13441, 13442, 13443, 13444, 13445, 13446, 13447, 13448, 13449, 13450, 13451, 13452, 13453, 13454, 13455, 13456, 13457, 13458, 13459, 13460, 13461, 13462, 13463, 13464, 13465, 13466, 13467, 13468, 13469, 13470, 13471, 13472, 13473, 13474, 13475, 13476, 13477, 13478, 13479, 13480, 13481, 13482, 13483, 13484, 13485, 13486, 13487, 13488, 13489, 13490, 13491, 13492, 13493, 13494, 13495, 13496, 13497, 13498, 13499, 13500, 13501, 13502, 13503, 13504, 13505, 13506, 13507, 13508, 13509, 13510, 13511, 13512, 13513, 13514, 13515, 13516, 13517, 13518, 13519, 13520, 13521, 13522, 13523, 13524, 13525, 13526, 13527, 13528, 13529, 13530, 13531, 13532, 13533, 13534, 13535, 13536, 13537, 13538, 13539, 13540, 13541, 13542, 13543, 13544, 13545, 13546, 13547, 13548, 13549, 13550, 13551, 13552, 13553, 13554, 13555, 13556, 13557, 13558, 13559, 13560, 13561, 13562, 13563, 13564, 13565, 13566, 13567, 13568, 13569, 13570, 13571, 13572, 13573, 13574, 13575, 13576, 13577, 13578, 13579, 13580, 13581, 13582, 13583, 13584, 13585, 13586, 13587, 13588, 13589, 13590, 13591, 13592, 13593, 13594, 13595, 13596, 13597, 13598, 13599, 13600, 13601, 13602, 13603, 13604, 13605, 13606, 13607, 13608, 13609, 13610, 13611, 13612, 13613, 13614, 13615, 13616, 13617, 13618, 13619, 13620, 13621, 13622, 13623, 13624, 13625, 13626, 13627, 13628, 13629, 13630, 13631, 13632, 13633, 13634, 13635, 13636, 13637, 13638, 13639, 13640, 13641, 13642, 13643, 13644, 13645, 13646, 13647, 13648, 13649, 13650, 13651, 13652, 13653, 13654, 13655, 13656, 13657, 13658, 13659, 13660, 13661, 13662, 13663, 13664, 13665, 13666, 13667, 13668, 13669, 13670, 13671, 13672, 13673, 13674, 13675, 13676, 13677, 13678, 13679, 13680, 13681, 13682, 13683, 13684, 13685, 13686, 13687, 13688, 13689, 13690, 13691, 13692, 13693, 13694, 13695, 13696, 13697, 13698, 13699, 13700, 13701, 13702, 13703, 13704, 13705, 13706, 13707, 13708, 13709, 13710, 13711, 13712, 13713, 13714, 13715, 13716, 13717, 13718, 13719, 13720, 13721, 13722, 13723, 13724, 13725, 13726, 13727, 13728, 13729, 13730, 13731, 13732, 13733, 13734, 13735, 13736, 13737, 13738, 13739, 13740, 13741, 13742, 13743, 13744, 13745, 13746, 13747, 13748, 13749, 13750, 13751, 13752, 13753, 13754, 13755, 13756, 13757, 13758, 13759, 13760, 13761, 13762, 13763, 13764, 13765, 13766, 13767, 13768, 13769, 13770, 13771, 13772, 13773, 13774, 13775, 13776, 13777, 13778, 13779, 13780, 13781, 13782, 13783, 13784, 13785, 13786, 13787, 13788, 13789, 13790, 13791, 13792, 13793, 13794, 13795, 13796, 13797, 13798, 13799, 13800, 13801, 13802, 13803, 13804, 13805, 13806, 13807, 13808, 13809, 13810, 13811, 13812, 13813, 13814, 13815, 13816, 13817, 13818, 13819, 13820, 13821, 13822, 13823, 13824, 13825, 13826, 13827, 13828, 13829, 13830, 13831, 13832, 13833, 13834, 13835, 13836, 13837, 13838, 13839, 13840, 13841, 13842, 13843, 13844, 13845, 13846, 13847, 13848, 13849, 13850, 13851, 13852, 13853, 13854, 13855, 13856, 13857, 13858, 13859, 13860, 13861, 13862, 13863, 13864, 13865, 13866, 13867, 13868, 13869, 13870, 13871, 13872, 13873, 13874, 13875, 13876, 13877, 13878, 13879, 13880, 13881, 13882, 13883, 13884, 13885, 13886, 13887, 13888, 13889, 13890, 13891, 13892, 13893, 13894, 13895, 13896, 13897, 13898, 13899, 13900, 13901, 13902, 13903, 13904, 13905, 13906, 13907, 13908, 13909, 13910, 13911, 13912, 13913, 13914, 13915, 13916, 13917, 13918, 13919, 13920, 13921, 13922, 13923, 13924, 13925, 13926, 13927, 13928, 13929, 13930, 13931, 13932, 13933, 13934, 13935, 13936, 13937, 13938, 13939, 13940, 13941, 13942, 13943, 13944, 13945, 13946, 13947, 13948, 13949, 13950, 13951, 13952, 13953, 13954, 13955, 13956, 13957, 13958, 13959, 13960, 13961, 13962, 13963, 13964, 13965, 13966, 13967, 13968, 13969, 13970, 13971, 13972, 13973, 13974, 13975, 13976, 13977, 13978, 13979, 13980, 13981, 13982, 13983, 13984, 13985, 13986, 13987, 13988, 13989, 13990, 13991, 13992, 13993, 13994, 13995, 13996, 13997, 13998, 13999, 14000, 14001, 14002, 14003, 14004, 14005, 14006, 14007, 14008, 14009, 14010, 14011, 14012, 14013, 14014, 14015, 14016, 14017, 14018, 14019, 14020, 14021, 14022, 14023, 14024, 14025, 14026, 14027, 14028, 14029, 14030, 14031, 14032, 14033, 14034, 14035, 14036, 14037, 14038, 14039, 14040, 14041, 14042, 14043, 14044, 14045, 14046, 14047, 14048, 14049, 14050, 14051, 14052, 14053, 14054, 14055, 14056, 14057, 14058, 14059, 14060, 14061, 14062, 14063, 14064, 14065, 14066, 14067, 14068, 14069, 14070, 14071, 14072, 14073, 14074, 14075, 14076, 14077, 14078, 14079, 14080, 14081, 14082, 14083, 14084, 14085, 14086, 14087, 14088, 14089, 14090, 14091, 14092, 14093, 14094, 14095, 14096, 14097, 14098, 14099, 14100, 14101, 14102, 14103, 14104, 14105, 14106, 14107, 14108, 14109, 14110, 14111, 14112, 14113, 14114, 14115, 14116, 14117, 14118, 14119, 14120, 14121, 14122, 14123, 14124, 14125, 14126, 14127, 14128, 14129, 14130, 14131, 14132, 14133, 14134, 14135, 14136, 14137, 14138, 14139, 14140, 14141, 14142, 14143, 14144, 14145, 14146, 14147, 14148, 14149, 14150, 14151, 14152, 14153, 14154, 14155, 14156, 14157, 14158, 14159, 14160, 14161, 14162, 14163, 14164, 14165, 14166, 14167, 14168, 14169, 14170, 14171, 14172, 14173, 14174, 14175, 14176, 14177, 14178, 14179, 14180, 14181, 14182, 14183, 14184, 14185, 14186, 14187, 14188, 14189, 14190, 14191, 14192, 14193, 14194, 14195, 14196, 14197, 14198, 14199, 14200, 14201, 14202, 14203, 14204, 14205, 14206, 14207, 14208, 14209, 14210, 14211, 14212, 14213, 14214, 14215, 14216, 14217, 14218, 14219, 14220, 14221, 14222, 14223, 14224, 14225, 14226, 14227, 14228, 14229, 14230, 14231, 14232, 14233, 14234, 14235, 14236, 14237, 14238, 14239, 14240, 14241, 14242, 14243, 14244, 14245, 14246, 14247, 14248, 14249, 14250, 14251, 14252, 14253, 14254, 14255, 14256, 14257, 14258, 14259, 14260, 14261, 14262, 14263, 14264, 14265, 14266, 14267, 14268, 14269, 14270, 14271, 14272, 14273, 14274, 14275, 14276, 14277, 14278, 14279, 14280, 14281, 14282, 14283, 14284, 14285, 14286, 14287, 14288, 14289, 14290, 14291, 14292, 14293, 14294, 14295, 14296, 14297, 14298, 14299, 14300, 14301, 14302, 14303, 14304, 14305, 14306, 14307, 14308, 14309, 14310, 14311, 14312, 14313, 14314, 14315, 14316, 14317, 14318, 14319, 14320, 14321, 14322, 14323, 14324, 14325, 14326, 14327, 14328, 14329, 14330, 14331, 14332, 14333, 14334, 14335, 14336, 14337, 14338, 14339, 14340, 14341, 14342, 14343, 14344, 14345, 14346, 14347, 14348, 14349, 14350, 14351, 14352, 14353, 14354, 14355, 14356, 14357, 14358, 14359, 14360, 14361, 14362, 14363, 14364, 14365, 14366, 14367, 14368, 14369, 14370, 14371, 14372, 14373, 14374, 14375, 14376, 14377, 14378, 14379, 14380, 14381, 14382, 14383, 14384, 14385, 14386, 14387, 14388, 14389, 14390, 14391, 14392, 14393, 14394, 14395, 14396, 14397, 14398, 14399, 14400, 14401, 14402, 14403, 14404, 14405, 14406, 14407, 14408, 14409, 14410, 14411, 14412, 14413, 14414, 14415, 14416, 14417, 14418, 14419, 14420, 14421, 14422, 14423, 14424, 14425, 14426, 14427, 14428, 14429, 14430, 14431, 14432, 14433, 14434, 14435, 14436, 14437, 14438, 14439, 14440, 14441, 14442, 14443, 14444, 14445, 14446, 14447, 14448, 14449, 14450, 14451, 14452, 14453, 14454, 14455, 14456, 14457, 14458, 14459, 14460, 14461, 14462, 14463, 14464, 14465, 14466, 14467, 14468, 14469, 14470, 14471, 14472, 14473, 14474, 14475, 14476, 14477, 14478, 14479, 14480, 14481, 14482, 14483, 14484, 14485, 14486, 14487, 14488, 14489, 14490, 14491, 14492, 14493, 14494, 14495, 14496, 14497, 14498, 14499, 14500, 14501, 14502, 14503, 14504, 14505, 14506, 14507, 14508, 14509, 14510, 14511, 14512, 14513, 14514, 14515, 14516, 14517, 14518, 14519, 14520, 14521, 14522, 14523, 14524, 14525, 14526, 14527, 14528, 14529, 14530, 14531, 14532, 14533, 14534, 14535, 14536, 14537, 14538, 14539, 14540, 14541, 14542, 14543, 14544, 14545, 14546, 14547, 14548, 14549, 14550, 14551, 14552, 14553, 14554, 14555, 14556, 14557, 14558, 14559, 14560, 14561, 14562, 14563, 14564, 14565, 14566, 14567, 14568, 14569, 14570, 14571, 14572, 14573, 14574, 14575, 14576, 14577, 14578, 14579, 14580, 14581, 14582, 14583, 14584, 14585, 14586, 14587, 14588, 14589, 14590, 14591, 14592, 14593, 14594, 14595, 14596, 14597, 14598, 14599, 14600, 14601, 14602, 14603, 14604, 14605, 14606, 14607, 14608, 14609, 14610, 14611, 14612, 14613, 14614, 14615, 14616, 14617, 14618, 14619, 14620, 14621, 14622, 14623, 14624, 14625, 14626, 14627, 14628, 14629, 14630, 14631, 14632, 14633, 14634, 14635, 14636, 14637, 14638, 14639, 14640, 14641, 14642, 14643, 14644, 14645, 14646, 14647, 14648, 14649, 14650, 14651, 14652, 14653, 14654, 14655, 14656, 14657, 14658, 14659, 14660, 14661, 14662, 14663, 14664, 14665, 14666, 14667, 14668, 14669, 14670, 14671, 14672, 14673, 14674, 14675, 14676, 14677, 14678, 14679, 14680, 14681, 14682, 14683, 14684, 14685, 14686, 14687, 14688, 14689, 14690, 14691, 14692, 14693, 14694, 14695, 14696, 14697, 14698, 14699, 14700, 14701, 14702, 14703, 14704, 14705, 14706, 14707, 14708, 14709, 14710, 14711, 14712, 14713, 14714, 14715, 14716, 14717, 14718, 14719, 14720, 14721, 14722, 14723, 14724, 14725, 14726, 14727, 14728, 14729, 14730, 14731, 14732, 14733, 14734, 14735, 14736, 14737, 14738, 14739, 14740, 14741, 14742, 14743, 14744, 14745, 14746, 14747, 14748, 14749, 14750, 14751, 14752, 14753, 14754, 14755, 14756, 14757, 14758, 14759, 14760, 14761, 14762, 14763, 14764, 14765, 14766, 14767, 14768, 14769, 14770, 14771, 14772, 14773, 14774, 14775, 14776, 14777, 14778, 14779, 14780, 14781, 14782, 14783, 14784, 14785, 14786, 14787, 14788, 14789, 14790, 14791, 14792, 14793, 14794, 14795, 14796, 14797, 14798, 14799, 14800, 14801, 14802, 14803, 14804, 14805, 14806, 14807, 14808, 14809, 14810, 14811, 14812, 14813, 14814, 14815, 14816, 14817, 14818, 14819, 14820, 14821, 14822, 14823, 14824, 14825, 14826, 14827, 14828, 14829, 14830, 14831, 14832, 14833, 14834, 14835, 14836, 14837, 14838, 14839, 14840, 14841, 14842, 14843, 14844, 14845, 14846, 14847, 14848, 14849, 14850, 14851, 14852, 14853, 14854, 14855, 14856, 14857, 14858, 14859, 14860, 14861, 14862, 14863, 14864, 14865, 14866, 14867, 14868, 14869, 14870, 14871, 14872, 14873, 14874, 14875, 14876, 14877, 14878, 14879, 14880, 14881, 14882, 14883, 14884, 14885, 14886, 14887, 14888, 14889, 14890, 14891, 14892, 14893, 14894, 14895, 14896, 14897, 14898, 14899, 14900, 14901, 14902, 14903, 14904, 14905, 14906, 14907, 14908, 14909, 14910, 14911, 14912, 14913, 14914, 14915, 14916, 14917, 14918, 14919, 14920, 14921, 14922, 14923, 14924, 14925, 14926, 14927, 14928, 14929, 14930, 14931, 14932, 14933, 14934, 14935, 14936, 14937, 14938, 14939, 14940, 14941, 14942, 14943, 14944, 14945, 14946, 14947, 14948, 14949, 14950, 14951, 14952, 14953, 14954, 14955, 14956, 14957, 14958, 14959, 14960, 14961, 14962, 14963, 14964, 14965, 14966, 14967, 14968, 14969, 14970, 14971, 14972, 14973, 14974, 14975, 14976, 14977, 14978, 14979, 14980, 14981, 14982, 14983, 14984, 14985, 14986, 14987, 14988, 14989, 14990, 14991, 14992, 14993, 14994, 14995, 14996, 14997, 14998, 14999, 15000, 15001, 15002, 15003, 15004, 15005, 15006, 15007, 15008, 15009, 15010, 15011, 15012, 15013, 15014, 15015, 15016, 15017, 15018, 15019, 15020, 15021, 15022, 15023, 15024, 15025, 15026, 15027, 15028, 15029, 15030, 15031, 15032, 15033, 15034, 15035, 15036, 15037, 15038, 15039, 15040, 15041, 15042, 15043, 15044, 15045, 15046, 15047, 15048, 15049, 15050, 15051, 15052, 15053, 15054, 15055, 15056, 15057, 15058, 15059, 15060, 15061, 15062, 15063, 15064, 15065, 15066, 15067, 15068, 15069, 15070, 15071, 15072, 15073, 15074, 15075, 15076, 15077, 15078, 15079, 15080, 15081, 15082, 15083, 15084, 15085, 15086, 15087, 15088, 15089, 15090, 15091, 15092, 15093, 15094, 15095, 15096, 15097, 15098, 15099, 15100, 15101, 15102, 15103, 15104, 15105, 15106, 15107, 15108, 15109, 15110, 15111, 15112, 15113, 15114, 15115, 15116, 15117, 15118, 15119, 15120, 15121, 15122, 15123, 15124, 15125, 15126, 15127, 15128, 15129, 15130, 15131, 15132, 15133, 15134, 15135, 15136, 15137, 15138, 15139, 15140, 15141, 15142, 15143, 15144, 15145, 15146, 15147, 15148, 15149, 15150, 15151, 15152, 15153, 15154, 15155, 15156, 15157, 15158, 15159, 15160, 15161, 15162, 15163, 15164, 15165, 15166, 15167, 15168, 15169, 15170, 15171, 15172, 15173, 15174, 15175, 15176, 15177, 15178, 15179, 15180, 15181, 15182, 15183, 15184, 15185, 15186, 15187, 15188, 15189, 15190, 15191, 15192, 15193, 15194, 15195, 15196, 15197, 15198, 15199, 15200, 15201, 15202, 15203, 15204, 15205, 15206, 15207, 15208, 15209, 15210, 15211, 15212, 15213, 15214, 15215, 15216, 15217, 15218, 15219, 15220, 15221, 15222, 15223, 15224, 15225, 15226, 15227, 15228, 15229, 15230, 15231, 15232, 15233, 15234, 15235, 15236, 15237, 15238, 15239, 15240, 15241, 15242, 15243, 15244, 15245, 15246, 15247, 15248, 15249, 15250, 15251, 15252, 15253, 15254, 15255, 15256, 15257, 15258, 15259, 15260, 15261, 15262, 15263, 15264, 15265, 15266, 15267, 15268, 15269, 15270, 15271, 15272, 15273, 15274, 15275, 15276, 15277, 15278, 15279, 15280, 15281, 15282, 15283, 15284, 15285, 15286, 15287, 15288, 15289, 15290, 15291, 15292, 15293, 15294, 15295, 15296, 15297, 15298, 15299, 15300, 15301, 15302, 15303, 15304, 15305, 15306, 15307, 15308, 15309, 15310, 15311, 15312, 15313, 15314, 15315, 15316, 15317, 15318, 15319, 15320, 15321, 15322, 15323, 15324, 15325, 15326, 15327, 15328, 15329, 15330, 15331, 15332, 15333, 15334, 15335, 15336, 15337, 15338, 15339, 15340, 15341, 15342, 15343, 15344, 15345, 15346, 15347, 15348, 15349, 15350, 15351, 15352, 15353, 15354, 15355, 15356, 15357, 15358, 15359, 15360, 15361, 15362, 15363, 15364, 15365, 15366, 15367, 15368, 15369, 15370, 15371, 15372, 15373, 15374, 15375, 15376, 15377, 15378, 15379, 15380, 15381, 15382, 15383, 15384, 15385, 15386, 15387, 15388, 15389, 15390, 15391, 15392, 15393, 15394, 15395, 15396, 15397, 15398, 15399, 15400, 15401, 15402, 15403, 15404, 15405, 15406, 15407, 15408, 15409, 15410, 15411, 15412, 15413, 15414, 15415, 15416, 15417, 15418, 15419, 15420, 15421, 15422, 15423, 15424, 15425, 15426, 15427, 15428, 15429, 15430, 15431, 15432, 15433, 15434, 15435, 15436, 15437, 15438, 15439, 15440, 15441, 15442, 15443, 15444, 15445, 15446, 15447, 15448, 15449, 15450, 15451, 15452, 15453, 15454, 15455, 15456, 15457, 15458, 15459, 15460, 15461, 15462, 15463, 15464, 15465, 15466, 15467, 15468, 15469, 15470, 15471, 15472, 15473, 15474, 15475, 15476, 15477, 15478, 15479, 15480, 15481, 15482, 15483, 15484, 15485, 15486, 15487, 15488, 15489, 15490, 15491, 15492, 15493, 15494, 15495, 15496, 15497, 15498, 15499, 15500, 15501, 15502, 15503, 15504, 15505, 15506, 15507, 15508, 15509, 15510, 15511, 15512, 15513, 15514, 15515, 15516, 15517, 15518, 15519, 15520, 15521, 15522, 15523, 15524, 15525, 15526, 15527, 15528, 15529, 15530, 15531, 15532, 15533, 15534, 15535, 15536, 15537, 15538, 15539, 15540, 15541, 15542, 15543, 15544, 15545, 15546, 15547, 15548, 15549, 15550, 15551, 15552, 15553, 15554, 15555, 15556, 15557, 15558, 15559, 15560, 15561, 15562, 15563, 15564, 15565, 15566, 15567, 15568, 15569, 15570, 15571, 15572, 15573, 15574, 15575, 15576, 15577, 15578, 15579, 15580, 15581, 15582, 15583, 15584, 15585, 15586, 15587, 15588, 15589, 15590, 15591, 15592, 15593, 15594, 15595, 15596, 15597, 15598, 15599, 15600, 15601, 15602, 15603, 15604, 15605, 15606, 15607, 15608, 15609, 15610, 15611, 15612, 15613, 15614, 15615, 15616, 15617, 15618, 15619, 15620, 15621, 15622, 15623, 15624, 15625, 15626, 15627, 15628, 15629, 15630, 15631, 15632, 15633, 15634, 15635, 15636, 15637, 15638, 15639, 15640, 15641, 15642, 15643, 15644, 15645, 15646, 15647, 15648, 15649, 15650, 15651, 15652, 15653, 15654, 15655, 15656, 15657, 15658, 15659, 15660, 15661, 15662, 15663, 15664, 15665, 15666, 15667, 15668, 15669, 15670, 15671, 15672, 15673, 15674, 15675, 15676, 15677, 15678, 15679, 15680, 15681, 15682, 15683, 15684, 15685, 15686, 15687, 15688, 15689, 15690, 15691, 15692, 15693, 15694, 15695, 15696, 15697, 15698, 15699, 15700, 15701, 15702, 15703, 15704, 15705, 15706, 15707, 15708, 15709, 15710, 15711, 15712, 15713, 15714, 15715, 15716, 15717, 15718, 15719, 15720, 15721, 15722, 15723, 15724, 15725, 15726, 15727, 15728, 15729, 15730, 15731, 15732, 15733, 15734, 15735, 15736, 15737, 15738, 15739, 15740, 15741, 15742, 15743, 15744, 15745, 15746, 15747, 15748, 15749, 15750, 15751, 15752, 15753, 15754, 15755, 15756, 15757, 15758, 15759, 15760, 15761, 15762, 15763, 15764, 15765, 15766, 15767, 15768, 15769, 15770, 15771, 15772, 15773, 15774, 15775, 15776, 15777, 15778, 15779, 15780, 15781, 15782, 15783, 15784, 15785, 15786, 15787, 15788, 15789, 15790, 15791, 15792, 15793, 15794, 15795, 15796, 15797, 15798, 15799, 15800, 15801, 15802, 15803, 15804, 15805, 15806, 15807, 15808, 15809, 15810, 15811, 15812, 15813, 15814, 15815, 15816, 15817, 15818, 15819, 15820, 15821, 15822, 15823, 15824, 15825, 15826, 15827, 15828, 15829, 15830, 15831, 15832, 15833, 15834, 15835, 15836, 15837, 15838, 15839, 15840, 15841, 15842, 15843, 15844, 15845, 15846, 15847, 15848, 15849, 15850, 15851, 15852, 15853, 15854, 15855, 15856, 15857, 15858, 15859, 15860, 15861, 15862, 15863, 15864, 15865, 15866, 15867, 15868, 15869, 15870, 15871, 15872, 15873, 15874, 15875, 15876, 15877, 15878, 15879, 15880, 15881, 15882, 15883, 15884, 15885, 15886, 15887, 15888, 15889, 15890, 15891, 15892, 15893, 15894, 15895, 15896, 15897, 15898, 15899, 15900, 15901, 15902, 15903, 15904, 15905, 15906, 15907, 15908, 15909, 15910, 15911, 15912, 15913, 15914, 15915, 15916, 15917, 15918, 15919, 15920, 15921, 15922, 15923, 15924, 15925, 15926, 15927, 15928, 15929, 15930, 15931, 15932, 15933, 15934, 15935, 15936, 15937, 15938, 15939, 15940, 15941, 15942, 15943, 15944, 15945, 15946, 15947, 15948, 15949, 15950, 15951, 15952, 15953, 15954, 15955, 15956, 15957, 15958, 15959, 15960, 15961, 15962, 15963, 15964, 15965, 15966, 15967, 15968, 15969, 15970, 15971, 15972, 15973, 15974, 15975, 15976, 15977, 15978, 15979, 15980, 15981, 15982, 15983, 15984, 15985, 15986, 15987, 15988, 15989, 15990, 15991, 15992, 15993, 15994, 15995, 15996, 15997, 15998, 15999, 16000, 16001, 16002, 16003, 16004, 16005, 16006, 16007, 16008, 16009, 16010, 16011, 16012, 16013, 16014, 16015, 16016, 16017, 16018, 16019, 16020, 16021, 16022, 16023, 16024, 16025, 16026, 16027, 16028, 16029, 16030, 16031, 16032, 16033, 16034, 16035, 16036, 16037, 16038, 16039, 16040, 16041, 16042, 16043, 16044, 16045, 16046, 16047, 16048, 16049, 16050, 16051, 16052, 16053, 16054, 16055, 16056, 16057, 16058, 16059, 16060, 16061, 16062, 16063, 16064, 16065, 16066, 16067, 16068, 16069, 16070, 16071, 16072, 16073, 16074, 16075, 16076, 16077, 16078, 16079, 16080, 16081, 16082, 16083, 16084, 16085, 16086, 16087, 16088, 16089, 16090, 16091, 16092, 16093, 16094, 16095, 16096, 16097, 16098, 16099, 16100, 16101, 16102, 16103, 16104, 16105, 16106, 16107, 16108, 16109, 16110, 16111, 16112, 16113, 16114, 16115, 16116, 16117, 16118, 16119, 16120, 16121, 16122, 16123, 16124, 16125, 16126, 16127, 16128, 16129, 16130, 16131, 16132, 16133, 16134, 16135, 16136, 16137, 16138, 16139, 16140, 16141, 16142, 16143, 16144, 16145, 16146, 16147, 16148, 16149, 16150, 16151, 16152, 16153, 16154, 16155, 16156, 16157, 16158, 16159, 16160, 16161, 16162, 16163, 16164, 16165, 16166, 16167, 16168, 16169, 16170, 16171, 16172, 16173, 16174, 16175, 16176, 16177, 16178, 16179, 16180, 16181, 16182, 16183, 16184, 16185, 16186, 16187, 16188, 16189, 16190, 16191, 16192, 16193, 16194, 16195, 16196, 16197, 16198, 16199, 16200, 16201, 16202, 16203, 16204, 16205, 16206, 16207, 16208, 16209, 16210, 16211, 16212, 16213, 16214, 16215, 16216, 16217, 16218, 16219, 16220, 16221, 16222, 16223, 16224, 16225, 16226, 16227, 16228, 16229, 16230, 16231, 16232, 16233, 16234, 16235, 16236, 16237, 16238, 16239, 16240, 16241, 16242, 16243, 16244, 16245, 16246, 16247, 16248, 16249, 16250, 16251, 16252, 16253, 16254, 16255, 16256, 16257, 16258, 16259, 16260, 16261, 16262, 16263, 16264, 16265, 16266, 16267, 16268, 16269, 16270, 16271, 16272, 16273, 16274, 16275, 16276, 16277, 16278, 16279, 16280, 16281, 16282, 16283, 16284, 16285, 16286, 16287, 16288, 16289, 16290, 16291, 16292, 16293, 16294, 16295, 16296, 16297, 16298, 16299, 16300, 16301, 16302, 16303, 16304, 16305, 16306, 16307, 16308, 16309, 16310, 16311, 16312, 16313, 16314, 16315, 16316, 16317, 16318, 16319, 16320, 16321, 16322, 16323, 16324, 16325, 16326, 16327, 16328, 16329, 16330, 16331, 16332, 16333, 16334, 16335, 16336, 16337, 16338, 16339, 16340, 16341, 16342, 16343, 16344, 16345, 16346, 16347, 16348, 16349, 16350, 16351, 16352, 16353, 16354, 16355, 16356, 16357, 16358, 16359, 16360, 16361, 16362, 16363, 16364, 16365, 16366, 16367, 16368, 16369, 16370, 16371, 16372, 16373, 16374, 16375, 16376, 16377, 16378, 16379, 16380, 16381, 16382, 16383, 16384, 16385, 16386, 16387, 16388, 16389, 16390, 16391, 16392, 16393, 16394, 16395, 16396, 16397, 16398, 16399, 16400, 16401, 16402, 16403, 16404, 16405, 16406, 16407, 16408, 16409, 16410, 16411, 16412, 16413, 16414, 16415, 16416, 16417, 16418, 16419, 16420, 16421, 16422, 16423, 16424, 16425, 16426, 16427, 16428, 16429, 16430, 16431, 16432, 16433, 16434, 16435, 16436, 16437, 16438, 16439, 16440, 16441, 16442, 16443, 16444, 16445, 16446, 16447, 16448, 16449, 16450, 16451, 16452, 16453, 16454, 16455, 16456, 16457, 16458, 16459, 16460, 16461, 16462, 16463, 16464, 16465, 16466, 16467, 16468, 16469, 16470, 16471, 16472, 16473, 16474, 16475, 16476, 16477, 16478, 16479, 16480, 16481, 16482, 16483, 16484, 16485, 16486, 16487, 16488, 16489, 16490, 16491, 16492, 16493, 16494, 16495, 16496, 16497, 16498, 16499, 16500, 16501, 16502, 16503, 16504, 16505, 16506, 16507, 16508, 16509, 16510, 16511, 16512, 16513, 16514, 16515, 16516, 16517, 16518, 16519, 16520, 16521, 16522, 16523, 16524, 16525, 16526, 16527, 16528, 16529, 16530, 16531, 16532, 16533, 16534, 16535, 16536, 16537, 16538, 16539, 16540, 16541, 16542, 16543, 16544, 16545, 16546, 16547, 16548, 16549, 16550, 16551, 16552, 16553, 16554, 16555, 16556, 16557, 16558, 16559, 16560, 16561, 16562, 16563, 16564, 16565, 16566, 16567, 16568, 16569, 16570, 16571, 16572, 16573, 16574, 16575, 16576, 16577, 16578, 16579, 16580, 16581, 16582, 16583, 16584, 16585, 16586, 16587, 16588, 16589, 16590, 16591, 16592, 16593, 16594, 16595, 16596, 16597, 16598, 16599, 16600, 16601, 16602, 16603, 16604, 16605, 16606, 16607, 16608, 16609, 16610, 16611, 16612, 16613, 16614, 16615, 16616, 16617, 16618, 16619, 16620, 16621, 16622, 16623, 16624, 16625, 16626, 16627, 16628, 16629, 16630, 16631, 16632, 16633, 16634, 16635, 16636, 16637, 16638, 16639, 16640, 16641, 16642, 16643, 16644, 16645, 16646, 16647, 16648, 16649, 16650, 16651, 16652, 16653, 16654, 16655, 16656, 16657, 16658, 16659, 16660, 16661, 16662, 16663, 16664, 16665, 16666, 16667, 16668, 16669, 16670, 16671, 16672, 16673, 16674, 16675, 16676, 16677, 16678, 16679, 16680, 16681, 16682, 16683, 16684, 16685, 16686, 16687, 16688, 16689, 16690, 16691, 16692, 16693, 16694, 16695, 16696, 16697, 16698, 16699, 16700, 16701, 16702, 16703, 16704, 16705, 16706, 16707, 16708, 16709, 16710, 16711, 16712, 16713, 16714, 16715, 16716, 16717, 16718, 16719, 16720, 16721, 16722, 16723, 16724, 16725, 16726, 16727, 16728, 16729, 16730, 16731, 16732, 16733, 16734, 16735, 16736, 16737, 16738, 16739, 16740, 16741, 16742, 16743, 16744, 16745, 16746, 16747, 16748, 16749, 16750, 16751, 16752, 16753, 16754, 16755, 16756, 16757, 16758, 16759, 16760, 16761, 16762, 16763, 16764, 16765, 16766, 16767, 16768, 16769, 16770, 16771, 16772, 16773, 16774, 16775, 16776, 16777, 16778, 16779, 16780, 16781, 16782, 16783, 16784, 16785, 16786, 16787, 16788, 16789, 16790, 16791, 16792, 16793, 16794, 16795, 16796, 16797, 16798, 16799, 16800, 16801, 16802, 16803, 16804, 16805, 16806, 16807, 16808, 16809, 16810, 16811, 16812, 16813, 16814, 16815, 16816, 16817, 16818, 16819, 16820, 16821, 16822, 16823, 16824, 16825, 16826, 16827, 16828, 16829, 16830, 16831, 16832, 16833, 16834, 16835, 16836, 16837, 16838, 16839, 16840, 16841, 16842, 16843, 16844, 16845, 16846, 16847, 16848, 16849, 16850, 16851, 16852, 16853, 16854, 16855, 16856, 16857, 16858, 16859, 16860, 16861, 16862, 16863, 16864, 16865, 16866, 16867, 16868, 16869, 16870, 16871, 16872, 16873, 16874, 16875, 16876, 16877, 16878, 16879, 16880, 16881, 16882, 16883, 16884, 16885, 16886, 16887, 16888, 16889, 16890, 16891, 16892, 16893, 16894, 16895, 16896, 16897, 16898, 16899, 16900, 16901, 16902, 16903, 16904, 16905, 16906, 16907, 16908, 16909, 16910, 16911, 16912, 16913, 16914, 16915, 16916, 16917, 16918, 16919, 16920, 16921, 16922, 16923, 16924, 16925, 16926, 16927, 16928, 16929, 16930, 16931, 16932, 16933, 16934, 16935, 16936, 16937, 16938, 16939, 16940, 16941, 16942, 16943, 16944, 16945, 16946, 16947, 16948, 16949, 16950, 16951, 16952, 16953, 16954, 16955, 16956, 16957, 16958, 16959, 16960, 16961, 16962, 16963, 16964, 16965, 16966, 16967, 16968, 16969, 16970, 16971, 16972, 16973, 16974, 16975, 16976, 16977, 16978, 16979, 16980, 16981, 16982, 16983, 16984, 16985, 16986, 16987, 16988, 16989, 16990, 16991, 16992, 16993, 16994, 16995, 16996, 16997, 16998, 16999, 17000, 17001, 17002, 17003, 17004, 17005, 17006, 17007, 17008, 17009, 17010, 17011, 17012, 17013, 17014, 17015, 17016, 17017, 17018, 17019, 17020, 17021, 17022, 17023, 17024, 17025, 17026, 17027, 17028, 17029, 17030, 17031, 17032, 17033, 17034, 17035, 17036, 17037, 17038, 17039, 17040, 17041, 17042, 17043, 17044, 17045, 17046, 17047, 17048, 17049, 17050, 17051, 17052, 17053, 17054, 17055, 17056, 17057, 17058, 17059, 17060, 17061, 17062, 17063, 17064, 17065, 17066, 17067, 17068, 17069, 17070, 17071, 17072, 17073, 17074, 17075, 17076, 17077, 17078, 17079, 17080, 17081, 17082, 17083, 17084, 17085, 17086, 17087, 17088, 17089, 17090, 17091, 17092, 17093, 17094, 17095, 17096, 17097, 17098, 17099, 17100, 17101, 17102, 17103, 17104, 17105, 17106, 17107, 17108, 17109, 17110, 17111, 17112, 17113, 17114, 17115, 17116, 17117, 17118, 17119, 17120, 17121, 17122, 17123, 17124, 17125, 17126, 17127, 17128, 17129, 17130, 17131, 17132, 17133, 17134, 17135, 17136, 17137, 17138, 17139, 17140, 17141, 17142, 17143, 17144, 17145, 17146, 17147, 17148, 17149, 17150, 17151, 17152, 17153, 17154, 17155, 17156, 17157, 17158, 17159, 17160, 17161, 17162, 17163, 17164, 17165, 17166, 17167, 17168, 17169, 17170, 17171, 17172, 17173, 17174, 17175, 17176, 17177, 17178, 17179, 17180, 17181, 17182, 17183, 17184, 17185, 17186, 17187, 17188, 17189, 17190, 17191, 17192, 17193, 17194, 17195, 17196, 17197, 17198, 17199, 17200, 17201, 17202, 17203, 17204, 17205, 17206, 17207, 17208, 17209, 17210, 17211, 17212, 17213, 17214, 17215, 17216, 17217, 17218, 17219, 17220, 17221, 17222, 17223, 17224, 17225, 17226, 17227, 17228, 17229, 17230, 17231, 17232, 17233, 17234, 17235, 17236, 17237, 17238, 17239, 17240, 17241, 17242, 17243, 17244, 17245, 17246, 17247, 17248, 17249, 17250, 17251, 17252, 17253, 17254, 17255, 17256, 17257, 17258, 17259, 17260, 17261, 17262, 17263, 17264, 17265, 17266, 17267, 17268, 17269, 17270, 17271, 17272, 17273, 17274, 17275, 17276, 17277, 17278, 17279, 17280, 17281, 17282, 17283, 17284, 17285, 17286, 17287, 17288, 17289, 17290, 17291, 17292, 17293, 17294, 17295, 17296, 17297, 17298, 17299, 17300, 17301, 17302, 17303, 17304, 17305, 17306, 17307, 17308, 17309, 17310, 17311, 17312, 17313, 17314, 17315, 17316, 17317, 17318, 17319, 17320, 17321, 17322, 17323, 17324, 17325, 17326, 17327, 17328, 17329, 17330, 17331, 17332, 17333, 17334, 17335, 17336, 17337, 17338, 17339, 17340, 17341, 17342, 17343, 17344, 17345, 17346, 17347, 17348, 17349, 17350, 17351, 17352, 17353, 17354, 17355, 17356, 17357, 17358, 17359, 17360, 17361, 17362, 17363, 17364, 17365, 17366, 17367, 17368, 17369, 17370, 17371, 17372, 17373, 17374, 17375, 17376, 17377, 17378, 17379, 17380, 17381, 17382, 17383, 17384, 17385, 17386, 17387, 17388, 17389, 17390, 17391, 17392, 17393, 17394, 17395, 17396, 17397, 17398, 17399, 17400, 17401, 17402, 17403, 17404, 17405, 17406, 17407, 17408, 17409, 17410, 17411, 17412, 17413, 17414, 17415, 17416, 17417, 17418, 17419, 17420, 17421, 17422, 17423, 17424, 17425, 17426, 17427, 17428, 17429, 17430, 17431, 17432, 17433, 17434, 17435, 17436, 17437, 17438, 17439, 17440, 17441, 17442, 17443, 17444, 17445, 17446, 17447, 17448, 17449, 17450, 17451, 17452, 17453, 17454, 17455, 17456, 17457, 17458, 17459, 17460, 17461, 17462, 17463, 17464, 17465, 17466, 17467, 17468, 17469, 17470, 17471, 17472, 17473, 17474, 17475, 17476, 17477, 17478, 17479, 17480, 17481, 17482, 17483, 17484, 17485, 17486, 17487, 17488, 17489, 17490, 17491, 17492, 17493, 17494, 17495, 17496, 17497, 17498, 17499, 17500, 17501, 17502, 17503, 17504, 17505, 17506, 17507, 17508, 17509, 17510, 17511, 17512, 17513, 17514, 17515, 17516, 17517, 17518, 17519, 17520, 17521, 17522, 17523, 17524, 17525, 17526, 17527, 17528, 17529, 17530, 17531, 17532, 17533, 17534, 17535, 17536, 17537, 17538, 17539, 17540, 17541, 17542, 17543, 17544, 17545, 17546, 17547, 17548, 17549, 17550, 17551, 17552, 17553, 17554, 17555, 17556, 17557, 17558, 17559, 17560, 17561, 17562, 17563, 17564, 17565, 17566, 17567, 17568, 17569, 17570, 17571, 17572, 17573, 17574, 17575, 17576, 17577, 17578, 17579, 17580, 17581, 17582, 17583, 17584, 17585, 17586, 17587, 17588, 17589, 17590, 17591, 17592, 17593, 17594, 17595, 17596, 17597, 17598, 17599, 17600, 17601, 17602, 17603, 17604, 17605, 17606, 17607, 17608, 17609, 17610, 17611, 17612, 17613, 17614, 17615, 17616, 17617, 17618, 17619, 17620, 17621, 17622, 17623, 17624, 17625, 17626, 17627, 17628, 17629, 17630, 17631, 17632, 17633, 17634, 17635, 17636, 17637, 17638, 17639, 17640, 17641, 17642, 17643, 17644, 17645, 17646, 17647, 17648, 17649, 17650, 17651, 17652, 17653, 17654, 17655, 17656, 17657, 17658, 17659, 17660, 17661, 17662, 17663, 17664, 17665, 17666, 17667, 17668, 17669, 17670, 17671, 17672, 17673, 17674, 17675, 17676, 17677, 17678, 17679, 17680, 17681, 17682, 17683, 17684, 17685, 17686, 17687, 17688, 17689, 17690, 17691, 17692, 17693, 17694, 17695, 17696, 17697, 17698, 17699, 17700, 17701, 17702, 17703, 17704, 17705, 17706, 17707, 17708, 17709, 17710, 17711, 17712, 17713, 17714, 17715, 17716, 17717, 17718, 17719, 17720, 17721, 17722, 17723, 17724, 17725, 17726, 17727, 17728, 17729, 17730, 17731, 17732, 17733, 17734, 17735, 17736, 17737, 17738, 17739, 17740, 17741, 17742, 17743, 17744, 17745, 17746, 17747, 17748, 17749, 17750, 17751, 17752, 17753, 17754, 17755, 17756, 17757, 17758, 17759, 17760, 17761, 17762, 17763, 17764, 17765, 17766, 17767, 17768, 17769, 17770, 17771, 17772, 17773, 17774, 17775, 17776, 17777, 17778, 17779, 17780, 17781, 17782, 17783, 17784, 17785, 17786, 17787, 17788, 17789, 17790, 17791, 17792, 17793, 17794, 17795, 17796, 17797, 17798, 17799, 17800, 17801, 17802, 17803, 17804, 17805, 17806, 17807, 17808, 17809, 17810, 17811, 17812, 17813, 17814, 17815, 17816, 17817, 17818, 17819, 17820, 17821, 17822, 17823, 17824, 17825, 17826, 17827, 17828, 17829, 17830, 17831, 17832, 17833, 17834, 17835, 17836, 17837, 17838, 17839, 17840, 17841, 17842, 17843, 17844, 17845, 17846, 17847, 17848, 17849, 17850, 17851, 17852, 17853, 17854, 17855, 17856, 17857, 17858, 17859, 17860, 17861, 17862, 17863, 17864, 17865, 17866, 17867, 17868, 17869, 17870, 17871, 17872, 17873, 17874, 17875, 17876, 17877, 17878, 17879, 17880, 17881, 17882, 17883, 17884, 17885, 17886, 17887, 17888, 17889, 17890, 17891, 17892, 17893, 17894, 17895, 17896, 17897, 17898, 17899, 17900, 17901, 17902, 17903, 17904, 17905, 17906, 17907, 17908, 17909, 17910, 17911, 17912, 17913, 17914, 17915, 17916, 17917, 17918, 17919, 17920, 17921, 17922, 17923, 17924, 17925, 17926, 17927, 17928, 17929, 17930, 17931, 17932, 17933, 17934, 17935, 17936, 17937, 17938, 17939, 17940, 17941, 17942, 17943, 17944, 17945, 17946, 17947, 17948, 17949, 17950, 17951, 17952, 17953, 17954, 17955, 17956, 17957, 17958, 17959, 17960, 17961, 17962, 17963, 17964, 17965, 17966, 17967, 17968, 17969, 17970, 17971, 17972, 17973, 17974, 17975, 17976, 17977, 17978, 17979, 17980, 17981, 17982, 17983, 17984, 17985, 17986, 17987, 17988, 17989, 17990, 17991, 17992, 17993, 17994, 17995, 17996, 17997, 17998, 17999, 18000, 18001, 18002, 18003, 18004, 18005, 18006, 18007, 18008, 18009, 18010, 18011, 18012, 18013, 18014, 18015, 18016, 18017, 18018, 18019, 18020, 18021, 18022, 18023, 18024, 18025, 18026, 18027, 18028, 18029, 18030, 18031, 18032, 18033, 18034, 18035, 18036, 18037, 18038, 18039, 18040, 18041, 18042, 18043, 18044, 18045, 18046, 18047, 18048, 18049, 18050, 18051, 18052, 18053, 18054, 18055, 18056, 18057, 18058, 18059, 18060, 18061, 18062, 18063, 18064, 18065, 18066, 18067, 18068, 18069, 18070, 18071, 18072, 18073, 18074, 18075, 18076, 18077, 18078, 18079, 18080, 18081, 18082, 18083, 18084, 18085, 18086, 18087, 18088, 18089, 18090, 18091, 18092, 18093, 18094, 18095, 18096, 18097, 18098, 18099, 18100, 18101, 18102, 18103, 18104, 18105, 18106, 18107, 18108, 18109, 18110, 18111, 18112, 18113, 18114, 18115, 18116, 18117, 18118, 18119, 18120, 18121, 18122, 18123, 18124, 18125, 18126, 18127, 18128, 18129, 18130, 18131, 18132, 18133, 18134, 18135, 18136, 18137, 18138, 18139, 18140, 18141, 18142, 18143, 18144, 18145, 18146, 18147, 18148, 18149, 18150, 18151, 18152, 18153, 18154, 18155, 18156, 18157, 18158, 18159, 18160, 18161, 18162, 18163, 18164, 18165, 18166, 18167, 18168, 18169, 18170, 18171, 18172, 18173, 18174, 18175, 18176, 18177, 18178, 18179, 18180, 18181, 18182, 18183, 18184, 18185, 18186, 18187, 18188, 18189, 18190, 18191, 18192, 18193, 18194, 18195, 18196, 18197, 18198, 18199, 18200, 18201, 18202, 18203, 18204, 18205, 18206, 18207, 18208, 18209, 18210, 18211, 18212, 18213, 18214, 18215, 18216, 18217, 18218, 18219, 18220, 18221, 18222, 18223, 18224, 18225, 18226, 18227, 18228, 18229, 18230, 18231, 18232, 18233, 18234, 18235, 18236, 18237, 18238, 18239, 18240, 18241, 18242, 18243, 18244, 18245, 18246, 18247, 18248, 18249, 18250, 18251, 18252, 18253, 18254, 18255, 18256, 18257, 18258, 18259, 18260, 18261, 18262, 18263, 18264, 18265, 18266, 18267, 18268, 18269, 18270, 18271, 18272, 18273, 18274, 18275, 18276, 18277, 18278, 18279, 18280, 18281, 18282, 18283, 18284, 18285, 18286, 18287, 18288, 18289, 18290, 18291, 18292, 18293, 18294, 18295, 18296, 18297, 18298, 18299, 18300, 18301, 18302, 18303, 18304, 18305, 18306, 18307, 18308, 18309, 18310, 18311, 18312, 18313, 18314, 18315, 18316, 18317, 18318, 18319, 18320, 18321, 18322, 18323, 18324, 18325, 18326, 18327, 18328, 18329, 18330, 18331, 18332, 18333, 18334, 18335, 18336, 18337, 18338, 18339, 18340, 18341, 18342, 18343, 18344, 18345, 18346, 18347, 18348, 18349, 18350, 18351, 18352, 18353, 18354, 18355, 18356, 18357, 18358, 18359, 18360, 18361, 18362, 18363, 18364, 18365, 18366, 18367, 18368, 18369, 18370, 18371, 18372, 18373, 18374, 18375, 18376, 18377, 18378, 18379, 18380, 18381, 18382, 18383, 18384, 18385, 18386, 18387, 18388, 18389, 18390, 18391, 18392, 18393, 18394, 18395, 18396, 18397, 18398, 18399, 18400, 18401, 18402, 18403, 18404, 18405, 18406, 18407, 18408, 18409, 18410, 18411, 18412, 18413, 18414, 18415, 18416, 18417, 18418, 18419, 18420, 18421, 18422, 18423, 18424, 18425, 18426, 18427, 18428, 18429, 18430, 18431, 18432, 18433, 18434, 18435, 18436, 18437, 18438, 18439, 18440, 18441, 18442, 18443, 18444, 18445, 18446, 18447, 18448, 18449, 18450, 18451, 18452, 18453, 18454, 18455, 18456, 18457, 18458, 18459, 18460, 18461, 18462, 18463, 18464, 18465, 18466, 18467, 18468, 18469, 18470, 18471, 18472, 18473, 18474, 18475, 18476, 18477, 18478, 18479, 18480, 18481, 18482, 18483, 18484, 18485, 18486, 18487, 18488, 18489, 18490, 18491, 18492, 18493, 18494, 18495, 18496, 18497, 18498, 18499, 18500, 18501, 18502, 18503, 18504, 18505, 18506, 18507, 18508, 18509, 18510, 18511, 18512, 18513, 18514, 18515, 18516, 18517, 18518, 18519, 18520, 18521, 18522, 18523, 18524, 18525, 18526, 18527, 18528, 18529, 18530, 18531, 18532, 18533, 18534, 18535, 18536, 18537, 18538, 18539, 18540, 18541, 18542, 18543, 18544, 18545, 18546, 18547, 18548, 18549, 18550, 18551, 18552, 18553, 18554, 18555, 18556, 18557, 18558, 18559, 18560, 18561, 18562, 18563, 18564, 18565, 18566, 18567, 18568, 18569, 18570, 18571, 18572, 18573, 18574, 18575, 18576, 18577, 18578, 18579, 18580, 18581, 18582, 18583, 18584, 18585, 18586, 18587, 18588, 18589, 18590, 18591, 18592, 18593, 18594, 18595, 18596, 18597, 18598, 18599, 18600, 18601, 18602, 18603, 18604, 18605, 18606, 18607, 18608, 18609, 18610, 18611, 18612, 18613, 18614, 18615, 18616, 18617, 18618, 18619, 18620, 18621, 18622, 18623, 18624, 18625, 18626, 18627, 18628, 18629, 18630, 18631, 18632, 18633, 18634, 18635, 18636, 18637, 18638, 18639, 18640, 18641, 18642, 18643, 18644, 18645, 18646, 18647, 18648, 18649, 18650, 18651, 18652, 18653, 18654, 18655, 18656, 18657, 18658, 18659, 18660, 18661, 18662, 18663, 18664, 18665, 18666, 18667, 18668, 18669, 18670, 18671, 18672, 18673, 18674, 18675, 18676, 18677, 18678, 18679, 18680, 18681, 18682, 18683, 18684, 18685, 18686, 18687, 18688, 18689, 18690, 18691, 18692, 18693, 18694, 18695, 18696, 18697, 18698, 18699, 18700, 18701, 18702, 18703, 18704, 18705, 18706, 18707, 18708, 18709, 18710, 18711, 18712, 18713, 18714, 18715, 18716, 18717, 18718, 18719, 18720, 18721, 18722, 18723, 18724, 18725, 18726, 18727, 18728, 18729, 18730, 18731, 18732, 18733, 18734, 18735, 18736, 18737, 18738, 18739, 18740, 18741, 18742, 18743, 18744, 18745, 18746, 18747, 18748, 18749, 18750, 18751, 18752, 18753, 18754, 18755, 18756, 18757, 18758, 18759, 18760, 18761, 18762, 18763, 18764, 18765, 18766, 18767, 18768, 18769, 18770, 18771, 18772, 18773, 18774, 18775, 18776, 18777, 18778, 18779, 18780, 18781, 18782, 18783, 18784, 18785, 18786, 18787, 18788, 18789, 18790, 18791, 18792, 18793, 18794, 18795, 18796, 18797, 18798, 18799, 18800, 18801, 18802, 18803, 18804, 18805, 18806, 18807, 18808, 18809, 18810, 18811, 18812, 18813, 18814, 18815, 18816, 18817, 18818, 18819, 18820, 18821, 18822, 18823, 18824, 18825, 18826, 18827, 18828, 18829, 18830, 18831, 18832, 18833, 18834, 18835, 18836, 18837, 18838, 18839, 18840, 18841, 18842, 18843, 18844, 18845, 18846, 18847, 18848, 18849, 18850, 18851, 18852, 18853, 18854, 18855, 18856, 18857, 18858, 18859, 18860, 18861, 18862, 18863, 18864, 18865, 18866, 18867, 18868, 18869, 18870, 18871, 18872, 18873, 18874, 18875, 18876, 18877, 18878, 18879, 18880, 18881, 18882, 18883, 18884, 18885, 18886, 18887, 18888, 18889, 18890, 18891, 18892, 18893, 18894, 18895, 18896, 18897, 18898, 18899, 18900, 18901, 18902, 18903, 18904, 18905, 18906, 18907, 18908, 18909, 18910, 18911, 18912, 18913, 18914, 18915, 18916, 18917, 18918, 18919, 18920, 18921, 18922, 18923, 18924, 18925, 18926, 18927, 18928, 18929, 18930, 18931, 18932, 18933, 18934, 18935, 18936, 18937, 18938, 18939, 18940, 18941, 18942, 18943, 18944, 18945, 18946, 18947, 18948, 18949, 18950, 18951, 18952, 18953, 18954, 18955, 18956, 18957, 18958, 18959, 18960, 18961, 18962, 18963, 18964, 18965, 18966, 18967, 18968, 18969, 18970, 18971, 18972, 18973, 18974, 18975, 18976, 18977, 18978, 18979, 18980, 18981, 18982, 18983, 18984, 18985, 18986, 18987, 18988, 18989, 18990, 18991, 18992, 18993, 18994, 18995, 18996, 18997, 18998, 18999, 19000, 19001, 19002, 19003, 19004, 19005, 19006, 19007, 19008, 19009, 19010, 19011, 19012, 19013, 19014, 19015, 19016, 19017, 19018, 19019, 19020, 19021, 19022, 19023, 19024, 19025, 19026, 19027, 19028, 19029, 19030, 19031, 19032, 19033, 19034, 19035, 19036, 19037, 19038, 19039, 19040, 19041, 19042, 19043, 19044, 19045, 19046, 19047, 19048, 19049, 19050, 19051, 19052, 19053, 19054, 19055, 19056, 19057, 19058, 19059, 19060, 19061, 19062, 19063, 19064, 19065, 19066, 19067, 19068, 19069, 19070, 19071, 19072, 19073, 19074, 19075, 19076, 19077, 19078, 19079, 19080, 19081, 19082, 19083, 19084, 19085, 19086, 19087, 19088, 19089, 19090, 19091, 19092, 19093, 19094, 19095, 19096, 19097, 19098, 19099, 19100, 19101, 19102, 19103, 19104, 19105, 19106, 19107, 19108, 19109, 19110, 19111, 19112, 19113, 19114, 19115, 19116, 19117, 19118, 19119, 19120, 19121, 19122, 19123, 19124, 19125, 19126, 19127, 19128, 19129, 19130, 19131, 19132, 19133, 19134, 19135, 19136, 19137, 19138, 19139, 19140, 19141, 19142, 19143, 19144, 19145, 19146, 19147, 19148, 19149, 19150, 19151, 19152, 19153, 19154, 19155, 19156, 19157, 19158, 19159, 19160, 19161, 19162, 19163, 19164, 19165, 19166, 19167, 19168, 19169, 19170, 19171, 19172, 19173, 19174, 19175, 19176, 19177, 19178, 19179, 19180, 19181, 19182, 19183, 19184, 19185, 19186, 19187, 19188, 19189, 19190, 19191, 19192, 19193, 19194, 19195, 19196, 19197, 19198, 19199, 19200, 19201, 19202, 19203, 19204, 19205, 19206, 19207, 19208, 19209, 19210, 19211, 19212, 19213, 19214, 19215, 19216, 19217, 19218, 19219, 19220, 19221, 19222, 19223, 19224, 19225, 19226, 19227, 19228, 19229, 19230, 19231, 19232, 19233, 19234, 19235, 19236, 19237, 19238, 19239, 19240, 19241, 19242, 19243, 19244, 19245, 19246, 19247, 19248, 19249, 19250, 19251, 19252, 19253, 19254, 19255, 19256, 19257, 19258, 19259, 19260, 19261, 19262, 19263, 19264, 19265, 19266, 19267, 19268, 19269, 19270, 19271, 19272, 19273, 19274, 19275, 19276, 19277, 19278, 19279, 19280, 19281, 19282, 19283, 19284, 19285, 19286, 19287, 19288, 19289, 19290, 19291, 19292, 19293, 19294, 19295, 19296, 19297, 19298, 19299, 19300, 19301, 19302, 19303, 19304, 19305, 19306, 19307, 19308, 19309, 19310, 19311, 19312, 19313, 19314, 19315, 19316, 19317, 19318, 19319, 19320, 19321, 19322, 19323, 19324, 19325, 19326, 19327, 19328, 19329, 19330, 19331, 19332, 19333, 19334, 19335, 19336, 19337, 19338, 19339, 19340, 19341, 19342, 19343, 19344, 19345, 19346, 19347, 19348, 19349, 19350, 19351, 19352, 19353, 19354, 19355, 19356, 19357, 19358, 19359, 19360, 19361, 19362, 19363, 19364, 19365, 19366, 19367, 19368, 19369, 19370, 19371, 19372, 19373, 19374, 19375, 19376, 19377, 19378, 19379, 19380, 19381, 19382, 19383, 19384, 19385, 19386, 19387, 19388, 19389, 19390, 19391, 19392, 19393, 19394, 19395, 19396, 19397, 19398, 19399, 19400, 19401, 19402, 19403, 19404, 19405, 19406, 19407, 19408, 19409, 19410, 19411, 19412, 19413, 19414, 19415, 19416, 19417, 19418, 19419, 19420, 19421, 19422, 19423, 19424, 19425, 19426, 19427, 19428, 19429, 19430, 19431, 19432, 19433, 19434, 19435, 19436, 19437, 19438, 19439, 19440, 19441, 19442, 19443, 19444, 19445, 19446, 19447, 19448, 19449, 19450, 19451, 19452, 19453, 19454, 19455, 19456, 19457, 19458, 19459, 19460, 19461, 19462, 19463, 19464, 19465, 19466, 19467, 19468, 19469, 19470, 19471, 19472, 19473, 19474, 19475, 19476, 19477, 19478, 19479, 19480, 19481, 19482, 19483, 19484, 19485, 19486, 19487, 19488, 19489, 19490, 19491, 19492, 19493, 19494, 19495, 19496, 19497, 19498, 19499, 19500, 19501, 19502, 19503, 19504, 19505, 19506, 19507, 19508, 19509, 19510, 19511, 19512, 19513, 19514, 19515, 19516, 19517, 19518, 19519, 19520, 19521, 19522, 19523, 19524, 19525, 19526, 19527, 19528, 19529, 19530, 19531, 19532, 19533, 19534, 19535, 19536, 19537, 19538, 19539, 19540, 19541, 19542, 19543, 19544, 19545, 19546, 19547, 19548, 19549, 19550, 19551, 19552, 19553, 19554, 19555, 19556, 19557, 19558, 19559, 19560, 19561, 19562, 19563, 19564, 19565, 19566, 19567, 19568, 19569, 19570, 19571, 19572, 19573, 19574, 19575, 19576, 19577, 19578, 19579, 19580, 19581, 19582, 19583, 19584, 19585, 19586, 19587, 19588, 19589, 19590, 19591, 19592, 19593, 19594, 19595, 19596, 19597, 19598, 19599, 19600, 19601, 19602, 19603, 19604, 19605, 19606, 19607, 19608, 19609, 19610, 19611, 19612, 19613, 19614, 19615, 19616, 19617, 19618, 19619, 19620, 19621, 19622, 19623, 19624, 19625, 19626, 19627, 19628, 19629, 19630, 19631, 19632, 19633, 19634, 19635, 19636, 19637, 19638, 19639, 19640, 19641, 19642, 19643, 19644, 19645, 19646, 19647, 19648, 19649, 19650, 19651, 19652, 19653, 19654, 19655, 19656, 19657, 19658, 19659, 19660, 19661, 19662, 19663, 19664, 19665, 19666, 19667, 19668, 19669, 19670, 19671, 19672, 19673, 19674, 19675, 19676, 19677, 19678, 19679, 19680, 19681, 19682, 19683, 19684, 19685, 19686, 19687, 19688, 19689, 19690, 19691, 19692, 19693, 19694, 19695, 19696, 19697, 19698, 19699, 19700, 19701, 19702, 19703, 19704, 19705, 19706, 19707, 19708, 19709, 19710, 19711, 19712, 19713, 19714, 19715, 19716, 19717, 19718, 19719, 19720, 19721, 19722, 19723, 19724, 19725, 19726, 19727, 19728, 19729, 19730, 19731, 19732, 19733, 19734, 19735, 19736, 19737, 19738, 19739, 19740, 19741, 19742, 19743, 19744, 19745, 19746, 19747, 19748, 19749, 19750, 19751, 19752, 19753, 19754, 19755, 19756, 19757, 19758, 19759, 19760, 19761, 19762, 19763, 19764, 19765, 19766, 19767, 19768, 19769, 19770, 19771, 19772, 19773, 19774, 19775, 19776, 19777, 19778, 19779, 19780, 19781, 19782, 19783, 19784, 19785, 19786, 19787, 19788, 19789, 19790, 19791, 19792, 19793, 19794, 19795, 19796, 19797, 19798, 19799, 19800, 19801, 19802, 19803, 19804, 19805, 19806, 19807, 19808, 19809, 19810, 19811, 19812, 19813, 19814, 19815, 19816, 19817, 19818, 19819, 19820, 19821, 19822, 19823, 19824, 19825, 19826, 19827, 19828, 19829, 19830, 19831, 19832, 19833, 19834, 19835, 19836, 19837, 19838, 19839, 19840, 19841, 19842, 19843, 19844, 19845, 19846, 19847, 19848, 19849, 19850, 19851, 19852, 19853, 19854, 19855, 19856, 19857, 19858, 19859, 19860, 19861, 19862, 19863, 19864, 19865, 19866, 19867, 19868, 19869, 19870, 19871, 19872, 19873, 19874, 19875, 19876, 19877, 19878, 19879, 19880, 19881, 19882, 19883, 19884, 19885, 19886, 19887, 19888, 19889, 19890, 19891, 19892, 19893, 19894, 19895, 19896, 19897, 19898, 19899, 19900, 19901, 19902, 19903, 19904, 19905, 19906, 19907, 19908, 19909, 19910, 19911, 19912, 19913, 19914, 19915, 19916, 19917, 19918, 19919, 19920, 19921, 19922, 19923, 19924, 19925, 19926, 19927, 19928, 19929, 19930, 19931, 19932, 19933, 19934, 19935, 19936, 19937, 19938, 19939, 19940, 19941, 19942, 19943, 19944, 19945, 19946, 19947, 19948, 19949, 19950, 19951, 19952, 19953, 19954, 19955, 19956, 19957, 19958, 19959, 19960, 19961, 19962, 19963, 19964, 19965, 19966, 19967, 19968, 19969, 19970, 19971, 19972, 19973, 19974, 19975, 19976, 19977, 19978, 19979, 19980, 19981, 19982, 19983, 19984, 19985, 19986, 19987, 19988, 19989, 19990, 19991, 19992, 19993, 19994, 19995, 19996, 19997, 19998, 19999, 20000, 20001, 20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012, 20013, 20014, 20015, 20016, 20017, 20018, 20019, 20020, 20021, 20022, 20023, 20024, 20025, 20026, 20027, 20028, 20029, 20030, 20031, 20032, 20033, 20034, 20035, 20036, 20037, 20038, 20039, 20040, 20041, 20042, 20043, 20044, 20045, 20046, 20047, 20048, 20049, 20050, 20051, 20052, 20053, 20054, 20055, 20056, 20057, 20058, 20059, 20060, 20061, 20062, 20063, 20064, 20065, 20066, 20067, 20068, 20069, 20070, 20071, 20072, 20073, 20074, 20075, 20076, 20077, 20078, 20079, 20080, 20081, 20082, 20083, 20084, 20085, 20086, 20087, 20088, 20089, 20090, 20091, 20092, 20093, 20094, 20095, 20096, 20097, 20098, 20099, 20100, 20101, 20102, 20103, 20104, 20105, 20106, 20107, 20108, 20109, 20110, 20111, 20112, 20113, 20114, 20115, 20116, 20117, 20118, 20119, 20120, 20121, 20122, 20123, 20124, 20125, 20126, 20127, 20128, 20129, 20130, 20131, 20132, 20133, 20134, 20135, 20136, 20137, 20138, 20139, 20140, 20141, 20142, 20143, 20144, 20145, 20146, 20147, 20148, 20149, 20150, 20151, 20152, 20153, 20154, 20155, 20156, 20157, 20158, 20159, 20160, 20161, 20162, 20163, 20164, 20165, 20166, 20167, 20168, 20169, 20170, 20171, 20172, 20173, 20174, 20175, 20176, 20177, 20178, 20179, 20180, 20181, 20182, 20183, 20184, 20185, 20186, 20187, 20188, 20189, 20190, 20191, 20192, 20193, 20194, 20195, 20196, 20197, 20198, 20199, 20200, 20201, 20202, 20203, 20204, 20205, 20206, 20207, 20208, 20209, 20210, 20211, 20212, 20213, 20214, 20215, 20216, 20217, 20218, 20219, 20220, 20221, 20222, 20223, 20224, 20225, 20226, 20227, 20228, 20229, 20230, 20231, 20232, 20233, 20234, 20235, 20236, 20237, 20238, 20239, 20240, 20241, 20242, 20243, 20244, 20245, 20246, 20247, 20248, 20249, 20250, 20251, 20252, 20253, 20254, 20255, 20256, 20257, 20258, 20259, 20260, 20261, 20262, 20263, 20264, 20265, 20266, 20267, 20268, 20269, 20270, 20271, 20272, 20273, 20274, 20275, 20276, 20277, 20278, 20279, 20280, 20281, 20282, 20283, 20284, 20285, 20286, 20287, 20288, 20289, 20290, 20291, 20292, 20293, 20294, 20295, 20296, 20297, 20298, 20299, 20300, 20301, 20302, 20303, 20304, 20305, 20306, 20307, 20308, 20309, 20310, 20311, 20312, 20313, 20314, 20315, 20316, 20317, 20318, 20319, 20320, 20321, 20322, 20323, 20324, 20325, 20326, 20327, 20328, 20329, 20330, 20331, 20332, 20333, 20334, 20335, 20336, 20337, 20338, 20339, 20340, 20341, 20342, 20343, 20344, 20345, 20346, 20347, 20348, 20349, 20350, 20351, 20352, 20353, 20354, 20355, 20356, 20357, 20358, 20359, 20360, 20361, 20362, 20363, 20364, 20365, 20366, 20367, 20368, 20369, 20370, 20371, 20372, 20373, 20374, 20375, 20376, 20377, 20378, 20379, 20380, 20381, 20382, 20383, 20384, 20385, 20386, 20387, 20388, 20389, 20390, 20391, 20392, 20393, 20394, 20395, 20396, 20397]\n"," This is the range of val:  [ 7700  7701  7702 ... 12097 12098 12099]\n","Starting training\n","shuffling\n","Epoch 1/350\n","2020-06-10 20:21:12.095476: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 20:21:12.322135: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","500/500 [==============================] - 36s 71ms/step - loss: 15.6970 - val_loss: 46.2086\n","\n","Epoch 00001: loss improved from inf to 15.69772, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/350\n","500/500 [==============================] - 31s 63ms/step - loss: 11.7820 - val_loss: 30.1330\n","\n","Epoch 00002: loss improved from 15.69772 to 11.78204, saving model to final.h5\n","Epoch 3/350\n","500/500 [==============================] - 32s 63ms/step - loss: 10.4735 - val_loss: 18.2412\n","\n","Epoch 00003: loss improved from 11.78204 to 10.47278, saving model to final.h5\n","Epoch 4/350\n","500/500 [==============================] - 32s 63ms/step - loss: 9.7398 - val_loss: 32.1081\n","\n","Epoch 00004: loss improved from 10.47278 to 9.73981, saving model to final.h5\n","Epoch 5/350\n","500/500 [==============================] - 32s 64ms/step - loss: 9.1209 - val_loss: 18.0939\n","\n","Epoch 00005: loss improved from 9.73981 to 9.12050, saving model to final.h5\n","Epoch 6/350\n","500/500 [==============================] - 32s 63ms/step - loss: 8.7911 - val_loss: 16.2118\n","\n","Epoch 00006: loss improved from 9.12050 to 8.79060, saving model to final.h5\n","Epoch 7/350\n","500/500 [==============================] - 32s 63ms/step - loss: 8.2409 - val_loss: 19.4390\n","\n","Epoch 00007: loss improved from 8.79060 to 8.24096, saving model to final.h5\n","Epoch 8/350\n","500/500 [==============================] - 32s 63ms/step - loss: 8.1364 - val_loss: 15.6365\n","\n","Epoch 00008: loss improved from 8.24096 to 8.13685, saving model to final.h5\n","Epoch 9/350\n","500/500 [==============================] - 32s 63ms/step - loss: 7.7303 - val_loss: 17.5891\n","\n","Epoch 00009: loss improved from 8.13685 to 7.72998, saving model to final.h5\n","Epoch 10/350\n","500/500 [==============================] - 32s 64ms/step - loss: 7.7515 - val_loss: 29.0810\n","\n","Epoch 00010: loss did not improve from 7.72998\n","Epoch 11/350\n","500/500 [==============================] - 31s 63ms/step - loss: 7.4534 - val_loss: 13.1760\n","\n","Epoch 00011: loss improved from 7.72998 to 7.45361, saving model to final.h5\n","Epoch 12/350\n","500/500 [==============================] - 31s 63ms/step - loss: 7.3802 - val_loss: 13.0265\n","\n","Epoch 00012: loss improved from 7.45361 to 7.37976, saving model to final.h5\n","Epoch 13/350\n","500/500 [==============================] - 32s 63ms/step - loss: 7.2942 - val_loss: 13.3324\n","\n","Epoch 00013: loss improved from 7.37976 to 7.29442, saving model to final.h5\n","Epoch 14/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.8123 - val_loss: 9.1351\n","\n","Epoch 00014: loss improved from 7.29442 to 6.81250, saving model to final.h5\n","Epoch 15/350\n","500/500 [==============================] - 31s 63ms/step - loss: 6.8851 - val_loss: 13.4171\n","\n","Epoch 00015: loss did not improve from 6.81250\n","Epoch 16/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.8618 - val_loss: 12.9458\n","\n","Epoch 00016: loss did not improve from 6.81250\n","Epoch 17/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.8072 - val_loss: 10.4414\n","\n","Epoch 00017: loss improved from 6.81250 to 6.80732, saving model to final.h5\n","Epoch 18/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.4236 - val_loss: 14.6433\n","\n","Epoch 00018: loss improved from 6.80732 to 6.42403, saving model to final.h5\n","Epoch 19/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.5718 - val_loss: 14.9198\n","\n","Epoch 00019: loss did not improve from 6.42403\n","Epoch 20/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.4624 - val_loss: 9.7566\n","\n","Epoch 00020: loss did not improve from 6.42403\n","Epoch 21/350\n","500/500 [==============================] - 31s 63ms/step - loss: 6.5515 - val_loss: 9.6459\n","\n","Epoch 00021: loss did not improve from 6.42403\n","Epoch 22/350\n","500/500 [==============================] - 31s 63ms/step - loss: 6.3303 - val_loss: 11.5742\n","\n","Epoch 00022: loss improved from 6.42403 to 6.33040, saving model to final.h5\n","Epoch 23/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.5930 - val_loss: 10.8930\n","\n","Epoch 00023: loss did not improve from 6.33040\n","Epoch 24/350\n","500/500 [==============================] - 32s 63ms/step - loss: 6.3093 - val_loss: 8.3312\n","\n","Epoch 00024: loss improved from 6.33040 to 6.30912, saving model to final.h5\n","Epoch 25/350\n","500/500 [==============================] - 32s 64ms/step - loss: 6.0087 - val_loss: 14.0521\n","\n","Epoch 00025: loss improved from 6.30912 to 6.00882, saving model to final.h5\n","Epoch 26/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.9653 - val_loss: 9.2660\n","\n","Epoch 00026: loss improved from 6.00882 to 5.96558, saving model to final.h5\n","Epoch 27/350\n","500/500 [==============================] - 31s 63ms/step - loss: 6.0249 - val_loss: 11.1309\n","\n","Epoch 00027: loss did not improve from 5.96558\n","Epoch 28/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.8520 - val_loss: 14.6451\n","\n","Epoch 00028: loss improved from 5.96558 to 5.85230, saving model to final.h5\n","Epoch 29/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.8251 - val_loss: 12.9060\n","\n","Epoch 00029: loss improved from 5.85230 to 5.82528, saving model to final.h5\n","Epoch 30/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.9177 - val_loss: 12.6323\n","\n","Epoch 00030: loss did not improve from 5.82528\n","Epoch 31/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.8239 - val_loss: 10.2932\n","\n","Epoch 00031: loss improved from 5.82528 to 5.82364, saving model to final.h5\n","Epoch 32/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.7460 - val_loss: 12.0993\n","\n","Epoch 00032: loss improved from 5.82364 to 5.74628, saving model to final.h5\n","Epoch 33/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.8148 - val_loss: 11.5518\n","\n","Epoch 00033: loss did not improve from 5.74628\n","Epoch 34/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.7813 - val_loss: 9.1293\n","\n","Epoch 00034: loss did not improve from 5.74628\n","Epoch 35/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.5746 - val_loss: 7.7020\n","\n","Epoch 00035: loss improved from 5.74628 to 5.57476, saving model to final.h5\n","Epoch 36/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.6230 - val_loss: 8.1478\n","\n","Epoch 00036: loss did not improve from 5.57476\n","Epoch 37/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.4966 - val_loss: 7.7552\n","\n","Epoch 00037: loss improved from 5.57476 to 5.49646, saving model to final.h5\n","Epoch 38/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.5365 - val_loss: 10.2439\n","\n","Epoch 00038: loss did not improve from 5.49646\n","Epoch 39/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.5070 - val_loss: 7.4682\n","\n","Epoch 00039: loss did not improve from 5.49646\n","Epoch 40/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.4025 - val_loss: 8.7872\n","\n","Epoch 00040: loss improved from 5.49646 to 5.40242, saving model to final.h5\n","Epoch 41/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.4834 - val_loss: 9.1452\n","\n","Epoch 00041: loss did not improve from 5.40242\n","Epoch 42/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.4112 - val_loss: 7.6434\n","\n","Epoch 00042: loss did not improve from 5.40242\n","Epoch 43/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.3370 - val_loss: 8.9290\n","\n","Epoch 00043: loss improved from 5.40242 to 5.33712, saving model to final.h5\n","Epoch 44/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.3050 - val_loss: 11.8411\n","\n","Epoch 00044: loss improved from 5.33712 to 5.30517, saving model to final.h5\n","Epoch 45/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.3942 - val_loss: 10.3242\n","\n","Epoch 00045: loss did not improve from 5.30517\n","Epoch 46/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.3762 - val_loss: 8.5680\n","\n","Epoch 00046: loss did not improve from 5.30517\n","Epoch 47/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.1853 - val_loss: 7.9766\n","\n","Epoch 00047: loss improved from 5.30517 to 5.18535, saving model to final.h5\n","Epoch 48/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.1913 - val_loss: 10.4068\n","\n","Epoch 00048: loss did not improve from 5.18535\n","Epoch 49/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.0289 - val_loss: 8.2126\n","\n","Epoch 00049: loss improved from 5.18535 to 5.02905, saving model to final.h5\n","Epoch 50/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.1119 - val_loss: 7.7690\n","\n","Epoch 00050: loss did not improve from 5.02905\n","Epoch 51/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.3804 - val_loss: 7.7828\n","\n","Epoch 00051: loss did not improve from 5.02905\n","Epoch 52/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.1715 - val_loss: 7.0486\n","\n","Epoch 00052: loss did not improve from 5.02905\n","Epoch 53/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.2508 - val_loss: 8.6345\n","\n","Epoch 00053: loss did not improve from 5.02905\n","Epoch 54/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.2628 - val_loss: 8.6154\n","\n","Epoch 00054: loss did not improve from 5.02905\n","Epoch 55/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.1081 - val_loss: 8.7969\n","\n","Epoch 00055: loss did not improve from 5.02905\n","Epoch 56/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.9423 - val_loss: 8.4257\n","\n","Epoch 00056: loss improved from 5.02905 to 4.94213, saving model to final.h5\n","Epoch 57/350\n","500/500 [==============================] - 32s 63ms/step - loss: 5.0685 - val_loss: 7.3993\n","\n","Epoch 00057: loss did not improve from 4.94213\n","Epoch 58/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.0661 - val_loss: 9.8583\n","\n","Epoch 00058: loss did not improve from 4.94213\n","Epoch 59/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.9930 - val_loss: 6.6228\n","\n","Epoch 00059: loss did not improve from 4.94213\n","Epoch 60/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.0471 - val_loss: 6.2641\n","\n","Epoch 00060: loss did not improve from 4.94213\n","Epoch 61/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.0062 - val_loss: 4.6789\n","\n","Epoch 00061: loss did not improve from 4.94213\n","Epoch 62/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.0853 - val_loss: 5.7637\n","\n","Epoch 00062: loss did not improve from 4.94213\n","Epoch 63/350\n","500/500 [==============================] - 32s 64ms/step - loss: 5.0660 - val_loss: 6.6854\n","\n","Epoch 00063: loss did not improve from 4.94213\n","Epoch 64/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.9637 - val_loss: 9.2588\n","\n","Epoch 00064: loss did not improve from 4.94213\n","Epoch 65/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.9952 - val_loss: 12.8652\n","\n","Epoch 00065: loss did not improve from 4.94213\n","Epoch 66/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.8778 - val_loss: 7.9838\n","\n","Epoch 00066: loss improved from 4.94213 to 4.87803, saving model to final.h5\n","Epoch 67/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.9552 - val_loss: 7.3926\n","\n","Epoch 00067: loss did not improve from 4.87803\n","Epoch 68/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.9340 - val_loss: 4.9387\n","\n","Epoch 00068: loss did not improve from 4.87803\n","Epoch 69/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.7702 - val_loss: 5.9172\n","\n","Epoch 00069: loss improved from 4.87803 to 4.77009, saving model to final.h5\n","Epoch 70/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.9201 - val_loss: 6.4305\n","\n","Epoch 00070: loss did not improve from 4.77009\n","Epoch 71/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.9172 - val_loss: 6.4638\n","\n","Epoch 00071: loss did not improve from 4.77009\n","Epoch 72/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.7031 - val_loss: 7.2226\n","\n","Epoch 00072: loss improved from 4.77009 to 4.70323, saving model to final.h5\n","Epoch 73/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.7161 - val_loss: 6.4104\n","\n","Epoch 00073: loss did not improve from 4.70323\n","Epoch 74/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.7674 - val_loss: 4.4960\n","\n","Epoch 00074: loss did not improve from 4.70323\n","Epoch 75/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.8161 - val_loss: 6.3216\n","\n","Epoch 00075: loss did not improve from 4.70323\n","Epoch 76/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.7612 - val_loss: 5.8252\n","\n","Epoch 00076: loss did not improve from 4.70323\n","Epoch 77/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.7118 - val_loss: 5.6362\n","\n","Epoch 00077: loss did not improve from 4.70323\n","Epoch 78/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.7568 - val_loss: 6.0895\n","\n","Epoch 00078: loss did not improve from 4.70323\n","Epoch 79/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.6534 - val_loss: 4.7289\n","\n","Epoch 00079: loss improved from 4.70323 to 4.65345, saving model to final.h5\n","Epoch 80/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.6303 - val_loss: 5.2006\n","\n","Epoch 00080: loss improved from 4.65345 to 4.63038, saving model to final.h5\n","Epoch 81/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.7206 - val_loss: 7.0311\n","\n","Epoch 00081: loss did not improve from 4.63038\n","Epoch 82/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.7543 - val_loss: 4.1827\n","\n","Epoch 00082: loss did not improve from 4.63038\n","Epoch 83/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.7158 - val_loss: 8.5081\n","\n","Epoch 00083: loss did not improve from 4.63038\n","Epoch 84/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.5342 - val_loss: 6.0985\n","\n","Epoch 00084: loss improved from 4.63038 to 4.53416, saving model to final.h5\n","Epoch 85/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.6922 - val_loss: 5.7869\n","\n","Epoch 00085: loss did not improve from 4.53416\n","Epoch 86/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.6276 - val_loss: 7.1136\n","\n","Epoch 00086: loss did not improve from 4.53416\n","Epoch 87/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.6326 - val_loss: 7.1489\n","\n","Epoch 00087: loss did not improve from 4.53416\n","Epoch 88/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.5461 - val_loss: 7.5763\n","\n","Epoch 00088: loss did not improve from 4.53416\n","Epoch 89/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.5968 - val_loss: 7.3340\n","\n","Epoch 00089: loss did not improve from 4.53416\n","Epoch 90/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.5156 - val_loss: 4.6355\n","\n","Epoch 00090: loss improved from 4.53416 to 4.51562, saving model to final.h5\n","Epoch 91/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.5737 - val_loss: 5.1399\n","\n","Epoch 00091: loss did not improve from 4.51562\n","Epoch 92/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.6347 - val_loss: 4.2003\n","\n","Epoch 00092: loss did not improve from 4.51562\n","Epoch 93/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.6034 - val_loss: 4.9933\n","\n","Epoch 00093: loss did not improve from 4.51562\n","Epoch 94/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.4938 - val_loss: 5.5793\n","\n","Epoch 00094: loss improved from 4.51562 to 4.49363, saving model to final.h5\n","Epoch 95/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.5396 - val_loss: 4.9286\n","\n","Epoch 00095: loss did not improve from 4.49363\n","Epoch 96/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.5520 - val_loss: 5.4783\n","\n","Epoch 00096: loss did not improve from 4.49363\n","Epoch 97/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.4009 - val_loss: 6.0705\n","\n","Epoch 00097: loss improved from 4.49363 to 4.40077, saving model to final.h5\n","Epoch 98/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.4487 - val_loss: 4.5278\n","\n","Epoch 00098: loss did not improve from 4.40077\n","Epoch 99/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.4591 - val_loss: 4.5246\n","\n","Epoch 00099: loss did not improve from 4.40077\n","Epoch 100/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.4048 - val_loss: 4.8421\n","\n","Epoch 00100: loss did not improve from 4.40077\n","Epoch 101/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.5039 - val_loss: 11.5628\n","\n","Epoch 00101: loss did not improve from 4.40077\n","Epoch 102/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.3880 - val_loss: 7.4795\n","\n","Epoch 00102: loss improved from 4.40077 to 4.38787, saving model to final.h5\n","Epoch 103/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.4703 - val_loss: 6.3681\n","\n","Epoch 00103: loss did not improve from 4.38787\n","Epoch 104/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.4881 - val_loss: 4.4837\n","\n","Epoch 00104: loss did not improve from 4.38787\n","Epoch 105/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.4196 - val_loss: 5.5805\n","\n","Epoch 00105: loss did not improve from 4.38787\n","Epoch 106/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.4009 - val_loss: 6.8176\n","\n","Epoch 00106: loss did not improve from 4.38787\n","Epoch 107/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.3073 - val_loss: 5.4935\n","\n","Epoch 00107: loss improved from 4.38787 to 4.30741, saving model to final.h5\n","Epoch 108/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.3222 - val_loss: 6.2512\n","\n","Epoch 00108: loss did not improve from 4.30741\n","Epoch 109/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.3256 - val_loss: 7.1440\n","\n","Epoch 00109: loss did not improve from 4.30741\n","Epoch 110/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.3379 - val_loss: 4.1649\n","\n","Epoch 00110: loss did not improve from 4.30741\n","Epoch 111/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.4132 - val_loss: 6.5079\n","\n","Epoch 00111: loss did not improve from 4.30741\n","Epoch 112/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.3532 - val_loss: 5.7282\n","\n","Epoch 00112: loss did not improve from 4.30741\n","Epoch 113/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.3310 - val_loss: 9.0983\n","\n","Epoch 00113: loss did not improve from 4.30741\n","Epoch 114/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.2789 - val_loss: 9.3676\n","\n","Epoch 00114: loss improved from 4.30741 to 4.27890, saving model to final.h5\n","Epoch 115/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.3025 - val_loss: 7.8959\n","\n","Epoch 00115: loss did not improve from 4.27890\n","Epoch 116/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.3310 - val_loss: 5.6851\n","\n","Epoch 00116: loss did not improve from 4.27890\n","Epoch 117/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.1319 - val_loss: 11.3732\n","\n","Epoch 00117: loss improved from 4.27890 to 4.13213, saving model to final.h5\n","Epoch 118/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.2869 - val_loss: 5.7704\n","\n","Epoch 00118: loss did not improve from 4.13213\n","Epoch 119/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.1917 - val_loss: 6.2147\n","\n","Epoch 00119: loss did not improve from 4.13213\n","Epoch 120/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2622 - val_loss: 6.4478\n","\n","Epoch 00120: loss did not improve from 4.13213\n","Epoch 121/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2557 - val_loss: 8.0667\n","\n","Epoch 00121: loss did not improve from 4.13213\n","Epoch 122/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3256 - val_loss: 5.1782\n","\n","Epoch 00122: loss did not improve from 4.13213\n","Epoch 123/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2749 - val_loss: 6.7985\n","\n","Epoch 00123: loss did not improve from 4.13213\n","Epoch 124/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2969 - val_loss: 7.4358\n","\n","Epoch 00124: loss did not improve from 4.13213\n","Epoch 125/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3379 - val_loss: 6.4997\n","\n","Epoch 00125: loss did not improve from 4.13213\n","Epoch 126/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2681 - val_loss: 7.6905\n","\n","Epoch 00126: loss did not improve from 4.13213\n","Epoch 127/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2091 - val_loss: 5.8701\n","\n","Epoch 00127: loss did not improve from 4.13213\n","Epoch 128/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2337 - val_loss: 8.1106\n","\n","Epoch 00128: loss did not improve from 4.13213\n","Epoch 129/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2370 - val_loss: 4.1325\n","\n","Epoch 00129: loss did not improve from 4.13213\n","Epoch 130/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2117 - val_loss: 6.2367\n","\n","Epoch 00130: loss did not improve from 4.13213\n","Epoch 131/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2414 - val_loss: 19.1038\n","\n","Epoch 00131: loss did not improve from 4.13213\n","Epoch 132/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.1044 - val_loss: 28.2691\n","\n","Epoch 00132: loss improved from 4.13213 to 4.10449, saving model to final.h5\n","Epoch 133/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.1980 - val_loss: 6.5711\n","\n","Epoch 00133: loss did not improve from 4.10449\n","Epoch 134/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.1663 - val_loss: 4.2115\n","\n","Epoch 00134: loss did not improve from 4.10449\n","Epoch 135/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.1206 - val_loss: 3.6551\n","\n","Epoch 00135: loss did not improve from 4.10449\n","Epoch 136/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2222 - val_loss: 3.2064\n","\n","Epoch 00136: loss did not improve from 4.10449\n","Epoch 137/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.1810 - val_loss: 4.3443\n","\n","Epoch 00137: loss did not improve from 4.10449\n","Epoch 138/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.1448 - val_loss: 2.7377\n","\n","Epoch 00138: loss did not improve from 4.10449\n","Epoch 139/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.1224 - val_loss: 2.7339\n","\n","Epoch 00139: loss did not improve from 4.10449\n","Epoch 140/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.1374 - val_loss: 2.8207\n","\n","Epoch 00140: loss did not improve from 4.10449\n","Epoch 141/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.1182 - val_loss: 5.2705\n","\n","Epoch 00141: loss did not improve from 4.10449\n","Epoch 142/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.1644 - val_loss: 3.6761\n","\n","Epoch 00142: loss did not improve from 4.10449\n","Epoch 143/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.0831 - val_loss: 2.3895\n","\n","Epoch 00143: loss improved from 4.10449 to 4.08319, saving model to final.h5\n","Epoch 144/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2746 - val_loss: 2.6197\n","\n","Epoch 00144: loss did not improve from 4.08319\n","Epoch 145/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.1903 - val_loss: 5.5586\n","\n","Epoch 00145: loss did not improve from 4.08319\n","Epoch 146/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.1752 - val_loss: 6.2074\n","\n","Epoch 00146: loss did not improve from 4.08319\n","Epoch 147/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0290 - val_loss: 6.4875\n","\n","Epoch 00147: loss improved from 4.08319 to 4.02871, saving model to final.h5\n","Epoch 148/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.1088 - val_loss: 9.3426\n","\n","Epoch 00148: loss did not improve from 4.02871\n","Epoch 149/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.9902 - val_loss: 5.4535\n","\n","Epoch 00149: loss improved from 4.02871 to 3.99035, saving model to final.h5\n","Epoch 150/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2063 - val_loss: 4.9486\n","\n","Epoch 00150: loss did not improve from 3.99035\n","Epoch 151/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.0804 - val_loss: 6.0984\n","\n","Epoch 00151: loss did not improve from 3.99035\n","Epoch 152/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.1663 - val_loss: 10.3450\n","\n","Epoch 00152: loss did not improve from 3.99035\n","Epoch 153/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.9694 - val_loss: 6.9021\n","\n","Epoch 00153: loss improved from 3.99035 to 3.96966, saving model to final.h5\n","Epoch 154/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0455 - val_loss: 6.0798\n","\n","Epoch 00154: loss did not improve from 3.96966\n","Epoch 155/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.0139 - val_loss: 5.4756\n","\n","Epoch 00155: loss did not improve from 3.96966\n","Epoch 156/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0070 - val_loss: 6.0074\n","\n","Epoch 00156: loss did not improve from 3.96966\n","Epoch 157/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.0540 - val_loss: 6.5134\n","\n","Epoch 00157: loss did not improve from 3.96966\n","Epoch 158/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.0587 - val_loss: 2.8172\n","\n","Epoch 00158: loss did not improve from 3.96966\n","Epoch 159/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.1283 - val_loss: 3.0071\n","\n","Epoch 00159: loss did not improve from 3.96966\n","Epoch 160/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0457 - val_loss: 3.5355\n","\n","Epoch 00160: loss did not improve from 3.96966\n","Epoch 161/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.9832 - val_loss: 5.7360\n","\n","Epoch 00161: loss did not improve from 3.96966\n","Epoch 162/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.0942 - val_loss: 7.2449\n","\n","Epoch 00162: loss did not improve from 3.96966\n","Epoch 163/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.9946 - val_loss: 4.2651\n","\n","Epoch 00163: loss did not improve from 3.96966\n","Epoch 164/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.9152 - val_loss: 4.6999\n","\n","Epoch 00164: loss improved from 3.96966 to 3.91502, saving model to final.h5\n","Epoch 165/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0969 - val_loss: 5.7692\n","\n","Epoch 00165: loss did not improve from 3.91502\n","Epoch 166/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0639 - val_loss: 7.2468\n","\n","Epoch 00166: loss did not improve from 3.91502\n","Epoch 167/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0303 - val_loss: 4.4422\n","\n","Epoch 00167: loss did not improve from 3.91502\n","Epoch 168/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.9646 - val_loss: 4.1574\n","\n","Epoch 00168: loss did not improve from 3.91502\n","Epoch 169/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0466 - val_loss: 3.6522\n","\n","Epoch 00169: loss did not improve from 3.91502\n","Epoch 170/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0325 - val_loss: 4.0804\n","\n","Epoch 00170: loss did not improve from 3.91502\n","Epoch 171/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.1743 - val_loss: 3.5174\n","\n","Epoch 00171: loss did not improve from 3.91502\n","Epoch 172/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.0484 - val_loss: 4.9362\n","\n","Epoch 00172: loss did not improve from 3.91502\n","Epoch 173/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0546 - val_loss: 4.5055\n","\n","Epoch 00173: loss did not improve from 3.91502\n","Epoch 174/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.9407 - val_loss: 4.8114\n","\n","Epoch 00174: loss did not improve from 3.91502\n","Epoch 175/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.9911 - val_loss: 2.2655\n","\n","Epoch 00175: loss did not improve from 3.91502\n","Epoch 176/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0526 - val_loss: 5.4742\n","\n","Epoch 00176: loss did not improve from 3.91502\n","Epoch 177/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0630 - val_loss: 4.0294\n","\n","Epoch 00177: loss did not improve from 3.91502\n","Epoch 178/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.0874 - val_loss: 4.5212\n","\n","Epoch 00178: loss did not improve from 3.91502\n","Epoch 179/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.8795 - val_loss: 3.8174\n","\n","Epoch 00179: loss improved from 3.91502 to 3.87944, saving model to final.h5\n","Epoch 180/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0872 - val_loss: 4.7775\n","\n","Epoch 00180: loss did not improve from 3.87944\n","Epoch 181/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0199 - val_loss: 4.5599\n","\n","Epoch 00181: loss did not improve from 3.87944\n","Epoch 182/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.0128 - val_loss: 8.5555\n","\n","Epoch 00182: loss did not improve from 3.87944\n","Epoch 183/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0001 - val_loss: 6.3101\n","\n","Epoch 00183: loss did not improve from 3.87944\n","Epoch 184/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0387 - val_loss: 4.1008\n","\n","Epoch 00184: loss did not improve from 3.87944\n","Epoch 185/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.9533 - val_loss: 6.0904\n","\n","Epoch 00185: loss did not improve from 3.87944\n","Epoch 186/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.9617 - val_loss: 3.9539\n","\n","Epoch 00186: loss did not improve from 3.87944\n","Epoch 187/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0703 - val_loss: 4.2275\n","\n","Epoch 00187: loss did not improve from 3.87944\n","Epoch 188/350\n","500/500 [==============================] - 32s 63ms/step - loss: 4.0731 - val_loss: 5.2724\n","\n","Epoch 00188: loss did not improve from 3.87944\n","Epoch 189/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0298 - val_loss: 5.4403\n","\n","Epoch 00189: loss did not improve from 3.87944\n","Epoch 190/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0321 - val_loss: 7.5466\n","\n","Epoch 00190: loss did not improve from 3.87944\n","Epoch 191/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.9942 - val_loss: 5.9794\n","\n","Epoch 00191: loss did not improve from 3.87944\n","Epoch 192/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0296 - val_loss: 4.1708\n","\n","Epoch 00192: loss did not improve from 3.87944\n","Epoch 193/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.9771 - val_loss: 5.8527\n","\n","Epoch 00193: loss did not improve from 3.87944\n","Epoch 194/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.0056 - val_loss: 5.9033\n","\n","Epoch 00194: loss did not improve from 3.87944\n","Epoch 195/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.9397 - val_loss: 3.8096\n","\n","Epoch 00195: loss did not improve from 3.87944\n","Epoch 196/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.9968 - val_loss: 3.0210\n","\n","Epoch 00196: loss did not improve from 3.87944\n","Epoch 197/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.9657 - val_loss: 5.7425\n","\n","Epoch 00197: loss did not improve from 3.87944\n","Epoch 198/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.8971 - val_loss: 4.3993\n","\n","Epoch 00198: loss did not improve from 3.87944\n","Epoch 199/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.7736 - val_loss: 3.6716\n","\n","Epoch 00199: loss improved from 3.87944 to 3.77374, saving model to final.h5\n","Epoch 200/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.9450 - val_loss: 3.0639\n","\n","Epoch 00200: loss did not improve from 3.77374\n","Epoch 201/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.8434 - val_loss: 1.9131\n","\n","Epoch 00201: loss did not improve from 3.77374\n","Epoch 202/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.8768 - val_loss: 3.5890\n","\n","Epoch 00202: loss did not improve from 3.77374\n","Epoch 203/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.8822 - val_loss: 11.3473\n","\n","Epoch 00203: loss did not improve from 3.77374\n","Epoch 204/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.8992 - val_loss: 4.2754\n","\n","Epoch 00204: loss did not improve from 3.77374\n","Epoch 205/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.8046 - val_loss: 3.1613\n","\n","Epoch 00205: loss did not improve from 3.77374\n","Epoch 206/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.8933 - val_loss: 2.4229\n","\n","Epoch 00206: loss did not improve from 3.77374\n","Epoch 207/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.8365 - val_loss: 8.2531\n","\n","Epoch 00207: loss did not improve from 3.77374\n","Epoch 208/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.8668 - val_loss: 11.5149\n","\n","Epoch 00208: loss did not improve from 3.77374\n","Epoch 209/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.8916 - val_loss: 18.4359\n","\n","Epoch 00209: loss did not improve from 3.77374\n","Epoch 210/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.7178 - val_loss: 4.5162\n","\n","Epoch 00210: loss improved from 3.77374 to 3.71784, saving model to final.h5\n","Epoch 211/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.7489 - val_loss: 3.9824\n","\n","Epoch 00211: loss did not improve from 3.71784\n","Epoch 212/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7357 - val_loss: 3.7136\n","\n","Epoch 00212: loss did not improve from 3.71784\n","Epoch 213/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.8744 - val_loss: 10.8176\n","\n","Epoch 00213: loss did not improve from 3.71784\n","Epoch 214/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7822 - val_loss: 4.6084\n","\n","Epoch 00214: loss did not improve from 3.71784\n","Epoch 215/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7691 - val_loss: 72.6822\n","\n","Epoch 00215: loss did not improve from 3.71784\n","Epoch 216/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.8229 - val_loss: 5.9024\n","\n","Epoch 00216: loss did not improve from 3.71784\n","Epoch 217/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.8288 - val_loss: 6.8725\n","\n","Epoch 00217: loss did not improve from 3.71784\n","Epoch 218/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7224 - val_loss: 12.0251\n","\n","Epoch 00218: loss did not improve from 3.71784\n","Epoch 219/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7706 - val_loss: 3.8875\n","\n","Epoch 00219: loss did not improve from 3.71784\n","Epoch 220/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7200 - val_loss: 28.3911\n","\n","Epoch 00220: loss did not improve from 3.71784\n","Epoch 221/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7144 - val_loss: 118.5927\n","\n","Epoch 00221: loss improved from 3.71784 to 3.71431, saving model to final.h5\n","Epoch 222/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.8705 - val_loss: 50.9275\n","\n","Epoch 00222: loss did not improve from 3.71431\n","Epoch 223/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7914 - val_loss: 114.8987\n","\n","Epoch 00223: loss did not improve from 3.71431\n","Epoch 224/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7296 - val_loss: 237.7447\n","\n","Epoch 00224: loss did not improve from 3.71431\n","Epoch 225/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.8272 - val_loss: 96.4456\n","\n","Epoch 00225: loss did not improve from 3.71431\n","Epoch 226/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6885 - val_loss: 259.7759\n","\n","Epoch 00226: loss improved from 3.71431 to 3.68849, saving model to final.h5\n","Epoch 227/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.7704 - val_loss: 88.2420\n","\n","Epoch 00227: loss did not improve from 3.68849\n","Epoch 228/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.6803 - val_loss: 41.0394\n","\n","Epoch 00228: loss improved from 3.68849 to 3.68034, saving model to final.h5\n","Epoch 229/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.7712 - val_loss: 174.3724\n","\n","Epoch 00229: loss did not improve from 3.68034\n","Epoch 230/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7200 - val_loss: 332.5987\n","\n","Epoch 00230: loss did not improve from 3.68034\n","Epoch 231/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.6171 - val_loss: 507.7786\n","\n","Epoch 00231: loss improved from 3.68034 to 3.61685, saving model to final.h5\n","Epoch 232/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.7752 - val_loss: 215.8008\n","\n","Epoch 00232: loss did not improve from 3.61685\n","Epoch 233/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.7201 - val_loss: 114.8751\n","\n","Epoch 00233: loss did not improve from 3.61685\n","Epoch 234/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.7227 - val_loss: 109.4330\n","\n","Epoch 00234: loss did not improve from 3.61685\n","Epoch 235/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6550 - val_loss: 23.0197\n","\n","Epoch 00235: loss did not improve from 3.61685\n","Epoch 236/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.7403 - val_loss: 234.0785\n","\n","Epoch 00236: loss did not improve from 3.61685\n","Epoch 237/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.7504 - val_loss: 290.6992\n","\n","Epoch 00237: loss did not improve from 3.61685\n","Epoch 238/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.5919 - val_loss: 344.9702\n","\n","Epoch 00238: loss improved from 3.61685 to 3.59188, saving model to final.h5\n","Epoch 239/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6194 - val_loss: 278.3082\n","\n","Epoch 00239: loss did not improve from 3.59188\n","Epoch 240/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6434 - val_loss: 104.4752\n","\n","Epoch 00240: loss did not improve from 3.59188\n","Epoch 241/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.6769 - val_loss: 77.2656\n","\n","Epoch 00241: loss did not improve from 3.59188\n","Epoch 242/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6877 - val_loss: 158.2370\n","\n","Epoch 00242: loss did not improve from 3.59188\n","Epoch 243/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6655 - val_loss: 50.7013\n","\n","Epoch 00243: loss did not improve from 3.59188\n","Epoch 244/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.7200 - val_loss: 310.8802\n","\n","Epoch 00244: loss did not improve from 3.59188\n","Epoch 245/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6109 - val_loss: 147.6834\n","\n","Epoch 00245: loss did not improve from 3.59188\n","Epoch 246/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6354 - val_loss: 78.3051\n","\n","Epoch 00246: loss did not improve from 3.59188\n","Epoch 247/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.7639 - val_loss: 37.8254\n","\n","Epoch 00247: loss did not improve from 3.59188\n","Epoch 248/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6956 - val_loss: 15.0845\n","\n","Epoch 00248: loss did not improve from 3.59188\n","Epoch 249/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6558 - val_loss: 61.0929\n","\n","Epoch 00249: loss did not improve from 3.59188\n","Epoch 250/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6168 - val_loss: 90.5396\n","\n","Epoch 00250: loss did not improve from 3.59188\n","Epoch 251/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.7398 - val_loss: 100.7786\n","\n","Epoch 00251: loss did not improve from 3.59188\n","Epoch 252/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.5776 - val_loss: 137.3626\n","\n","Epoch 00252: loss improved from 3.59188 to 3.57766, saving model to final.h5\n","Epoch 253/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6509 - val_loss: 5.1275\n","\n","Epoch 00253: loss did not improve from 3.57766\n","Epoch 254/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6551 - val_loss: 4.3336\n","\n","Epoch 00254: loss did not improve from 3.57766\n","Epoch 255/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.6726 - val_loss: 5.0342\n","\n","Epoch 00255: loss did not improve from 3.57766\n","Epoch 256/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.7646 - val_loss: 2.9806\n","\n","Epoch 00256: loss did not improve from 3.57766\n","Epoch 257/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.6519 - val_loss: 28.8075\n","\n","Epoch 00257: loss did not improve from 3.57766\n","Epoch 258/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.6187 - val_loss: 116.8325\n","\n","Epoch 00258: loss did not improve from 3.57766\n","Epoch 259/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.5517 - val_loss: 286.6837\n","\n","Epoch 00259: loss improved from 3.57766 to 3.55173, saving model to final.h5\n","Epoch 260/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5744 - val_loss: 236.8316\n","\n","Epoch 00260: loss did not improve from 3.55173\n","Epoch 261/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.6401 - val_loss: 17.6327\n","\n","Epoch 00261: loss did not improve from 3.55173\n","Epoch 262/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5864 - val_loss: 320.9723\n","\n","Epoch 00262: loss did not improve from 3.55173\n","Epoch 263/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.6472 - val_loss: 247.6446\n","\n","Epoch 00263: loss did not improve from 3.55173\n","Epoch 264/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.6046 - val_loss: 76.7060\n","\n","Epoch 00264: loss did not improve from 3.55173\n","Epoch 265/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.6875 - val_loss: 58.4085\n","\n","Epoch 00265: loss did not improve from 3.55173\n","Epoch 266/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5886 - val_loss: 77.1249\n","\n","Epoch 00266: loss did not improve from 3.55173\n","Epoch 267/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.6852 - val_loss: 36.0310\n","\n","Epoch 00267: loss did not improve from 3.55173\n","Epoch 268/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.6133 - val_loss: 184.7330\n","\n","Epoch 00268: loss did not improve from 3.55173\n","Epoch 269/350\n","500/500 [==============================] - 32s 64ms/step - loss: 4.7596 - val_loss: 13.2027\n","\n","Epoch 00269: loss did not improve from 3.55173\n","Epoch 270/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.6462 - val_loss: 5.8335\n","\n","Epoch 00270: loss did not improve from 3.55173\n","Epoch 271/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.6756 - val_loss: 12.7147\n","\n","Epoch 00271: loss did not improve from 3.55173\n","Epoch 272/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.6506 - val_loss: 39.6288\n","\n","Epoch 00272: loss did not improve from 3.55173\n","Epoch 273/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5108 - val_loss: 170.7889\n","\n","Epoch 00273: loss improved from 3.55173 to 3.51088, saving model to final.h5\n","Epoch 274/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.5425 - val_loss: 35.5893\n","\n","Epoch 00274: loss did not improve from 3.51088\n","Epoch 275/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.5443 - val_loss: 30.7298\n","\n","Epoch 00275: loss did not improve from 3.51088\n","Epoch 276/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.6416 - val_loss: 23.2292\n","\n","Epoch 00276: loss did not improve from 3.51088\n","Epoch 277/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.5525 - val_loss: 9.1110\n","\n","Epoch 00277: loss did not improve from 3.51088\n","Epoch 278/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.5965 - val_loss: 29.7722\n","\n","Epoch 00278: loss did not improve from 3.51088\n","Epoch 279/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5534 - val_loss: 110.3521\n","\n","Epoch 00279: loss did not improve from 3.51088\n","Epoch 280/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5816 - val_loss: 22.5636\n","\n","Epoch 00280: loss did not improve from 3.51088\n","Epoch 281/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5640 - val_loss: 21.4398\n","\n","Epoch 00281: loss did not improve from 3.51088\n","Epoch 282/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5472 - val_loss: 12.1014\n","\n","Epoch 00282: loss did not improve from 3.51088\n","Epoch 283/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5241 - val_loss: 31.7536\n","\n","Epoch 00283: loss did not improve from 3.51088\n","Epoch 284/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5503 - val_loss: 49.5011\n","\n","Epoch 00284: loss did not improve from 3.51088\n","Epoch 285/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4986 - val_loss: 88.1399\n","\n","Epoch 00285: loss improved from 3.51088 to 3.49871, saving model to final.h5\n","Epoch 286/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4392 - val_loss: 25.4033\n","\n","Epoch 00286: loss improved from 3.49871 to 3.43901, saving model to final.h5\n","Epoch 287/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.5786 - val_loss: 84.9935\n","\n","Epoch 00287: loss did not improve from 3.43901\n","Epoch 288/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.4987 - val_loss: 10.0386\n","\n","Epoch 00288: loss did not improve from 3.43901\n","Epoch 289/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.6032 - val_loss: 5.7094\n","\n","Epoch 00289: loss did not improve from 3.43901\n","Epoch 290/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5736 - val_loss: 4.2916\n","\n","Epoch 00290: loss did not improve from 3.43901\n","Epoch 291/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.6115 - val_loss: 2.7610\n","\n","Epoch 00291: loss did not improve from 3.43901\n","Epoch 292/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5727 - val_loss: 1.7473\n","\n","Epoch 00292: loss did not improve from 3.43901\n","Epoch 293/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5289 - val_loss: 3.7026\n","\n","Epoch 00293: loss did not improve from 3.43901\n","Epoch 294/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4711 - val_loss: 3.3176\n","\n","Epoch 00294: loss did not improve from 3.43901\n","Epoch 295/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4904 - val_loss: 7.1972\n","\n","Epoch 00295: loss did not improve from 3.43901\n","Epoch 296/350\n","500/500 [==============================] - 32s 65ms/step - loss: 3.4909 - val_loss: 4.6984\n","\n","Epoch 00296: loss did not improve from 3.43901\n","Epoch 297/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4525 - val_loss: 4.2377\n","\n","Epoch 00297: loss did not improve from 3.43901\n","Epoch 298/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4825 - val_loss: 18.8196\n","\n","Epoch 00298: loss did not improve from 3.43901\n","Epoch 299/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5750 - val_loss: 12.3444\n","\n","Epoch 00299: loss did not improve from 3.43901\n","Epoch 300/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4891 - val_loss: 4.6017\n","\n","Epoch 00300: loss did not improve from 3.43901\n","Epoch 301/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5207 - val_loss: 2.8683\n","\n","Epoch 00301: loss did not improve from 3.43901\n","Epoch 302/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5356 - val_loss: 2.4418\n","\n","Epoch 00302: loss did not improve from 3.43901\n","Epoch 303/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5398 - val_loss: 5.2668\n","\n","Epoch 00303: loss did not improve from 3.43901\n","Epoch 304/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4659 - val_loss: 6.6355\n","\n","Epoch 00304: loss did not improve from 3.43901\n","Epoch 305/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4107 - val_loss: 8.5519\n","\n","Epoch 00305: loss improved from 3.43901 to 3.41080, saving model to final.h5\n","Epoch 306/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.5156 - val_loss: 4.5690\n","\n","Epoch 00306: loss did not improve from 3.41080\n","Epoch 307/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.5243 - val_loss: 2.2797\n","\n","Epoch 00307: loss did not improve from 3.41080\n","Epoch 308/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.4960 - val_loss: 6.5852\n","\n","Epoch 00308: loss did not improve from 3.41080\n","Epoch 309/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.4865 - val_loss: 5.1950\n","\n","Epoch 00309: loss did not improve from 3.41080\n","Epoch 310/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.4375 - val_loss: 118.4416\n","\n","Epoch 00310: loss did not improve from 3.41080\n","Epoch 311/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.5496 - val_loss: 36.5006\n","\n","Epoch 00311: loss did not improve from 3.41080\n","Epoch 312/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.4454 - val_loss: 142.5014\n","\n","Epoch 00312: loss did not improve from 3.41080\n","Epoch 313/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.5395 - val_loss: 23.8652\n","\n","Epoch 00313: loss did not improve from 3.41080\n","Epoch 314/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.4589 - val_loss: 14.8578\n","\n","Epoch 00314: loss did not improve from 3.41080\n","Epoch 315/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4292 - val_loss: 15.9891\n","\n","Epoch 00315: loss did not improve from 3.41080\n","Epoch 316/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.5034 - val_loss: 11.3003\n","\n","Epoch 00316: loss did not improve from 3.41080\n","Epoch 317/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.4651 - val_loss: 37.6375\n","\n","Epoch 00317: loss did not improve from 3.41080\n","Epoch 318/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.4180 - val_loss: 14.9100\n","\n","Epoch 00318: loss did not improve from 3.41080\n","Epoch 319/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.4899 - val_loss: 5.6220\n","\n","Epoch 00319: loss did not improve from 3.41080\n","Epoch 320/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.5930 - val_loss: 4.4217\n","\n","Epoch 00320: loss did not improve from 3.41080\n","Epoch 321/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.4620 - val_loss: 5.0236\n","\n","Epoch 00321: loss did not improve from 3.41080\n","Epoch 322/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.4423 - val_loss: 3.9371\n","\n","Epoch 00322: loss did not improve from 3.41080\n","Epoch 323/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.3903 - val_loss: 3.7892\n","\n","Epoch 00323: loss improved from 3.41080 to 3.39024, saving model to final.h5\n","Epoch 324/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.5506 - val_loss: 3.3574\n","\n","Epoch 00324: loss did not improve from 3.39024\n","Epoch 325/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5508 - val_loss: 4.1038\n","\n","Epoch 00325: loss did not improve from 3.39024\n","Epoch 326/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.3858 - val_loss: 3.9442\n","\n","Epoch 00326: loss improved from 3.39024 to 3.38565, saving model to final.h5\n","Epoch 327/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.3517 - val_loss: 5.2168\n","\n","Epoch 00327: loss improved from 3.38565 to 3.35177, saving model to final.h5\n","Epoch 328/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.4501 - val_loss: 4.5250\n","\n","Epoch 00328: loss did not improve from 3.35177\n","Epoch 329/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.3891 - val_loss: 2.8874\n","\n","Epoch 00329: loss did not improve from 3.35177\n","Epoch 330/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.3326 - val_loss: 2.6998\n","\n","Epoch 00330: loss improved from 3.35177 to 3.33285, saving model to final.h5\n","Epoch 331/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.4339 - val_loss: 3.0705\n","\n","Epoch 00331: loss did not improve from 3.33285\n","Epoch 332/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.3993 - val_loss: 2.9317\n","\n","Epoch 00332: loss did not improve from 3.33285\n","Epoch 333/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.4095 - val_loss: 3.7170\n","\n","Epoch 00333: loss did not improve from 3.33285\n","Epoch 334/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.4805 - val_loss: 4.4966\n","\n","Epoch 00334: loss did not improve from 3.33285\n","Epoch 335/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.5077 - val_loss: 7.4871\n","\n","Epoch 00335: loss did not improve from 3.33285\n","Epoch 336/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.5205 - val_loss: 3.9681\n","\n","Epoch 00336: loss did not improve from 3.33285\n","Epoch 337/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4397 - val_loss: 3.1814\n","\n","Epoch 00337: loss did not improve from 3.33285\n","Epoch 338/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4456 - val_loss: 4.6100\n","\n","Epoch 00338: loss did not improve from 3.33285\n","Epoch 339/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4412 - val_loss: 3.2266\n","\n","Epoch 00339: loss did not improve from 3.33285\n","Epoch 340/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.3696 - val_loss: 4.4982\n","\n","Epoch 00340: loss did not improve from 3.33285\n","Epoch 341/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.3900 - val_loss: 3.5187\n","\n","Epoch 00341: loss did not improve from 3.33285\n","Epoch 342/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.3444 - val_loss: 4.7373\n","\n","Epoch 00342: loss did not improve from 3.33285\n","Epoch 343/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.4070 - val_loss: 6.1773\n","\n","Epoch 00343: loss did not improve from 3.33285\n","Epoch 344/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.3439 - val_loss: 5.9588\n","\n","Epoch 00344: loss did not improve from 3.33285\n","Epoch 345/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4079 - val_loss: 3.5713\n","\n","Epoch 00345: loss did not improve from 3.33285\n","Epoch 346/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.3693 - val_loss: 8.6379\n","\n","Epoch 00346: loss did not improve from 3.33285\n","Epoch 347/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4499 - val_loss: 4.2581\n","\n","Epoch 00347: loss did not improve from 3.33285\n","Epoch 348/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.3831 - val_loss: 4.0083\n","\n","Epoch 00348: loss did not improve from 3.33285\n","Epoch 349/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.3425 - val_loss: 5.7293\n","\n","Epoch 00349: loss did not improve from 3.33285\n","Epoch 350/350\n","500/500 [==============================] - 32s 64ms/step - loss: 3.4590 - val_loss: 2.1623\n","\n","Epoch 00350: loss did not improve from 3.33285\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s8M_eIB20Uk7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4dac6e3c-54f9-48ac-d95a-c30129429c91","executionInfo":{"status":"ok","timestamp":1591831649758,"user_tz":360,"elapsed":24386396,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}}},"source":["!python speedchallenge.py --mode=test  --history {nhistory}  --model final.h5  train_Flipped_for_testing.mp4 train_Flipped_for_testing.txt"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-06-10 23:25:33.154638: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:217: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(32, (8, 8), return_sequences=True, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(32, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=True,activation=\"relu\", dropout=0.5))(flow_inp)\n","2020-06-10 23:25:35.350992: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-06-10 23:25:35.366779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:25:35.367761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 23:25:35.367808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 23:25:35.369780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 23:25:35.372071: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 23:25:35.372479: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 23:25:35.374499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 23:25:35.375900: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 23:25:35.380606: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 23:25:35.380749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:25:35.381755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:25:35.382637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 23:25:35.388960: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-06-10 23:25:35.389360: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e252c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 23:25:35.389418: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-06-10 23:25:35.490550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:25:35.491784: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e25480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 23:25:35.491827: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-06-10 23:25:35.492180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:25:35.493196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 23:25:35.493282: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 23:25:35.493351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 23:25:35.493407: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 23:25:35.493454: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 23:25:35.493504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 23:25:35.493552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 23:25:35.493601: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 23:25:35.493705: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:25:35.494703: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:25:35.495602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 23:25:35.495671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 23:25:36.082358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-10 23:25:36.082421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-06-10 23:25:36.082442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-06-10 23:25:36.082708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:25:36.083775: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:25:36.084704: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-06-10 23:25:36.084764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:221: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(128, (8, 8), return_sequences=False, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(128, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=False,activation=\"relu\", dropout=0.5))(flow)\n","speedchallenge.py:234: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","speedchallenge.py:241: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 100, 100, 2)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 25, 25, 32)   4128        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 25, 25, 32)   0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 25, 25, 32)   128         activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 25, 25, 32)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 2, 100, 100,  0                                            \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 32)   0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv_lst_m2d_1 (ConvLSTM2D)     (None, 2, 25, 25, 32 286848      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 6, 6, 128)    262272      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 2, 25, 25, 32 128         conv_lst_m2d_1[0][0]             \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 6, 6, 128)    0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv_lst_m2d_2 (ConvLSTM2D)     (None, 7, 7, 128)    5243392     batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 6, 6, 128)    512         activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 7, 7, 128)    512         conv_lst_m2d_2[0][0]             \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 6, 6, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 6272)         0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 128)    0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          1605888     flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 5, 5, 256)    33024       max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 256)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 6400)         0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 6656)         0           activation_1[0][0]               \n","                                                                 flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 6656)         0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 6656)         0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 128)          852096      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 128)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1)            129         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 8,289,057\n","Trainable params: 8,288,417\n","Non-trainable params: 640\n","__________________________________________________________________________________________________\n","None\n","Prepping data\n","Decoding speed data\n","loaded 4399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 4398 frames\n","done processing 4400 frames\n","Done prepping data\n","loading weights\n","Starting testing\n","Number of frames to be evaluated:  4398\n","2020-06-10 23:26:27.406658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 23:26:27.628997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","The mean square error is:  52.89571543585437\n","Done testing\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_yPi2-giMTA2","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"13520678-b386-41ee-996e-2dda7e249f71","executionInfo":{"status":"ok","timestamp":1591832080499,"user_tz":360,"elapsed":24817135,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}}},"source":["!python speedchallenge.py --mode=test  --history {nhistory}  --model final.h5  train.mp4 train.txt"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-06-10 23:27:11.330623: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:217: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(32, (8, 8), return_sequences=True, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(32, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=True,activation=\"relu\", dropout=0.5))(flow_inp)\n","2020-06-10 23:27:13.491351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-06-10 23:27:13.507082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:27:13.508070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 23:27:13.508139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 23:27:13.509989: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 23:27:13.512050: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 23:27:13.512419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 23:27:13.514414: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 23:27:13.515675: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 23:27:13.519912: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 23:27:13.520018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:27:13.520989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:27:13.521924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 23:27:13.527792: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-06-10 23:27:13.528165: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e3f2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 23:27:13.528205: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-06-10 23:27:13.622015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:27:13.623305: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1e3f480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 23:27:13.623345: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-06-10 23:27:13.623532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:27:13.624426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 23:27:13.624479: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 23:27:13.624534: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 23:27:13.624565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 23:27:13.624599: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 23:27:13.624627: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 23:27:13.624684: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 23:27:13.624722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 23:27:13.624793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:27:13.625809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:27:13.626650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 23:27:13.626717: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 23:27:14.187035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-10 23:27:14.187101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-06-10 23:27:14.187134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-06-10 23:27:14.187388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:27:14.188412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:27:14.189287: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-06-10 23:27:14.189334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:221: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(128, (8, 8), return_sequences=False, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(128, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=False,activation=\"relu\", dropout=0.5))(flow)\n","speedchallenge.py:234: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","speedchallenge.py:241: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 100, 100, 2)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 25, 25, 32)   4128        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 25, 25, 32)   0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 25, 25, 32)   128         activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 25, 25, 32)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 2, 100, 100,  0                                            \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 32)   0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv_lst_m2d_1 (ConvLSTM2D)     (None, 2, 25, 25, 32 286848      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 6, 6, 128)    262272      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 2, 25, 25, 32 128         conv_lst_m2d_1[0][0]             \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 6, 6, 128)    0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv_lst_m2d_2 (ConvLSTM2D)     (None, 7, 7, 128)    5243392     batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 6, 6, 128)    512         activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 7, 7, 128)    512         conv_lst_m2d_2[0][0]             \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 6, 6, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 6272)         0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 128)    0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          1605888     flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 5, 5, 256)    33024       max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 256)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 6400)         0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 6656)         0           activation_1[0][0]               \n","                                                                 flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 6656)         0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 6656)         0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 128)          852096      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 128)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1)            129         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 8,289,057\n","Trainable params: 8,288,417\n","Non-trainable params: 640\n","__________________________________________________________________________________________________\n","None\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400 frames\n","Done prepping data\n","loading weights\n","Starting testing\n","Number of frames to be evaluated:  20398\n","2020-06-10 23:31:11.618235: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 23:31:11.852193: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","The mean square error is:  135.560957593858\n","Done testing\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s7kOQxOMDECc","colab_type":"text"},"source":["## **Training with normal image** "]},{"cell_type":"code","metadata":{"id":"t_RTlrpYBc6I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"695659cf-a024-4f13-8fae-c50711524b22","executionInfo":{"status":"ok","timestamp":1591842903414,"user_tz":360,"elapsed":35640048,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}}},"source":["!python speedchallenge.py --mode=train --epoch {nepoch} --history {nhistory}  --split_start=7700 --split_end=12100 --model final.h5 --resume --LR {LR} train.mp4 train.txt"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-06-10 23:34:21.948252: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:217: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(32, (8, 8), return_sequences=True, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(32, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=True,activation=\"relu\", dropout=0.5))(flow_inp)\n","2020-06-10 23:34:24.088288: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-06-10 23:34:24.103536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:34:24.104498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 23:34:24.104539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 23:34:24.106466: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 23:34:24.108509: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 23:34:24.108873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 23:34:24.110984: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 23:34:24.112272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 23:34:24.116708: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 23:34:24.116815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:34:24.117870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:34:24.118722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 23:34:24.124725: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-06-10 23:34:24.125066: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ccb2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 23:34:24.125104: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-06-10 23:34:24.220200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:34:24.221397: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ccb480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-06-10 23:34:24.221457: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-06-10 23:34:24.221637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:34:24.222499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-10 23:34:24.222551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 23:34:24.222607: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 23:34:24.222637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-10 23:34:24.222669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-10 23:34:24.222698: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-10 23:34:24.222725: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-10 23:34:24.222753: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-10 23:34:24.222821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:34:24.223739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:34:24.224619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-10 23:34:24.224670: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-10 23:34:24.776257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-10 23:34:24.776305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-06-10 23:34:24.776328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-06-10 23:34:24.776558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:34:24.777485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-10 23:34:24.778302: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-06-10 23:34:24.778348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:221: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(128, (8, 8), return_sequences=False, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(128, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=False,activation=\"relu\", dropout=0.5))(flow)\n","speedchallenge.py:234: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","speedchallenge.py:241: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 100, 100, 2)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 25, 25, 32)   4128        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 25, 25, 32)   0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 25, 25, 32)   128         activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 25, 25, 32)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 2, 100, 100,  0                                            \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 32)   0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv_lst_m2d_1 (ConvLSTM2D)     (None, 2, 25, 25, 32 286848      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 6, 6, 128)    262272      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 2, 25, 25, 32 128         conv_lst_m2d_1[0][0]             \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 6, 6, 128)    0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv_lst_m2d_2 (ConvLSTM2D)     (None, 7, 7, 128)    5243392     batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 6, 6, 128)    512         activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 7, 7, 128)    512         conv_lst_m2d_2[0][0]             \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 6, 6, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 6272)         0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 128)    0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          1605888     flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 5, 5, 256)    33024       max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 256)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 6400)         0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 6656)         0           activation_1[0][0]               \n","                                                                 flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 6656)         0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 6656)         0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 128)          852096      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 128)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1)            129         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 8,289,057\n","Trainable params: 8,288,417\n","Non-trainable params: 640\n","__________________________________________________________________________________________________\n","None\n","loading weights\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","Found preprocessed data\n","tcmalloc: large alloc 2447884288 bytes == 0x21e28000 @  0x7f98450ea1e7 0x7f9842b905e1 0x7f9842bf4c78 0x7f9842bf4f37 0x7f9842c8cf28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f9844ce7b97 0x5b250a\n","tcmalloc: large alloc 2447884288 bytes == 0xb3ca4000 @  0x7f98450ea1e7 0x7f9842b905e1 0x7f9842bf4c78 0x7f9842bf4f37 0x7f9842c8cf28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f9844ce7b97 0x5b250a\n","tcmalloc: large alloc 2447884288 bytes == 0x145b20000 @  0x7f98450ea1e7 0x7f9842b905e1 0x7f9842bf4c78 0x7f9842bf4f37 0x7f9842c8cf28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f9844ce7b97 0x5b250a\n","Loading frame 20398\n","done loading 20399 frames\n","Done prepping data\n","tcmalloc: large alloc 1631920128 bytes == 0x7f96f8bae000 @  0x7f98450ea1e7 0x7f9842b905e1 0x7f9842bf4c78 0x7f9842bf4d93 0x7f9842c7fed6 0x7f9842c80338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f9844ce7b97 0x5b250a\n","20398 Training data size per Aug\n","15998 Train indices size\n","4400 Val indices size\n"," This is the range of train:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119, 6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129, 6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139, 6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169, 6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209, 6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229, 6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239, 6240, 6241, 6242, 6243, 6244, 6245, 6246, 6247, 6248, 6249, 6250, 6251, 6252, 6253, 6254, 6255, 6256, 6257, 6258, 6259, 6260, 6261, 6262, 6263, 6264, 6265, 6266, 6267, 6268, 6269, 6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289, 6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299, 6300, 6301, 6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309, 6310, 6311, 6312, 6313, 6314, 6315, 6316, 6317, 6318, 6319, 6320, 6321, 6322, 6323, 6324, 6325, 6326, 6327, 6328, 6329, 6330, 6331, 6332, 6333, 6334, 6335, 6336, 6337, 6338, 6339, 6340, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349, 6350, 6351, 6352, 6353, 6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363, 6364, 6365, 6366, 6367, 6368, 6369, 6370, 6371, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6379, 6380, 6381, 6382, 6383, 6384, 6385, 6386, 6387, 6388, 6389, 6390, 6391, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399, 6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419, 6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429, 6430, 6431, 6432, 6433, 6434, 6435, 6436, 6437, 6438, 6439, 6440, 6441, 6442, 6443, 6444, 6445, 6446, 6447, 6448, 6449, 6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459, 6460, 6461, 6462, 6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473, 6474, 6475, 6476, 6477, 6478, 6479, 6480, 6481, 6482, 6483, 6484, 6485, 6486, 6487, 6488, 6489, 6490, 6491, 6492, 6493, 6494, 6495, 6496, 6497, 6498, 6499, 6500, 6501, 6502, 6503, 6504, 6505, 6506, 6507, 6508, 6509, 6510, 6511, 6512, 6513, 6514, 6515, 6516, 6517, 6518, 6519, 6520, 6521, 6522, 6523, 6524, 6525, 6526, 6527, 6528, 6529, 6530, 6531, 6532, 6533, 6534, 6535, 6536, 6537, 6538, 6539, 6540, 6541, 6542, 6543, 6544, 6545, 6546, 6547, 6548, 6549, 6550, 6551, 6552, 6553, 6554, 6555, 6556, 6557, 6558, 6559, 6560, 6561, 6562, 6563, 6564, 6565, 6566, 6567, 6568, 6569, 6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577, 6578, 6579, 6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588, 6589, 6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599, 6600, 6601, 6602, 6603, 6604, 6605, 6606, 6607, 6608, 6609, 6610, 6611, 6612, 6613, 6614, 6615, 6616, 6617, 6618, 6619, 6620, 6621, 6622, 6623, 6624, 6625, 6626, 6627, 6628, 6629, 6630, 6631, 6632, 6633, 6634, 6635, 6636, 6637, 6638, 6639, 6640, 6641, 6642, 6643, 6644, 6645, 6646, 6647, 6648, 6649, 6650, 6651, 6652, 6653, 6654, 6655, 6656, 6657, 6658, 6659, 6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6669, 6670, 6671, 6672, 6673, 6674, 6675, 6676, 6677, 6678, 6679, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689, 6690, 6691, 6692, 6693, 6694, 6695, 6696, 6697, 6698, 6699, 6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6708, 6709, 6710, 6711, 6712, 6713, 6714, 6715, 6716, 6717, 6718, 6719, 6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729, 6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739, 6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6748, 6749, 6750, 6751, 6752, 6753, 6754, 6755, 6756, 6757, 6758, 6759, 6760, 6761, 6762, 6763, 6764, 6765, 6766, 6767, 6768, 6769, 6770, 6771, 6772, 6773, 6774, 6775, 6776, 6777, 6778, 6779, 6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6788, 6789, 6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 6804, 6805, 6806, 6807, 6808, 6809, 6810, 6811, 6812, 6813, 6814, 6815, 6816, 6817, 6818, 6819, 6820, 6821, 6822, 6823, 6824, 6825, 6826, 6827, 6828, 6829, 6830, 6831, 6832, 6833, 6834, 6835, 6836, 6837, 6838, 6839, 6840, 6841, 6842, 6843, 6844, 6845, 6846, 6847, 6848, 6849, 6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6859, 6860, 6861, 6862, 6863, 6864, 6865, 6866, 6867, 6868, 6869, 6870, 6871, 6872, 6873, 6874, 6875, 6876, 6877, 6878, 6879, 6880, 6881, 6882, 6883, 6884, 6885, 6886, 6887, 6888, 6889, 6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898, 6899, 6900, 6901, 6902, 6903, 6904, 6905, 6906, 6907, 6908, 6909, 6910, 6911, 6912, 6913, 6914, 6915, 6916, 6917, 6918, 6919, 6920, 6921, 6922, 6923, 6924, 6925, 6926, 6927, 6928, 6929, 6930, 6931, 6932, 6933, 6934, 6935, 6936, 6937, 6938, 6939, 6940, 6941, 6942, 6943, 6944, 6945, 6946, 6947, 6948, 6949, 6950, 6951, 6952, 6953, 6954, 6955, 6956, 6957, 6958, 6959, 6960, 6961, 6962, 6963, 6964, 6965, 6966, 6967, 6968, 6969, 6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979, 6980, 6981, 6982, 6983, 6984, 6985, 6986, 6987, 6988, 6989, 6990, 6991, 6992, 6993, 6994, 6995, 6996, 6997, 6998, 6999, 7000, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009, 7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019, 7020, 7021, 7022, 7023, 7024, 7025, 7026, 7027, 7028, 7029, 7030, 7031, 7032, 7033, 7034, 7035, 7036, 7037, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 7045, 7046, 7047, 7048, 7049, 7050, 7051, 7052, 7053, 7054, 7055, 7056, 7057, 7058, 7059, 7060, 7061, 7062, 7063, 7064, 7065, 7066, 7067, 7068, 7069, 7070, 7071, 7072, 7073, 7074, 7075, 7076, 7077, 7078, 7079, 7080, 7081, 7082, 7083, 7084, 7085, 7086, 7087, 7088, 7089, 7090, 7091, 7092, 7093, 7094, 7095, 7096, 7097, 7098, 7099, 7100, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7108, 7109, 7110, 7111, 7112, 7113, 7114, 7115, 7116, 7117, 7118, 7119, 7120, 7121, 7122, 7123, 7124, 7125, 7126, 7127, 7128, 7129, 7130, 7131, 7132, 7133, 7134, 7135, 7136, 7137, 7138, 7139, 7140, 7141, 7142, 7143, 7144, 7145, 7146, 7147, 7148, 7149, 7150, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7161, 7162, 7163, 7164, 7165, 7166, 7167, 7168, 7169, 7170, 7171, 7172, 7173, 7174, 7175, 7176, 7177, 7178, 7179, 7180, 7181, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189, 7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199, 7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209, 7210, 7211, 7212, 7213, 7214, 7215, 7216, 7217, 7218, 7219, 7220, 7221, 7222, 7223, 7224, 7225, 7226, 7227, 7228, 7229, 7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7239, 7240, 7241, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249, 7250, 7251, 7252, 7253, 7254, 7255, 7256, 7257, 7258, 7259, 7260, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7268, 7269, 7270, 7271, 7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281, 7282, 7283, 7284, 7285, 7286, 7287, 7288, 7289, 7290, 7291, 7292, 7293, 7294, 7295, 7296, 7297, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309, 7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318, 7319, 7320, 7321, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7329, 7330, 7331, 7332, 7333, 7334, 7335, 7336, 7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7349, 7350, 7351, 7352, 7353, 7354, 7355, 7356, 7357, 7358, 7359, 7360, 7361, 7362, 7363, 7364, 7365, 7366, 7367, 7368, 7369, 7370, 7371, 7372, 7373, 7374, 7375, 7376, 7377, 7378, 7379, 7380, 7381, 7382, 7383, 7384, 7385, 7386, 7387, 7388, 7389, 7390, 7391, 7392, 7393, 7394, 7395, 7396, 7397, 7398, 7399, 7400, 7401, 7402, 7403, 7404, 7405, 7406, 7407, 7408, 7409, 7410, 7411, 7412, 7413, 7414, 7415, 7416, 7417, 7418, 7419, 7420, 7421, 7422, 7423, 7424, 7425, 7426, 7427, 7428, 7429, 7430, 7431, 7432, 7433, 7434, 7435, 7436, 7437, 7438, 7439, 7440, 7441, 7442, 7443, 7444, 7445, 7446, 7447, 7448, 7449, 7450, 7451, 7452, 7453, 7454, 7455, 7456, 7457, 7458, 7459, 7460, 7461, 7462, 7463, 7464, 7465, 7466, 7467, 7468, 7469, 7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7479, 7480, 7481, 7482, 7483, 7484, 7485, 7486, 7487, 7488, 7489, 7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499, 7500, 7501, 7502, 7503, 7504, 7505, 7506, 7507, 7508, 7509, 7510, 7511, 7512, 7513, 7514, 7515, 7516, 7517, 7518, 7519, 7520, 7521, 7522, 7523, 7524, 7525, 7526, 7527, 7528, 7529, 7530, 7531, 7532, 7533, 7534, 7535, 7536, 7537, 7538, 7539, 7540, 7541, 7542, 7543, 7544, 7545, 7546, 7547, 7548, 7549, 7550, 7551, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559, 7560, 7561, 7562, 7563, 7564, 7565, 7566, 7567, 7568, 7569, 7570, 7571, 7572, 7573, 7574, 7575, 7576, 7577, 7578, 7579, 7580, 7581, 7582, 7583, 7584, 7585, 7586, 7587, 7588, 7589, 7590, 7591, 7592, 7593, 7594, 7595, 7596, 7597, 7598, 7599, 7600, 7601, 7602, 7603, 7604, 7605, 7606, 7607, 7608, 7609, 7610, 7611, 7612, 7613, 7614, 7615, 7616, 7617, 7618, 7619, 7620, 7621, 7622, 7623, 7624, 7625, 7626, 7627, 7628, 7629, 7630, 7631, 7632, 7633, 7634, 7635, 7636, 7637, 7638, 7639, 7640, 7641, 7642, 7643, 7644, 7645, 7646, 7647, 7648, 7649, 7650, 7651, 7652, 7653, 7654, 7655, 7656, 7657, 7658, 7659, 7660, 7661, 7662, 7663, 7664, 7665, 7666, 7667, 7668, 7669, 7670, 7671, 7672, 7673, 7674, 7675, 7676, 7677, 7678, 7679, 7680, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689, 7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699, 12100, 12101, 12102, 12103, 12104, 12105, 12106, 12107, 12108, 12109, 12110, 12111, 12112, 12113, 12114, 12115, 12116, 12117, 12118, 12119, 12120, 12121, 12122, 12123, 12124, 12125, 12126, 12127, 12128, 12129, 12130, 12131, 12132, 12133, 12134, 12135, 12136, 12137, 12138, 12139, 12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149, 12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159, 12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169, 12170, 12171, 12172, 12173, 12174, 12175, 12176, 12177, 12178, 12179, 12180, 12181, 12182, 12183, 12184, 12185, 12186, 12187, 12188, 12189, 12190, 12191, 12192, 12193, 12194, 12195, 12196, 12197, 12198, 12199, 12200, 12201, 12202, 12203, 12204, 12205, 12206, 12207, 12208, 12209, 12210, 12211, 12212, 12213, 12214, 12215, 12216, 12217, 12218, 12219, 12220, 12221, 12222, 12223, 12224, 12225, 12226, 12227, 12228, 12229, 12230, 12231, 12232, 12233, 12234, 12235, 12236, 12237, 12238, 12239, 12240, 12241, 12242, 12243, 12244, 12245, 12246, 12247, 12248, 12249, 12250, 12251, 12252, 12253, 12254, 12255, 12256, 12257, 12258, 12259, 12260, 12261, 12262, 12263, 12264, 12265, 12266, 12267, 12268, 12269, 12270, 12271, 12272, 12273, 12274, 12275, 12276, 12277, 12278, 12279, 12280, 12281, 12282, 12283, 12284, 12285, 12286, 12287, 12288, 12289, 12290, 12291, 12292, 12293, 12294, 12295, 12296, 12297, 12298, 12299, 12300, 12301, 12302, 12303, 12304, 12305, 12306, 12307, 12308, 12309, 12310, 12311, 12312, 12313, 12314, 12315, 12316, 12317, 12318, 12319, 12320, 12321, 12322, 12323, 12324, 12325, 12326, 12327, 12328, 12329, 12330, 12331, 12332, 12333, 12334, 12335, 12336, 12337, 12338, 12339, 12340, 12341, 12342, 12343, 12344, 12345, 12346, 12347, 12348, 12349, 12350, 12351, 12352, 12353, 12354, 12355, 12356, 12357, 12358, 12359, 12360, 12361, 12362, 12363, 12364, 12365, 12366, 12367, 12368, 12369, 12370, 12371, 12372, 12373, 12374, 12375, 12376, 12377, 12378, 12379, 12380, 12381, 12382, 12383, 12384, 12385, 12386, 12387, 12388, 12389, 12390, 12391, 12392, 12393, 12394, 12395, 12396, 12397, 12398, 12399, 12400, 12401, 12402, 12403, 12404, 12405, 12406, 12407, 12408, 12409, 12410, 12411, 12412, 12413, 12414, 12415, 12416, 12417, 12418, 12419, 12420, 12421, 12422, 12423, 12424, 12425, 12426, 12427, 12428, 12429, 12430, 12431, 12432, 12433, 12434, 12435, 12436, 12437, 12438, 12439, 12440, 12441, 12442, 12443, 12444, 12445, 12446, 12447, 12448, 12449, 12450, 12451, 12452, 12453, 12454, 12455, 12456, 12457, 12458, 12459, 12460, 12461, 12462, 12463, 12464, 12465, 12466, 12467, 12468, 12469, 12470, 12471, 12472, 12473, 12474, 12475, 12476, 12477, 12478, 12479, 12480, 12481, 12482, 12483, 12484, 12485, 12486, 12487, 12488, 12489, 12490, 12491, 12492, 12493, 12494, 12495, 12496, 12497, 12498, 12499, 12500, 12501, 12502, 12503, 12504, 12505, 12506, 12507, 12508, 12509, 12510, 12511, 12512, 12513, 12514, 12515, 12516, 12517, 12518, 12519, 12520, 12521, 12522, 12523, 12524, 12525, 12526, 12527, 12528, 12529, 12530, 12531, 12532, 12533, 12534, 12535, 12536, 12537, 12538, 12539, 12540, 12541, 12542, 12543, 12544, 12545, 12546, 12547, 12548, 12549, 12550, 12551, 12552, 12553, 12554, 12555, 12556, 12557, 12558, 12559, 12560, 12561, 12562, 12563, 12564, 12565, 12566, 12567, 12568, 12569, 12570, 12571, 12572, 12573, 12574, 12575, 12576, 12577, 12578, 12579, 12580, 12581, 12582, 12583, 12584, 12585, 12586, 12587, 12588, 12589, 12590, 12591, 12592, 12593, 12594, 12595, 12596, 12597, 12598, 12599, 12600, 12601, 12602, 12603, 12604, 12605, 12606, 12607, 12608, 12609, 12610, 12611, 12612, 12613, 12614, 12615, 12616, 12617, 12618, 12619, 12620, 12621, 12622, 12623, 12624, 12625, 12626, 12627, 12628, 12629, 12630, 12631, 12632, 12633, 12634, 12635, 12636, 12637, 12638, 12639, 12640, 12641, 12642, 12643, 12644, 12645, 12646, 12647, 12648, 12649, 12650, 12651, 12652, 12653, 12654, 12655, 12656, 12657, 12658, 12659, 12660, 12661, 12662, 12663, 12664, 12665, 12666, 12667, 12668, 12669, 12670, 12671, 12672, 12673, 12674, 12675, 12676, 12677, 12678, 12679, 12680, 12681, 12682, 12683, 12684, 12685, 12686, 12687, 12688, 12689, 12690, 12691, 12692, 12693, 12694, 12695, 12696, 12697, 12698, 12699, 12700, 12701, 12702, 12703, 12704, 12705, 12706, 12707, 12708, 12709, 12710, 12711, 12712, 12713, 12714, 12715, 12716, 12717, 12718, 12719, 12720, 12721, 12722, 12723, 12724, 12725, 12726, 12727, 12728, 12729, 12730, 12731, 12732, 12733, 12734, 12735, 12736, 12737, 12738, 12739, 12740, 12741, 12742, 12743, 12744, 12745, 12746, 12747, 12748, 12749, 12750, 12751, 12752, 12753, 12754, 12755, 12756, 12757, 12758, 12759, 12760, 12761, 12762, 12763, 12764, 12765, 12766, 12767, 12768, 12769, 12770, 12771, 12772, 12773, 12774, 12775, 12776, 12777, 12778, 12779, 12780, 12781, 12782, 12783, 12784, 12785, 12786, 12787, 12788, 12789, 12790, 12791, 12792, 12793, 12794, 12795, 12796, 12797, 12798, 12799, 12800, 12801, 12802, 12803, 12804, 12805, 12806, 12807, 12808, 12809, 12810, 12811, 12812, 12813, 12814, 12815, 12816, 12817, 12818, 12819, 12820, 12821, 12822, 12823, 12824, 12825, 12826, 12827, 12828, 12829, 12830, 12831, 12832, 12833, 12834, 12835, 12836, 12837, 12838, 12839, 12840, 12841, 12842, 12843, 12844, 12845, 12846, 12847, 12848, 12849, 12850, 12851, 12852, 12853, 12854, 12855, 12856, 12857, 12858, 12859, 12860, 12861, 12862, 12863, 12864, 12865, 12866, 12867, 12868, 12869, 12870, 12871, 12872, 12873, 12874, 12875, 12876, 12877, 12878, 12879, 12880, 12881, 12882, 12883, 12884, 12885, 12886, 12887, 12888, 12889, 12890, 12891, 12892, 12893, 12894, 12895, 12896, 12897, 12898, 12899, 12900, 12901, 12902, 12903, 12904, 12905, 12906, 12907, 12908, 12909, 12910, 12911, 12912, 12913, 12914, 12915, 12916, 12917, 12918, 12919, 12920, 12921, 12922, 12923, 12924, 12925, 12926, 12927, 12928, 12929, 12930, 12931, 12932, 12933, 12934, 12935, 12936, 12937, 12938, 12939, 12940, 12941, 12942, 12943, 12944, 12945, 12946, 12947, 12948, 12949, 12950, 12951, 12952, 12953, 12954, 12955, 12956, 12957, 12958, 12959, 12960, 12961, 12962, 12963, 12964, 12965, 12966, 12967, 12968, 12969, 12970, 12971, 12972, 12973, 12974, 12975, 12976, 12977, 12978, 12979, 12980, 12981, 12982, 12983, 12984, 12985, 12986, 12987, 12988, 12989, 12990, 12991, 12992, 12993, 12994, 12995, 12996, 12997, 12998, 12999, 13000, 13001, 13002, 13003, 13004, 13005, 13006, 13007, 13008, 13009, 13010, 13011, 13012, 13013, 13014, 13015, 13016, 13017, 13018, 13019, 13020, 13021, 13022, 13023, 13024, 13025, 13026, 13027, 13028, 13029, 13030, 13031, 13032, 13033, 13034, 13035, 13036, 13037, 13038, 13039, 13040, 13041, 13042, 13043, 13044, 13045, 13046, 13047, 13048, 13049, 13050, 13051, 13052, 13053, 13054, 13055, 13056, 13057, 13058, 13059, 13060, 13061, 13062, 13063, 13064, 13065, 13066, 13067, 13068, 13069, 13070, 13071, 13072, 13073, 13074, 13075, 13076, 13077, 13078, 13079, 13080, 13081, 13082, 13083, 13084, 13085, 13086, 13087, 13088, 13089, 13090, 13091, 13092, 13093, 13094, 13095, 13096, 13097, 13098, 13099, 13100, 13101, 13102, 13103, 13104, 13105, 13106, 13107, 13108, 13109, 13110, 13111, 13112, 13113, 13114, 13115, 13116, 13117, 13118, 13119, 13120, 13121, 13122, 13123, 13124, 13125, 13126, 13127, 13128, 13129, 13130, 13131, 13132, 13133, 13134, 13135, 13136, 13137, 13138, 13139, 13140, 13141, 13142, 13143, 13144, 13145, 13146, 13147, 13148, 13149, 13150, 13151, 13152, 13153, 13154, 13155, 13156, 13157, 13158, 13159, 13160, 13161, 13162, 13163, 13164, 13165, 13166, 13167, 13168, 13169, 13170, 13171, 13172, 13173, 13174, 13175, 13176, 13177, 13178, 13179, 13180, 13181, 13182, 13183, 13184, 13185, 13186, 13187, 13188, 13189, 13190, 13191, 13192, 13193, 13194, 13195, 13196, 13197, 13198, 13199, 13200, 13201, 13202, 13203, 13204, 13205, 13206, 13207, 13208, 13209, 13210, 13211, 13212, 13213, 13214, 13215, 13216, 13217, 13218, 13219, 13220, 13221, 13222, 13223, 13224, 13225, 13226, 13227, 13228, 13229, 13230, 13231, 13232, 13233, 13234, 13235, 13236, 13237, 13238, 13239, 13240, 13241, 13242, 13243, 13244, 13245, 13246, 13247, 13248, 13249, 13250, 13251, 13252, 13253, 13254, 13255, 13256, 13257, 13258, 13259, 13260, 13261, 13262, 13263, 13264, 13265, 13266, 13267, 13268, 13269, 13270, 13271, 13272, 13273, 13274, 13275, 13276, 13277, 13278, 13279, 13280, 13281, 13282, 13283, 13284, 13285, 13286, 13287, 13288, 13289, 13290, 13291, 13292, 13293, 13294, 13295, 13296, 13297, 13298, 13299, 13300, 13301, 13302, 13303, 13304, 13305, 13306, 13307, 13308, 13309, 13310, 13311, 13312, 13313, 13314, 13315, 13316, 13317, 13318, 13319, 13320, 13321, 13322, 13323, 13324, 13325, 13326, 13327, 13328, 13329, 13330, 13331, 13332, 13333, 13334, 13335, 13336, 13337, 13338, 13339, 13340, 13341, 13342, 13343, 13344, 13345, 13346, 13347, 13348, 13349, 13350, 13351, 13352, 13353, 13354, 13355, 13356, 13357, 13358, 13359, 13360, 13361, 13362, 13363, 13364, 13365, 13366, 13367, 13368, 13369, 13370, 13371, 13372, 13373, 13374, 13375, 13376, 13377, 13378, 13379, 13380, 13381, 13382, 13383, 13384, 13385, 13386, 13387, 13388, 13389, 13390, 13391, 13392, 13393, 13394, 13395, 13396, 13397, 13398, 13399, 13400, 13401, 13402, 13403, 13404, 13405, 13406, 13407, 13408, 13409, 13410, 13411, 13412, 13413, 13414, 13415, 13416, 13417, 13418, 13419, 13420, 13421, 13422, 13423, 13424, 13425, 13426, 13427, 13428, 13429, 13430, 13431, 13432, 13433, 13434, 13435, 13436, 13437, 13438, 13439, 13440, 13441, 13442, 13443, 13444, 13445, 13446, 13447, 13448, 13449, 13450, 13451, 13452, 13453, 13454, 13455, 13456, 13457, 13458, 13459, 13460, 13461, 13462, 13463, 13464, 13465, 13466, 13467, 13468, 13469, 13470, 13471, 13472, 13473, 13474, 13475, 13476, 13477, 13478, 13479, 13480, 13481, 13482, 13483, 13484, 13485, 13486, 13487, 13488, 13489, 13490, 13491, 13492, 13493, 13494, 13495, 13496, 13497, 13498, 13499, 13500, 13501, 13502, 13503, 13504, 13505, 13506, 13507, 13508, 13509, 13510, 13511, 13512, 13513, 13514, 13515, 13516, 13517, 13518, 13519, 13520, 13521, 13522, 13523, 13524, 13525, 13526, 13527, 13528, 13529, 13530, 13531, 13532, 13533, 13534, 13535, 13536, 13537, 13538, 13539, 13540, 13541, 13542, 13543, 13544, 13545, 13546, 13547, 13548, 13549, 13550, 13551, 13552, 13553, 13554, 13555, 13556, 13557, 13558, 13559, 13560, 13561, 13562, 13563, 13564, 13565, 13566, 13567, 13568, 13569, 13570, 13571, 13572, 13573, 13574, 13575, 13576, 13577, 13578, 13579, 13580, 13581, 13582, 13583, 13584, 13585, 13586, 13587, 13588, 13589, 13590, 13591, 13592, 13593, 13594, 13595, 13596, 13597, 13598, 13599, 13600, 13601, 13602, 13603, 13604, 13605, 13606, 13607, 13608, 13609, 13610, 13611, 13612, 13613, 13614, 13615, 13616, 13617, 13618, 13619, 13620, 13621, 13622, 13623, 13624, 13625, 13626, 13627, 13628, 13629, 13630, 13631, 13632, 13633, 13634, 13635, 13636, 13637, 13638, 13639, 13640, 13641, 13642, 13643, 13644, 13645, 13646, 13647, 13648, 13649, 13650, 13651, 13652, 13653, 13654, 13655, 13656, 13657, 13658, 13659, 13660, 13661, 13662, 13663, 13664, 13665, 13666, 13667, 13668, 13669, 13670, 13671, 13672, 13673, 13674, 13675, 13676, 13677, 13678, 13679, 13680, 13681, 13682, 13683, 13684, 13685, 13686, 13687, 13688, 13689, 13690, 13691, 13692, 13693, 13694, 13695, 13696, 13697, 13698, 13699, 13700, 13701, 13702, 13703, 13704, 13705, 13706, 13707, 13708, 13709, 13710, 13711, 13712, 13713, 13714, 13715, 13716, 13717, 13718, 13719, 13720, 13721, 13722, 13723, 13724, 13725, 13726, 13727, 13728, 13729, 13730, 13731, 13732, 13733, 13734, 13735, 13736, 13737, 13738, 13739, 13740, 13741, 13742, 13743, 13744, 13745, 13746, 13747, 13748, 13749, 13750, 13751, 13752, 13753, 13754, 13755, 13756, 13757, 13758, 13759, 13760, 13761, 13762, 13763, 13764, 13765, 13766, 13767, 13768, 13769, 13770, 13771, 13772, 13773, 13774, 13775, 13776, 13777, 13778, 13779, 13780, 13781, 13782, 13783, 13784, 13785, 13786, 13787, 13788, 13789, 13790, 13791, 13792, 13793, 13794, 13795, 13796, 13797, 13798, 13799, 13800, 13801, 13802, 13803, 13804, 13805, 13806, 13807, 13808, 13809, 13810, 13811, 13812, 13813, 13814, 13815, 13816, 13817, 13818, 13819, 13820, 13821, 13822, 13823, 13824, 13825, 13826, 13827, 13828, 13829, 13830, 13831, 13832, 13833, 13834, 13835, 13836, 13837, 13838, 13839, 13840, 13841, 13842, 13843, 13844, 13845, 13846, 13847, 13848, 13849, 13850, 13851, 13852, 13853, 13854, 13855, 13856, 13857, 13858, 13859, 13860, 13861, 13862, 13863, 13864, 13865, 13866, 13867, 13868, 13869, 13870, 13871, 13872, 13873, 13874, 13875, 13876, 13877, 13878, 13879, 13880, 13881, 13882, 13883, 13884, 13885, 13886, 13887, 13888, 13889, 13890, 13891, 13892, 13893, 13894, 13895, 13896, 13897, 13898, 13899, 13900, 13901, 13902, 13903, 13904, 13905, 13906, 13907, 13908, 13909, 13910, 13911, 13912, 13913, 13914, 13915, 13916, 13917, 13918, 13919, 13920, 13921, 13922, 13923, 13924, 13925, 13926, 13927, 13928, 13929, 13930, 13931, 13932, 13933, 13934, 13935, 13936, 13937, 13938, 13939, 13940, 13941, 13942, 13943, 13944, 13945, 13946, 13947, 13948, 13949, 13950, 13951, 13952, 13953, 13954, 13955, 13956, 13957, 13958, 13959, 13960, 13961, 13962, 13963, 13964, 13965, 13966, 13967, 13968, 13969, 13970, 13971, 13972, 13973, 13974, 13975, 13976, 13977, 13978, 13979, 13980, 13981, 13982, 13983, 13984, 13985, 13986, 13987, 13988, 13989, 13990, 13991, 13992, 13993, 13994, 13995, 13996, 13997, 13998, 13999, 14000, 14001, 14002, 14003, 14004, 14005, 14006, 14007, 14008, 14009, 14010, 14011, 14012, 14013, 14014, 14015, 14016, 14017, 14018, 14019, 14020, 14021, 14022, 14023, 14024, 14025, 14026, 14027, 14028, 14029, 14030, 14031, 14032, 14033, 14034, 14035, 14036, 14037, 14038, 14039, 14040, 14041, 14042, 14043, 14044, 14045, 14046, 14047, 14048, 14049, 14050, 14051, 14052, 14053, 14054, 14055, 14056, 14057, 14058, 14059, 14060, 14061, 14062, 14063, 14064, 14065, 14066, 14067, 14068, 14069, 14070, 14071, 14072, 14073, 14074, 14075, 14076, 14077, 14078, 14079, 14080, 14081, 14082, 14083, 14084, 14085, 14086, 14087, 14088, 14089, 14090, 14091, 14092, 14093, 14094, 14095, 14096, 14097, 14098, 14099, 14100, 14101, 14102, 14103, 14104, 14105, 14106, 14107, 14108, 14109, 14110, 14111, 14112, 14113, 14114, 14115, 14116, 14117, 14118, 14119, 14120, 14121, 14122, 14123, 14124, 14125, 14126, 14127, 14128, 14129, 14130, 14131, 14132, 14133, 14134, 14135, 14136, 14137, 14138, 14139, 14140, 14141, 14142, 14143, 14144, 14145, 14146, 14147, 14148, 14149, 14150, 14151, 14152, 14153, 14154, 14155, 14156, 14157, 14158, 14159, 14160, 14161, 14162, 14163, 14164, 14165, 14166, 14167, 14168, 14169, 14170, 14171, 14172, 14173, 14174, 14175, 14176, 14177, 14178, 14179, 14180, 14181, 14182, 14183, 14184, 14185, 14186, 14187, 14188, 14189, 14190, 14191, 14192, 14193, 14194, 14195, 14196, 14197, 14198, 14199, 14200, 14201, 14202, 14203, 14204, 14205, 14206, 14207, 14208, 14209, 14210, 14211, 14212, 14213, 14214, 14215, 14216, 14217, 14218, 14219, 14220, 14221, 14222, 14223, 14224, 14225, 14226, 14227, 14228, 14229, 14230, 14231, 14232, 14233, 14234, 14235, 14236, 14237, 14238, 14239, 14240, 14241, 14242, 14243, 14244, 14245, 14246, 14247, 14248, 14249, 14250, 14251, 14252, 14253, 14254, 14255, 14256, 14257, 14258, 14259, 14260, 14261, 14262, 14263, 14264, 14265, 14266, 14267, 14268, 14269, 14270, 14271, 14272, 14273, 14274, 14275, 14276, 14277, 14278, 14279, 14280, 14281, 14282, 14283, 14284, 14285, 14286, 14287, 14288, 14289, 14290, 14291, 14292, 14293, 14294, 14295, 14296, 14297, 14298, 14299, 14300, 14301, 14302, 14303, 14304, 14305, 14306, 14307, 14308, 14309, 14310, 14311, 14312, 14313, 14314, 14315, 14316, 14317, 14318, 14319, 14320, 14321, 14322, 14323, 14324, 14325, 14326, 14327, 14328, 14329, 14330, 14331, 14332, 14333, 14334, 14335, 14336, 14337, 14338, 14339, 14340, 14341, 14342, 14343, 14344, 14345, 14346, 14347, 14348, 14349, 14350, 14351, 14352, 14353, 14354, 14355, 14356, 14357, 14358, 14359, 14360, 14361, 14362, 14363, 14364, 14365, 14366, 14367, 14368, 14369, 14370, 14371, 14372, 14373, 14374, 14375, 14376, 14377, 14378, 14379, 14380, 14381, 14382, 14383, 14384, 14385, 14386, 14387, 14388, 14389, 14390, 14391, 14392, 14393, 14394, 14395, 14396, 14397, 14398, 14399, 14400, 14401, 14402, 14403, 14404, 14405, 14406, 14407, 14408, 14409, 14410, 14411, 14412, 14413, 14414, 14415, 14416, 14417, 14418, 14419, 14420, 14421, 14422, 14423, 14424, 14425, 14426, 14427, 14428, 14429, 14430, 14431, 14432, 14433, 14434, 14435, 14436, 14437, 14438, 14439, 14440, 14441, 14442, 14443, 14444, 14445, 14446, 14447, 14448, 14449, 14450, 14451, 14452, 14453, 14454, 14455, 14456, 14457, 14458, 14459, 14460, 14461, 14462, 14463, 14464, 14465, 14466, 14467, 14468, 14469, 14470, 14471, 14472, 14473, 14474, 14475, 14476, 14477, 14478, 14479, 14480, 14481, 14482, 14483, 14484, 14485, 14486, 14487, 14488, 14489, 14490, 14491, 14492, 14493, 14494, 14495, 14496, 14497, 14498, 14499, 14500, 14501, 14502, 14503, 14504, 14505, 14506, 14507, 14508, 14509, 14510, 14511, 14512, 14513, 14514, 14515, 14516, 14517, 14518, 14519, 14520, 14521, 14522, 14523, 14524, 14525, 14526, 14527, 14528, 14529, 14530, 14531, 14532, 14533, 14534, 14535, 14536, 14537, 14538, 14539, 14540, 14541, 14542, 14543, 14544, 14545, 14546, 14547, 14548, 14549, 14550, 14551, 14552, 14553, 14554, 14555, 14556, 14557, 14558, 14559, 14560, 14561, 14562, 14563, 14564, 14565, 14566, 14567, 14568, 14569, 14570, 14571, 14572, 14573, 14574, 14575, 14576, 14577, 14578, 14579, 14580, 14581, 14582, 14583, 14584, 14585, 14586, 14587, 14588, 14589, 14590, 14591, 14592, 14593, 14594, 14595, 14596, 14597, 14598, 14599, 14600, 14601, 14602, 14603, 14604, 14605, 14606, 14607, 14608, 14609, 14610, 14611, 14612, 14613, 14614, 14615, 14616, 14617, 14618, 14619, 14620, 14621, 14622, 14623, 14624, 14625, 14626, 14627, 14628, 14629, 14630, 14631, 14632, 14633, 14634, 14635, 14636, 14637, 14638, 14639, 14640, 14641, 14642, 14643, 14644, 14645, 14646, 14647, 14648, 14649, 14650, 14651, 14652, 14653, 14654, 14655, 14656, 14657, 14658, 14659, 14660, 14661, 14662, 14663, 14664, 14665, 14666, 14667, 14668, 14669, 14670, 14671, 14672, 14673, 14674, 14675, 14676, 14677, 14678, 14679, 14680, 14681, 14682, 14683, 14684, 14685, 14686, 14687, 14688, 14689, 14690, 14691, 14692, 14693, 14694, 14695, 14696, 14697, 14698, 14699, 14700, 14701, 14702, 14703, 14704, 14705, 14706, 14707, 14708, 14709, 14710, 14711, 14712, 14713, 14714, 14715, 14716, 14717, 14718, 14719, 14720, 14721, 14722, 14723, 14724, 14725, 14726, 14727, 14728, 14729, 14730, 14731, 14732, 14733, 14734, 14735, 14736, 14737, 14738, 14739, 14740, 14741, 14742, 14743, 14744, 14745, 14746, 14747, 14748, 14749, 14750, 14751, 14752, 14753, 14754, 14755, 14756, 14757, 14758, 14759, 14760, 14761, 14762, 14763, 14764, 14765, 14766, 14767, 14768, 14769, 14770, 14771, 14772, 14773, 14774, 14775, 14776, 14777, 14778, 14779, 14780, 14781, 14782, 14783, 14784, 14785, 14786, 14787, 14788, 14789, 14790, 14791, 14792, 14793, 14794, 14795, 14796, 14797, 14798, 14799, 14800, 14801, 14802, 14803, 14804, 14805, 14806, 14807, 14808, 14809, 14810, 14811, 14812, 14813, 14814, 14815, 14816, 14817, 14818, 14819, 14820, 14821, 14822, 14823, 14824, 14825, 14826, 14827, 14828, 14829, 14830, 14831, 14832, 14833, 14834, 14835, 14836, 14837, 14838, 14839, 14840, 14841, 14842, 14843, 14844, 14845, 14846, 14847, 14848, 14849, 14850, 14851, 14852, 14853, 14854, 14855, 14856, 14857, 14858, 14859, 14860, 14861, 14862, 14863, 14864, 14865, 14866, 14867, 14868, 14869, 14870, 14871, 14872, 14873, 14874, 14875, 14876, 14877, 14878, 14879, 14880, 14881, 14882, 14883, 14884, 14885, 14886, 14887, 14888, 14889, 14890, 14891, 14892, 14893, 14894, 14895, 14896, 14897, 14898, 14899, 14900, 14901, 14902, 14903, 14904, 14905, 14906, 14907, 14908, 14909, 14910, 14911, 14912, 14913, 14914, 14915, 14916, 14917, 14918, 14919, 14920, 14921, 14922, 14923, 14924, 14925, 14926, 14927, 14928, 14929, 14930, 14931, 14932, 14933, 14934, 14935, 14936, 14937, 14938, 14939, 14940, 14941, 14942, 14943, 14944, 14945, 14946, 14947, 14948, 14949, 14950, 14951, 14952, 14953, 14954, 14955, 14956, 14957, 14958, 14959, 14960, 14961, 14962, 14963, 14964, 14965, 14966, 14967, 14968, 14969, 14970, 14971, 14972, 14973, 14974, 14975, 14976, 14977, 14978, 14979, 14980, 14981, 14982, 14983, 14984, 14985, 14986, 14987, 14988, 14989, 14990, 14991, 14992, 14993, 14994, 14995, 14996, 14997, 14998, 14999, 15000, 15001, 15002, 15003, 15004, 15005, 15006, 15007, 15008, 15009, 15010, 15011, 15012, 15013, 15014, 15015, 15016, 15017, 15018, 15019, 15020, 15021, 15022, 15023, 15024, 15025, 15026, 15027, 15028, 15029, 15030, 15031, 15032, 15033, 15034, 15035, 15036, 15037, 15038, 15039, 15040, 15041, 15042, 15043, 15044, 15045, 15046, 15047, 15048, 15049, 15050, 15051, 15052, 15053, 15054, 15055, 15056, 15057, 15058, 15059, 15060, 15061, 15062, 15063, 15064, 15065, 15066, 15067, 15068, 15069, 15070, 15071, 15072, 15073, 15074, 15075, 15076, 15077, 15078, 15079, 15080, 15081, 15082, 15083, 15084, 15085, 15086, 15087, 15088, 15089, 15090, 15091, 15092, 15093, 15094, 15095, 15096, 15097, 15098, 15099, 15100, 15101, 15102, 15103, 15104, 15105, 15106, 15107, 15108, 15109, 15110, 15111, 15112, 15113, 15114, 15115, 15116, 15117, 15118, 15119, 15120, 15121, 15122, 15123, 15124, 15125, 15126, 15127, 15128, 15129, 15130, 15131, 15132, 15133, 15134, 15135, 15136, 15137, 15138, 15139, 15140, 15141, 15142, 15143, 15144, 15145, 15146, 15147, 15148, 15149, 15150, 15151, 15152, 15153, 15154, 15155, 15156, 15157, 15158, 15159, 15160, 15161, 15162, 15163, 15164, 15165, 15166, 15167, 15168, 15169, 15170, 15171, 15172, 15173, 15174, 15175, 15176, 15177, 15178, 15179, 15180, 15181, 15182, 15183, 15184, 15185, 15186, 15187, 15188, 15189, 15190, 15191, 15192, 15193, 15194, 15195, 15196, 15197, 15198, 15199, 15200, 15201, 15202, 15203, 15204, 15205, 15206, 15207, 15208, 15209, 15210, 15211, 15212, 15213, 15214, 15215, 15216, 15217, 15218, 15219, 15220, 15221, 15222, 15223, 15224, 15225, 15226, 15227, 15228, 15229, 15230, 15231, 15232, 15233, 15234, 15235, 15236, 15237, 15238, 15239, 15240, 15241, 15242, 15243, 15244, 15245, 15246, 15247, 15248, 15249, 15250, 15251, 15252, 15253, 15254, 15255, 15256, 15257, 15258, 15259, 15260, 15261, 15262, 15263, 15264, 15265, 15266, 15267, 15268, 15269, 15270, 15271, 15272, 15273, 15274, 15275, 15276, 15277, 15278, 15279, 15280, 15281, 15282, 15283, 15284, 15285, 15286, 15287, 15288, 15289, 15290, 15291, 15292, 15293, 15294, 15295, 15296, 15297, 15298, 15299, 15300, 15301, 15302, 15303, 15304, 15305, 15306, 15307, 15308, 15309, 15310, 15311, 15312, 15313, 15314, 15315, 15316, 15317, 15318, 15319, 15320, 15321, 15322, 15323, 15324, 15325, 15326, 15327, 15328, 15329, 15330, 15331, 15332, 15333, 15334, 15335, 15336, 15337, 15338, 15339, 15340, 15341, 15342, 15343, 15344, 15345, 15346, 15347, 15348, 15349, 15350, 15351, 15352, 15353, 15354, 15355, 15356, 15357, 15358, 15359, 15360, 15361, 15362, 15363, 15364, 15365, 15366, 15367, 15368, 15369, 15370, 15371, 15372, 15373, 15374, 15375, 15376, 15377, 15378, 15379, 15380, 15381, 15382, 15383, 15384, 15385, 15386, 15387, 15388, 15389, 15390, 15391, 15392, 15393, 15394, 15395, 15396, 15397, 15398, 15399, 15400, 15401, 15402, 15403, 15404, 15405, 15406, 15407, 15408, 15409, 15410, 15411, 15412, 15413, 15414, 15415, 15416, 15417, 15418, 15419, 15420, 15421, 15422, 15423, 15424, 15425, 15426, 15427, 15428, 15429, 15430, 15431, 15432, 15433, 15434, 15435, 15436, 15437, 15438, 15439, 15440, 15441, 15442, 15443, 15444, 15445, 15446, 15447, 15448, 15449, 15450, 15451, 15452, 15453, 15454, 15455, 15456, 15457, 15458, 15459, 15460, 15461, 15462, 15463, 15464, 15465, 15466, 15467, 15468, 15469, 15470, 15471, 15472, 15473, 15474, 15475, 15476, 15477, 15478, 15479, 15480, 15481, 15482, 15483, 15484, 15485, 15486, 15487, 15488, 15489, 15490, 15491, 15492, 15493, 15494, 15495, 15496, 15497, 15498, 15499, 15500, 15501, 15502, 15503, 15504, 15505, 15506, 15507, 15508, 15509, 15510, 15511, 15512, 15513, 15514, 15515, 15516, 15517, 15518, 15519, 15520, 15521, 15522, 15523, 15524, 15525, 15526, 15527, 15528, 15529, 15530, 15531, 15532, 15533, 15534, 15535, 15536, 15537, 15538, 15539, 15540, 15541, 15542, 15543, 15544, 15545, 15546, 15547, 15548, 15549, 15550, 15551, 15552, 15553, 15554, 15555, 15556, 15557, 15558, 15559, 15560, 15561, 15562, 15563, 15564, 15565, 15566, 15567, 15568, 15569, 15570, 15571, 15572, 15573, 15574, 15575, 15576, 15577, 15578, 15579, 15580, 15581, 15582, 15583, 15584, 15585, 15586, 15587, 15588, 15589, 15590, 15591, 15592, 15593, 15594, 15595, 15596, 15597, 15598, 15599, 15600, 15601, 15602, 15603, 15604, 15605, 15606, 15607, 15608, 15609, 15610, 15611, 15612, 15613, 15614, 15615, 15616, 15617, 15618, 15619, 15620, 15621, 15622, 15623, 15624, 15625, 15626, 15627, 15628, 15629, 15630, 15631, 15632, 15633, 15634, 15635, 15636, 15637, 15638, 15639, 15640, 15641, 15642, 15643, 15644, 15645, 15646, 15647, 15648, 15649, 15650, 15651, 15652, 15653, 15654, 15655, 15656, 15657, 15658, 15659, 15660, 15661, 15662, 15663, 15664, 15665, 15666, 15667, 15668, 15669, 15670, 15671, 15672, 15673, 15674, 15675, 15676, 15677, 15678, 15679, 15680, 15681, 15682, 15683, 15684, 15685, 15686, 15687, 15688, 15689, 15690, 15691, 15692, 15693, 15694, 15695, 15696, 15697, 15698, 15699, 15700, 15701, 15702, 15703, 15704, 15705, 15706, 15707, 15708, 15709, 15710, 15711, 15712, 15713, 15714, 15715, 15716, 15717, 15718, 15719, 15720, 15721, 15722, 15723, 15724, 15725, 15726, 15727, 15728, 15729, 15730, 15731, 15732, 15733, 15734, 15735, 15736, 15737, 15738, 15739, 15740, 15741, 15742, 15743, 15744, 15745, 15746, 15747, 15748, 15749, 15750, 15751, 15752, 15753, 15754, 15755, 15756, 15757, 15758, 15759, 15760, 15761, 15762, 15763, 15764, 15765, 15766, 15767, 15768, 15769, 15770, 15771, 15772, 15773, 15774, 15775, 15776, 15777, 15778, 15779, 15780, 15781, 15782, 15783, 15784, 15785, 15786, 15787, 15788, 15789, 15790, 15791, 15792, 15793, 15794, 15795, 15796, 15797, 15798, 15799, 15800, 15801, 15802, 15803, 15804, 15805, 15806, 15807, 15808, 15809, 15810, 15811, 15812, 15813, 15814, 15815, 15816, 15817, 15818, 15819, 15820, 15821, 15822, 15823, 15824, 15825, 15826, 15827, 15828, 15829, 15830, 15831, 15832, 15833, 15834, 15835, 15836, 15837, 15838, 15839, 15840, 15841, 15842, 15843, 15844, 15845, 15846, 15847, 15848, 15849, 15850, 15851, 15852, 15853, 15854, 15855, 15856, 15857, 15858, 15859, 15860, 15861, 15862, 15863, 15864, 15865, 15866, 15867, 15868, 15869, 15870, 15871, 15872, 15873, 15874, 15875, 15876, 15877, 15878, 15879, 15880, 15881, 15882, 15883, 15884, 15885, 15886, 15887, 15888, 15889, 15890, 15891, 15892, 15893, 15894, 15895, 15896, 15897, 15898, 15899, 15900, 15901, 15902, 15903, 15904, 15905, 15906, 15907, 15908, 15909, 15910, 15911, 15912, 15913, 15914, 15915, 15916, 15917, 15918, 15919, 15920, 15921, 15922, 15923, 15924, 15925, 15926, 15927, 15928, 15929, 15930, 15931, 15932, 15933, 15934, 15935, 15936, 15937, 15938, 15939, 15940, 15941, 15942, 15943, 15944, 15945, 15946, 15947, 15948, 15949, 15950, 15951, 15952, 15953, 15954, 15955, 15956, 15957, 15958, 15959, 15960, 15961, 15962, 15963, 15964, 15965, 15966, 15967, 15968, 15969, 15970, 15971, 15972, 15973, 15974, 15975, 15976, 15977, 15978, 15979, 15980, 15981, 15982, 15983, 15984, 15985, 15986, 15987, 15988, 15989, 15990, 15991, 15992, 15993, 15994, 15995, 15996, 15997, 15998, 15999, 16000, 16001, 16002, 16003, 16004, 16005, 16006, 16007, 16008, 16009, 16010, 16011, 16012, 16013, 16014, 16015, 16016, 16017, 16018, 16019, 16020, 16021, 16022, 16023, 16024, 16025, 16026, 16027, 16028, 16029, 16030, 16031, 16032, 16033, 16034, 16035, 16036, 16037, 16038, 16039, 16040, 16041, 16042, 16043, 16044, 16045, 16046, 16047, 16048, 16049, 16050, 16051, 16052, 16053, 16054, 16055, 16056, 16057, 16058, 16059, 16060, 16061, 16062, 16063, 16064, 16065, 16066, 16067, 16068, 16069, 16070, 16071, 16072, 16073, 16074, 16075, 16076, 16077, 16078, 16079, 16080, 16081, 16082, 16083, 16084, 16085, 16086, 16087, 16088, 16089, 16090, 16091, 16092, 16093, 16094, 16095, 16096, 16097, 16098, 16099, 16100, 16101, 16102, 16103, 16104, 16105, 16106, 16107, 16108, 16109, 16110, 16111, 16112, 16113, 16114, 16115, 16116, 16117, 16118, 16119, 16120, 16121, 16122, 16123, 16124, 16125, 16126, 16127, 16128, 16129, 16130, 16131, 16132, 16133, 16134, 16135, 16136, 16137, 16138, 16139, 16140, 16141, 16142, 16143, 16144, 16145, 16146, 16147, 16148, 16149, 16150, 16151, 16152, 16153, 16154, 16155, 16156, 16157, 16158, 16159, 16160, 16161, 16162, 16163, 16164, 16165, 16166, 16167, 16168, 16169, 16170, 16171, 16172, 16173, 16174, 16175, 16176, 16177, 16178, 16179, 16180, 16181, 16182, 16183, 16184, 16185, 16186, 16187, 16188, 16189, 16190, 16191, 16192, 16193, 16194, 16195, 16196, 16197, 16198, 16199, 16200, 16201, 16202, 16203, 16204, 16205, 16206, 16207, 16208, 16209, 16210, 16211, 16212, 16213, 16214, 16215, 16216, 16217, 16218, 16219, 16220, 16221, 16222, 16223, 16224, 16225, 16226, 16227, 16228, 16229, 16230, 16231, 16232, 16233, 16234, 16235, 16236, 16237, 16238, 16239, 16240, 16241, 16242, 16243, 16244, 16245, 16246, 16247, 16248, 16249, 16250, 16251, 16252, 16253, 16254, 16255, 16256, 16257, 16258, 16259, 16260, 16261, 16262, 16263, 16264, 16265, 16266, 16267, 16268, 16269, 16270, 16271, 16272, 16273, 16274, 16275, 16276, 16277, 16278, 16279, 16280, 16281, 16282, 16283, 16284, 16285, 16286, 16287, 16288, 16289, 16290, 16291, 16292, 16293, 16294, 16295, 16296, 16297, 16298, 16299, 16300, 16301, 16302, 16303, 16304, 16305, 16306, 16307, 16308, 16309, 16310, 16311, 16312, 16313, 16314, 16315, 16316, 16317, 16318, 16319, 16320, 16321, 16322, 16323, 16324, 16325, 16326, 16327, 16328, 16329, 16330, 16331, 16332, 16333, 16334, 16335, 16336, 16337, 16338, 16339, 16340, 16341, 16342, 16343, 16344, 16345, 16346, 16347, 16348, 16349, 16350, 16351, 16352, 16353, 16354, 16355, 16356, 16357, 16358, 16359, 16360, 16361, 16362, 16363, 16364, 16365, 16366, 16367, 16368, 16369, 16370, 16371, 16372, 16373, 16374, 16375, 16376, 16377, 16378, 16379, 16380, 16381, 16382, 16383, 16384, 16385, 16386, 16387, 16388, 16389, 16390, 16391, 16392, 16393, 16394, 16395, 16396, 16397, 16398, 16399, 16400, 16401, 16402, 16403, 16404, 16405, 16406, 16407, 16408, 16409, 16410, 16411, 16412, 16413, 16414, 16415, 16416, 16417, 16418, 16419, 16420, 16421, 16422, 16423, 16424, 16425, 16426, 16427, 16428, 16429, 16430, 16431, 16432, 16433, 16434, 16435, 16436, 16437, 16438, 16439, 16440, 16441, 16442, 16443, 16444, 16445, 16446, 16447, 16448, 16449, 16450, 16451, 16452, 16453, 16454, 16455, 16456, 16457, 16458, 16459, 16460, 16461, 16462, 16463, 16464, 16465, 16466, 16467, 16468, 16469, 16470, 16471, 16472, 16473, 16474, 16475, 16476, 16477, 16478, 16479, 16480, 16481, 16482, 16483, 16484, 16485, 16486, 16487, 16488, 16489, 16490, 16491, 16492, 16493, 16494, 16495, 16496, 16497, 16498, 16499, 16500, 16501, 16502, 16503, 16504, 16505, 16506, 16507, 16508, 16509, 16510, 16511, 16512, 16513, 16514, 16515, 16516, 16517, 16518, 16519, 16520, 16521, 16522, 16523, 16524, 16525, 16526, 16527, 16528, 16529, 16530, 16531, 16532, 16533, 16534, 16535, 16536, 16537, 16538, 16539, 16540, 16541, 16542, 16543, 16544, 16545, 16546, 16547, 16548, 16549, 16550, 16551, 16552, 16553, 16554, 16555, 16556, 16557, 16558, 16559, 16560, 16561, 16562, 16563, 16564, 16565, 16566, 16567, 16568, 16569, 16570, 16571, 16572, 16573, 16574, 16575, 16576, 16577, 16578, 16579, 16580, 16581, 16582, 16583, 16584, 16585, 16586, 16587, 16588, 16589, 16590, 16591, 16592, 16593, 16594, 16595, 16596, 16597, 16598, 16599, 16600, 16601, 16602, 16603, 16604, 16605, 16606, 16607, 16608, 16609, 16610, 16611, 16612, 16613, 16614, 16615, 16616, 16617, 16618, 16619, 16620, 16621, 16622, 16623, 16624, 16625, 16626, 16627, 16628, 16629, 16630, 16631, 16632, 16633, 16634, 16635, 16636, 16637, 16638, 16639, 16640, 16641, 16642, 16643, 16644, 16645, 16646, 16647, 16648, 16649, 16650, 16651, 16652, 16653, 16654, 16655, 16656, 16657, 16658, 16659, 16660, 16661, 16662, 16663, 16664, 16665, 16666, 16667, 16668, 16669, 16670, 16671, 16672, 16673, 16674, 16675, 16676, 16677, 16678, 16679, 16680, 16681, 16682, 16683, 16684, 16685, 16686, 16687, 16688, 16689, 16690, 16691, 16692, 16693, 16694, 16695, 16696, 16697, 16698, 16699, 16700, 16701, 16702, 16703, 16704, 16705, 16706, 16707, 16708, 16709, 16710, 16711, 16712, 16713, 16714, 16715, 16716, 16717, 16718, 16719, 16720, 16721, 16722, 16723, 16724, 16725, 16726, 16727, 16728, 16729, 16730, 16731, 16732, 16733, 16734, 16735, 16736, 16737, 16738, 16739, 16740, 16741, 16742, 16743, 16744, 16745, 16746, 16747, 16748, 16749, 16750, 16751, 16752, 16753, 16754, 16755, 16756, 16757, 16758, 16759, 16760, 16761, 16762, 16763, 16764, 16765, 16766, 16767, 16768, 16769, 16770, 16771, 16772, 16773, 16774, 16775, 16776, 16777, 16778, 16779, 16780, 16781, 16782, 16783, 16784, 16785, 16786, 16787, 16788, 16789, 16790, 16791, 16792, 16793, 16794, 16795, 16796, 16797, 16798, 16799, 16800, 16801, 16802, 16803, 16804, 16805, 16806, 16807, 16808, 16809, 16810, 16811, 16812, 16813, 16814, 16815, 16816, 16817, 16818, 16819, 16820, 16821, 16822, 16823, 16824, 16825, 16826, 16827, 16828, 16829, 16830, 16831, 16832, 16833, 16834, 16835, 16836, 16837, 16838, 16839, 16840, 16841, 16842, 16843, 16844, 16845, 16846, 16847, 16848, 16849, 16850, 16851, 16852, 16853, 16854, 16855, 16856, 16857, 16858, 16859, 16860, 16861, 16862, 16863, 16864, 16865, 16866, 16867, 16868, 16869, 16870, 16871, 16872, 16873, 16874, 16875, 16876, 16877, 16878, 16879, 16880, 16881, 16882, 16883, 16884, 16885, 16886, 16887, 16888, 16889, 16890, 16891, 16892, 16893, 16894, 16895, 16896, 16897, 16898, 16899, 16900, 16901, 16902, 16903, 16904, 16905, 16906, 16907, 16908, 16909, 16910, 16911, 16912, 16913, 16914, 16915, 16916, 16917, 16918, 16919, 16920, 16921, 16922, 16923, 16924, 16925, 16926, 16927, 16928, 16929, 16930, 16931, 16932, 16933, 16934, 16935, 16936, 16937, 16938, 16939, 16940, 16941, 16942, 16943, 16944, 16945, 16946, 16947, 16948, 16949, 16950, 16951, 16952, 16953, 16954, 16955, 16956, 16957, 16958, 16959, 16960, 16961, 16962, 16963, 16964, 16965, 16966, 16967, 16968, 16969, 16970, 16971, 16972, 16973, 16974, 16975, 16976, 16977, 16978, 16979, 16980, 16981, 16982, 16983, 16984, 16985, 16986, 16987, 16988, 16989, 16990, 16991, 16992, 16993, 16994, 16995, 16996, 16997, 16998, 16999, 17000, 17001, 17002, 17003, 17004, 17005, 17006, 17007, 17008, 17009, 17010, 17011, 17012, 17013, 17014, 17015, 17016, 17017, 17018, 17019, 17020, 17021, 17022, 17023, 17024, 17025, 17026, 17027, 17028, 17029, 17030, 17031, 17032, 17033, 17034, 17035, 17036, 17037, 17038, 17039, 17040, 17041, 17042, 17043, 17044, 17045, 17046, 17047, 17048, 17049, 17050, 17051, 17052, 17053, 17054, 17055, 17056, 17057, 17058, 17059, 17060, 17061, 17062, 17063, 17064, 17065, 17066, 17067, 17068, 17069, 17070, 17071, 17072, 17073, 17074, 17075, 17076, 17077, 17078, 17079, 17080, 17081, 17082, 17083, 17084, 17085, 17086, 17087, 17088, 17089, 17090, 17091, 17092, 17093, 17094, 17095, 17096, 17097, 17098, 17099, 17100, 17101, 17102, 17103, 17104, 17105, 17106, 17107, 17108, 17109, 17110, 17111, 17112, 17113, 17114, 17115, 17116, 17117, 17118, 17119, 17120, 17121, 17122, 17123, 17124, 17125, 17126, 17127, 17128, 17129, 17130, 17131, 17132, 17133, 17134, 17135, 17136, 17137, 17138, 17139, 17140, 17141, 17142, 17143, 17144, 17145, 17146, 17147, 17148, 17149, 17150, 17151, 17152, 17153, 17154, 17155, 17156, 17157, 17158, 17159, 17160, 17161, 17162, 17163, 17164, 17165, 17166, 17167, 17168, 17169, 17170, 17171, 17172, 17173, 17174, 17175, 17176, 17177, 17178, 17179, 17180, 17181, 17182, 17183, 17184, 17185, 17186, 17187, 17188, 17189, 17190, 17191, 17192, 17193, 17194, 17195, 17196, 17197, 17198, 17199, 17200, 17201, 17202, 17203, 17204, 17205, 17206, 17207, 17208, 17209, 17210, 17211, 17212, 17213, 17214, 17215, 17216, 17217, 17218, 17219, 17220, 17221, 17222, 17223, 17224, 17225, 17226, 17227, 17228, 17229, 17230, 17231, 17232, 17233, 17234, 17235, 17236, 17237, 17238, 17239, 17240, 17241, 17242, 17243, 17244, 17245, 17246, 17247, 17248, 17249, 17250, 17251, 17252, 17253, 17254, 17255, 17256, 17257, 17258, 17259, 17260, 17261, 17262, 17263, 17264, 17265, 17266, 17267, 17268, 17269, 17270, 17271, 17272, 17273, 17274, 17275, 17276, 17277, 17278, 17279, 17280, 17281, 17282, 17283, 17284, 17285, 17286, 17287, 17288, 17289, 17290, 17291, 17292, 17293, 17294, 17295, 17296, 17297, 17298, 17299, 17300, 17301, 17302, 17303, 17304, 17305, 17306, 17307, 17308, 17309, 17310, 17311, 17312, 17313, 17314, 17315, 17316, 17317, 17318, 17319, 17320, 17321, 17322, 17323, 17324, 17325, 17326, 17327, 17328, 17329, 17330, 17331, 17332, 17333, 17334, 17335, 17336, 17337, 17338, 17339, 17340, 17341, 17342, 17343, 17344, 17345, 17346, 17347, 17348, 17349, 17350, 17351, 17352, 17353, 17354, 17355, 17356, 17357, 17358, 17359, 17360, 17361, 17362, 17363, 17364, 17365, 17366, 17367, 17368, 17369, 17370, 17371, 17372, 17373, 17374, 17375, 17376, 17377, 17378, 17379, 17380, 17381, 17382, 17383, 17384, 17385, 17386, 17387, 17388, 17389, 17390, 17391, 17392, 17393, 17394, 17395, 17396, 17397, 17398, 17399, 17400, 17401, 17402, 17403, 17404, 17405, 17406, 17407, 17408, 17409, 17410, 17411, 17412, 17413, 17414, 17415, 17416, 17417, 17418, 17419, 17420, 17421, 17422, 17423, 17424, 17425, 17426, 17427, 17428, 17429, 17430, 17431, 17432, 17433, 17434, 17435, 17436, 17437, 17438, 17439, 17440, 17441, 17442, 17443, 17444, 17445, 17446, 17447, 17448, 17449, 17450, 17451, 17452, 17453, 17454, 17455, 17456, 17457, 17458, 17459, 17460, 17461, 17462, 17463, 17464, 17465, 17466, 17467, 17468, 17469, 17470, 17471, 17472, 17473, 17474, 17475, 17476, 17477, 17478, 17479, 17480, 17481, 17482, 17483, 17484, 17485, 17486, 17487, 17488, 17489, 17490, 17491, 17492, 17493, 17494, 17495, 17496, 17497, 17498, 17499, 17500, 17501, 17502, 17503, 17504, 17505, 17506, 17507, 17508, 17509, 17510, 17511, 17512, 17513, 17514, 17515, 17516, 17517, 17518, 17519, 17520, 17521, 17522, 17523, 17524, 17525, 17526, 17527, 17528, 17529, 17530, 17531, 17532, 17533, 17534, 17535, 17536, 17537, 17538, 17539, 17540, 17541, 17542, 17543, 17544, 17545, 17546, 17547, 17548, 17549, 17550, 17551, 17552, 17553, 17554, 17555, 17556, 17557, 17558, 17559, 17560, 17561, 17562, 17563, 17564, 17565, 17566, 17567, 17568, 17569, 17570, 17571, 17572, 17573, 17574, 17575, 17576, 17577, 17578, 17579, 17580, 17581, 17582, 17583, 17584, 17585, 17586, 17587, 17588, 17589, 17590, 17591, 17592, 17593, 17594, 17595, 17596, 17597, 17598, 17599, 17600, 17601, 17602, 17603, 17604, 17605, 17606, 17607, 17608, 17609, 17610, 17611, 17612, 17613, 17614, 17615, 17616, 17617, 17618, 17619, 17620, 17621, 17622, 17623, 17624, 17625, 17626, 17627, 17628, 17629, 17630, 17631, 17632, 17633, 17634, 17635, 17636, 17637, 17638, 17639, 17640, 17641, 17642, 17643, 17644, 17645, 17646, 17647, 17648, 17649, 17650, 17651, 17652, 17653, 17654, 17655, 17656, 17657, 17658, 17659, 17660, 17661, 17662, 17663, 17664, 17665, 17666, 17667, 17668, 17669, 17670, 17671, 17672, 17673, 17674, 17675, 17676, 17677, 17678, 17679, 17680, 17681, 17682, 17683, 17684, 17685, 17686, 17687, 17688, 17689, 17690, 17691, 17692, 17693, 17694, 17695, 17696, 17697, 17698, 17699, 17700, 17701, 17702, 17703, 17704, 17705, 17706, 17707, 17708, 17709, 17710, 17711, 17712, 17713, 17714, 17715, 17716, 17717, 17718, 17719, 17720, 17721, 17722, 17723, 17724, 17725, 17726, 17727, 17728, 17729, 17730, 17731, 17732, 17733, 17734, 17735, 17736, 17737, 17738, 17739, 17740, 17741, 17742, 17743, 17744, 17745, 17746, 17747, 17748, 17749, 17750, 17751, 17752, 17753, 17754, 17755, 17756, 17757, 17758, 17759, 17760, 17761, 17762, 17763, 17764, 17765, 17766, 17767, 17768, 17769, 17770, 17771, 17772, 17773, 17774, 17775, 17776, 17777, 17778, 17779, 17780, 17781, 17782, 17783, 17784, 17785, 17786, 17787, 17788, 17789, 17790, 17791, 17792, 17793, 17794, 17795, 17796, 17797, 17798, 17799, 17800, 17801, 17802, 17803, 17804, 17805, 17806, 17807, 17808, 17809, 17810, 17811, 17812, 17813, 17814, 17815, 17816, 17817, 17818, 17819, 17820, 17821, 17822, 17823, 17824, 17825, 17826, 17827, 17828, 17829, 17830, 17831, 17832, 17833, 17834, 17835, 17836, 17837, 17838, 17839, 17840, 17841, 17842, 17843, 17844, 17845, 17846, 17847, 17848, 17849, 17850, 17851, 17852, 17853, 17854, 17855, 17856, 17857, 17858, 17859, 17860, 17861, 17862, 17863, 17864, 17865, 17866, 17867, 17868, 17869, 17870, 17871, 17872, 17873, 17874, 17875, 17876, 17877, 17878, 17879, 17880, 17881, 17882, 17883, 17884, 17885, 17886, 17887, 17888, 17889, 17890, 17891, 17892, 17893, 17894, 17895, 17896, 17897, 17898, 17899, 17900, 17901, 17902, 17903, 17904, 17905, 17906, 17907, 17908, 17909, 17910, 17911, 17912, 17913, 17914, 17915, 17916, 17917, 17918, 17919, 17920, 17921, 17922, 17923, 17924, 17925, 17926, 17927, 17928, 17929, 17930, 17931, 17932, 17933, 17934, 17935, 17936, 17937, 17938, 17939, 17940, 17941, 17942, 17943, 17944, 17945, 17946, 17947, 17948, 17949, 17950, 17951, 17952, 17953, 17954, 17955, 17956, 17957, 17958, 17959, 17960, 17961, 17962, 17963, 17964, 17965, 17966, 17967, 17968, 17969, 17970, 17971, 17972, 17973, 17974, 17975, 17976, 17977, 17978, 17979, 17980, 17981, 17982, 17983, 17984, 17985, 17986, 17987, 17988, 17989, 17990, 17991, 17992, 17993, 17994, 17995, 17996, 17997, 17998, 17999, 18000, 18001, 18002, 18003, 18004, 18005, 18006, 18007, 18008, 18009, 18010, 18011, 18012, 18013, 18014, 18015, 18016, 18017, 18018, 18019, 18020, 18021, 18022, 18023, 18024, 18025, 18026, 18027, 18028, 18029, 18030, 18031, 18032, 18033, 18034, 18035, 18036, 18037, 18038, 18039, 18040, 18041, 18042, 18043, 18044, 18045, 18046, 18047, 18048, 18049, 18050, 18051, 18052, 18053, 18054, 18055, 18056, 18057, 18058, 18059, 18060, 18061, 18062, 18063, 18064, 18065, 18066, 18067, 18068, 18069, 18070, 18071, 18072, 18073, 18074, 18075, 18076, 18077, 18078, 18079, 18080, 18081, 18082, 18083, 18084, 18085, 18086, 18087, 18088, 18089, 18090, 18091, 18092, 18093, 18094, 18095, 18096, 18097, 18098, 18099, 18100, 18101, 18102, 18103, 18104, 18105, 18106, 18107, 18108, 18109, 18110, 18111, 18112, 18113, 18114, 18115, 18116, 18117, 18118, 18119, 18120, 18121, 18122, 18123, 18124, 18125, 18126, 18127, 18128, 18129, 18130, 18131, 18132, 18133, 18134, 18135, 18136, 18137, 18138, 18139, 18140, 18141, 18142, 18143, 18144, 18145, 18146, 18147, 18148, 18149, 18150, 18151, 18152, 18153, 18154, 18155, 18156, 18157, 18158, 18159, 18160, 18161, 18162, 18163, 18164, 18165, 18166, 18167, 18168, 18169, 18170, 18171, 18172, 18173, 18174, 18175, 18176, 18177, 18178, 18179, 18180, 18181, 18182, 18183, 18184, 18185, 18186, 18187, 18188, 18189, 18190, 18191, 18192, 18193, 18194, 18195, 18196, 18197, 18198, 18199, 18200, 18201, 18202, 18203, 18204, 18205, 18206, 18207, 18208, 18209, 18210, 18211, 18212, 18213, 18214, 18215, 18216, 18217, 18218, 18219, 18220, 18221, 18222, 18223, 18224, 18225, 18226, 18227, 18228, 18229, 18230, 18231, 18232, 18233, 18234, 18235, 18236, 18237, 18238, 18239, 18240, 18241, 18242, 18243, 18244, 18245, 18246, 18247, 18248, 18249, 18250, 18251, 18252, 18253, 18254, 18255, 18256, 18257, 18258, 18259, 18260, 18261, 18262, 18263, 18264, 18265, 18266, 18267, 18268, 18269, 18270, 18271, 18272, 18273, 18274, 18275, 18276, 18277, 18278, 18279, 18280, 18281, 18282, 18283, 18284, 18285, 18286, 18287, 18288, 18289, 18290, 18291, 18292, 18293, 18294, 18295, 18296, 18297, 18298, 18299, 18300, 18301, 18302, 18303, 18304, 18305, 18306, 18307, 18308, 18309, 18310, 18311, 18312, 18313, 18314, 18315, 18316, 18317, 18318, 18319, 18320, 18321, 18322, 18323, 18324, 18325, 18326, 18327, 18328, 18329, 18330, 18331, 18332, 18333, 18334, 18335, 18336, 18337, 18338, 18339, 18340, 18341, 18342, 18343, 18344, 18345, 18346, 18347, 18348, 18349, 18350, 18351, 18352, 18353, 18354, 18355, 18356, 18357, 18358, 18359, 18360, 18361, 18362, 18363, 18364, 18365, 18366, 18367, 18368, 18369, 18370, 18371, 18372, 18373, 18374, 18375, 18376, 18377, 18378, 18379, 18380, 18381, 18382, 18383, 18384, 18385, 18386, 18387, 18388, 18389, 18390, 18391, 18392, 18393, 18394, 18395, 18396, 18397, 18398, 18399, 18400, 18401, 18402, 18403, 18404, 18405, 18406, 18407, 18408, 18409, 18410, 18411, 18412, 18413, 18414, 18415, 18416, 18417, 18418, 18419, 18420, 18421, 18422, 18423, 18424, 18425, 18426, 18427, 18428, 18429, 18430, 18431, 18432, 18433, 18434, 18435, 18436, 18437, 18438, 18439, 18440, 18441, 18442, 18443, 18444, 18445, 18446, 18447, 18448, 18449, 18450, 18451, 18452, 18453, 18454, 18455, 18456, 18457, 18458, 18459, 18460, 18461, 18462, 18463, 18464, 18465, 18466, 18467, 18468, 18469, 18470, 18471, 18472, 18473, 18474, 18475, 18476, 18477, 18478, 18479, 18480, 18481, 18482, 18483, 18484, 18485, 18486, 18487, 18488, 18489, 18490, 18491, 18492, 18493, 18494, 18495, 18496, 18497, 18498, 18499, 18500, 18501, 18502, 18503, 18504, 18505, 18506, 18507, 18508, 18509, 18510, 18511, 18512, 18513, 18514, 18515, 18516, 18517, 18518, 18519, 18520, 18521, 18522, 18523, 18524, 18525, 18526, 18527, 18528, 18529, 18530, 18531, 18532, 18533, 18534, 18535, 18536, 18537, 18538, 18539, 18540, 18541, 18542, 18543, 18544, 18545, 18546, 18547, 18548, 18549, 18550, 18551, 18552, 18553, 18554, 18555, 18556, 18557, 18558, 18559, 18560, 18561, 18562, 18563, 18564, 18565, 18566, 18567, 18568, 18569, 18570, 18571, 18572, 18573, 18574, 18575, 18576, 18577, 18578, 18579, 18580, 18581, 18582, 18583, 18584, 18585, 18586, 18587, 18588, 18589, 18590, 18591, 18592, 18593, 18594, 18595, 18596, 18597, 18598, 18599, 18600, 18601, 18602, 18603, 18604, 18605, 18606, 18607, 18608, 18609, 18610, 18611, 18612, 18613, 18614, 18615, 18616, 18617, 18618, 18619, 18620, 18621, 18622, 18623, 18624, 18625, 18626, 18627, 18628, 18629, 18630, 18631, 18632, 18633, 18634, 18635, 18636, 18637, 18638, 18639, 18640, 18641, 18642, 18643, 18644, 18645, 18646, 18647, 18648, 18649, 18650, 18651, 18652, 18653, 18654, 18655, 18656, 18657, 18658, 18659, 18660, 18661, 18662, 18663, 18664, 18665, 18666, 18667, 18668, 18669, 18670, 18671, 18672, 18673, 18674, 18675, 18676, 18677, 18678, 18679, 18680, 18681, 18682, 18683, 18684, 18685, 18686, 18687, 18688, 18689, 18690, 18691, 18692, 18693, 18694, 18695, 18696, 18697, 18698, 18699, 18700, 18701, 18702, 18703, 18704, 18705, 18706, 18707, 18708, 18709, 18710, 18711, 18712, 18713, 18714, 18715, 18716, 18717, 18718, 18719, 18720, 18721, 18722, 18723, 18724, 18725, 18726, 18727, 18728, 18729, 18730, 18731, 18732, 18733, 18734, 18735, 18736, 18737, 18738, 18739, 18740, 18741, 18742, 18743, 18744, 18745, 18746, 18747, 18748, 18749, 18750, 18751, 18752, 18753, 18754, 18755, 18756, 18757, 18758, 18759, 18760, 18761, 18762, 18763, 18764, 18765, 18766, 18767, 18768, 18769, 18770, 18771, 18772, 18773, 18774, 18775, 18776, 18777, 18778, 18779, 18780, 18781, 18782, 18783, 18784, 18785, 18786, 18787, 18788, 18789, 18790, 18791, 18792, 18793, 18794, 18795, 18796, 18797, 18798, 18799, 18800, 18801, 18802, 18803, 18804, 18805, 18806, 18807, 18808, 18809, 18810, 18811, 18812, 18813, 18814, 18815, 18816, 18817, 18818, 18819, 18820, 18821, 18822, 18823, 18824, 18825, 18826, 18827, 18828, 18829, 18830, 18831, 18832, 18833, 18834, 18835, 18836, 18837, 18838, 18839, 18840, 18841, 18842, 18843, 18844, 18845, 18846, 18847, 18848, 18849, 18850, 18851, 18852, 18853, 18854, 18855, 18856, 18857, 18858, 18859, 18860, 18861, 18862, 18863, 18864, 18865, 18866, 18867, 18868, 18869, 18870, 18871, 18872, 18873, 18874, 18875, 18876, 18877, 18878, 18879, 18880, 18881, 18882, 18883, 18884, 18885, 18886, 18887, 18888, 18889, 18890, 18891, 18892, 18893, 18894, 18895, 18896, 18897, 18898, 18899, 18900, 18901, 18902, 18903, 18904, 18905, 18906, 18907, 18908, 18909, 18910, 18911, 18912, 18913, 18914, 18915, 18916, 18917, 18918, 18919, 18920, 18921, 18922, 18923, 18924, 18925, 18926, 18927, 18928, 18929, 18930, 18931, 18932, 18933, 18934, 18935, 18936, 18937, 18938, 18939, 18940, 18941, 18942, 18943, 18944, 18945, 18946, 18947, 18948, 18949, 18950, 18951, 18952, 18953, 18954, 18955, 18956, 18957, 18958, 18959, 18960, 18961, 18962, 18963, 18964, 18965, 18966, 18967, 18968, 18969, 18970, 18971, 18972, 18973, 18974, 18975, 18976, 18977, 18978, 18979, 18980, 18981, 18982, 18983, 18984, 18985, 18986, 18987, 18988, 18989, 18990, 18991, 18992, 18993, 18994, 18995, 18996, 18997, 18998, 18999, 19000, 19001, 19002, 19003, 19004, 19005, 19006, 19007, 19008, 19009, 19010, 19011, 19012, 19013, 19014, 19015, 19016, 19017, 19018, 19019, 19020, 19021, 19022, 19023, 19024, 19025, 19026, 19027, 19028, 19029, 19030, 19031, 19032, 19033, 19034, 19035, 19036, 19037, 19038, 19039, 19040, 19041, 19042, 19043, 19044, 19045, 19046, 19047, 19048, 19049, 19050, 19051, 19052, 19053, 19054, 19055, 19056, 19057, 19058, 19059, 19060, 19061, 19062, 19063, 19064, 19065, 19066, 19067, 19068, 19069, 19070, 19071, 19072, 19073, 19074, 19075, 19076, 19077, 19078, 19079, 19080, 19081, 19082, 19083, 19084, 19085, 19086, 19087, 19088, 19089, 19090, 19091, 19092, 19093, 19094, 19095, 19096, 19097, 19098, 19099, 19100, 19101, 19102, 19103, 19104, 19105, 19106, 19107, 19108, 19109, 19110, 19111, 19112, 19113, 19114, 19115, 19116, 19117, 19118, 19119, 19120, 19121, 19122, 19123, 19124, 19125, 19126, 19127, 19128, 19129, 19130, 19131, 19132, 19133, 19134, 19135, 19136, 19137, 19138, 19139, 19140, 19141, 19142, 19143, 19144, 19145, 19146, 19147, 19148, 19149, 19150, 19151, 19152, 19153, 19154, 19155, 19156, 19157, 19158, 19159, 19160, 19161, 19162, 19163, 19164, 19165, 19166, 19167, 19168, 19169, 19170, 19171, 19172, 19173, 19174, 19175, 19176, 19177, 19178, 19179, 19180, 19181, 19182, 19183, 19184, 19185, 19186, 19187, 19188, 19189, 19190, 19191, 19192, 19193, 19194, 19195, 19196, 19197, 19198, 19199, 19200, 19201, 19202, 19203, 19204, 19205, 19206, 19207, 19208, 19209, 19210, 19211, 19212, 19213, 19214, 19215, 19216, 19217, 19218, 19219, 19220, 19221, 19222, 19223, 19224, 19225, 19226, 19227, 19228, 19229, 19230, 19231, 19232, 19233, 19234, 19235, 19236, 19237, 19238, 19239, 19240, 19241, 19242, 19243, 19244, 19245, 19246, 19247, 19248, 19249, 19250, 19251, 19252, 19253, 19254, 19255, 19256, 19257, 19258, 19259, 19260, 19261, 19262, 19263, 19264, 19265, 19266, 19267, 19268, 19269, 19270, 19271, 19272, 19273, 19274, 19275, 19276, 19277, 19278, 19279, 19280, 19281, 19282, 19283, 19284, 19285, 19286, 19287, 19288, 19289, 19290, 19291, 19292, 19293, 19294, 19295, 19296, 19297, 19298, 19299, 19300, 19301, 19302, 19303, 19304, 19305, 19306, 19307, 19308, 19309, 19310, 19311, 19312, 19313, 19314, 19315, 19316, 19317, 19318, 19319, 19320, 19321, 19322, 19323, 19324, 19325, 19326, 19327, 19328, 19329, 19330, 19331, 19332, 19333, 19334, 19335, 19336, 19337, 19338, 19339, 19340, 19341, 19342, 19343, 19344, 19345, 19346, 19347, 19348, 19349, 19350, 19351, 19352, 19353, 19354, 19355, 19356, 19357, 19358, 19359, 19360, 19361, 19362, 19363, 19364, 19365, 19366, 19367, 19368, 19369, 19370, 19371, 19372, 19373, 19374, 19375, 19376, 19377, 19378, 19379, 19380, 19381, 19382, 19383, 19384, 19385, 19386, 19387, 19388, 19389, 19390, 19391, 19392, 19393, 19394, 19395, 19396, 19397, 19398, 19399, 19400, 19401, 19402, 19403, 19404, 19405, 19406, 19407, 19408, 19409, 19410, 19411, 19412, 19413, 19414, 19415, 19416, 19417, 19418, 19419, 19420, 19421, 19422, 19423, 19424, 19425, 19426, 19427, 19428, 19429, 19430, 19431, 19432, 19433, 19434, 19435, 19436, 19437, 19438, 19439, 19440, 19441, 19442, 19443, 19444, 19445, 19446, 19447, 19448, 19449, 19450, 19451, 19452, 19453, 19454, 19455, 19456, 19457, 19458, 19459, 19460, 19461, 19462, 19463, 19464, 19465, 19466, 19467, 19468, 19469, 19470, 19471, 19472, 19473, 19474, 19475, 19476, 19477, 19478, 19479, 19480, 19481, 19482, 19483, 19484, 19485, 19486, 19487, 19488, 19489, 19490, 19491, 19492, 19493, 19494, 19495, 19496, 19497, 19498, 19499, 19500, 19501, 19502, 19503, 19504, 19505, 19506, 19507, 19508, 19509, 19510, 19511, 19512, 19513, 19514, 19515, 19516, 19517, 19518, 19519, 19520, 19521, 19522, 19523, 19524, 19525, 19526, 19527, 19528, 19529, 19530, 19531, 19532, 19533, 19534, 19535, 19536, 19537, 19538, 19539, 19540, 19541, 19542, 19543, 19544, 19545, 19546, 19547, 19548, 19549, 19550, 19551, 19552, 19553, 19554, 19555, 19556, 19557, 19558, 19559, 19560, 19561, 19562, 19563, 19564, 19565, 19566, 19567, 19568, 19569, 19570, 19571, 19572, 19573, 19574, 19575, 19576, 19577, 19578, 19579, 19580, 19581, 19582, 19583, 19584, 19585, 19586, 19587, 19588, 19589, 19590, 19591, 19592, 19593, 19594, 19595, 19596, 19597, 19598, 19599, 19600, 19601, 19602, 19603, 19604, 19605, 19606, 19607, 19608, 19609, 19610, 19611, 19612, 19613, 19614, 19615, 19616, 19617, 19618, 19619, 19620, 19621, 19622, 19623, 19624, 19625, 19626, 19627, 19628, 19629, 19630, 19631, 19632, 19633, 19634, 19635, 19636, 19637, 19638, 19639, 19640, 19641, 19642, 19643, 19644, 19645, 19646, 19647, 19648, 19649, 19650, 19651, 19652, 19653, 19654, 19655, 19656, 19657, 19658, 19659, 19660, 19661, 19662, 19663, 19664, 19665, 19666, 19667, 19668, 19669, 19670, 19671, 19672, 19673, 19674, 19675, 19676, 19677, 19678, 19679, 19680, 19681, 19682, 19683, 19684, 19685, 19686, 19687, 19688, 19689, 19690, 19691, 19692, 19693, 19694, 19695, 19696, 19697, 19698, 19699, 19700, 19701, 19702, 19703, 19704, 19705, 19706, 19707, 19708, 19709, 19710, 19711, 19712, 19713, 19714, 19715, 19716, 19717, 19718, 19719, 19720, 19721, 19722, 19723, 19724, 19725, 19726, 19727, 19728, 19729, 19730, 19731, 19732, 19733, 19734, 19735, 19736, 19737, 19738, 19739, 19740, 19741, 19742, 19743, 19744, 19745, 19746, 19747, 19748, 19749, 19750, 19751, 19752, 19753, 19754, 19755, 19756, 19757, 19758, 19759, 19760, 19761, 19762, 19763, 19764, 19765, 19766, 19767, 19768, 19769, 19770, 19771, 19772, 19773, 19774, 19775, 19776, 19777, 19778, 19779, 19780, 19781, 19782, 19783, 19784, 19785, 19786, 19787, 19788, 19789, 19790, 19791, 19792, 19793, 19794, 19795, 19796, 19797, 19798, 19799, 19800, 19801, 19802, 19803, 19804, 19805, 19806, 19807, 19808, 19809, 19810, 19811, 19812, 19813, 19814, 19815, 19816, 19817, 19818, 19819, 19820, 19821, 19822, 19823, 19824, 19825, 19826, 19827, 19828, 19829, 19830, 19831, 19832, 19833, 19834, 19835, 19836, 19837, 19838, 19839, 19840, 19841, 19842, 19843, 19844, 19845, 19846, 19847, 19848, 19849, 19850, 19851, 19852, 19853, 19854, 19855, 19856, 19857, 19858, 19859, 19860, 19861, 19862, 19863, 19864, 19865, 19866, 19867, 19868, 19869, 19870, 19871, 19872, 19873, 19874, 19875, 19876, 19877, 19878, 19879, 19880, 19881, 19882, 19883, 19884, 19885, 19886, 19887, 19888, 19889, 19890, 19891, 19892, 19893, 19894, 19895, 19896, 19897, 19898, 19899, 19900, 19901, 19902, 19903, 19904, 19905, 19906, 19907, 19908, 19909, 19910, 19911, 19912, 19913, 19914, 19915, 19916, 19917, 19918, 19919, 19920, 19921, 19922, 19923, 19924, 19925, 19926, 19927, 19928, 19929, 19930, 19931, 19932, 19933, 19934, 19935, 19936, 19937, 19938, 19939, 19940, 19941, 19942, 19943, 19944, 19945, 19946, 19947, 19948, 19949, 19950, 19951, 19952, 19953, 19954, 19955, 19956, 19957, 19958, 19959, 19960, 19961, 19962, 19963, 19964, 19965, 19966, 19967, 19968, 19969, 19970, 19971, 19972, 19973, 19974, 19975, 19976, 19977, 19978, 19979, 19980, 19981, 19982, 19983, 19984, 19985, 19986, 19987, 19988, 19989, 19990, 19991, 19992, 19993, 19994, 19995, 19996, 19997, 19998, 19999, 20000, 20001, 20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012, 20013, 20014, 20015, 20016, 20017, 20018, 20019, 20020, 20021, 20022, 20023, 20024, 20025, 20026, 20027, 20028, 20029, 20030, 20031, 20032, 20033, 20034, 20035, 20036, 20037, 20038, 20039, 20040, 20041, 20042, 20043, 20044, 20045, 20046, 20047, 20048, 20049, 20050, 20051, 20052, 20053, 20054, 20055, 20056, 20057, 20058, 20059, 20060, 20061, 20062, 20063, 20064, 20065, 20066, 20067, 20068, 20069, 20070, 20071, 20072, 20073, 20074, 20075, 20076, 20077, 20078, 20079, 20080, 20081, 20082, 20083, 20084, 20085, 20086, 20087, 20088, 20089, 20090, 20091, 20092, 20093, 20094, 20095, 20096, 20097, 20098, 20099, 20100, 20101, 20102, 20103, 20104, 20105, 20106, 20107, 20108, 20109, 20110, 20111, 20112, 20113, 20114, 20115, 20116, 20117, 20118, 20119, 20120, 20121, 20122, 20123, 20124, 20125, 20126, 20127, 20128, 20129, 20130, 20131, 20132, 20133, 20134, 20135, 20136, 20137, 20138, 20139, 20140, 20141, 20142, 20143, 20144, 20145, 20146, 20147, 20148, 20149, 20150, 20151, 20152, 20153, 20154, 20155, 20156, 20157, 20158, 20159, 20160, 20161, 20162, 20163, 20164, 20165, 20166, 20167, 20168, 20169, 20170, 20171, 20172, 20173, 20174, 20175, 20176, 20177, 20178, 20179, 20180, 20181, 20182, 20183, 20184, 20185, 20186, 20187, 20188, 20189, 20190, 20191, 20192, 20193, 20194, 20195, 20196, 20197, 20198, 20199, 20200, 20201, 20202, 20203, 20204, 20205, 20206, 20207, 20208, 20209, 20210, 20211, 20212, 20213, 20214, 20215, 20216, 20217, 20218, 20219, 20220, 20221, 20222, 20223, 20224, 20225, 20226, 20227, 20228, 20229, 20230, 20231, 20232, 20233, 20234, 20235, 20236, 20237, 20238, 20239, 20240, 20241, 20242, 20243, 20244, 20245, 20246, 20247, 20248, 20249, 20250, 20251, 20252, 20253, 20254, 20255, 20256, 20257, 20258, 20259, 20260, 20261, 20262, 20263, 20264, 20265, 20266, 20267, 20268, 20269, 20270, 20271, 20272, 20273, 20274, 20275, 20276, 20277, 20278, 20279, 20280, 20281, 20282, 20283, 20284, 20285, 20286, 20287, 20288, 20289, 20290, 20291, 20292, 20293, 20294, 20295, 20296, 20297, 20298, 20299, 20300, 20301, 20302, 20303, 20304, 20305, 20306, 20307, 20308, 20309, 20310, 20311, 20312, 20313, 20314, 20315, 20316, 20317, 20318, 20319, 20320, 20321, 20322, 20323, 20324, 20325, 20326, 20327, 20328, 20329, 20330, 20331, 20332, 20333, 20334, 20335, 20336, 20337, 20338, 20339, 20340, 20341, 20342, 20343, 20344, 20345, 20346, 20347, 20348, 20349, 20350, 20351, 20352, 20353, 20354, 20355, 20356, 20357, 20358, 20359, 20360, 20361, 20362, 20363, 20364, 20365, 20366, 20367, 20368, 20369, 20370, 20371, 20372, 20373, 20374, 20375, 20376, 20377, 20378, 20379, 20380, 20381, 20382, 20383, 20384, 20385, 20386, 20387, 20388, 20389, 20390, 20391, 20392, 20393, 20394, 20395, 20396, 20397]\n"," This is the range of val:  [ 7700  7701  7702 ... 12097 12098 12099]\n","Starting training\n","shuffling\n","Epoch 1/350\n","2020-06-10 23:34:52.387675: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-10 23:34:52.607798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","500/500 [==============================] - 35s 70ms/step - loss: 30.6163 - val_loss: 385.8664\n","\n","Epoch 00001: loss improved from inf to 30.61551, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/350\n","500/500 [==============================] - 31s 62ms/step - loss: 13.6733 - val_loss: 213.6159\n","\n","Epoch 00002: loss improved from 30.61551 to 13.67366, saving model to final.h5\n","Epoch 3/350\n","500/500 [==============================] - 31s 62ms/step - loss: 10.6374 - val_loss: 520.2335\n","\n","Epoch 00003: loss improved from 13.67366 to 10.63721, saving model to final.h5\n","Epoch 4/350\n","500/500 [==============================] - 31s 62ms/step - loss: 9.1605 - val_loss: 65.6903\n","\n","Epoch 00004: loss improved from 10.63721 to 9.16025, saving model to final.h5\n","Epoch 5/350\n","500/500 [==============================] - 31s 62ms/step - loss: 8.2893 - val_loss: 61.0313\n","\n","Epoch 00005: loss improved from 9.16025 to 8.28952, saving model to final.h5\n","Epoch 6/350\n","500/500 [==============================] - 31s 62ms/step - loss: 7.6590 - val_loss: 26.8418\n","\n","Epoch 00006: loss improved from 8.28952 to 7.65895, saving model to final.h5\n","Epoch 7/350\n","500/500 [==============================] - 31s 62ms/step - loss: 7.0174 - val_loss: 24.9527\n","\n","Epoch 00007: loss improved from 7.65895 to 7.01765, saving model to final.h5\n","Epoch 8/350\n","500/500 [==============================] - 31s 62ms/step - loss: 6.7408 - val_loss: 27.9341\n","\n","Epoch 00008: loss improved from 7.01765 to 6.74093, saving model to final.h5\n","Epoch 9/350\n","500/500 [==============================] - 31s 62ms/step - loss: 6.5059 - val_loss: 16.0532\n","\n","Epoch 00009: loss improved from 6.74093 to 6.50624, saving model to final.h5\n","Epoch 10/350\n","500/500 [==============================] - 31s 62ms/step - loss: 6.3509 - val_loss: 48.4342\n","\n","Epoch 00010: loss improved from 6.50624 to 6.35083, saving model to final.h5\n","Epoch 11/350\n","500/500 [==============================] - 31s 62ms/step - loss: 6.0553 - val_loss: 21.7715\n","\n","Epoch 00011: loss improved from 6.35083 to 6.05547, saving model to final.h5\n","Epoch 12/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.7705 - val_loss: 393.8835\n","\n","Epoch 00012: loss improved from 6.05547 to 5.77074, saving model to final.h5\n","Epoch 13/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.7193 - val_loss: 351.7117\n","\n","Epoch 00013: loss improved from 5.77074 to 5.71946, saving model to final.h5\n","Epoch 14/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.3872 - val_loss: 509.5118\n","\n","Epoch 00014: loss improved from 5.71946 to 5.38714, saving model to final.h5\n","Epoch 15/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.3701 - val_loss: 12.5426\n","\n","Epoch 00015: loss improved from 5.38714 to 5.37011, saving model to final.h5\n","Epoch 16/350\n","500/500 [==============================] - 31s 63ms/step - loss: 5.2983 - val_loss: 72.4716\n","\n","Epoch 00016: loss improved from 5.37011 to 5.29824, saving model to final.h5\n","Epoch 17/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.1294 - val_loss: 148.6443\n","\n","Epoch 00017: loss improved from 5.29824 to 5.12976, saving model to final.h5\n","Epoch 18/350\n","500/500 [==============================] - 31s 62ms/step - loss: 5.1137 - val_loss: 660.0217\n","\n","Epoch 00018: loss improved from 5.12976 to 5.11423, saving model to final.h5\n","Epoch 19/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.8929 - val_loss: 213.9962\n","\n","Epoch 00019: loss improved from 5.11423 to 4.89296, saving model to final.h5\n","Epoch 20/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7992 - val_loss: 61.7067\n","\n","Epoch 00020: loss improved from 4.89296 to 4.79921, saving model to final.h5\n","Epoch 21/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7637 - val_loss: 25.3192\n","\n","Epoch 00021: loss improved from 4.79921 to 4.76358, saving model to final.h5\n","Epoch 22/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7752 - val_loss: 25.8723\n","\n","Epoch 00022: loss did not improve from 4.76358\n","Epoch 23/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.6908 - val_loss: 29.7741\n","\n","Epoch 00023: loss improved from 4.76358 to 4.69078, saving model to final.h5\n","Epoch 24/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.7678 - val_loss: 66.6710\n","\n","Epoch 00024: loss did not improve from 4.69078\n","Epoch 25/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.4907 - val_loss: 25.8079\n","\n","Epoch 00025: loss improved from 4.69078 to 4.49072, saving model to final.h5\n","Epoch 26/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.5572 - val_loss: 21.6388\n","\n","Epoch 00026: loss did not improve from 4.49072\n","Epoch 27/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3635 - val_loss: 34.7162\n","\n","Epoch 00027: loss improved from 4.49072 to 4.36363, saving model to final.h5\n","Epoch 28/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.4023 - val_loss: 62.6879\n","\n","Epoch 00028: loss did not improve from 4.36363\n","Epoch 29/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.4147 - val_loss: 79.8263\n","\n","Epoch 00029: loss did not improve from 4.36363\n","Epoch 30/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.4440 - val_loss: 127.9261\n","\n","Epoch 00030: loss did not improve from 4.36363\n","Epoch 31/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2569 - val_loss: 116.2968\n","\n","Epoch 00031: loss improved from 4.36363 to 4.25652, saving model to final.h5\n","Epoch 32/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.1721 - val_loss: 218.8879\n","\n","Epoch 00032: loss improved from 4.25652 to 4.17176, saving model to final.h5\n","Epoch 33/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.2415 - val_loss: 346.2571\n","\n","Epoch 00033: loss did not improve from 4.17176\n","Epoch 34/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.1836 - val_loss: 60.8345\n","\n","Epoch 00034: loss did not improve from 4.17176\n","Epoch 35/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.0731 - val_loss: 47.4973\n","\n","Epoch 00035: loss improved from 4.17176 to 4.07319, saving model to final.h5\n","Epoch 36/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.1422 - val_loss: 164.5663\n","\n","Epoch 00036: loss did not improve from 4.07319\n","Epoch 37/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.0922 - val_loss: 90.1153\n","\n","Epoch 00037: loss did not improve from 4.07319\n","Epoch 38/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.0272 - val_loss: 80.9217\n","\n","Epoch 00038: loss improved from 4.07319 to 4.02732, saving model to final.h5\n","Epoch 39/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.0071 - val_loss: 215.8610\n","\n","Epoch 00039: loss improved from 4.02732 to 4.00685, saving model to final.h5\n","Epoch 40/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.0849 - val_loss: 253.0431\n","\n","Epoch 00040: loss did not improve from 4.00685\n","Epoch 41/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.9678 - val_loss: 238.6864\n","\n","Epoch 00041: loss improved from 4.00685 to 3.96781, saving model to final.h5\n","Epoch 42/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.8936 - val_loss: 819.1196\n","\n","Epoch 00042: loss improved from 3.96781 to 3.89349, saving model to final.h5\n","Epoch 43/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.9860 - val_loss: 214.6047\n","\n","Epoch 00043: loss did not improve from 3.89349\n","Epoch 44/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.9317 - val_loss: 28.7560\n","\n","Epoch 00044: loss did not improve from 3.89349\n","Epoch 45/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.8297 - val_loss: 180.3714\n","\n","Epoch 00045: loss improved from 3.89349 to 3.82970, saving model to final.h5\n","Epoch 46/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.8459 - val_loss: 268.3993\n","\n","Epoch 00046: loss did not improve from 3.82970\n","Epoch 47/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.8295 - val_loss: 194.4122\n","\n","Epoch 00047: loss improved from 3.82970 to 3.82952, saving model to final.h5\n","Epoch 48/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.6895 - val_loss: 93.4355\n","\n","Epoch 00048: loss improved from 3.82952 to 3.68939, saving model to final.h5\n","Epoch 49/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7973 - val_loss: 259.3030\n","\n","Epoch 00049: loss did not improve from 3.68939\n","Epoch 50/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.8583 - val_loss: 16.4741\n","\n","Epoch 00050: loss did not improve from 3.68939\n","Epoch 51/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7992 - val_loss: 320.2598\n","\n","Epoch 00051: loss did not improve from 3.68939\n","Epoch 52/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7421 - val_loss: 148.7729\n","\n","Epoch 00052: loss did not improve from 3.68939\n","Epoch 53/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.7240 - val_loss: 8.7073\n","\n","Epoch 00053: loss did not improve from 3.68939\n","Epoch 54/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5397 - val_loss: 206.2147\n","\n","Epoch 00054: loss improved from 3.68939 to 3.53984, saving model to final.h5\n","Epoch 55/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5691 - val_loss: 0.8684\n","\n","Epoch 00055: loss did not improve from 3.53984\n","Epoch 56/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.6175 - val_loss: 197.3524\n","\n","Epoch 00056: loss did not improve from 3.53984\n","Epoch 57/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5738 - val_loss: 5.2771\n","\n","Epoch 00057: loss did not improve from 3.53984\n","Epoch 58/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.6762 - val_loss: 2.8477\n","\n","Epoch 00058: loss did not improve from 3.53984\n","Epoch 59/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5948 - val_loss: 7.8737\n","\n","Epoch 00059: loss did not improve from 3.53984\n","Epoch 60/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5970 - val_loss: 25.1818\n","\n","Epoch 00060: loss did not improve from 3.53984\n","Epoch 61/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5948 - val_loss: 1.6762\n","\n","Epoch 00061: loss did not improve from 3.53984\n","Epoch 62/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5337 - val_loss: 7.5650\n","\n","Epoch 00062: loss improved from 3.53984 to 3.53377, saving model to final.h5\n","Epoch 63/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5386 - val_loss: 2.3092\n","\n","Epoch 00063: loss did not improve from 3.53377\n","Epoch 64/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5367 - val_loss: 2.9430\n","\n","Epoch 00064: loss did not improve from 3.53377\n","Epoch 65/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.4822 - val_loss: 2.6658\n","\n","Epoch 00065: loss improved from 3.53377 to 3.48245, saving model to final.h5\n","Epoch 66/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.5756 - val_loss: 14.1218\n","\n","Epoch 00066: loss did not improve from 3.48245\n","Epoch 67/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5212 - val_loss: 2.1589\n","\n","Epoch 00067: loss did not improve from 3.48245\n","Epoch 68/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.4282 - val_loss: 33.2452\n","\n","Epoch 00068: loss improved from 3.48245 to 3.42809, saving model to final.h5\n","Epoch 69/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5260 - val_loss: 19.3049\n","\n","Epoch 00069: loss did not improve from 3.42809\n","Epoch 70/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.4241 - val_loss: 30.6218\n","\n","Epoch 00070: loss improved from 3.42809 to 3.42422, saving model to final.h5\n","Epoch 71/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.4808 - val_loss: 1.7851\n","\n","Epoch 00071: loss did not improve from 3.42422\n","Epoch 72/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.4606 - val_loss: 71.4243\n","\n","Epoch 00072: loss did not improve from 3.42422\n","Epoch 73/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.4296 - val_loss: 33.6015\n","\n","Epoch 00073: loss did not improve from 3.42422\n","Epoch 74/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.4818 - val_loss: 19.9309\n","\n","Epoch 00074: loss did not improve from 3.42422\n","Epoch 75/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.3910 - val_loss: 36.2876\n","\n","Epoch 00075: loss improved from 3.42422 to 3.39107, saving model to final.h5\n","Epoch 76/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.3981 - val_loss: 170.1161\n","\n","Epoch 00076: loss did not improve from 3.39107\n","Epoch 77/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.4117 - val_loss: 196.7591\n","\n","Epoch 00077: loss did not improve from 3.39107\n","Epoch 78/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.4148 - val_loss: 40.2450\n","\n","Epoch 00078: loss did not improve from 3.39107\n","Epoch 79/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.4145 - val_loss: 9.1876\n","\n","Epoch 00079: loss did not improve from 3.39107\n","Epoch 80/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.3688 - val_loss: 12.9495\n","\n","Epoch 00080: loss improved from 3.39107 to 3.36877, saving model to final.h5\n","Epoch 81/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.3797 - val_loss: 74.0474\n","\n","Epoch 00081: loss did not improve from 3.36877\n","Epoch 82/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.3679 - val_loss: 159.7496\n","\n","Epoch 00082: loss improved from 3.36877 to 3.36789, saving model to final.h5\n","Epoch 83/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.3697 - val_loss: 6.2638\n","\n","Epoch 00083: loss did not improve from 3.36789\n","Epoch 84/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.3771 - val_loss: 8.0243\n","\n","Epoch 00084: loss did not improve from 3.36789\n","Epoch 85/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.3513 - val_loss: 8.0025\n","\n","Epoch 00085: loss improved from 3.36789 to 3.35136, saving model to final.h5\n","Epoch 86/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.4352 - val_loss: 12.9413\n","\n","Epoch 00086: loss did not improve from 3.35136\n","Epoch 87/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2908 - val_loss: 3.6328\n","\n","Epoch 00087: loss improved from 3.35136 to 3.29104, saving model to final.h5\n","Epoch 88/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2832 - val_loss: 5.9043\n","\n","Epoch 00088: loss improved from 3.29104 to 3.28303, saving model to final.h5\n","Epoch 89/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2573 - val_loss: 16.0413\n","\n","Epoch 00089: loss improved from 3.28303 to 3.25712, saving model to final.h5\n","Epoch 90/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.3128 - val_loss: 6.6720\n","\n","Epoch 00090: loss did not improve from 3.25712\n","Epoch 91/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2558 - val_loss: 1.0017\n","\n","Epoch 00091: loss improved from 3.25712 to 3.25582, saving model to final.h5\n","Epoch 92/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.3604 - val_loss: 29.4841\n","\n","Epoch 00092: loss did not improve from 3.25582\n","Epoch 93/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.3316 - val_loss: 435.9285\n","\n","Epoch 00093: loss did not improve from 3.25582\n","Epoch 94/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.3341 - val_loss: 0.4213\n","\n","Epoch 00094: loss did not improve from 3.25582\n","Epoch 95/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2924 - val_loss: 60.8008\n","\n","Epoch 00095: loss did not improve from 3.25582\n","Epoch 96/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2609 - val_loss: 186.8134\n","\n","Epoch 00096: loss did not improve from 3.25582\n","Epoch 97/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2610 - val_loss: 5.0834\n","\n","Epoch 00097: loss did not improve from 3.25582\n","Epoch 98/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2373 - val_loss: 75.8016\n","\n","Epoch 00098: loss improved from 3.25582 to 3.23723, saving model to final.h5\n","Epoch 99/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2080 - val_loss: 172.4764\n","\n","Epoch 00099: loss improved from 3.23723 to 3.20775, saving model to final.h5\n","Epoch 100/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2213 - val_loss: 30.9012\n","\n","Epoch 00100: loss did not improve from 3.20775\n","Epoch 101/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2047 - val_loss: 4.3381\n","\n","Epoch 00101: loss improved from 3.20775 to 3.20461, saving model to final.h5\n","Epoch 102/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2589 - val_loss: 913.0339\n","\n","Epoch 00102: loss did not improve from 3.20461\n","Epoch 103/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1798 - val_loss: 2.2703\n","\n","Epoch 00103: loss improved from 3.20461 to 3.17993, saving model to final.h5\n","Epoch 104/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2632 - val_loss: 73.2721\n","\n","Epoch 00104: loss did not improve from 3.17993\n","Epoch 105/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2042 - val_loss: 721.7731\n","\n","Epoch 00105: loss did not improve from 3.17993\n","Epoch 106/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2460 - val_loss: 514.5533\n","\n","Epoch 00106: loss did not improve from 3.17993\n","Epoch 107/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.2140 - val_loss: 200.2454\n","\n","Epoch 00107: loss did not improve from 3.17993\n","Epoch 108/350\n","500/500 [==============================] - 30s 61ms/step - loss: 3.2459 - val_loss: 4.0504\n","\n","Epoch 00108: loss did not improve from 3.17993\n","Epoch 109/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1409 - val_loss: 75.1608\n","\n","Epoch 00109: loss improved from 3.17993 to 3.14099, saving model to final.h5\n","Epoch 110/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2483 - val_loss: 3.3902\n","\n","Epoch 00110: loss did not improve from 3.14099\n","Epoch 111/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2104 - val_loss: 878.9513\n","\n","Epoch 00111: loss did not improve from 3.14099\n","Epoch 112/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1576 - val_loss: 1.4458\n","\n","Epoch 00112: loss did not improve from 3.14099\n","Epoch 113/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1497 - val_loss: 286.1701\n","\n","Epoch 00113: loss did not improve from 3.14099\n","Epoch 114/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1784 - val_loss: 184.1906\n","\n","Epoch 00114: loss did not improve from 3.14099\n","Epoch 115/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2136 - val_loss: 1044.1122\n","\n","Epoch 00115: loss did not improve from 3.14099\n","Epoch 116/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2378 - val_loss: 161.4759\n","\n","Epoch 00116: loss did not improve from 3.14099\n","Epoch 117/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.2201 - val_loss: 6070.7363\n","\n","Epoch 00117: loss did not improve from 3.14099\n","Epoch 118/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1684 - val_loss: 2945.0879\n","\n","Epoch 00118: loss did not improve from 3.14099\n","Epoch 119/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1509 - val_loss: 4057.6294\n","\n","Epoch 00119: loss did not improve from 3.14099\n","Epoch 120/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1625 - val_loss: 1271.2543\n","\n","Epoch 00120: loss did not improve from 3.14099\n","Epoch 121/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1713 - val_loss: 2.3027\n","\n","Epoch 00121: loss did not improve from 3.14099\n","Epoch 122/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1588 - val_loss: 8.0494\n","\n","Epoch 00122: loss did not improve from 3.14099\n","Epoch 123/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1314 - val_loss: 66.0773\n","\n","Epoch 00123: loss improved from 3.14099 to 3.13127, saving model to final.h5\n","Epoch 124/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1270 - val_loss: 718.1490\n","\n","Epoch 00124: loss improved from 3.13127 to 3.12715, saving model to final.h5\n","Epoch 125/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1777 - val_loss: 1766.7623\n","\n","Epoch 00125: loss did not improve from 3.12715\n","Epoch 126/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2261 - val_loss: 5750.0317\n","\n","Epoch 00126: loss did not improve from 3.12715\n","Epoch 127/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1334 - val_loss: 123.5682\n","\n","Epoch 00127: loss did not improve from 3.12715\n","Epoch 128/350\n","500/500 [==============================] - 30s 61ms/step - loss: 3.1464 - val_loss: 72.1681\n","\n","Epoch 00128: loss did not improve from 3.12715\n","Epoch 129/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2094 - val_loss: 493.8914\n","\n","Epoch 00129: loss did not improve from 3.12715\n","Epoch 130/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1780 - val_loss: 155.7629\n","\n","Epoch 00130: loss did not improve from 3.12715\n","Epoch 131/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1954 - val_loss: 7.6715\n","\n","Epoch 00131: loss did not improve from 3.12715\n","Epoch 132/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0642 - val_loss: 10.0955\n","\n","Epoch 00132: loss improved from 3.12715 to 3.06414, saving model to final.h5\n","Epoch 133/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1100 - val_loss: 9.5776\n","\n","Epoch 00133: loss did not improve from 3.06414\n","Epoch 134/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1185 - val_loss: 0.6393\n","\n","Epoch 00134: loss did not improve from 3.06414\n","Epoch 135/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.2153 - val_loss: 0.9826\n","\n","Epoch 00135: loss did not improve from 3.06414\n","Epoch 136/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.0732 - val_loss: 1.1173\n","\n","Epoch 00136: loss did not improve from 3.06414\n","Epoch 137/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0885 - val_loss: 6.5242\n","\n","Epoch 00137: loss did not improve from 3.06414\n","Epoch 138/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.0799 - val_loss: 2.0128\n","\n","Epoch 00138: loss did not improve from 3.06414\n","Epoch 139/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.0702 - val_loss: 4.0035\n","\n","Epoch 00139: loss did not improve from 3.06414\n","Epoch 140/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.0837 - val_loss: 2.5330\n","\n","Epoch 00140: loss did not improve from 3.06414\n","Epoch 141/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1142 - val_loss: 1.0541\n","\n","Epoch 00141: loss did not improve from 3.06414\n","Epoch 142/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.1314 - val_loss: 8.1607\n","\n","Epoch 00142: loss did not improve from 3.06414\n","Epoch 143/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.0871 - val_loss: 0.6094\n","\n","Epoch 00143: loss did not improve from 3.06414\n","Epoch 144/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0535 - val_loss: 6.9574\n","\n","Epoch 00144: loss improved from 3.06414 to 3.05360, saving model to final.h5\n","Epoch 145/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0312 - val_loss: 0.2322\n","\n","Epoch 00145: loss improved from 3.05360 to 3.03118, saving model to final.h5\n","Epoch 146/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0421 - val_loss: 3.9270\n","\n","Epoch 00146: loss did not improve from 3.03118\n","Epoch 147/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0646 - val_loss: 2.4613\n","\n","Epoch 00147: loss did not improve from 3.03118\n","Epoch 148/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0516 - val_loss: 5.9249\n","\n","Epoch 00148: loss did not improve from 3.03118\n","Epoch 149/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0264 - val_loss: 2.2661\n","\n","Epoch 00149: loss improved from 3.03118 to 3.02652, saving model to final.h5\n","Epoch 150/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1179 - val_loss: 5.0502\n","\n","Epoch 00150: loss did not improve from 3.02652\n","Epoch 151/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0720 - val_loss: 0.2088\n","\n","Epoch 00151: loss did not improve from 3.02652\n","Epoch 152/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0330 - val_loss: 6.0570\n","\n","Epoch 00152: loss did not improve from 3.02652\n","Epoch 153/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0251 - val_loss: 4.7812\n","\n","Epoch 00153: loss improved from 3.02652 to 3.02506, saving model to final.h5\n","Epoch 154/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0560 - val_loss: 4.9911\n","\n","Epoch 00154: loss did not improve from 3.02506\n","Epoch 155/350\n","500/500 [==============================] - 31s 61ms/step - loss: 3.0635 - val_loss: 0.3419\n","\n","Epoch 00155: loss did not improve from 3.02506\n","Epoch 156/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9650 - val_loss: 3.0355\n","\n","Epoch 00156: loss improved from 3.02506 to 2.96502, saving model to final.h5\n","Epoch 157/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1127 - val_loss: 1.9664\n","\n","Epoch 00157: loss did not improve from 2.96502\n","Epoch 158/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0137 - val_loss: 9.6423\n","\n","Epoch 00158: loss did not improve from 2.96502\n","Epoch 159/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9839 - val_loss: 7.2392\n","\n","Epoch 00159: loss did not improve from 2.96502\n","Epoch 160/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0264 - val_loss: 4.4785\n","\n","Epoch 00160: loss did not improve from 2.96502\n","Epoch 161/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9987 - val_loss: 2.5229\n","\n","Epoch 00161: loss did not improve from 2.96502\n","Epoch 162/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9349 - val_loss: 7.6597\n","\n","Epoch 00162: loss improved from 2.96502 to 2.93483, saving model to final.h5\n","Epoch 163/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0310 - val_loss: 4.4144\n","\n","Epoch 00163: loss did not improve from 2.93483\n","Epoch 164/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9566 - val_loss: 6.5617\n","\n","Epoch 00164: loss did not improve from 2.93483\n","Epoch 165/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9391 - val_loss: 4.1363\n","\n","Epoch 00165: loss did not improve from 2.93483\n","Epoch 166/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9737 - val_loss: 1.0691\n","\n","Epoch 00166: loss did not improve from 2.93483\n","Epoch 167/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0369 - val_loss: 11.0335\n","\n","Epoch 00167: loss did not improve from 2.93483\n","Epoch 168/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9668 - val_loss: 43.9890\n","\n","Epoch 00168: loss did not improve from 2.93483\n","Epoch 169/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9501 - val_loss: 14.9304\n","\n","Epoch 00169: loss did not improve from 2.93483\n","Epoch 170/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9160 - val_loss: 0.4812\n","\n","Epoch 00170: loss improved from 2.93483 to 2.91600, saving model to final.h5\n","Epoch 171/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0216 - val_loss: 39.1165\n","\n","Epoch 00171: loss did not improve from 2.91600\n","Epoch 172/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9815 - val_loss: 112.7941\n","\n","Epoch 00172: loss did not improve from 2.91600\n","Epoch 173/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9148 - val_loss: 7.7989\n","\n","Epoch 00173: loss improved from 2.91600 to 2.91464, saving model to final.h5\n","Epoch 174/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0010 - val_loss: 4.3832\n","\n","Epoch 00174: loss did not improve from 2.91464\n","Epoch 175/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9297 - val_loss: 28.4808\n","\n","Epoch 00175: loss did not improve from 2.91464\n","Epoch 176/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0116 - val_loss: 1.2752\n","\n","Epoch 00176: loss did not improve from 2.91464\n","Epoch 177/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0122 - val_loss: 35.5727\n","\n","Epoch 00177: loss did not improve from 2.91464\n","Epoch 178/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9362 - val_loss: 3.8161\n","\n","Epoch 00178: loss did not improve from 2.91464\n","Epoch 179/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0302 - val_loss: 4.0229\n","\n","Epoch 00179: loss did not improve from 2.91464\n","Epoch 180/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9309 - val_loss: 10.7257\n","\n","Epoch 00180: loss did not improve from 2.91464\n","Epoch 181/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9657 - val_loss: 25.3333\n","\n","Epoch 00181: loss did not improve from 2.91464\n","Epoch 182/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9545 - val_loss: 28.6781\n","\n","Epoch 00182: loss did not improve from 2.91464\n","Epoch 183/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9864 - val_loss: 17.3812\n","\n","Epoch 00183: loss did not improve from 2.91464\n","Epoch 184/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9146 - val_loss: 0.9195\n","\n","Epoch 00184: loss improved from 2.91464 to 2.91462, saving model to final.h5\n","Epoch 185/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8938 - val_loss: 2.1640\n","\n","Epoch 00185: loss improved from 2.91462 to 2.89377, saving model to final.h5\n","Epoch 186/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0058 - val_loss: 17.6675\n","\n","Epoch 00186: loss did not improve from 2.89377\n","Epoch 187/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9779 - val_loss: 0.4775\n","\n","Epoch 00187: loss did not improve from 2.89377\n","Epoch 188/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9022 - val_loss: 7.4171\n","\n","Epoch 00188: loss did not improve from 2.89377\n","Epoch 189/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9348 - val_loss: 98.1517\n","\n","Epoch 00189: loss did not improve from 2.89377\n","Epoch 190/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9441 - val_loss: 33.3553\n","\n","Epoch 00190: loss did not improve from 2.89377\n","Epoch 191/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8984 - val_loss: 2.0743\n","\n","Epoch 00191: loss did not improve from 2.89377\n","Epoch 192/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9594 - val_loss: 48.2702\n","\n","Epoch 00192: loss did not improve from 2.89377\n","Epoch 193/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0477 - val_loss: 2.1625\n","\n","Epoch 00193: loss did not improve from 2.89377\n","Epoch 194/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9148 - val_loss: 5.1278\n","\n","Epoch 00194: loss did not improve from 2.89377\n","Epoch 195/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9278 - val_loss: 3.1256\n","\n","Epoch 00195: loss did not improve from 2.89377\n","Epoch 196/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9174 - val_loss: 14.0082\n","\n","Epoch 00196: loss did not improve from 2.89377\n","Epoch 197/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8789 - val_loss: 4.4365\n","\n","Epoch 00197: loss improved from 2.89377 to 2.87893, saving model to final.h5\n","Epoch 198/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8924 - val_loss: 17.2742\n","\n","Epoch 00198: loss did not improve from 2.87893\n","Epoch 199/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9048 - val_loss: 6.5017\n","\n","Epoch 00199: loss did not improve from 2.87893\n","Epoch 200/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8834 - val_loss: 5.8192\n","\n","Epoch 00200: loss did not improve from 2.87893\n","Epoch 201/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9844 - val_loss: 51.0385\n","\n","Epoch 00201: loss did not improve from 2.87893\n","Epoch 202/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9415 - val_loss: 8.8195\n","\n","Epoch 00202: loss did not improve from 2.87893\n","Epoch 203/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8568 - val_loss: 14.4153\n","\n","Epoch 00203: loss improved from 2.87893 to 2.85699, saving model to final.h5\n","Epoch 204/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9256 - val_loss: 5.1016\n","\n","Epoch 00204: loss did not improve from 2.85699\n","Epoch 205/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9720 - val_loss: 8.1264\n","\n","Epoch 00205: loss did not improve from 2.85699\n","Epoch 206/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8642 - val_loss: 9.7260\n","\n","Epoch 00206: loss did not improve from 2.85699\n","Epoch 207/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8553 - val_loss: 20.2617\n","\n","Epoch 00207: loss improved from 2.85699 to 2.85535, saving model to final.h5\n","Epoch 208/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8726 - val_loss: 5.4568\n","\n","Epoch 00208: loss did not improve from 2.85535\n","Epoch 209/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9604 - val_loss: 3.9621\n","\n","Epoch 00209: loss did not improve from 2.85535\n","Epoch 210/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9020 - val_loss: 5.5092\n","\n","Epoch 00210: loss did not improve from 2.85535\n","Epoch 211/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9011 - val_loss: 10.4526\n","\n","Epoch 00211: loss did not improve from 2.85535\n","Epoch 212/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8533 - val_loss: 4.8063\n","\n","Epoch 00212: loss improved from 2.85535 to 2.85307, saving model to final.h5\n","Epoch 213/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9425 - val_loss: 127.1119\n","\n","Epoch 00213: loss did not improve from 2.85307\n","Epoch 214/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8481 - val_loss: 2.7266\n","\n","Epoch 00214: loss improved from 2.85307 to 2.84806, saving model to final.h5\n","Epoch 215/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8936 - val_loss: 6.6508\n","\n","Epoch 00215: loss did not improve from 2.84806\n","Epoch 216/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9043 - val_loss: 5.3981\n","\n","Epoch 00216: loss did not improve from 2.84806\n","Epoch 217/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8975 - val_loss: 5.4965\n","\n","Epoch 00217: loss did not improve from 2.84806\n","Epoch 218/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9653 - val_loss: 13.1214\n","\n","Epoch 00218: loss did not improve from 2.84806\n","Epoch 219/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8811 - val_loss: 111.9128\n","\n","Epoch 00219: loss did not improve from 2.84806\n","Epoch 220/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9476 - val_loss: 1.6152\n","\n","Epoch 00220: loss did not improve from 2.84806\n","Epoch 221/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8653 - val_loss: 2.7820\n","\n","Epoch 00221: loss did not improve from 2.84806\n","Epoch 222/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8710 - val_loss: 1.5647\n","\n","Epoch 00222: loss did not improve from 2.84806\n","Epoch 223/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8409 - val_loss: 6.0968\n","\n","Epoch 00223: loss improved from 2.84806 to 2.84096, saving model to final.h5\n","Epoch 224/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8367 - val_loss: 10.0099\n","\n","Epoch 00224: loss improved from 2.84096 to 2.83688, saving model to final.h5\n","Epoch 225/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8920 - val_loss: 271.9465\n","\n","Epoch 00225: loss did not improve from 2.83688\n","Epoch 226/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9012 - val_loss: 246.8259\n","\n","Epoch 00226: loss did not improve from 2.83688\n","Epoch 227/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7601 - val_loss: 29.2959\n","\n","Epoch 00227: loss improved from 2.83688 to 2.76021, saving model to final.h5\n","Epoch 228/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8324 - val_loss: 31.0122\n","\n","Epoch 00228: loss did not improve from 2.76021\n","Epoch 229/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9055 - val_loss: 2.0810\n","\n","Epoch 00229: loss did not improve from 2.76021\n","Epoch 230/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9046 - val_loss: 24.1387\n","\n","Epoch 00230: loss did not improve from 2.76021\n","Epoch 231/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9260 - val_loss: 2.7018\n","\n","Epoch 00231: loss did not improve from 2.76021\n","Epoch 232/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8825 - val_loss: 4.2585\n","\n","Epoch 00232: loss did not improve from 2.76021\n","Epoch 233/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8726 - val_loss: 5.2413\n","\n","Epoch 00233: loss did not improve from 2.76021\n","Epoch 234/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8303 - val_loss: 5.1004\n","\n","Epoch 00234: loss did not improve from 2.76021\n","Epoch 235/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8793 - val_loss: 6.0849\n","\n","Epoch 00235: loss did not improve from 2.76021\n","Epoch 236/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8533 - val_loss: 88.0972\n","\n","Epoch 00236: loss did not improve from 2.76021\n","Epoch 237/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8631 - val_loss: 1.0467\n","\n","Epoch 00237: loss did not improve from 2.76021\n","Epoch 238/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8539 - val_loss: 598.3826\n","\n","Epoch 00238: loss did not improve from 2.76021\n","Epoch 239/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8952 - val_loss: 968.6395\n","\n","Epoch 00239: loss did not improve from 2.76021\n","Epoch 240/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8370 - val_loss: 74.0176\n","\n","Epoch 00240: loss did not improve from 2.76021\n","Epoch 241/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8588 - val_loss: 200.0621\n","\n","Epoch 00241: loss did not improve from 2.76021\n","Epoch 242/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7762 - val_loss: 302.8431\n","\n","Epoch 00242: loss did not improve from 2.76021\n","Epoch 243/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8493 - val_loss: 26.6503\n","\n","Epoch 00243: loss did not improve from 2.76021\n","Epoch 244/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8857 - val_loss: 4.7516\n","\n","Epoch 00244: loss did not improve from 2.76021\n","Epoch 245/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8420 - val_loss: 2.7374\n","\n","Epoch 00245: loss did not improve from 2.76021\n","Epoch 246/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8756 - val_loss: 31.2566\n","\n","Epoch 00246: loss did not improve from 2.76021\n","Epoch 247/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8740 - val_loss: 15.6829\n","\n","Epoch 00247: loss did not improve from 2.76021\n","Epoch 248/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8216 - val_loss: 61.9521\n","\n","Epoch 00248: loss did not improve from 2.76021\n","Epoch 249/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8272 - val_loss: 56.5468\n","\n","Epoch 00249: loss did not improve from 2.76021\n","Epoch 250/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.9350 - val_loss: 20.1236\n","\n","Epoch 00250: loss did not improve from 2.76021\n","Epoch 251/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8348 - val_loss: 109.9061\n","\n","Epoch 00251: loss did not improve from 2.76021\n","Epoch 252/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8179 - val_loss: 119.5887\n","\n","Epoch 00252: loss did not improve from 2.76021\n","Epoch 253/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8022 - val_loss: 30.2965\n","\n","Epoch 00253: loss did not improve from 2.76021\n","Epoch 254/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8355 - val_loss: 334.7720\n","\n","Epoch 00254: loss did not improve from 2.76021\n","Epoch 255/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8524 - val_loss: 373.2660\n","\n","Epoch 00255: loss did not improve from 2.76021\n","Epoch 256/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8670 - val_loss: 64.4850\n","\n","Epoch 00256: loss did not improve from 2.76021\n","Epoch 257/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8631 - val_loss: 155.1855\n","\n","Epoch 00257: loss did not improve from 2.76021\n","Epoch 258/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8406 - val_loss: 97.3308\n","\n","Epoch 00258: loss did not improve from 2.76021\n","Epoch 259/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8854 - val_loss: 87.7475\n","\n","Epoch 00259: loss did not improve from 2.76021\n","Epoch 260/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8468 - val_loss: 113.1542\n","\n","Epoch 00260: loss did not improve from 2.76021\n","Epoch 261/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8364 - val_loss: 119.9473\n","\n","Epoch 00261: loss did not improve from 2.76021\n","Epoch 262/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8280 - val_loss: 22.6364\n","\n","Epoch 00262: loss did not improve from 2.76021\n","Epoch 263/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8127 - val_loss: 284.0029\n","\n","Epoch 00263: loss did not improve from 2.76021\n","Epoch 264/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8783 - val_loss: 0.7301\n","\n","Epoch 00264: loss did not improve from 2.76021\n","Epoch 265/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8289 - val_loss: 44.3614\n","\n","Epoch 00265: loss did not improve from 2.76021\n","Epoch 266/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8750 - val_loss: 145.2330\n","\n","Epoch 00266: loss did not improve from 2.76021\n","Epoch 267/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7929 - val_loss: 585.5269\n","\n","Epoch 00267: loss did not improve from 2.76021\n","Epoch 268/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8755 - val_loss: 5571.4209\n","\n","Epoch 00268: loss did not improve from 2.76021\n","Epoch 269/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7443 - val_loss: 1440.5729\n","\n","Epoch 00269: loss improved from 2.76021 to 2.74434, saving model to final.h5\n","Epoch 270/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7835 - val_loss: 1172.8489\n","\n","Epoch 00270: loss did not improve from 2.74434\n","Epoch 271/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8520 - val_loss: 96.5641\n","\n","Epoch 00271: loss did not improve from 2.74434\n","Epoch 272/350\n","500/500 [==============================] - 30s 61ms/step - loss: 2.8014 - val_loss: 410.2357\n","\n","Epoch 00272: loss did not improve from 2.74434\n","Epoch 273/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7834 - val_loss: 163.6701\n","\n","Epoch 00273: loss did not improve from 2.74434\n","Epoch 274/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8021 - val_loss: 639.2540\n","\n","Epoch 00274: loss did not improve from 2.74434\n","Epoch 275/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8473 - val_loss: 1284.4161\n","\n","Epoch 00275: loss did not improve from 2.74434\n","Epoch 276/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8708 - val_loss: 1094.3260\n","\n","Epoch 00276: loss did not improve from 2.74434\n","Epoch 277/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8127 - val_loss: 232.8729\n","\n","Epoch 00277: loss did not improve from 2.74434\n","Epoch 278/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7549 - val_loss: 586.2859\n","\n","Epoch 00278: loss did not improve from 2.74434\n","Epoch 279/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7931 - val_loss: 282.5611\n","\n","Epoch 00279: loss did not improve from 2.74434\n","Epoch 280/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7803 - val_loss: 756.2601\n","\n","Epoch 00280: loss did not improve from 2.74434\n","Epoch 281/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7600 - val_loss: 398.1406\n","\n","Epoch 00281: loss did not improve from 2.74434\n","Epoch 282/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8450 - val_loss: 475.7595\n","\n","Epoch 00282: loss did not improve from 2.74434\n","Epoch 283/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7497 - val_loss: 190.4063\n","\n","Epoch 00283: loss did not improve from 2.74434\n","Epoch 284/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7461 - val_loss: 359.9654\n","\n","Epoch 00284: loss did not improve from 2.74434\n","Epoch 285/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8550 - val_loss: 81.6564\n","\n","Epoch 00285: loss did not improve from 2.74434\n","Epoch 286/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8027 - val_loss: 213.1371\n","\n","Epoch 00286: loss did not improve from 2.74434\n","Epoch 287/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7984 - val_loss: 254.8743\n","\n","Epoch 00287: loss did not improve from 2.74434\n","Epoch 288/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7917 - val_loss: 300.2456\n","\n","Epoch 00288: loss did not improve from 2.74434\n","Epoch 289/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7473 - val_loss: 27.8465\n","\n","Epoch 00289: loss did not improve from 2.74434\n","Epoch 290/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7553 - val_loss: 90.1542\n","\n","Epoch 00290: loss did not improve from 2.74434\n","Epoch 291/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8174 - val_loss: 431.9597\n","\n","Epoch 00291: loss did not improve from 2.74434\n","Epoch 292/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7352 - val_loss: 184.8172\n","\n","Epoch 00292: loss improved from 2.74434 to 2.73529, saving model to final.h5\n","Epoch 293/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7873 - val_loss: 536.6000\n","\n","Epoch 00293: loss did not improve from 2.73529\n","Epoch 294/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7156 - val_loss: 112.0953\n","\n","Epoch 00294: loss improved from 2.73529 to 2.71568, saving model to final.h5\n","Epoch 295/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7753 - val_loss: 236.9799\n","\n","Epoch 00295: loss did not improve from 2.71568\n","Epoch 296/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7665 - val_loss: 412.2044\n","\n","Epoch 00296: loss did not improve from 2.71568\n","Epoch 297/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7769 - val_loss: 74.7515\n","\n","Epoch 00297: loss did not improve from 2.71568\n","Epoch 298/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7550 - val_loss: 201.2467\n","\n","Epoch 00298: loss did not improve from 2.71568\n","Epoch 299/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7875 - val_loss: 595.9849\n","\n","Epoch 00299: loss did not improve from 2.71568\n","Epoch 300/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7479 - val_loss: 310.6441\n","\n","Epoch 00300: loss did not improve from 2.71568\n","Epoch 301/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7522 - val_loss: 81.6362\n","\n","Epoch 00301: loss did not improve from 2.71568\n","Epoch 302/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8114 - val_loss: 196.5641\n","\n","Epoch 00302: loss did not improve from 2.71568\n","Epoch 303/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7904 - val_loss: 95.1670\n","\n","Epoch 00303: loss did not improve from 2.71568\n","Epoch 304/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7491 - val_loss: 259.5215\n","\n","Epoch 00304: loss did not improve from 2.71568\n","Epoch 305/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7148 - val_loss: 30.2732\n","\n","Epoch 00305: loss improved from 2.71568 to 2.71497, saving model to final.h5\n","Epoch 306/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7655 - val_loss: 15.5756\n","\n","Epoch 00306: loss did not improve from 2.71497\n","Epoch 307/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7362 - val_loss: 48.7936\n","\n","Epoch 00307: loss did not improve from 2.71497\n","Epoch 308/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7794 - val_loss: 104.8359\n","\n","Epoch 00308: loss did not improve from 2.71497\n","Epoch 309/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8087 - val_loss: 43.4696\n","\n","Epoch 00309: loss did not improve from 2.71497\n","Epoch 310/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7380 - val_loss: 61.2197\n","\n","Epoch 00310: loss did not improve from 2.71497\n","Epoch 311/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7916 - val_loss: 21.2486\n","\n","Epoch 00311: loss did not improve from 2.71497\n","Epoch 312/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7877 - val_loss: 124.5947\n","\n","Epoch 00312: loss did not improve from 2.71497\n","Epoch 313/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7432 - val_loss: 49.2969\n","\n","Epoch 00313: loss did not improve from 2.71497\n","Epoch 314/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7029 - val_loss: 53.0337\n","\n","Epoch 00314: loss improved from 2.71497 to 2.70283, saving model to final.h5\n","Epoch 315/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7168 - val_loss: 8.5557\n","\n","Epoch 00315: loss did not improve from 2.70283\n","Epoch 316/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7485 - val_loss: 56.4049\n","\n","Epoch 00316: loss did not improve from 2.70283\n","Epoch 317/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7236 - val_loss: 13.2271\n","\n","Epoch 00317: loss did not improve from 2.70283\n","Epoch 318/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7261 - val_loss: 2.5844\n","\n","Epoch 00318: loss did not improve from 2.70283\n","Epoch 319/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.6575 - val_loss: 12.1859\n","\n","Epoch 00319: loss improved from 2.70283 to 2.65760, saving model to final.h5\n","Epoch 320/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7332 - val_loss: 7.8900\n","\n","Epoch 00320: loss did not improve from 2.65760\n","Epoch 321/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7388 - val_loss: 16.7722\n","\n","Epoch 00321: loss did not improve from 2.65760\n","Epoch 322/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7380 - val_loss: 29.0449\n","\n","Epoch 00322: loss did not improve from 2.65760\n","Epoch 323/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7237 - val_loss: 28.1343\n","\n","Epoch 00323: loss did not improve from 2.65760\n","Epoch 324/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7244 - val_loss: 6.3274\n","\n","Epoch 00324: loss did not improve from 2.65760\n","Epoch 325/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7024 - val_loss: 47.5641\n","\n","Epoch 00325: loss did not improve from 2.65760\n","Epoch 326/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.6972 - val_loss: 1.5871\n","\n","Epoch 00326: loss did not improve from 2.65760\n","Epoch 327/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7036 - val_loss: 6.8306\n","\n","Epoch 00327: loss did not improve from 2.65760\n","Epoch 328/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7672 - val_loss: 8.7688\n","\n","Epoch 00328: loss did not improve from 2.65760\n","Epoch 329/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7192 - val_loss: 11.1443\n","\n","Epoch 00329: loss did not improve from 2.65760\n","Epoch 330/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7671 - val_loss: 1.3844\n","\n","Epoch 00330: loss did not improve from 2.65760\n","Epoch 331/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7499 - val_loss: 1.5552\n","\n","Epoch 00331: loss did not improve from 2.65760\n","Epoch 332/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7216 - val_loss: 7.9256\n","\n","Epoch 00332: loss did not improve from 2.65760\n","Epoch 333/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.6718 - val_loss: 3.2344\n","\n","Epoch 00333: loss did not improve from 2.65760\n","Epoch 334/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7516 - val_loss: 1.0459\n","\n","Epoch 00334: loss did not improve from 2.65760\n","Epoch 335/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7177 - val_loss: 3.0526\n","\n","Epoch 00335: loss did not improve from 2.65760\n","Epoch 336/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7147 - val_loss: 3.0028\n","\n","Epoch 00336: loss did not improve from 2.65760\n","Epoch 337/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.6898 - val_loss: 3.2030\n","\n","Epoch 00337: loss did not improve from 2.65760\n","Epoch 338/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7360 - val_loss: 2.6418\n","\n","Epoch 00338: loss did not improve from 2.65760\n","Epoch 339/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7537 - val_loss: 6.8731\n","\n","Epoch 00339: loss did not improve from 2.65760\n","Epoch 340/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.6953 - val_loss: 7.8279\n","\n","Epoch 00340: loss did not improve from 2.65760\n","Epoch 341/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.6799 - val_loss: 6.2660\n","\n","Epoch 00341: loss did not improve from 2.65760\n","Epoch 342/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7169 - val_loss: 2.0824\n","\n","Epoch 00342: loss did not improve from 2.65760\n","Epoch 343/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7652 - val_loss: 5.4442\n","\n","Epoch 00343: loss did not improve from 2.65760\n","Epoch 344/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.6833 - val_loss: 2.6481\n","\n","Epoch 00344: loss did not improve from 2.65760\n","Epoch 345/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.8050 - val_loss: 9.3834\n","\n","Epoch 00345: loss did not improve from 2.65760\n","Epoch 346/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7758 - val_loss: 198.5839\n","\n","Epoch 00346: loss did not improve from 2.65760\n","Epoch 347/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.6492 - val_loss: 3.7496\n","\n","Epoch 00347: loss improved from 2.65760 to 2.64925, saving model to final.h5\n","Epoch 348/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7400 - val_loss: 62.8141\n","\n","Epoch 00348: loss did not improve from 2.64925\n","Epoch 349/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.6941 - val_loss: 291.9925\n","\n","Epoch 00349: loss did not improve from 2.64925\n","Epoch 350/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7353 - val_loss: 601.7730\n","\n","Epoch 00350: loss did not improve from 2.64925\n","Done training. Saved weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XVZUGm5nzfz_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c92c048c-2311-4592-b6e5-8f3f4ddf7840","executionInfo":{"status":"ok","timestamp":1591843002150,"user_tz":360,"elapsed":35738782,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}}},"source":["!python speedchallenge.py --mode=test  --history {nhistory}  --model final.h5  train_Flipped_for_testing.mp4 train_Flipped_for_testing.txt"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-06-11 02:34:45.016145: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:217: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(32, (8, 8), return_sequences=True, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(32, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=True,activation=\"relu\", dropout=0.5))(flow_inp)\n","2020-06-11 02:34:47.172289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-06-11 02:34:47.187986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:34:47.188930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-11 02:34:47.188970: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-11 02:34:47.191048: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-11 02:34:47.193259: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-11 02:34:47.193619: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-11 02:34:47.195554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-11 02:34:47.196863: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-11 02:34:47.200967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-11 02:34:47.201071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:34:47.201976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:34:47.202808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-11 02:34:47.208648: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-06-11 02:34:47.208968: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15172c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-06-11 02:34:47.209008: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-06-11 02:34:47.308379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:34:47.309592: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1517480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-06-11 02:34:47.309632: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-06-11 02:34:47.309823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:34:47.310747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-11 02:34:47.310805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-11 02:34:47.310861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-11 02:34:47.310897: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-11 02:34:47.310935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-11 02:34:47.310967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-11 02:34:47.310998: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-11 02:34:47.311029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-11 02:34:47.311100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:34:47.312150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:34:47.313247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-11 02:34:47.313320: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-11 02:34:47.865747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-11 02:34:47.865823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-06-11 02:34:47.865879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-06-11 02:34:47.866240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:34:47.867313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:34:47.868233: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-06-11 02:34:47.868290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:221: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(128, (8, 8), return_sequences=False, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(128, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=False,activation=\"relu\", dropout=0.5))(flow)\n","speedchallenge.py:234: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","speedchallenge.py:241: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 100, 100, 2)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 25, 25, 32)   4128        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 25, 25, 32)   0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 25, 25, 32)   128         activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 25, 25, 32)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 2, 100, 100,  0                                            \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 32)   0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv_lst_m2d_1 (ConvLSTM2D)     (None, 2, 25, 25, 32 286848      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 6, 6, 128)    262272      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 2, 25, 25, 32 128         conv_lst_m2d_1[0][0]             \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 6, 6, 128)    0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv_lst_m2d_2 (ConvLSTM2D)     (None, 7, 7, 128)    5243392     batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 6, 6, 128)    512         activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 7, 7, 128)    512         conv_lst_m2d_2[0][0]             \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 6, 6, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 6272)         0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 128)    0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          1605888     flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 5, 5, 256)    33024       max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 256)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 6400)         0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 6656)         0           activation_1[0][0]               \n","                                                                 flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 6656)         0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 6656)         0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 128)          852096      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 128)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1)            129         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 8,289,057\n","Trainable params: 8,288,417\n","Non-trainable params: 640\n","__________________________________________________________________________________________________\n","None\n","Prepping data\n","Decoding speed data\n","loaded 4399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 4398 frames\n","done processing 4400 frames\n","Done prepping data\n","loading weights\n","Starting testing\n","Number of frames to be evaluated:  4398\n","2020-06-11 02:35:39.864926: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-11 02:35:40.091177: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","The mean square error is:  325098739.2737527\n","Done testing\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X6lPTaiXMHWg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b3f83048-17bd-4375-e00a-5816bb64e760","executionInfo":{"status":"ok","timestamp":1591843432490,"user_tz":360,"elapsed":36169120,"user":{"displayName":"Nihar Turumella","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjkRVVhA0Wnk-CDzQkT6BOY3_J0TM4n_LksmLKhFw=s64","userId":"04644814609749384435"}}},"source":["!python speedchallenge.py --mode=test  --history {nhistory}  --model final.h5  train.mp4 train.txt"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-06-11 02:36:23.745446: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:217: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(32, (8, 8), return_sequences=True, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(32, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=True,activation=\"relu\", dropout=0.5))(flow_inp)\n","2020-06-11 02:36:25.908711: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-06-11 02:36:25.924756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:36:25.925743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-11 02:36:25.925792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-11 02:36:25.928486: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-11 02:36:25.931232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-11 02:36:25.931732: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-11 02:36:25.934009: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-11 02:36:25.935412: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-11 02:36:25.940817: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-11 02:36:25.940934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:36:25.942002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:36:25.942855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-11 02:36:25.949297: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-06-11 02:36:25.949771: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18ef2c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-06-11 02:36:25.949838: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-06-11 02:36:26.049528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:36:26.050772: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x18ef480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-06-11 02:36:26.050816: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-06-11 02:36:26.051039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:36:26.052028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-11 02:36:26.052081: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-11 02:36:26.052169: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-11 02:36:26.052208: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-11 02:36:26.052240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-11 02:36:26.052269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-11 02:36:26.052295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-11 02:36:26.052322: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-11 02:36:26.052396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:36:26.053386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:36:26.054275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-11 02:36:26.054335: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-11 02:36:26.605599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-11 02:36:26.605666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-06-11 02:36:26.605687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-06-11 02:36:26.605932: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:36:26.606884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:36:26.607752: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-06-11 02:36:26.607810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:221: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(128, (8, 8), return_sequences=False, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(128, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=False,activation=\"relu\", dropout=0.5))(flow)\n","speedchallenge.py:234: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","speedchallenge.py:241: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 100, 100, 2)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 25, 25, 32)   4128        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 25, 25, 32)   0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 25, 25, 32)   128         activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 25, 25, 32)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 2, 100, 100,  0                                            \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 32)   0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv_lst_m2d_1 (ConvLSTM2D)     (None, 2, 25, 25, 32 286848      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 6, 6, 128)    262272      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 2, 25, 25, 32 128         conv_lst_m2d_1[0][0]             \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 6, 6, 128)    0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv_lst_m2d_2 (ConvLSTM2D)     (None, 7, 7, 128)    5243392     batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 6, 6, 128)    512         activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 7, 7, 128)    512         conv_lst_m2d_2[0][0]             \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 6, 6, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 6272)         0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 128)    0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          1605888     flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 5, 5, 256)    33024       max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 256)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 6400)         0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 6656)         0           activation_1[0][0]               \n","                                                                 flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 6656)         0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 6656)         0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 128)          852096      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 128)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1)            129         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 8,289,057\n","Trainable params: 8,288,417\n","Non-trainable params: 640\n","__________________________________________________________________________________________________\n","None\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","wiping preprocessed data...\n","preprocessing data...\n","Processed 20398 frames\n","done processing 20400 frames\n","Done prepping data\n","loading weights\n","Starting testing\n","Number of frames to be evaluated:  20398\n","2020-06-11 02:40:26.040527: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-11 02:40:26.278071: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","The mean square error is:  69089538.34103663\n","Done testing\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FyE30U_r0Bp5","colab_type":"text"},"source":["## **Training with flipped image**"]},{"cell_type":"code","metadata":{"id":"cXpb18aXDSFJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"fbf1a434-4122-4675-a4e0-d5a7ec4d9678"},"source":["!python speedchallenge.py --mode=train --epoch {nepoch} --history {nhistory} --split_start=7700 --split_end=12100 --resume --model final.h5  --LR {LR} train_Flipped_for_training.mp4 train_Flipped_for_training.txt"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","2020-06-11 02:43:34.085969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","ML for Speed\n","Compiling Model\n","speedchallenge.py:217: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(32, (8, 8), return_sequences=True, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(32, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=True,activation=\"relu\", dropout=0.5))(flow_inp)\n","2020-06-11 02:43:36.252657: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2020-06-11 02:43:36.267509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:43:36.268545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-11 02:43:36.268589: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-11 02:43:36.270357: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-11 02:43:36.272430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-11 02:43:36.272767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-11 02:43:36.274743: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-11 02:43:36.275967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-11 02:43:36.280215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-11 02:43:36.280328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:43:36.281280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:43:36.282140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-11 02:43:36.292639: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n","2020-06-11 02:43:36.292969: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x31392c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2020-06-11 02:43:36.293005: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2020-06-11 02:43:36.388321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:43:36.389581: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3139480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2020-06-11 02:43:36.389617: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n","2020-06-11 02:43:36.389848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:43:36.390760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n","coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n","2020-06-11 02:43:36.390828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-11 02:43:36.390883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-11 02:43:36.390933: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2020-06-11 02:43:36.390974: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2020-06-11 02:43:36.391024: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2020-06-11 02:43:36.391071: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2020-06-11 02:43:36.391133: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2020-06-11 02:43:36.391227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:43:36.392166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:43:36.393011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n","2020-06-11 02:43:36.393064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2020-06-11 02:43:36.951935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2020-06-11 02:43:36.952000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 \n","2020-06-11 02:43:36.952020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N \n","2020-06-11 02:43:36.952262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:43:36.953286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2020-06-11 02:43:36.954194: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","2020-06-11 02:43:36.954253: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14974 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n","speedchallenge.py:221: UserWarning: Update your `ConvLSTM2D` call to the Keras 2 API: `ConvLSTM2D(128, (8, 8), return_sequences=False, activation=\"relu\", dropout=0.5, strides=(4, 4), padding=\"same\")`\n","  flow=(ConvLSTM2D(128, 8,8 ,border_mode='same', subsample=(4,4),return_sequences=False,activation=\"relu\", dropout=0.5))(flow)\n","speedchallenge.py:234: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(32, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow_inp)\n","speedchallenge.py:241: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (8, 8), strides=(4, 4), padding=\"same\")`\n","  op_flow=(Convolution2D(128, 8,8 ,border_mode='same', subsample=(4,4)))(op_flow)\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            (None, 100, 100, 2)  0                                            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 25, 25, 32)   4128        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 25, 25, 32)   0           conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 25, 25, 32)   128         activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 25, 25, 32)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            (None, 2, 100, 100,  0                                            \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 24, 24, 32)   0           dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","conv_lst_m2d_1 (ConvLSTM2D)     (None, 2, 25, 25, 32 286848      input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 6, 6, 128)    262272      max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 2, 25, 25, 32 128         conv_lst_m2d_1[0][0]             \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 6, 6, 128)    0           conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","conv_lst_m2d_2 (ConvLSTM2D)     (None, 7, 7, 128)    5243392     batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 6, 6, 128)    512         activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 7, 7, 128)    512         conv_lst_m2d_2[0][0]             \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 6, 6, 128)    0           batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","flatten_1 (Flatten)             (None, 6272)         0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 128)    0           dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          1605888     flatten_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 5, 5, 256)    33024       max_pooling2d_2[0][0]            \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 256)          0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","flatten_2 (Flatten)             (None, 6400)         0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 6656)         0           activation_1[0][0]               \n","                                                                 flatten_2[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 6656)         0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 6656)         0           activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 128)          852096      dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 128)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 128)          0           activation_5[0][0]               \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 1)            129         dropout_4[0][0]                  \n","==================================================================================================\n","Total params: 8,289,057\n","Trainable params: 8,288,417\n","Non-trainable params: 640\n","__________________________________________________________________________________________________\n","None\n","loading weights\n","Prepping data\n","Decoding speed data\n","loaded 20399 speed entries\n","Found preprocessed data\n","tcmalloc: large alloc 2447884288 bytes == 0x222a0000 @  0x7ff163ff91e7 0x7ff161a9f5e1 0x7ff161b03c78 0x7ff161b03f37 0x7ff161b9bf28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7ff163bf6b97 0x5b250a\n","tcmalloc: large alloc 2447884288 bytes == 0xb411c000 @  0x7ff163ff91e7 0x7ff161a9f5e1 0x7ff161b03c78 0x7ff161b03f37 0x7ff161b9bf28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7ff163bf6b97 0x5b250a\n","tcmalloc: large alloc 2447884288 bytes == 0x145f98000 @  0x7ff163ff91e7 0x7ff161a9f5e1 0x7ff161b03c78 0x7ff161b03f37 0x7ff161b9bf28 0x50a635 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7ff163bf6b97 0x5b250a\n","Loading frame 20398\n","done loading 20399 frames\n","Done prepping data\n","tcmalloc: large alloc 1631920128 bytes == 0x7ff018bae000 @  0x7ff163ff91e7 0x7ff161a9f5e1 0x7ff161b03c78 0x7ff161b03d93 0x7ff161b8eed6 0x7ff161b8f338 0x50c29e 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x509758 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7ff163bf6b97 0x5b250a\n","20398 Training data size per Aug\n","15998 Train indices size\n","4400 Val indices size\n"," This is the range of train:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748, 5749, 5750, 5751, 5752, 5753, 5754, 5755, 5756, 5757, 5758, 5759, 5760, 5761, 5762, 5763, 5764, 5765, 5766, 5767, 5768, 5769, 5770, 5771, 5772, 5773, 5774, 5775, 5776, 5777, 5778, 5779, 5780, 5781, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5793, 5794, 5795, 5796, 5797, 5798, 5799, 5800, 5801, 5802, 5803, 5804, 5805, 5806, 5807, 5808, 5809, 5810, 5811, 5812, 5813, 5814, 5815, 5816, 5817, 5818, 5819, 5820, 5821, 5822, 5823, 5824, 5825, 5826, 5827, 5828, 5829, 5830, 5831, 5832, 5833, 5834, 5835, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 5843, 5844, 5845, 5846, 5847, 5848, 5849, 5850, 5851, 5852, 5853, 5854, 5855, 5856, 5857, 5858, 5859, 5860, 5861, 5862, 5863, 5864, 5865, 5866, 5867, 5868, 5869, 5870, 5871, 5872, 5873, 5874, 5875, 5876, 5877, 5878, 5879, 5880, 5881, 5882, 5883, 5884, 5885, 5886, 5887, 5888, 5889, 5890, 5891, 5892, 5893, 5894, 5895, 5896, 5897, 5898, 5899, 5900, 5901, 5902, 5903, 5904, 5905, 5906, 5907, 5908, 5909, 5910, 5911, 5912, 5913, 5914, 5915, 5916, 5917, 5918, 5919, 5920, 5921, 5922, 5923, 5924, 5925, 5926, 5927, 5928, 5929, 5930, 5931, 5932, 5933, 5934, 5935, 5936, 5937, 5938, 5939, 5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955, 5956, 5957, 5958, 5959, 5960, 5961, 5962, 5963, 5964, 5965, 5966, 5967, 5968, 5969, 5970, 5971, 5972, 5973, 5974, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995, 5996, 5997, 5998, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008, 6009, 6010, 6011, 6012, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021, 6022, 6023, 6024, 6025, 6026, 6027, 6028, 6029, 6030, 6031, 6032, 6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6059, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6070, 6071, 6072, 6073, 6074, 6075, 6076, 6077, 6078, 6079, 6080, 6081, 6082, 6083, 6084, 6085, 6086, 6087, 6088, 6089, 6090, 6091, 6092, 6093, 6094, 6095, 6096, 6097, 6098, 6099, 6100, 6101, 6102, 6103, 6104, 6105, 6106, 6107, 6108, 6109, 6110, 6111, 6112, 6113, 6114, 6115, 6116, 6117, 6118, 6119, 6120, 6121, 6122, 6123, 6124, 6125, 6126, 6127, 6128, 6129, 6130, 6131, 6132, 6133, 6134, 6135, 6136, 6137, 6138, 6139, 6140, 6141, 6142, 6143, 6144, 6145, 6146, 6147, 6148, 6149, 6150, 6151, 6152, 6153, 6154, 6155, 6156, 6157, 6158, 6159, 6160, 6161, 6162, 6163, 6164, 6165, 6166, 6167, 6168, 6169, 6170, 6171, 6172, 6173, 6174, 6175, 6176, 6177, 6178, 6179, 6180, 6181, 6182, 6183, 6184, 6185, 6186, 6187, 6188, 6189, 6190, 6191, 6192, 6193, 6194, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 6202, 6203, 6204, 6205, 6206, 6207, 6208, 6209, 6210, 6211, 6212, 6213, 6214, 6215, 6216, 6217, 6218, 6219, 6220, 6221, 6222, 6223, 6224, 6225, 6226, 6227, 6228, 6229, 6230, 6231, 6232, 6233, 6234, 6235, 6236, 6237, 6238, 6239, 6240, 6241, 6242, 6243, 6244, 6245, 6246, 6247, 6248, 6249, 6250, 6251, 6252, 6253, 6254, 6255, 6256, 6257, 6258, 6259, 6260, 6261, 6262, 6263, 6264, 6265, 6266, 6267, 6268, 6269, 6270, 6271, 6272, 6273, 6274, 6275, 6276, 6277, 6278, 6279, 6280, 6281, 6282, 6283, 6284, 6285, 6286, 6287, 6288, 6289, 6290, 6291, 6292, 6293, 6294, 6295, 6296, 6297, 6298, 6299, 6300, 6301, 6302, 6303, 6304, 6305, 6306, 6307, 6308, 6309, 6310, 6311, 6312, 6313, 6314, 6315, 6316, 6317, 6318, 6319, 6320, 6321, 6322, 6323, 6324, 6325, 6326, 6327, 6328, 6329, 6330, 6331, 6332, 6333, 6334, 6335, 6336, 6337, 6338, 6339, 6340, 6341, 6342, 6343, 6344, 6345, 6346, 6347, 6348, 6349, 6350, 6351, 6352, 6353, 6354, 6355, 6356, 6357, 6358, 6359, 6360, 6361, 6362, 6363, 6364, 6365, 6366, 6367, 6368, 6369, 6370, 6371, 6372, 6373, 6374, 6375, 6376, 6377, 6378, 6379, 6380, 6381, 6382, 6383, 6384, 6385, 6386, 6387, 6388, 6389, 6390, 6391, 6392, 6393, 6394, 6395, 6396, 6397, 6398, 6399, 6400, 6401, 6402, 6403, 6404, 6405, 6406, 6407, 6408, 6409, 6410, 6411, 6412, 6413, 6414, 6415, 6416, 6417, 6418, 6419, 6420, 6421, 6422, 6423, 6424, 6425, 6426, 6427, 6428, 6429, 6430, 6431, 6432, 6433, 6434, 6435, 6436, 6437, 6438, 6439, 6440, 6441, 6442, 6443, 6444, 6445, 6446, 6447, 6448, 6449, 6450, 6451, 6452, 6453, 6454, 6455, 6456, 6457, 6458, 6459, 6460, 6461, 6462, 6463, 6464, 6465, 6466, 6467, 6468, 6469, 6470, 6471, 6472, 6473, 6474, 6475, 6476, 6477, 6478, 6479, 6480, 6481, 6482, 6483, 6484, 6485, 6486, 6487, 6488, 6489, 6490, 6491, 6492, 6493, 6494, 6495, 6496, 6497, 6498, 6499, 6500, 6501, 6502, 6503, 6504, 6505, 6506, 6507, 6508, 6509, 6510, 6511, 6512, 6513, 6514, 6515, 6516, 6517, 6518, 6519, 6520, 6521, 6522, 6523, 6524, 6525, 6526, 6527, 6528, 6529, 6530, 6531, 6532, 6533, 6534, 6535, 6536, 6537, 6538, 6539, 6540, 6541, 6542, 6543, 6544, 6545, 6546, 6547, 6548, 6549, 6550, 6551, 6552, 6553, 6554, 6555, 6556, 6557, 6558, 6559, 6560, 6561, 6562, 6563, 6564, 6565, 6566, 6567, 6568, 6569, 6570, 6571, 6572, 6573, 6574, 6575, 6576, 6577, 6578, 6579, 6580, 6581, 6582, 6583, 6584, 6585, 6586, 6587, 6588, 6589, 6590, 6591, 6592, 6593, 6594, 6595, 6596, 6597, 6598, 6599, 6600, 6601, 6602, 6603, 6604, 6605, 6606, 6607, 6608, 6609, 6610, 6611, 6612, 6613, 6614, 6615, 6616, 6617, 6618, 6619, 6620, 6621, 6622, 6623, 6624, 6625, 6626, 6627, 6628, 6629, 6630, 6631, 6632, 6633, 6634, 6635, 6636, 6637, 6638, 6639, 6640, 6641, 6642, 6643, 6644, 6645, 6646, 6647, 6648, 6649, 6650, 6651, 6652, 6653, 6654, 6655, 6656, 6657, 6658, 6659, 6660, 6661, 6662, 6663, 6664, 6665, 6666, 6667, 6668, 6669, 6670, 6671, 6672, 6673, 6674, 6675, 6676, 6677, 6678, 6679, 6680, 6681, 6682, 6683, 6684, 6685, 6686, 6687, 6688, 6689, 6690, 6691, 6692, 6693, 6694, 6695, 6696, 6697, 6698, 6699, 6700, 6701, 6702, 6703, 6704, 6705, 6706, 6707, 6708, 6709, 6710, 6711, 6712, 6713, 6714, 6715, 6716, 6717, 6718, 6719, 6720, 6721, 6722, 6723, 6724, 6725, 6726, 6727, 6728, 6729, 6730, 6731, 6732, 6733, 6734, 6735, 6736, 6737, 6738, 6739, 6740, 6741, 6742, 6743, 6744, 6745, 6746, 6747, 6748, 6749, 6750, 6751, 6752, 6753, 6754, 6755, 6756, 6757, 6758, 6759, 6760, 6761, 6762, 6763, 6764, 6765, 6766, 6767, 6768, 6769, 6770, 6771, 6772, 6773, 6774, 6775, 6776, 6777, 6778, 6779, 6780, 6781, 6782, 6783, 6784, 6785, 6786, 6787, 6788, 6789, 6790, 6791, 6792, 6793, 6794, 6795, 6796, 6797, 6798, 6799, 6800, 6801, 6802, 6803, 6804, 6805, 6806, 6807, 6808, 6809, 6810, 6811, 6812, 6813, 6814, 6815, 6816, 6817, 6818, 6819, 6820, 6821, 6822, 6823, 6824, 6825, 6826, 6827, 6828, 6829, 6830, 6831, 6832, 6833, 6834, 6835, 6836, 6837, 6838, 6839, 6840, 6841, 6842, 6843, 6844, 6845, 6846, 6847, 6848, 6849, 6850, 6851, 6852, 6853, 6854, 6855, 6856, 6857, 6858, 6859, 6860, 6861, 6862, 6863, 6864, 6865, 6866, 6867, 6868, 6869, 6870, 6871, 6872, 6873, 6874, 6875, 6876, 6877, 6878, 6879, 6880, 6881, 6882, 6883, 6884, 6885, 6886, 6887, 6888, 6889, 6890, 6891, 6892, 6893, 6894, 6895, 6896, 6897, 6898, 6899, 6900, 6901, 6902, 6903, 6904, 6905, 6906, 6907, 6908, 6909, 6910, 6911, 6912, 6913, 6914, 6915, 6916, 6917, 6918, 6919, 6920, 6921, 6922, 6923, 6924, 6925, 6926, 6927, 6928, 6929, 6930, 6931, 6932, 6933, 6934, 6935, 6936, 6937, 6938, 6939, 6940, 6941, 6942, 6943, 6944, 6945, 6946, 6947, 6948, 6949, 6950, 6951, 6952, 6953, 6954, 6955, 6956, 6957, 6958, 6959, 6960, 6961, 6962, 6963, 6964, 6965, 6966, 6967, 6968, 6969, 6970, 6971, 6972, 6973, 6974, 6975, 6976, 6977, 6978, 6979, 6980, 6981, 6982, 6983, 6984, 6985, 6986, 6987, 6988, 6989, 6990, 6991, 6992, 6993, 6994, 6995, 6996, 6997, 6998, 6999, 7000, 7001, 7002, 7003, 7004, 7005, 7006, 7007, 7008, 7009, 7010, 7011, 7012, 7013, 7014, 7015, 7016, 7017, 7018, 7019, 7020, 7021, 7022, 7023, 7024, 7025, 7026, 7027, 7028, 7029, 7030, 7031, 7032, 7033, 7034, 7035, 7036, 7037, 7038, 7039, 7040, 7041, 7042, 7043, 7044, 7045, 7046, 7047, 7048, 7049, 7050, 7051, 7052, 7053, 7054, 7055, 7056, 7057, 7058, 7059, 7060, 7061, 7062, 7063, 7064, 7065, 7066, 7067, 7068, 7069, 7070, 7071, 7072, 7073, 7074, 7075, 7076, 7077, 7078, 7079, 7080, 7081, 7082, 7083, 7084, 7085, 7086, 7087, 7088, 7089, 7090, 7091, 7092, 7093, 7094, 7095, 7096, 7097, 7098, 7099, 7100, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7108, 7109, 7110, 7111, 7112, 7113, 7114, 7115, 7116, 7117, 7118, 7119, 7120, 7121, 7122, 7123, 7124, 7125, 7126, 7127, 7128, 7129, 7130, 7131, 7132, 7133, 7134, 7135, 7136, 7137, 7138, 7139, 7140, 7141, 7142, 7143, 7144, 7145, 7146, 7147, 7148, 7149, 7150, 7151, 7152, 7153, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7161, 7162, 7163, 7164, 7165, 7166, 7167, 7168, 7169, 7170, 7171, 7172, 7173, 7174, 7175, 7176, 7177, 7178, 7179, 7180, 7181, 7182, 7183, 7184, 7185, 7186, 7187, 7188, 7189, 7190, 7191, 7192, 7193, 7194, 7195, 7196, 7197, 7198, 7199, 7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209, 7210, 7211, 7212, 7213, 7214, 7215, 7216, 7217, 7218, 7219, 7220, 7221, 7222, 7223, 7224, 7225, 7226, 7227, 7228, 7229, 7230, 7231, 7232, 7233, 7234, 7235, 7236, 7237, 7238, 7239, 7240, 7241, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249, 7250, 7251, 7252, 7253, 7254, 7255, 7256, 7257, 7258, 7259, 7260, 7261, 7262, 7263, 7264, 7265, 7266, 7267, 7268, 7269, 7270, 7271, 7272, 7273, 7274, 7275, 7276, 7277, 7278, 7279, 7280, 7281, 7282, 7283, 7284, 7285, 7286, 7287, 7288, 7289, 7290, 7291, 7292, 7293, 7294, 7295, 7296, 7297, 7298, 7299, 7300, 7301, 7302, 7303, 7304, 7305, 7306, 7307, 7308, 7309, 7310, 7311, 7312, 7313, 7314, 7315, 7316, 7317, 7318, 7319, 7320, 7321, 7322, 7323, 7324, 7325, 7326, 7327, 7328, 7329, 7330, 7331, 7332, 7333, 7334, 7335, 7336, 7337, 7338, 7339, 7340, 7341, 7342, 7343, 7344, 7345, 7346, 7347, 7348, 7349, 7350, 7351, 7352, 7353, 7354, 7355, 7356, 7357, 7358, 7359, 7360, 7361, 7362, 7363, 7364, 7365, 7366, 7367, 7368, 7369, 7370, 7371, 7372, 7373, 7374, 7375, 7376, 7377, 7378, 7379, 7380, 7381, 7382, 7383, 7384, 7385, 7386, 7387, 7388, 7389, 7390, 7391, 7392, 7393, 7394, 7395, 7396, 7397, 7398, 7399, 7400, 7401, 7402, 7403, 7404, 7405, 7406, 7407, 7408, 7409, 7410, 7411, 7412, 7413, 7414, 7415, 7416, 7417, 7418, 7419, 7420, 7421, 7422, 7423, 7424, 7425, 7426, 7427, 7428, 7429, 7430, 7431, 7432, 7433, 7434, 7435, 7436, 7437, 7438, 7439, 7440, 7441, 7442, 7443, 7444, 7445, 7446, 7447, 7448, 7449, 7450, 7451, 7452, 7453, 7454, 7455, 7456, 7457, 7458, 7459, 7460, 7461, 7462, 7463, 7464, 7465, 7466, 7467, 7468, 7469, 7470, 7471, 7472, 7473, 7474, 7475, 7476, 7477, 7478, 7479, 7480, 7481, 7482, 7483, 7484, 7485, 7486, 7487, 7488, 7489, 7490, 7491, 7492, 7493, 7494, 7495, 7496, 7497, 7498, 7499, 7500, 7501, 7502, 7503, 7504, 7505, 7506, 7507, 7508, 7509, 7510, 7511, 7512, 7513, 7514, 7515, 7516, 7517, 7518, 7519, 7520, 7521, 7522, 7523, 7524, 7525, 7526, 7527, 7528, 7529, 7530, 7531, 7532, 7533, 7534, 7535, 7536, 7537, 7538, 7539, 7540, 7541, 7542, 7543, 7544, 7545, 7546, 7547, 7548, 7549, 7550, 7551, 7552, 7553, 7554, 7555, 7556, 7557, 7558, 7559, 7560, 7561, 7562, 7563, 7564, 7565, 7566, 7567, 7568, 7569, 7570, 7571, 7572, 7573, 7574, 7575, 7576, 7577, 7578, 7579, 7580, 7581, 7582, 7583, 7584, 7585, 7586, 7587, 7588, 7589, 7590, 7591, 7592, 7593, 7594, 7595, 7596, 7597, 7598, 7599, 7600, 7601, 7602, 7603, 7604, 7605, 7606, 7607, 7608, 7609, 7610, 7611, 7612, 7613, 7614, 7615, 7616, 7617, 7618, 7619, 7620, 7621, 7622, 7623, 7624, 7625, 7626, 7627, 7628, 7629, 7630, 7631, 7632, 7633, 7634, 7635, 7636, 7637, 7638, 7639, 7640, 7641, 7642, 7643, 7644, 7645, 7646, 7647, 7648, 7649, 7650, 7651, 7652, 7653, 7654, 7655, 7656, 7657, 7658, 7659, 7660, 7661, 7662, 7663, 7664, 7665, 7666, 7667, 7668, 7669, 7670, 7671, 7672, 7673, 7674, 7675, 7676, 7677, 7678, 7679, 7680, 7681, 7682, 7683, 7684, 7685, 7686, 7687, 7688, 7689, 7690, 7691, 7692, 7693, 7694, 7695, 7696, 7697, 7698, 7699, 12100, 12101, 12102, 12103, 12104, 12105, 12106, 12107, 12108, 12109, 12110, 12111, 12112, 12113, 12114, 12115, 12116, 12117, 12118, 12119, 12120, 12121, 12122, 12123, 12124, 12125, 12126, 12127, 12128, 12129, 12130, 12131, 12132, 12133, 12134, 12135, 12136, 12137, 12138, 12139, 12140, 12141, 12142, 12143, 12144, 12145, 12146, 12147, 12148, 12149, 12150, 12151, 12152, 12153, 12154, 12155, 12156, 12157, 12158, 12159, 12160, 12161, 12162, 12163, 12164, 12165, 12166, 12167, 12168, 12169, 12170, 12171, 12172, 12173, 12174, 12175, 12176, 12177, 12178, 12179, 12180, 12181, 12182, 12183, 12184, 12185, 12186, 12187, 12188, 12189, 12190, 12191, 12192, 12193, 12194, 12195, 12196, 12197, 12198, 12199, 12200, 12201, 12202, 12203, 12204, 12205, 12206, 12207, 12208, 12209, 12210, 12211, 12212, 12213, 12214, 12215, 12216, 12217, 12218, 12219, 12220, 12221, 12222, 12223, 12224, 12225, 12226, 12227, 12228, 12229, 12230, 12231, 12232, 12233, 12234, 12235, 12236, 12237, 12238, 12239, 12240, 12241, 12242, 12243, 12244, 12245, 12246, 12247, 12248, 12249, 12250, 12251, 12252, 12253, 12254, 12255, 12256, 12257, 12258, 12259, 12260, 12261, 12262, 12263, 12264, 12265, 12266, 12267, 12268, 12269, 12270, 12271, 12272, 12273, 12274, 12275, 12276, 12277, 12278, 12279, 12280, 12281, 12282, 12283, 12284, 12285, 12286, 12287, 12288, 12289, 12290, 12291, 12292, 12293, 12294, 12295, 12296, 12297, 12298, 12299, 12300, 12301, 12302, 12303, 12304, 12305, 12306, 12307, 12308, 12309, 12310, 12311, 12312, 12313, 12314, 12315, 12316, 12317, 12318, 12319, 12320, 12321, 12322, 12323, 12324, 12325, 12326, 12327, 12328, 12329, 12330, 12331, 12332, 12333, 12334, 12335, 12336, 12337, 12338, 12339, 12340, 12341, 12342, 12343, 12344, 12345, 12346, 12347, 12348, 12349, 12350, 12351, 12352, 12353, 12354, 12355, 12356, 12357, 12358, 12359, 12360, 12361, 12362, 12363, 12364, 12365, 12366, 12367, 12368, 12369, 12370, 12371, 12372, 12373, 12374, 12375, 12376, 12377, 12378, 12379, 12380, 12381, 12382, 12383, 12384, 12385, 12386, 12387, 12388, 12389, 12390, 12391, 12392, 12393, 12394, 12395, 12396, 12397, 12398, 12399, 12400, 12401, 12402, 12403, 12404, 12405, 12406, 12407, 12408, 12409, 12410, 12411, 12412, 12413, 12414, 12415, 12416, 12417, 12418, 12419, 12420, 12421, 12422, 12423, 12424, 12425, 12426, 12427, 12428, 12429, 12430, 12431, 12432, 12433, 12434, 12435, 12436, 12437, 12438, 12439, 12440, 12441, 12442, 12443, 12444, 12445, 12446, 12447, 12448, 12449, 12450, 12451, 12452, 12453, 12454, 12455, 12456, 12457, 12458, 12459, 12460, 12461, 12462, 12463, 12464, 12465, 12466, 12467, 12468, 12469, 12470, 12471, 12472, 12473, 12474, 12475, 12476, 12477, 12478, 12479, 12480, 12481, 12482, 12483, 12484, 12485, 12486, 12487, 12488, 12489, 12490, 12491, 12492, 12493, 12494, 12495, 12496, 12497, 12498, 12499, 12500, 12501, 12502, 12503, 12504, 12505, 12506, 12507, 12508, 12509, 12510, 12511, 12512, 12513, 12514, 12515, 12516, 12517, 12518, 12519, 12520, 12521, 12522, 12523, 12524, 12525, 12526, 12527, 12528, 12529, 12530, 12531, 12532, 12533, 12534, 12535, 12536, 12537, 12538, 12539, 12540, 12541, 12542, 12543, 12544, 12545, 12546, 12547, 12548, 12549, 12550, 12551, 12552, 12553, 12554, 12555, 12556, 12557, 12558, 12559, 12560, 12561, 12562, 12563, 12564, 12565, 12566, 12567, 12568, 12569, 12570, 12571, 12572, 12573, 12574, 12575, 12576, 12577, 12578, 12579, 12580, 12581, 12582, 12583, 12584, 12585, 12586, 12587, 12588, 12589, 12590, 12591, 12592, 12593, 12594, 12595, 12596, 12597, 12598, 12599, 12600, 12601, 12602, 12603, 12604, 12605, 12606, 12607, 12608, 12609, 12610, 12611, 12612, 12613, 12614, 12615, 12616, 12617, 12618, 12619, 12620, 12621, 12622, 12623, 12624, 12625, 12626, 12627, 12628, 12629, 12630, 12631, 12632, 12633, 12634, 12635, 12636, 12637, 12638, 12639, 12640, 12641, 12642, 12643, 12644, 12645, 12646, 12647, 12648, 12649, 12650, 12651, 12652, 12653, 12654, 12655, 12656, 12657, 12658, 12659, 12660, 12661, 12662, 12663, 12664, 12665, 12666, 12667, 12668, 12669, 12670, 12671, 12672, 12673, 12674, 12675, 12676, 12677, 12678, 12679, 12680, 12681, 12682, 12683, 12684, 12685, 12686, 12687, 12688, 12689, 12690, 12691, 12692, 12693, 12694, 12695, 12696, 12697, 12698, 12699, 12700, 12701, 12702, 12703, 12704, 12705, 12706, 12707, 12708, 12709, 12710, 12711, 12712, 12713, 12714, 12715, 12716, 12717, 12718, 12719, 12720, 12721, 12722, 12723, 12724, 12725, 12726, 12727, 12728, 12729, 12730, 12731, 12732, 12733, 12734, 12735, 12736, 12737, 12738, 12739, 12740, 12741, 12742, 12743, 12744, 12745, 12746, 12747, 12748, 12749, 12750, 12751, 12752, 12753, 12754, 12755, 12756, 12757, 12758, 12759, 12760, 12761, 12762, 12763, 12764, 12765, 12766, 12767, 12768, 12769, 12770, 12771, 12772, 12773, 12774, 12775, 12776, 12777, 12778, 12779, 12780, 12781, 12782, 12783, 12784, 12785, 12786, 12787, 12788, 12789, 12790, 12791, 12792, 12793, 12794, 12795, 12796, 12797, 12798, 12799, 12800, 12801, 12802, 12803, 12804, 12805, 12806, 12807, 12808, 12809, 12810, 12811, 12812, 12813, 12814, 12815, 12816, 12817, 12818, 12819, 12820, 12821, 12822, 12823, 12824, 12825, 12826, 12827, 12828, 12829, 12830, 12831, 12832, 12833, 12834, 12835, 12836, 12837, 12838, 12839, 12840, 12841, 12842, 12843, 12844, 12845, 12846, 12847, 12848, 12849, 12850, 12851, 12852, 12853, 12854, 12855, 12856, 12857, 12858, 12859, 12860, 12861, 12862, 12863, 12864, 12865, 12866, 12867, 12868, 12869, 12870, 12871, 12872, 12873, 12874, 12875, 12876, 12877, 12878, 12879, 12880, 12881, 12882, 12883, 12884, 12885, 12886, 12887, 12888, 12889, 12890, 12891, 12892, 12893, 12894, 12895, 12896, 12897, 12898, 12899, 12900, 12901, 12902, 12903, 12904, 12905, 12906, 12907, 12908, 12909, 12910, 12911, 12912, 12913, 12914, 12915, 12916, 12917, 12918, 12919, 12920, 12921, 12922, 12923, 12924, 12925, 12926, 12927, 12928, 12929, 12930, 12931, 12932, 12933, 12934, 12935, 12936, 12937, 12938, 12939, 12940, 12941, 12942, 12943, 12944, 12945, 12946, 12947, 12948, 12949, 12950, 12951, 12952, 12953, 12954, 12955, 12956, 12957, 12958, 12959, 12960, 12961, 12962, 12963, 12964, 12965, 12966, 12967, 12968, 12969, 12970, 12971, 12972, 12973, 12974, 12975, 12976, 12977, 12978, 12979, 12980, 12981, 12982, 12983, 12984, 12985, 12986, 12987, 12988, 12989, 12990, 12991, 12992, 12993, 12994, 12995, 12996, 12997, 12998, 12999, 13000, 13001, 13002, 13003, 13004, 13005, 13006, 13007, 13008, 13009, 13010, 13011, 13012, 13013, 13014, 13015, 13016, 13017, 13018, 13019, 13020, 13021, 13022, 13023, 13024, 13025, 13026, 13027, 13028, 13029, 13030, 13031, 13032, 13033, 13034, 13035, 13036, 13037, 13038, 13039, 13040, 13041, 13042, 13043, 13044, 13045, 13046, 13047, 13048, 13049, 13050, 13051, 13052, 13053, 13054, 13055, 13056, 13057, 13058, 13059, 13060, 13061, 13062, 13063, 13064, 13065, 13066, 13067, 13068, 13069, 13070, 13071, 13072, 13073, 13074, 13075, 13076, 13077, 13078, 13079, 13080, 13081, 13082, 13083, 13084, 13085, 13086, 13087, 13088, 13089, 13090, 13091, 13092, 13093, 13094, 13095, 13096, 13097, 13098, 13099, 13100, 13101, 13102, 13103, 13104, 13105, 13106, 13107, 13108, 13109, 13110, 13111, 13112, 13113, 13114, 13115, 13116, 13117, 13118, 13119, 13120, 13121, 13122, 13123, 13124, 13125, 13126, 13127, 13128, 13129, 13130, 13131, 13132, 13133, 13134, 13135, 13136, 13137, 13138, 13139, 13140, 13141, 13142, 13143, 13144, 13145, 13146, 13147, 13148, 13149, 13150, 13151, 13152, 13153, 13154, 13155, 13156, 13157, 13158, 13159, 13160, 13161, 13162, 13163, 13164, 13165, 13166, 13167, 13168, 13169, 13170, 13171, 13172, 13173, 13174, 13175, 13176, 13177, 13178, 13179, 13180, 13181, 13182, 13183, 13184, 13185, 13186, 13187, 13188, 13189, 13190, 13191, 13192, 13193, 13194, 13195, 13196, 13197, 13198, 13199, 13200, 13201, 13202, 13203, 13204, 13205, 13206, 13207, 13208, 13209, 13210, 13211, 13212, 13213, 13214, 13215, 13216, 13217, 13218, 13219, 13220, 13221, 13222, 13223, 13224, 13225, 13226, 13227, 13228, 13229, 13230, 13231, 13232, 13233, 13234, 13235, 13236, 13237, 13238, 13239, 13240, 13241, 13242, 13243, 13244, 13245, 13246, 13247, 13248, 13249, 13250, 13251, 13252, 13253, 13254, 13255, 13256, 13257, 13258, 13259, 13260, 13261, 13262, 13263, 13264, 13265, 13266, 13267, 13268, 13269, 13270, 13271, 13272, 13273, 13274, 13275, 13276, 13277, 13278, 13279, 13280, 13281, 13282, 13283, 13284, 13285, 13286, 13287, 13288, 13289, 13290, 13291, 13292, 13293, 13294, 13295, 13296, 13297, 13298, 13299, 13300, 13301, 13302, 13303, 13304, 13305, 13306, 13307, 13308, 13309, 13310, 13311, 13312, 13313, 13314, 13315, 13316, 13317, 13318, 13319, 13320, 13321, 13322, 13323, 13324, 13325, 13326, 13327, 13328, 13329, 13330, 13331, 13332, 13333, 13334, 13335, 13336, 13337, 13338, 13339, 13340, 13341, 13342, 13343, 13344, 13345, 13346, 13347, 13348, 13349, 13350, 13351, 13352, 13353, 13354, 13355, 13356, 13357, 13358, 13359, 13360, 13361, 13362, 13363, 13364, 13365, 13366, 13367, 13368, 13369, 13370, 13371, 13372, 13373, 13374, 13375, 13376, 13377, 13378, 13379, 13380, 13381, 13382, 13383, 13384, 13385, 13386, 13387, 13388, 13389, 13390, 13391, 13392, 13393, 13394, 13395, 13396, 13397, 13398, 13399, 13400, 13401, 13402, 13403, 13404, 13405, 13406, 13407, 13408, 13409, 13410, 13411, 13412, 13413, 13414, 13415, 13416, 13417, 13418, 13419, 13420, 13421, 13422, 13423, 13424, 13425, 13426, 13427, 13428, 13429, 13430, 13431, 13432, 13433, 13434, 13435, 13436, 13437, 13438, 13439, 13440, 13441, 13442, 13443, 13444, 13445, 13446, 13447, 13448, 13449, 13450, 13451, 13452, 13453, 13454, 13455, 13456, 13457, 13458, 13459, 13460, 13461, 13462, 13463, 13464, 13465, 13466, 13467, 13468, 13469, 13470, 13471, 13472, 13473, 13474, 13475, 13476, 13477, 13478, 13479, 13480, 13481, 13482, 13483, 13484, 13485, 13486, 13487, 13488, 13489, 13490, 13491, 13492, 13493, 13494, 13495, 13496, 13497, 13498, 13499, 13500, 13501, 13502, 13503, 13504, 13505, 13506, 13507, 13508, 13509, 13510, 13511, 13512, 13513, 13514, 13515, 13516, 13517, 13518, 13519, 13520, 13521, 13522, 13523, 13524, 13525, 13526, 13527, 13528, 13529, 13530, 13531, 13532, 13533, 13534, 13535, 13536, 13537, 13538, 13539, 13540, 13541, 13542, 13543, 13544, 13545, 13546, 13547, 13548, 13549, 13550, 13551, 13552, 13553, 13554, 13555, 13556, 13557, 13558, 13559, 13560, 13561, 13562, 13563, 13564, 13565, 13566, 13567, 13568, 13569, 13570, 13571, 13572, 13573, 13574, 13575, 13576, 13577, 13578, 13579, 13580, 13581, 13582, 13583, 13584, 13585, 13586, 13587, 13588, 13589, 13590, 13591, 13592, 13593, 13594, 13595, 13596, 13597, 13598, 13599, 13600, 13601, 13602, 13603, 13604, 13605, 13606, 13607, 13608, 13609, 13610, 13611, 13612, 13613, 13614, 13615, 13616, 13617, 13618, 13619, 13620, 13621, 13622, 13623, 13624, 13625, 13626, 13627, 13628, 13629, 13630, 13631, 13632, 13633, 13634, 13635, 13636, 13637, 13638, 13639, 13640, 13641, 13642, 13643, 13644, 13645, 13646, 13647, 13648, 13649, 13650, 13651, 13652, 13653, 13654, 13655, 13656, 13657, 13658, 13659, 13660, 13661, 13662, 13663, 13664, 13665, 13666, 13667, 13668, 13669, 13670, 13671, 13672, 13673, 13674, 13675, 13676, 13677, 13678, 13679, 13680, 13681, 13682, 13683, 13684, 13685, 13686, 13687, 13688, 13689, 13690, 13691, 13692, 13693, 13694, 13695, 13696, 13697, 13698, 13699, 13700, 13701, 13702, 13703, 13704, 13705, 13706, 13707, 13708, 13709, 13710, 13711, 13712, 13713, 13714, 13715, 13716, 13717, 13718, 13719, 13720, 13721, 13722, 13723, 13724, 13725, 13726, 13727, 13728, 13729, 13730, 13731, 13732, 13733, 13734, 13735, 13736, 13737, 13738, 13739, 13740, 13741, 13742, 13743, 13744, 13745, 13746, 13747, 13748, 13749, 13750, 13751, 13752, 13753, 13754, 13755, 13756, 13757, 13758, 13759, 13760, 13761, 13762, 13763, 13764, 13765, 13766, 13767, 13768, 13769, 13770, 13771, 13772, 13773, 13774, 13775, 13776, 13777, 13778, 13779, 13780, 13781, 13782, 13783, 13784, 13785, 13786, 13787, 13788, 13789, 13790, 13791, 13792, 13793, 13794, 13795, 13796, 13797, 13798, 13799, 13800, 13801, 13802, 13803, 13804, 13805, 13806, 13807, 13808, 13809, 13810, 13811, 13812, 13813, 13814, 13815, 13816, 13817, 13818, 13819, 13820, 13821, 13822, 13823, 13824, 13825, 13826, 13827, 13828, 13829, 13830, 13831, 13832, 13833, 13834, 13835, 13836, 13837, 13838, 13839, 13840, 13841, 13842, 13843, 13844, 13845, 13846, 13847, 13848, 13849, 13850, 13851, 13852, 13853, 13854, 13855, 13856, 13857, 13858, 13859, 13860, 13861, 13862, 13863, 13864, 13865, 13866, 13867, 13868, 13869, 13870, 13871, 13872, 13873, 13874, 13875, 13876, 13877, 13878, 13879, 13880, 13881, 13882, 13883, 13884, 13885, 13886, 13887, 13888, 13889, 13890, 13891, 13892, 13893, 13894, 13895, 13896, 13897, 13898, 13899, 13900, 13901, 13902, 13903, 13904, 13905, 13906, 13907, 13908, 13909, 13910, 13911, 13912, 13913, 13914, 13915, 13916, 13917, 13918, 13919, 13920, 13921, 13922, 13923, 13924, 13925, 13926, 13927, 13928, 13929, 13930, 13931, 13932, 13933, 13934, 13935, 13936, 13937, 13938, 13939, 13940, 13941, 13942, 13943, 13944, 13945, 13946, 13947, 13948, 13949, 13950, 13951, 13952, 13953, 13954, 13955, 13956, 13957, 13958, 13959, 13960, 13961, 13962, 13963, 13964, 13965, 13966, 13967, 13968, 13969, 13970, 13971, 13972, 13973, 13974, 13975, 13976, 13977, 13978, 13979, 13980, 13981, 13982, 13983, 13984, 13985, 13986, 13987, 13988, 13989, 13990, 13991, 13992, 13993, 13994, 13995, 13996, 13997, 13998, 13999, 14000, 14001, 14002, 14003, 14004, 14005, 14006, 14007, 14008, 14009, 14010, 14011, 14012, 14013, 14014, 14015, 14016, 14017, 14018, 14019, 14020, 14021, 14022, 14023, 14024, 14025, 14026, 14027, 14028, 14029, 14030, 14031, 14032, 14033, 14034, 14035, 14036, 14037, 14038, 14039, 14040, 14041, 14042, 14043, 14044, 14045, 14046, 14047, 14048, 14049, 14050, 14051, 14052, 14053, 14054, 14055, 14056, 14057, 14058, 14059, 14060, 14061, 14062, 14063, 14064, 14065, 14066, 14067, 14068, 14069, 14070, 14071, 14072, 14073, 14074, 14075, 14076, 14077, 14078, 14079, 14080, 14081, 14082, 14083, 14084, 14085, 14086, 14087, 14088, 14089, 14090, 14091, 14092, 14093, 14094, 14095, 14096, 14097, 14098, 14099, 14100, 14101, 14102, 14103, 14104, 14105, 14106, 14107, 14108, 14109, 14110, 14111, 14112, 14113, 14114, 14115, 14116, 14117, 14118, 14119, 14120, 14121, 14122, 14123, 14124, 14125, 14126, 14127, 14128, 14129, 14130, 14131, 14132, 14133, 14134, 14135, 14136, 14137, 14138, 14139, 14140, 14141, 14142, 14143, 14144, 14145, 14146, 14147, 14148, 14149, 14150, 14151, 14152, 14153, 14154, 14155, 14156, 14157, 14158, 14159, 14160, 14161, 14162, 14163, 14164, 14165, 14166, 14167, 14168, 14169, 14170, 14171, 14172, 14173, 14174, 14175, 14176, 14177, 14178, 14179, 14180, 14181, 14182, 14183, 14184, 14185, 14186, 14187, 14188, 14189, 14190, 14191, 14192, 14193, 14194, 14195, 14196, 14197, 14198, 14199, 14200, 14201, 14202, 14203, 14204, 14205, 14206, 14207, 14208, 14209, 14210, 14211, 14212, 14213, 14214, 14215, 14216, 14217, 14218, 14219, 14220, 14221, 14222, 14223, 14224, 14225, 14226, 14227, 14228, 14229, 14230, 14231, 14232, 14233, 14234, 14235, 14236, 14237, 14238, 14239, 14240, 14241, 14242, 14243, 14244, 14245, 14246, 14247, 14248, 14249, 14250, 14251, 14252, 14253, 14254, 14255, 14256, 14257, 14258, 14259, 14260, 14261, 14262, 14263, 14264, 14265, 14266, 14267, 14268, 14269, 14270, 14271, 14272, 14273, 14274, 14275, 14276, 14277, 14278, 14279, 14280, 14281, 14282, 14283, 14284, 14285, 14286, 14287, 14288, 14289, 14290, 14291, 14292, 14293, 14294, 14295, 14296, 14297, 14298, 14299, 14300, 14301, 14302, 14303, 14304, 14305, 14306, 14307, 14308, 14309, 14310, 14311, 14312, 14313, 14314, 14315, 14316, 14317, 14318, 14319, 14320, 14321, 14322, 14323, 14324, 14325, 14326, 14327, 14328, 14329, 14330, 14331, 14332, 14333, 14334, 14335, 14336, 14337, 14338, 14339, 14340, 14341, 14342, 14343, 14344, 14345, 14346, 14347, 14348, 14349, 14350, 14351, 14352, 14353, 14354, 14355, 14356, 14357, 14358, 14359, 14360, 14361, 14362, 14363, 14364, 14365, 14366, 14367, 14368, 14369, 14370, 14371, 14372, 14373, 14374, 14375, 14376, 14377, 14378, 14379, 14380, 14381, 14382, 14383, 14384, 14385, 14386, 14387, 14388, 14389, 14390, 14391, 14392, 14393, 14394, 14395, 14396, 14397, 14398, 14399, 14400, 14401, 14402, 14403, 14404, 14405, 14406, 14407, 14408, 14409, 14410, 14411, 14412, 14413, 14414, 14415, 14416, 14417, 14418, 14419, 14420, 14421, 14422, 14423, 14424, 14425, 14426, 14427, 14428, 14429, 14430, 14431, 14432, 14433, 14434, 14435, 14436, 14437, 14438, 14439, 14440, 14441, 14442, 14443, 14444, 14445, 14446, 14447, 14448, 14449, 14450, 14451, 14452, 14453, 14454, 14455, 14456, 14457, 14458, 14459, 14460, 14461, 14462, 14463, 14464, 14465, 14466, 14467, 14468, 14469, 14470, 14471, 14472, 14473, 14474, 14475, 14476, 14477, 14478, 14479, 14480, 14481, 14482, 14483, 14484, 14485, 14486, 14487, 14488, 14489, 14490, 14491, 14492, 14493, 14494, 14495, 14496, 14497, 14498, 14499, 14500, 14501, 14502, 14503, 14504, 14505, 14506, 14507, 14508, 14509, 14510, 14511, 14512, 14513, 14514, 14515, 14516, 14517, 14518, 14519, 14520, 14521, 14522, 14523, 14524, 14525, 14526, 14527, 14528, 14529, 14530, 14531, 14532, 14533, 14534, 14535, 14536, 14537, 14538, 14539, 14540, 14541, 14542, 14543, 14544, 14545, 14546, 14547, 14548, 14549, 14550, 14551, 14552, 14553, 14554, 14555, 14556, 14557, 14558, 14559, 14560, 14561, 14562, 14563, 14564, 14565, 14566, 14567, 14568, 14569, 14570, 14571, 14572, 14573, 14574, 14575, 14576, 14577, 14578, 14579, 14580, 14581, 14582, 14583, 14584, 14585, 14586, 14587, 14588, 14589, 14590, 14591, 14592, 14593, 14594, 14595, 14596, 14597, 14598, 14599, 14600, 14601, 14602, 14603, 14604, 14605, 14606, 14607, 14608, 14609, 14610, 14611, 14612, 14613, 14614, 14615, 14616, 14617, 14618, 14619, 14620, 14621, 14622, 14623, 14624, 14625, 14626, 14627, 14628, 14629, 14630, 14631, 14632, 14633, 14634, 14635, 14636, 14637, 14638, 14639, 14640, 14641, 14642, 14643, 14644, 14645, 14646, 14647, 14648, 14649, 14650, 14651, 14652, 14653, 14654, 14655, 14656, 14657, 14658, 14659, 14660, 14661, 14662, 14663, 14664, 14665, 14666, 14667, 14668, 14669, 14670, 14671, 14672, 14673, 14674, 14675, 14676, 14677, 14678, 14679, 14680, 14681, 14682, 14683, 14684, 14685, 14686, 14687, 14688, 14689, 14690, 14691, 14692, 14693, 14694, 14695, 14696, 14697, 14698, 14699, 14700, 14701, 14702, 14703, 14704, 14705, 14706, 14707, 14708, 14709, 14710, 14711, 14712, 14713, 14714, 14715, 14716, 14717, 14718, 14719, 14720, 14721, 14722, 14723, 14724, 14725, 14726, 14727, 14728, 14729, 14730, 14731, 14732, 14733, 14734, 14735, 14736, 14737, 14738, 14739, 14740, 14741, 14742, 14743, 14744, 14745, 14746, 14747, 14748, 14749, 14750, 14751, 14752, 14753, 14754, 14755, 14756, 14757, 14758, 14759, 14760, 14761, 14762, 14763, 14764, 14765, 14766, 14767, 14768, 14769, 14770, 14771, 14772, 14773, 14774, 14775, 14776, 14777, 14778, 14779, 14780, 14781, 14782, 14783, 14784, 14785, 14786, 14787, 14788, 14789, 14790, 14791, 14792, 14793, 14794, 14795, 14796, 14797, 14798, 14799, 14800, 14801, 14802, 14803, 14804, 14805, 14806, 14807, 14808, 14809, 14810, 14811, 14812, 14813, 14814, 14815, 14816, 14817, 14818, 14819, 14820, 14821, 14822, 14823, 14824, 14825, 14826, 14827, 14828, 14829, 14830, 14831, 14832, 14833, 14834, 14835, 14836, 14837, 14838, 14839, 14840, 14841, 14842, 14843, 14844, 14845, 14846, 14847, 14848, 14849, 14850, 14851, 14852, 14853, 14854, 14855, 14856, 14857, 14858, 14859, 14860, 14861, 14862, 14863, 14864, 14865, 14866, 14867, 14868, 14869, 14870, 14871, 14872, 14873, 14874, 14875, 14876, 14877, 14878, 14879, 14880, 14881, 14882, 14883, 14884, 14885, 14886, 14887, 14888, 14889, 14890, 14891, 14892, 14893, 14894, 14895, 14896, 14897, 14898, 14899, 14900, 14901, 14902, 14903, 14904, 14905, 14906, 14907, 14908, 14909, 14910, 14911, 14912, 14913, 14914, 14915, 14916, 14917, 14918, 14919, 14920, 14921, 14922, 14923, 14924, 14925, 14926, 14927, 14928, 14929, 14930, 14931, 14932, 14933, 14934, 14935, 14936, 14937, 14938, 14939, 14940, 14941, 14942, 14943, 14944, 14945, 14946, 14947, 14948, 14949, 14950, 14951, 14952, 14953, 14954, 14955, 14956, 14957, 14958, 14959, 14960, 14961, 14962, 14963, 14964, 14965, 14966, 14967, 14968, 14969, 14970, 14971, 14972, 14973, 14974, 14975, 14976, 14977, 14978, 14979, 14980, 14981, 14982, 14983, 14984, 14985, 14986, 14987, 14988, 14989, 14990, 14991, 14992, 14993, 14994, 14995, 14996, 14997, 14998, 14999, 15000, 15001, 15002, 15003, 15004, 15005, 15006, 15007, 15008, 15009, 15010, 15011, 15012, 15013, 15014, 15015, 15016, 15017, 15018, 15019, 15020, 15021, 15022, 15023, 15024, 15025, 15026, 15027, 15028, 15029, 15030, 15031, 15032, 15033, 15034, 15035, 15036, 15037, 15038, 15039, 15040, 15041, 15042, 15043, 15044, 15045, 15046, 15047, 15048, 15049, 15050, 15051, 15052, 15053, 15054, 15055, 15056, 15057, 15058, 15059, 15060, 15061, 15062, 15063, 15064, 15065, 15066, 15067, 15068, 15069, 15070, 15071, 15072, 15073, 15074, 15075, 15076, 15077, 15078, 15079, 15080, 15081, 15082, 15083, 15084, 15085, 15086, 15087, 15088, 15089, 15090, 15091, 15092, 15093, 15094, 15095, 15096, 15097, 15098, 15099, 15100, 15101, 15102, 15103, 15104, 15105, 15106, 15107, 15108, 15109, 15110, 15111, 15112, 15113, 15114, 15115, 15116, 15117, 15118, 15119, 15120, 15121, 15122, 15123, 15124, 15125, 15126, 15127, 15128, 15129, 15130, 15131, 15132, 15133, 15134, 15135, 15136, 15137, 15138, 15139, 15140, 15141, 15142, 15143, 15144, 15145, 15146, 15147, 15148, 15149, 15150, 15151, 15152, 15153, 15154, 15155, 15156, 15157, 15158, 15159, 15160, 15161, 15162, 15163, 15164, 15165, 15166, 15167, 15168, 15169, 15170, 15171, 15172, 15173, 15174, 15175, 15176, 15177, 15178, 15179, 15180, 15181, 15182, 15183, 15184, 15185, 15186, 15187, 15188, 15189, 15190, 15191, 15192, 15193, 15194, 15195, 15196, 15197, 15198, 15199, 15200, 15201, 15202, 15203, 15204, 15205, 15206, 15207, 15208, 15209, 15210, 15211, 15212, 15213, 15214, 15215, 15216, 15217, 15218, 15219, 15220, 15221, 15222, 15223, 15224, 15225, 15226, 15227, 15228, 15229, 15230, 15231, 15232, 15233, 15234, 15235, 15236, 15237, 15238, 15239, 15240, 15241, 15242, 15243, 15244, 15245, 15246, 15247, 15248, 15249, 15250, 15251, 15252, 15253, 15254, 15255, 15256, 15257, 15258, 15259, 15260, 15261, 15262, 15263, 15264, 15265, 15266, 15267, 15268, 15269, 15270, 15271, 15272, 15273, 15274, 15275, 15276, 15277, 15278, 15279, 15280, 15281, 15282, 15283, 15284, 15285, 15286, 15287, 15288, 15289, 15290, 15291, 15292, 15293, 15294, 15295, 15296, 15297, 15298, 15299, 15300, 15301, 15302, 15303, 15304, 15305, 15306, 15307, 15308, 15309, 15310, 15311, 15312, 15313, 15314, 15315, 15316, 15317, 15318, 15319, 15320, 15321, 15322, 15323, 15324, 15325, 15326, 15327, 15328, 15329, 15330, 15331, 15332, 15333, 15334, 15335, 15336, 15337, 15338, 15339, 15340, 15341, 15342, 15343, 15344, 15345, 15346, 15347, 15348, 15349, 15350, 15351, 15352, 15353, 15354, 15355, 15356, 15357, 15358, 15359, 15360, 15361, 15362, 15363, 15364, 15365, 15366, 15367, 15368, 15369, 15370, 15371, 15372, 15373, 15374, 15375, 15376, 15377, 15378, 15379, 15380, 15381, 15382, 15383, 15384, 15385, 15386, 15387, 15388, 15389, 15390, 15391, 15392, 15393, 15394, 15395, 15396, 15397, 15398, 15399, 15400, 15401, 15402, 15403, 15404, 15405, 15406, 15407, 15408, 15409, 15410, 15411, 15412, 15413, 15414, 15415, 15416, 15417, 15418, 15419, 15420, 15421, 15422, 15423, 15424, 15425, 15426, 15427, 15428, 15429, 15430, 15431, 15432, 15433, 15434, 15435, 15436, 15437, 15438, 15439, 15440, 15441, 15442, 15443, 15444, 15445, 15446, 15447, 15448, 15449, 15450, 15451, 15452, 15453, 15454, 15455, 15456, 15457, 15458, 15459, 15460, 15461, 15462, 15463, 15464, 15465, 15466, 15467, 15468, 15469, 15470, 15471, 15472, 15473, 15474, 15475, 15476, 15477, 15478, 15479, 15480, 15481, 15482, 15483, 15484, 15485, 15486, 15487, 15488, 15489, 15490, 15491, 15492, 15493, 15494, 15495, 15496, 15497, 15498, 15499, 15500, 15501, 15502, 15503, 15504, 15505, 15506, 15507, 15508, 15509, 15510, 15511, 15512, 15513, 15514, 15515, 15516, 15517, 15518, 15519, 15520, 15521, 15522, 15523, 15524, 15525, 15526, 15527, 15528, 15529, 15530, 15531, 15532, 15533, 15534, 15535, 15536, 15537, 15538, 15539, 15540, 15541, 15542, 15543, 15544, 15545, 15546, 15547, 15548, 15549, 15550, 15551, 15552, 15553, 15554, 15555, 15556, 15557, 15558, 15559, 15560, 15561, 15562, 15563, 15564, 15565, 15566, 15567, 15568, 15569, 15570, 15571, 15572, 15573, 15574, 15575, 15576, 15577, 15578, 15579, 15580, 15581, 15582, 15583, 15584, 15585, 15586, 15587, 15588, 15589, 15590, 15591, 15592, 15593, 15594, 15595, 15596, 15597, 15598, 15599, 15600, 15601, 15602, 15603, 15604, 15605, 15606, 15607, 15608, 15609, 15610, 15611, 15612, 15613, 15614, 15615, 15616, 15617, 15618, 15619, 15620, 15621, 15622, 15623, 15624, 15625, 15626, 15627, 15628, 15629, 15630, 15631, 15632, 15633, 15634, 15635, 15636, 15637, 15638, 15639, 15640, 15641, 15642, 15643, 15644, 15645, 15646, 15647, 15648, 15649, 15650, 15651, 15652, 15653, 15654, 15655, 15656, 15657, 15658, 15659, 15660, 15661, 15662, 15663, 15664, 15665, 15666, 15667, 15668, 15669, 15670, 15671, 15672, 15673, 15674, 15675, 15676, 15677, 15678, 15679, 15680, 15681, 15682, 15683, 15684, 15685, 15686, 15687, 15688, 15689, 15690, 15691, 15692, 15693, 15694, 15695, 15696, 15697, 15698, 15699, 15700, 15701, 15702, 15703, 15704, 15705, 15706, 15707, 15708, 15709, 15710, 15711, 15712, 15713, 15714, 15715, 15716, 15717, 15718, 15719, 15720, 15721, 15722, 15723, 15724, 15725, 15726, 15727, 15728, 15729, 15730, 15731, 15732, 15733, 15734, 15735, 15736, 15737, 15738, 15739, 15740, 15741, 15742, 15743, 15744, 15745, 15746, 15747, 15748, 15749, 15750, 15751, 15752, 15753, 15754, 15755, 15756, 15757, 15758, 15759, 15760, 15761, 15762, 15763, 15764, 15765, 15766, 15767, 15768, 15769, 15770, 15771, 15772, 15773, 15774, 15775, 15776, 15777, 15778, 15779, 15780, 15781, 15782, 15783, 15784, 15785, 15786, 15787, 15788, 15789, 15790, 15791, 15792, 15793, 15794, 15795, 15796, 15797, 15798, 15799, 15800, 15801, 15802, 15803, 15804, 15805, 15806, 15807, 15808, 15809, 15810, 15811, 15812, 15813, 15814, 15815, 15816, 15817, 15818, 15819, 15820, 15821, 15822, 15823, 15824, 15825, 15826, 15827, 15828, 15829, 15830, 15831, 15832, 15833, 15834, 15835, 15836, 15837, 15838, 15839, 15840, 15841, 15842, 15843, 15844, 15845, 15846, 15847, 15848, 15849, 15850, 15851, 15852, 15853, 15854, 15855, 15856, 15857, 15858, 15859, 15860, 15861, 15862, 15863, 15864, 15865, 15866, 15867, 15868, 15869, 15870, 15871, 15872, 15873, 15874, 15875, 15876, 15877, 15878, 15879, 15880, 15881, 15882, 15883, 15884, 15885, 15886, 15887, 15888, 15889, 15890, 15891, 15892, 15893, 15894, 15895, 15896, 15897, 15898, 15899, 15900, 15901, 15902, 15903, 15904, 15905, 15906, 15907, 15908, 15909, 15910, 15911, 15912, 15913, 15914, 15915, 15916, 15917, 15918, 15919, 15920, 15921, 15922, 15923, 15924, 15925, 15926, 15927, 15928, 15929, 15930, 15931, 15932, 15933, 15934, 15935, 15936, 15937, 15938, 15939, 15940, 15941, 15942, 15943, 15944, 15945, 15946, 15947, 15948, 15949, 15950, 15951, 15952, 15953, 15954, 15955, 15956, 15957, 15958, 15959, 15960, 15961, 15962, 15963, 15964, 15965, 15966, 15967, 15968, 15969, 15970, 15971, 15972, 15973, 15974, 15975, 15976, 15977, 15978, 15979, 15980, 15981, 15982, 15983, 15984, 15985, 15986, 15987, 15988, 15989, 15990, 15991, 15992, 15993, 15994, 15995, 15996, 15997, 15998, 15999, 16000, 16001, 16002, 16003, 16004, 16005, 16006, 16007, 16008, 16009, 16010, 16011, 16012, 16013, 16014, 16015, 16016, 16017, 16018, 16019, 16020, 16021, 16022, 16023, 16024, 16025, 16026, 16027, 16028, 16029, 16030, 16031, 16032, 16033, 16034, 16035, 16036, 16037, 16038, 16039, 16040, 16041, 16042, 16043, 16044, 16045, 16046, 16047, 16048, 16049, 16050, 16051, 16052, 16053, 16054, 16055, 16056, 16057, 16058, 16059, 16060, 16061, 16062, 16063, 16064, 16065, 16066, 16067, 16068, 16069, 16070, 16071, 16072, 16073, 16074, 16075, 16076, 16077, 16078, 16079, 16080, 16081, 16082, 16083, 16084, 16085, 16086, 16087, 16088, 16089, 16090, 16091, 16092, 16093, 16094, 16095, 16096, 16097, 16098, 16099, 16100, 16101, 16102, 16103, 16104, 16105, 16106, 16107, 16108, 16109, 16110, 16111, 16112, 16113, 16114, 16115, 16116, 16117, 16118, 16119, 16120, 16121, 16122, 16123, 16124, 16125, 16126, 16127, 16128, 16129, 16130, 16131, 16132, 16133, 16134, 16135, 16136, 16137, 16138, 16139, 16140, 16141, 16142, 16143, 16144, 16145, 16146, 16147, 16148, 16149, 16150, 16151, 16152, 16153, 16154, 16155, 16156, 16157, 16158, 16159, 16160, 16161, 16162, 16163, 16164, 16165, 16166, 16167, 16168, 16169, 16170, 16171, 16172, 16173, 16174, 16175, 16176, 16177, 16178, 16179, 16180, 16181, 16182, 16183, 16184, 16185, 16186, 16187, 16188, 16189, 16190, 16191, 16192, 16193, 16194, 16195, 16196, 16197, 16198, 16199, 16200, 16201, 16202, 16203, 16204, 16205, 16206, 16207, 16208, 16209, 16210, 16211, 16212, 16213, 16214, 16215, 16216, 16217, 16218, 16219, 16220, 16221, 16222, 16223, 16224, 16225, 16226, 16227, 16228, 16229, 16230, 16231, 16232, 16233, 16234, 16235, 16236, 16237, 16238, 16239, 16240, 16241, 16242, 16243, 16244, 16245, 16246, 16247, 16248, 16249, 16250, 16251, 16252, 16253, 16254, 16255, 16256, 16257, 16258, 16259, 16260, 16261, 16262, 16263, 16264, 16265, 16266, 16267, 16268, 16269, 16270, 16271, 16272, 16273, 16274, 16275, 16276, 16277, 16278, 16279, 16280, 16281, 16282, 16283, 16284, 16285, 16286, 16287, 16288, 16289, 16290, 16291, 16292, 16293, 16294, 16295, 16296, 16297, 16298, 16299, 16300, 16301, 16302, 16303, 16304, 16305, 16306, 16307, 16308, 16309, 16310, 16311, 16312, 16313, 16314, 16315, 16316, 16317, 16318, 16319, 16320, 16321, 16322, 16323, 16324, 16325, 16326, 16327, 16328, 16329, 16330, 16331, 16332, 16333, 16334, 16335, 16336, 16337, 16338, 16339, 16340, 16341, 16342, 16343, 16344, 16345, 16346, 16347, 16348, 16349, 16350, 16351, 16352, 16353, 16354, 16355, 16356, 16357, 16358, 16359, 16360, 16361, 16362, 16363, 16364, 16365, 16366, 16367, 16368, 16369, 16370, 16371, 16372, 16373, 16374, 16375, 16376, 16377, 16378, 16379, 16380, 16381, 16382, 16383, 16384, 16385, 16386, 16387, 16388, 16389, 16390, 16391, 16392, 16393, 16394, 16395, 16396, 16397, 16398, 16399, 16400, 16401, 16402, 16403, 16404, 16405, 16406, 16407, 16408, 16409, 16410, 16411, 16412, 16413, 16414, 16415, 16416, 16417, 16418, 16419, 16420, 16421, 16422, 16423, 16424, 16425, 16426, 16427, 16428, 16429, 16430, 16431, 16432, 16433, 16434, 16435, 16436, 16437, 16438, 16439, 16440, 16441, 16442, 16443, 16444, 16445, 16446, 16447, 16448, 16449, 16450, 16451, 16452, 16453, 16454, 16455, 16456, 16457, 16458, 16459, 16460, 16461, 16462, 16463, 16464, 16465, 16466, 16467, 16468, 16469, 16470, 16471, 16472, 16473, 16474, 16475, 16476, 16477, 16478, 16479, 16480, 16481, 16482, 16483, 16484, 16485, 16486, 16487, 16488, 16489, 16490, 16491, 16492, 16493, 16494, 16495, 16496, 16497, 16498, 16499, 16500, 16501, 16502, 16503, 16504, 16505, 16506, 16507, 16508, 16509, 16510, 16511, 16512, 16513, 16514, 16515, 16516, 16517, 16518, 16519, 16520, 16521, 16522, 16523, 16524, 16525, 16526, 16527, 16528, 16529, 16530, 16531, 16532, 16533, 16534, 16535, 16536, 16537, 16538, 16539, 16540, 16541, 16542, 16543, 16544, 16545, 16546, 16547, 16548, 16549, 16550, 16551, 16552, 16553, 16554, 16555, 16556, 16557, 16558, 16559, 16560, 16561, 16562, 16563, 16564, 16565, 16566, 16567, 16568, 16569, 16570, 16571, 16572, 16573, 16574, 16575, 16576, 16577, 16578, 16579, 16580, 16581, 16582, 16583, 16584, 16585, 16586, 16587, 16588, 16589, 16590, 16591, 16592, 16593, 16594, 16595, 16596, 16597, 16598, 16599, 16600, 16601, 16602, 16603, 16604, 16605, 16606, 16607, 16608, 16609, 16610, 16611, 16612, 16613, 16614, 16615, 16616, 16617, 16618, 16619, 16620, 16621, 16622, 16623, 16624, 16625, 16626, 16627, 16628, 16629, 16630, 16631, 16632, 16633, 16634, 16635, 16636, 16637, 16638, 16639, 16640, 16641, 16642, 16643, 16644, 16645, 16646, 16647, 16648, 16649, 16650, 16651, 16652, 16653, 16654, 16655, 16656, 16657, 16658, 16659, 16660, 16661, 16662, 16663, 16664, 16665, 16666, 16667, 16668, 16669, 16670, 16671, 16672, 16673, 16674, 16675, 16676, 16677, 16678, 16679, 16680, 16681, 16682, 16683, 16684, 16685, 16686, 16687, 16688, 16689, 16690, 16691, 16692, 16693, 16694, 16695, 16696, 16697, 16698, 16699, 16700, 16701, 16702, 16703, 16704, 16705, 16706, 16707, 16708, 16709, 16710, 16711, 16712, 16713, 16714, 16715, 16716, 16717, 16718, 16719, 16720, 16721, 16722, 16723, 16724, 16725, 16726, 16727, 16728, 16729, 16730, 16731, 16732, 16733, 16734, 16735, 16736, 16737, 16738, 16739, 16740, 16741, 16742, 16743, 16744, 16745, 16746, 16747, 16748, 16749, 16750, 16751, 16752, 16753, 16754, 16755, 16756, 16757, 16758, 16759, 16760, 16761, 16762, 16763, 16764, 16765, 16766, 16767, 16768, 16769, 16770, 16771, 16772, 16773, 16774, 16775, 16776, 16777, 16778, 16779, 16780, 16781, 16782, 16783, 16784, 16785, 16786, 16787, 16788, 16789, 16790, 16791, 16792, 16793, 16794, 16795, 16796, 16797, 16798, 16799, 16800, 16801, 16802, 16803, 16804, 16805, 16806, 16807, 16808, 16809, 16810, 16811, 16812, 16813, 16814, 16815, 16816, 16817, 16818, 16819, 16820, 16821, 16822, 16823, 16824, 16825, 16826, 16827, 16828, 16829, 16830, 16831, 16832, 16833, 16834, 16835, 16836, 16837, 16838, 16839, 16840, 16841, 16842, 16843, 16844, 16845, 16846, 16847, 16848, 16849, 16850, 16851, 16852, 16853, 16854, 16855, 16856, 16857, 16858, 16859, 16860, 16861, 16862, 16863, 16864, 16865, 16866, 16867, 16868, 16869, 16870, 16871, 16872, 16873, 16874, 16875, 16876, 16877, 16878, 16879, 16880, 16881, 16882, 16883, 16884, 16885, 16886, 16887, 16888, 16889, 16890, 16891, 16892, 16893, 16894, 16895, 16896, 16897, 16898, 16899, 16900, 16901, 16902, 16903, 16904, 16905, 16906, 16907, 16908, 16909, 16910, 16911, 16912, 16913, 16914, 16915, 16916, 16917, 16918, 16919, 16920, 16921, 16922, 16923, 16924, 16925, 16926, 16927, 16928, 16929, 16930, 16931, 16932, 16933, 16934, 16935, 16936, 16937, 16938, 16939, 16940, 16941, 16942, 16943, 16944, 16945, 16946, 16947, 16948, 16949, 16950, 16951, 16952, 16953, 16954, 16955, 16956, 16957, 16958, 16959, 16960, 16961, 16962, 16963, 16964, 16965, 16966, 16967, 16968, 16969, 16970, 16971, 16972, 16973, 16974, 16975, 16976, 16977, 16978, 16979, 16980, 16981, 16982, 16983, 16984, 16985, 16986, 16987, 16988, 16989, 16990, 16991, 16992, 16993, 16994, 16995, 16996, 16997, 16998, 16999, 17000, 17001, 17002, 17003, 17004, 17005, 17006, 17007, 17008, 17009, 17010, 17011, 17012, 17013, 17014, 17015, 17016, 17017, 17018, 17019, 17020, 17021, 17022, 17023, 17024, 17025, 17026, 17027, 17028, 17029, 17030, 17031, 17032, 17033, 17034, 17035, 17036, 17037, 17038, 17039, 17040, 17041, 17042, 17043, 17044, 17045, 17046, 17047, 17048, 17049, 17050, 17051, 17052, 17053, 17054, 17055, 17056, 17057, 17058, 17059, 17060, 17061, 17062, 17063, 17064, 17065, 17066, 17067, 17068, 17069, 17070, 17071, 17072, 17073, 17074, 17075, 17076, 17077, 17078, 17079, 17080, 17081, 17082, 17083, 17084, 17085, 17086, 17087, 17088, 17089, 17090, 17091, 17092, 17093, 17094, 17095, 17096, 17097, 17098, 17099, 17100, 17101, 17102, 17103, 17104, 17105, 17106, 17107, 17108, 17109, 17110, 17111, 17112, 17113, 17114, 17115, 17116, 17117, 17118, 17119, 17120, 17121, 17122, 17123, 17124, 17125, 17126, 17127, 17128, 17129, 17130, 17131, 17132, 17133, 17134, 17135, 17136, 17137, 17138, 17139, 17140, 17141, 17142, 17143, 17144, 17145, 17146, 17147, 17148, 17149, 17150, 17151, 17152, 17153, 17154, 17155, 17156, 17157, 17158, 17159, 17160, 17161, 17162, 17163, 17164, 17165, 17166, 17167, 17168, 17169, 17170, 17171, 17172, 17173, 17174, 17175, 17176, 17177, 17178, 17179, 17180, 17181, 17182, 17183, 17184, 17185, 17186, 17187, 17188, 17189, 17190, 17191, 17192, 17193, 17194, 17195, 17196, 17197, 17198, 17199, 17200, 17201, 17202, 17203, 17204, 17205, 17206, 17207, 17208, 17209, 17210, 17211, 17212, 17213, 17214, 17215, 17216, 17217, 17218, 17219, 17220, 17221, 17222, 17223, 17224, 17225, 17226, 17227, 17228, 17229, 17230, 17231, 17232, 17233, 17234, 17235, 17236, 17237, 17238, 17239, 17240, 17241, 17242, 17243, 17244, 17245, 17246, 17247, 17248, 17249, 17250, 17251, 17252, 17253, 17254, 17255, 17256, 17257, 17258, 17259, 17260, 17261, 17262, 17263, 17264, 17265, 17266, 17267, 17268, 17269, 17270, 17271, 17272, 17273, 17274, 17275, 17276, 17277, 17278, 17279, 17280, 17281, 17282, 17283, 17284, 17285, 17286, 17287, 17288, 17289, 17290, 17291, 17292, 17293, 17294, 17295, 17296, 17297, 17298, 17299, 17300, 17301, 17302, 17303, 17304, 17305, 17306, 17307, 17308, 17309, 17310, 17311, 17312, 17313, 17314, 17315, 17316, 17317, 17318, 17319, 17320, 17321, 17322, 17323, 17324, 17325, 17326, 17327, 17328, 17329, 17330, 17331, 17332, 17333, 17334, 17335, 17336, 17337, 17338, 17339, 17340, 17341, 17342, 17343, 17344, 17345, 17346, 17347, 17348, 17349, 17350, 17351, 17352, 17353, 17354, 17355, 17356, 17357, 17358, 17359, 17360, 17361, 17362, 17363, 17364, 17365, 17366, 17367, 17368, 17369, 17370, 17371, 17372, 17373, 17374, 17375, 17376, 17377, 17378, 17379, 17380, 17381, 17382, 17383, 17384, 17385, 17386, 17387, 17388, 17389, 17390, 17391, 17392, 17393, 17394, 17395, 17396, 17397, 17398, 17399, 17400, 17401, 17402, 17403, 17404, 17405, 17406, 17407, 17408, 17409, 17410, 17411, 17412, 17413, 17414, 17415, 17416, 17417, 17418, 17419, 17420, 17421, 17422, 17423, 17424, 17425, 17426, 17427, 17428, 17429, 17430, 17431, 17432, 17433, 17434, 17435, 17436, 17437, 17438, 17439, 17440, 17441, 17442, 17443, 17444, 17445, 17446, 17447, 17448, 17449, 17450, 17451, 17452, 17453, 17454, 17455, 17456, 17457, 17458, 17459, 17460, 17461, 17462, 17463, 17464, 17465, 17466, 17467, 17468, 17469, 17470, 17471, 17472, 17473, 17474, 17475, 17476, 17477, 17478, 17479, 17480, 17481, 17482, 17483, 17484, 17485, 17486, 17487, 17488, 17489, 17490, 17491, 17492, 17493, 17494, 17495, 17496, 17497, 17498, 17499, 17500, 17501, 17502, 17503, 17504, 17505, 17506, 17507, 17508, 17509, 17510, 17511, 17512, 17513, 17514, 17515, 17516, 17517, 17518, 17519, 17520, 17521, 17522, 17523, 17524, 17525, 17526, 17527, 17528, 17529, 17530, 17531, 17532, 17533, 17534, 17535, 17536, 17537, 17538, 17539, 17540, 17541, 17542, 17543, 17544, 17545, 17546, 17547, 17548, 17549, 17550, 17551, 17552, 17553, 17554, 17555, 17556, 17557, 17558, 17559, 17560, 17561, 17562, 17563, 17564, 17565, 17566, 17567, 17568, 17569, 17570, 17571, 17572, 17573, 17574, 17575, 17576, 17577, 17578, 17579, 17580, 17581, 17582, 17583, 17584, 17585, 17586, 17587, 17588, 17589, 17590, 17591, 17592, 17593, 17594, 17595, 17596, 17597, 17598, 17599, 17600, 17601, 17602, 17603, 17604, 17605, 17606, 17607, 17608, 17609, 17610, 17611, 17612, 17613, 17614, 17615, 17616, 17617, 17618, 17619, 17620, 17621, 17622, 17623, 17624, 17625, 17626, 17627, 17628, 17629, 17630, 17631, 17632, 17633, 17634, 17635, 17636, 17637, 17638, 17639, 17640, 17641, 17642, 17643, 17644, 17645, 17646, 17647, 17648, 17649, 17650, 17651, 17652, 17653, 17654, 17655, 17656, 17657, 17658, 17659, 17660, 17661, 17662, 17663, 17664, 17665, 17666, 17667, 17668, 17669, 17670, 17671, 17672, 17673, 17674, 17675, 17676, 17677, 17678, 17679, 17680, 17681, 17682, 17683, 17684, 17685, 17686, 17687, 17688, 17689, 17690, 17691, 17692, 17693, 17694, 17695, 17696, 17697, 17698, 17699, 17700, 17701, 17702, 17703, 17704, 17705, 17706, 17707, 17708, 17709, 17710, 17711, 17712, 17713, 17714, 17715, 17716, 17717, 17718, 17719, 17720, 17721, 17722, 17723, 17724, 17725, 17726, 17727, 17728, 17729, 17730, 17731, 17732, 17733, 17734, 17735, 17736, 17737, 17738, 17739, 17740, 17741, 17742, 17743, 17744, 17745, 17746, 17747, 17748, 17749, 17750, 17751, 17752, 17753, 17754, 17755, 17756, 17757, 17758, 17759, 17760, 17761, 17762, 17763, 17764, 17765, 17766, 17767, 17768, 17769, 17770, 17771, 17772, 17773, 17774, 17775, 17776, 17777, 17778, 17779, 17780, 17781, 17782, 17783, 17784, 17785, 17786, 17787, 17788, 17789, 17790, 17791, 17792, 17793, 17794, 17795, 17796, 17797, 17798, 17799, 17800, 17801, 17802, 17803, 17804, 17805, 17806, 17807, 17808, 17809, 17810, 17811, 17812, 17813, 17814, 17815, 17816, 17817, 17818, 17819, 17820, 17821, 17822, 17823, 17824, 17825, 17826, 17827, 17828, 17829, 17830, 17831, 17832, 17833, 17834, 17835, 17836, 17837, 17838, 17839, 17840, 17841, 17842, 17843, 17844, 17845, 17846, 17847, 17848, 17849, 17850, 17851, 17852, 17853, 17854, 17855, 17856, 17857, 17858, 17859, 17860, 17861, 17862, 17863, 17864, 17865, 17866, 17867, 17868, 17869, 17870, 17871, 17872, 17873, 17874, 17875, 17876, 17877, 17878, 17879, 17880, 17881, 17882, 17883, 17884, 17885, 17886, 17887, 17888, 17889, 17890, 17891, 17892, 17893, 17894, 17895, 17896, 17897, 17898, 17899, 17900, 17901, 17902, 17903, 17904, 17905, 17906, 17907, 17908, 17909, 17910, 17911, 17912, 17913, 17914, 17915, 17916, 17917, 17918, 17919, 17920, 17921, 17922, 17923, 17924, 17925, 17926, 17927, 17928, 17929, 17930, 17931, 17932, 17933, 17934, 17935, 17936, 17937, 17938, 17939, 17940, 17941, 17942, 17943, 17944, 17945, 17946, 17947, 17948, 17949, 17950, 17951, 17952, 17953, 17954, 17955, 17956, 17957, 17958, 17959, 17960, 17961, 17962, 17963, 17964, 17965, 17966, 17967, 17968, 17969, 17970, 17971, 17972, 17973, 17974, 17975, 17976, 17977, 17978, 17979, 17980, 17981, 17982, 17983, 17984, 17985, 17986, 17987, 17988, 17989, 17990, 17991, 17992, 17993, 17994, 17995, 17996, 17997, 17998, 17999, 18000, 18001, 18002, 18003, 18004, 18005, 18006, 18007, 18008, 18009, 18010, 18011, 18012, 18013, 18014, 18015, 18016, 18017, 18018, 18019, 18020, 18021, 18022, 18023, 18024, 18025, 18026, 18027, 18028, 18029, 18030, 18031, 18032, 18033, 18034, 18035, 18036, 18037, 18038, 18039, 18040, 18041, 18042, 18043, 18044, 18045, 18046, 18047, 18048, 18049, 18050, 18051, 18052, 18053, 18054, 18055, 18056, 18057, 18058, 18059, 18060, 18061, 18062, 18063, 18064, 18065, 18066, 18067, 18068, 18069, 18070, 18071, 18072, 18073, 18074, 18075, 18076, 18077, 18078, 18079, 18080, 18081, 18082, 18083, 18084, 18085, 18086, 18087, 18088, 18089, 18090, 18091, 18092, 18093, 18094, 18095, 18096, 18097, 18098, 18099, 18100, 18101, 18102, 18103, 18104, 18105, 18106, 18107, 18108, 18109, 18110, 18111, 18112, 18113, 18114, 18115, 18116, 18117, 18118, 18119, 18120, 18121, 18122, 18123, 18124, 18125, 18126, 18127, 18128, 18129, 18130, 18131, 18132, 18133, 18134, 18135, 18136, 18137, 18138, 18139, 18140, 18141, 18142, 18143, 18144, 18145, 18146, 18147, 18148, 18149, 18150, 18151, 18152, 18153, 18154, 18155, 18156, 18157, 18158, 18159, 18160, 18161, 18162, 18163, 18164, 18165, 18166, 18167, 18168, 18169, 18170, 18171, 18172, 18173, 18174, 18175, 18176, 18177, 18178, 18179, 18180, 18181, 18182, 18183, 18184, 18185, 18186, 18187, 18188, 18189, 18190, 18191, 18192, 18193, 18194, 18195, 18196, 18197, 18198, 18199, 18200, 18201, 18202, 18203, 18204, 18205, 18206, 18207, 18208, 18209, 18210, 18211, 18212, 18213, 18214, 18215, 18216, 18217, 18218, 18219, 18220, 18221, 18222, 18223, 18224, 18225, 18226, 18227, 18228, 18229, 18230, 18231, 18232, 18233, 18234, 18235, 18236, 18237, 18238, 18239, 18240, 18241, 18242, 18243, 18244, 18245, 18246, 18247, 18248, 18249, 18250, 18251, 18252, 18253, 18254, 18255, 18256, 18257, 18258, 18259, 18260, 18261, 18262, 18263, 18264, 18265, 18266, 18267, 18268, 18269, 18270, 18271, 18272, 18273, 18274, 18275, 18276, 18277, 18278, 18279, 18280, 18281, 18282, 18283, 18284, 18285, 18286, 18287, 18288, 18289, 18290, 18291, 18292, 18293, 18294, 18295, 18296, 18297, 18298, 18299, 18300, 18301, 18302, 18303, 18304, 18305, 18306, 18307, 18308, 18309, 18310, 18311, 18312, 18313, 18314, 18315, 18316, 18317, 18318, 18319, 18320, 18321, 18322, 18323, 18324, 18325, 18326, 18327, 18328, 18329, 18330, 18331, 18332, 18333, 18334, 18335, 18336, 18337, 18338, 18339, 18340, 18341, 18342, 18343, 18344, 18345, 18346, 18347, 18348, 18349, 18350, 18351, 18352, 18353, 18354, 18355, 18356, 18357, 18358, 18359, 18360, 18361, 18362, 18363, 18364, 18365, 18366, 18367, 18368, 18369, 18370, 18371, 18372, 18373, 18374, 18375, 18376, 18377, 18378, 18379, 18380, 18381, 18382, 18383, 18384, 18385, 18386, 18387, 18388, 18389, 18390, 18391, 18392, 18393, 18394, 18395, 18396, 18397, 18398, 18399, 18400, 18401, 18402, 18403, 18404, 18405, 18406, 18407, 18408, 18409, 18410, 18411, 18412, 18413, 18414, 18415, 18416, 18417, 18418, 18419, 18420, 18421, 18422, 18423, 18424, 18425, 18426, 18427, 18428, 18429, 18430, 18431, 18432, 18433, 18434, 18435, 18436, 18437, 18438, 18439, 18440, 18441, 18442, 18443, 18444, 18445, 18446, 18447, 18448, 18449, 18450, 18451, 18452, 18453, 18454, 18455, 18456, 18457, 18458, 18459, 18460, 18461, 18462, 18463, 18464, 18465, 18466, 18467, 18468, 18469, 18470, 18471, 18472, 18473, 18474, 18475, 18476, 18477, 18478, 18479, 18480, 18481, 18482, 18483, 18484, 18485, 18486, 18487, 18488, 18489, 18490, 18491, 18492, 18493, 18494, 18495, 18496, 18497, 18498, 18499, 18500, 18501, 18502, 18503, 18504, 18505, 18506, 18507, 18508, 18509, 18510, 18511, 18512, 18513, 18514, 18515, 18516, 18517, 18518, 18519, 18520, 18521, 18522, 18523, 18524, 18525, 18526, 18527, 18528, 18529, 18530, 18531, 18532, 18533, 18534, 18535, 18536, 18537, 18538, 18539, 18540, 18541, 18542, 18543, 18544, 18545, 18546, 18547, 18548, 18549, 18550, 18551, 18552, 18553, 18554, 18555, 18556, 18557, 18558, 18559, 18560, 18561, 18562, 18563, 18564, 18565, 18566, 18567, 18568, 18569, 18570, 18571, 18572, 18573, 18574, 18575, 18576, 18577, 18578, 18579, 18580, 18581, 18582, 18583, 18584, 18585, 18586, 18587, 18588, 18589, 18590, 18591, 18592, 18593, 18594, 18595, 18596, 18597, 18598, 18599, 18600, 18601, 18602, 18603, 18604, 18605, 18606, 18607, 18608, 18609, 18610, 18611, 18612, 18613, 18614, 18615, 18616, 18617, 18618, 18619, 18620, 18621, 18622, 18623, 18624, 18625, 18626, 18627, 18628, 18629, 18630, 18631, 18632, 18633, 18634, 18635, 18636, 18637, 18638, 18639, 18640, 18641, 18642, 18643, 18644, 18645, 18646, 18647, 18648, 18649, 18650, 18651, 18652, 18653, 18654, 18655, 18656, 18657, 18658, 18659, 18660, 18661, 18662, 18663, 18664, 18665, 18666, 18667, 18668, 18669, 18670, 18671, 18672, 18673, 18674, 18675, 18676, 18677, 18678, 18679, 18680, 18681, 18682, 18683, 18684, 18685, 18686, 18687, 18688, 18689, 18690, 18691, 18692, 18693, 18694, 18695, 18696, 18697, 18698, 18699, 18700, 18701, 18702, 18703, 18704, 18705, 18706, 18707, 18708, 18709, 18710, 18711, 18712, 18713, 18714, 18715, 18716, 18717, 18718, 18719, 18720, 18721, 18722, 18723, 18724, 18725, 18726, 18727, 18728, 18729, 18730, 18731, 18732, 18733, 18734, 18735, 18736, 18737, 18738, 18739, 18740, 18741, 18742, 18743, 18744, 18745, 18746, 18747, 18748, 18749, 18750, 18751, 18752, 18753, 18754, 18755, 18756, 18757, 18758, 18759, 18760, 18761, 18762, 18763, 18764, 18765, 18766, 18767, 18768, 18769, 18770, 18771, 18772, 18773, 18774, 18775, 18776, 18777, 18778, 18779, 18780, 18781, 18782, 18783, 18784, 18785, 18786, 18787, 18788, 18789, 18790, 18791, 18792, 18793, 18794, 18795, 18796, 18797, 18798, 18799, 18800, 18801, 18802, 18803, 18804, 18805, 18806, 18807, 18808, 18809, 18810, 18811, 18812, 18813, 18814, 18815, 18816, 18817, 18818, 18819, 18820, 18821, 18822, 18823, 18824, 18825, 18826, 18827, 18828, 18829, 18830, 18831, 18832, 18833, 18834, 18835, 18836, 18837, 18838, 18839, 18840, 18841, 18842, 18843, 18844, 18845, 18846, 18847, 18848, 18849, 18850, 18851, 18852, 18853, 18854, 18855, 18856, 18857, 18858, 18859, 18860, 18861, 18862, 18863, 18864, 18865, 18866, 18867, 18868, 18869, 18870, 18871, 18872, 18873, 18874, 18875, 18876, 18877, 18878, 18879, 18880, 18881, 18882, 18883, 18884, 18885, 18886, 18887, 18888, 18889, 18890, 18891, 18892, 18893, 18894, 18895, 18896, 18897, 18898, 18899, 18900, 18901, 18902, 18903, 18904, 18905, 18906, 18907, 18908, 18909, 18910, 18911, 18912, 18913, 18914, 18915, 18916, 18917, 18918, 18919, 18920, 18921, 18922, 18923, 18924, 18925, 18926, 18927, 18928, 18929, 18930, 18931, 18932, 18933, 18934, 18935, 18936, 18937, 18938, 18939, 18940, 18941, 18942, 18943, 18944, 18945, 18946, 18947, 18948, 18949, 18950, 18951, 18952, 18953, 18954, 18955, 18956, 18957, 18958, 18959, 18960, 18961, 18962, 18963, 18964, 18965, 18966, 18967, 18968, 18969, 18970, 18971, 18972, 18973, 18974, 18975, 18976, 18977, 18978, 18979, 18980, 18981, 18982, 18983, 18984, 18985, 18986, 18987, 18988, 18989, 18990, 18991, 18992, 18993, 18994, 18995, 18996, 18997, 18998, 18999, 19000, 19001, 19002, 19003, 19004, 19005, 19006, 19007, 19008, 19009, 19010, 19011, 19012, 19013, 19014, 19015, 19016, 19017, 19018, 19019, 19020, 19021, 19022, 19023, 19024, 19025, 19026, 19027, 19028, 19029, 19030, 19031, 19032, 19033, 19034, 19035, 19036, 19037, 19038, 19039, 19040, 19041, 19042, 19043, 19044, 19045, 19046, 19047, 19048, 19049, 19050, 19051, 19052, 19053, 19054, 19055, 19056, 19057, 19058, 19059, 19060, 19061, 19062, 19063, 19064, 19065, 19066, 19067, 19068, 19069, 19070, 19071, 19072, 19073, 19074, 19075, 19076, 19077, 19078, 19079, 19080, 19081, 19082, 19083, 19084, 19085, 19086, 19087, 19088, 19089, 19090, 19091, 19092, 19093, 19094, 19095, 19096, 19097, 19098, 19099, 19100, 19101, 19102, 19103, 19104, 19105, 19106, 19107, 19108, 19109, 19110, 19111, 19112, 19113, 19114, 19115, 19116, 19117, 19118, 19119, 19120, 19121, 19122, 19123, 19124, 19125, 19126, 19127, 19128, 19129, 19130, 19131, 19132, 19133, 19134, 19135, 19136, 19137, 19138, 19139, 19140, 19141, 19142, 19143, 19144, 19145, 19146, 19147, 19148, 19149, 19150, 19151, 19152, 19153, 19154, 19155, 19156, 19157, 19158, 19159, 19160, 19161, 19162, 19163, 19164, 19165, 19166, 19167, 19168, 19169, 19170, 19171, 19172, 19173, 19174, 19175, 19176, 19177, 19178, 19179, 19180, 19181, 19182, 19183, 19184, 19185, 19186, 19187, 19188, 19189, 19190, 19191, 19192, 19193, 19194, 19195, 19196, 19197, 19198, 19199, 19200, 19201, 19202, 19203, 19204, 19205, 19206, 19207, 19208, 19209, 19210, 19211, 19212, 19213, 19214, 19215, 19216, 19217, 19218, 19219, 19220, 19221, 19222, 19223, 19224, 19225, 19226, 19227, 19228, 19229, 19230, 19231, 19232, 19233, 19234, 19235, 19236, 19237, 19238, 19239, 19240, 19241, 19242, 19243, 19244, 19245, 19246, 19247, 19248, 19249, 19250, 19251, 19252, 19253, 19254, 19255, 19256, 19257, 19258, 19259, 19260, 19261, 19262, 19263, 19264, 19265, 19266, 19267, 19268, 19269, 19270, 19271, 19272, 19273, 19274, 19275, 19276, 19277, 19278, 19279, 19280, 19281, 19282, 19283, 19284, 19285, 19286, 19287, 19288, 19289, 19290, 19291, 19292, 19293, 19294, 19295, 19296, 19297, 19298, 19299, 19300, 19301, 19302, 19303, 19304, 19305, 19306, 19307, 19308, 19309, 19310, 19311, 19312, 19313, 19314, 19315, 19316, 19317, 19318, 19319, 19320, 19321, 19322, 19323, 19324, 19325, 19326, 19327, 19328, 19329, 19330, 19331, 19332, 19333, 19334, 19335, 19336, 19337, 19338, 19339, 19340, 19341, 19342, 19343, 19344, 19345, 19346, 19347, 19348, 19349, 19350, 19351, 19352, 19353, 19354, 19355, 19356, 19357, 19358, 19359, 19360, 19361, 19362, 19363, 19364, 19365, 19366, 19367, 19368, 19369, 19370, 19371, 19372, 19373, 19374, 19375, 19376, 19377, 19378, 19379, 19380, 19381, 19382, 19383, 19384, 19385, 19386, 19387, 19388, 19389, 19390, 19391, 19392, 19393, 19394, 19395, 19396, 19397, 19398, 19399, 19400, 19401, 19402, 19403, 19404, 19405, 19406, 19407, 19408, 19409, 19410, 19411, 19412, 19413, 19414, 19415, 19416, 19417, 19418, 19419, 19420, 19421, 19422, 19423, 19424, 19425, 19426, 19427, 19428, 19429, 19430, 19431, 19432, 19433, 19434, 19435, 19436, 19437, 19438, 19439, 19440, 19441, 19442, 19443, 19444, 19445, 19446, 19447, 19448, 19449, 19450, 19451, 19452, 19453, 19454, 19455, 19456, 19457, 19458, 19459, 19460, 19461, 19462, 19463, 19464, 19465, 19466, 19467, 19468, 19469, 19470, 19471, 19472, 19473, 19474, 19475, 19476, 19477, 19478, 19479, 19480, 19481, 19482, 19483, 19484, 19485, 19486, 19487, 19488, 19489, 19490, 19491, 19492, 19493, 19494, 19495, 19496, 19497, 19498, 19499, 19500, 19501, 19502, 19503, 19504, 19505, 19506, 19507, 19508, 19509, 19510, 19511, 19512, 19513, 19514, 19515, 19516, 19517, 19518, 19519, 19520, 19521, 19522, 19523, 19524, 19525, 19526, 19527, 19528, 19529, 19530, 19531, 19532, 19533, 19534, 19535, 19536, 19537, 19538, 19539, 19540, 19541, 19542, 19543, 19544, 19545, 19546, 19547, 19548, 19549, 19550, 19551, 19552, 19553, 19554, 19555, 19556, 19557, 19558, 19559, 19560, 19561, 19562, 19563, 19564, 19565, 19566, 19567, 19568, 19569, 19570, 19571, 19572, 19573, 19574, 19575, 19576, 19577, 19578, 19579, 19580, 19581, 19582, 19583, 19584, 19585, 19586, 19587, 19588, 19589, 19590, 19591, 19592, 19593, 19594, 19595, 19596, 19597, 19598, 19599, 19600, 19601, 19602, 19603, 19604, 19605, 19606, 19607, 19608, 19609, 19610, 19611, 19612, 19613, 19614, 19615, 19616, 19617, 19618, 19619, 19620, 19621, 19622, 19623, 19624, 19625, 19626, 19627, 19628, 19629, 19630, 19631, 19632, 19633, 19634, 19635, 19636, 19637, 19638, 19639, 19640, 19641, 19642, 19643, 19644, 19645, 19646, 19647, 19648, 19649, 19650, 19651, 19652, 19653, 19654, 19655, 19656, 19657, 19658, 19659, 19660, 19661, 19662, 19663, 19664, 19665, 19666, 19667, 19668, 19669, 19670, 19671, 19672, 19673, 19674, 19675, 19676, 19677, 19678, 19679, 19680, 19681, 19682, 19683, 19684, 19685, 19686, 19687, 19688, 19689, 19690, 19691, 19692, 19693, 19694, 19695, 19696, 19697, 19698, 19699, 19700, 19701, 19702, 19703, 19704, 19705, 19706, 19707, 19708, 19709, 19710, 19711, 19712, 19713, 19714, 19715, 19716, 19717, 19718, 19719, 19720, 19721, 19722, 19723, 19724, 19725, 19726, 19727, 19728, 19729, 19730, 19731, 19732, 19733, 19734, 19735, 19736, 19737, 19738, 19739, 19740, 19741, 19742, 19743, 19744, 19745, 19746, 19747, 19748, 19749, 19750, 19751, 19752, 19753, 19754, 19755, 19756, 19757, 19758, 19759, 19760, 19761, 19762, 19763, 19764, 19765, 19766, 19767, 19768, 19769, 19770, 19771, 19772, 19773, 19774, 19775, 19776, 19777, 19778, 19779, 19780, 19781, 19782, 19783, 19784, 19785, 19786, 19787, 19788, 19789, 19790, 19791, 19792, 19793, 19794, 19795, 19796, 19797, 19798, 19799, 19800, 19801, 19802, 19803, 19804, 19805, 19806, 19807, 19808, 19809, 19810, 19811, 19812, 19813, 19814, 19815, 19816, 19817, 19818, 19819, 19820, 19821, 19822, 19823, 19824, 19825, 19826, 19827, 19828, 19829, 19830, 19831, 19832, 19833, 19834, 19835, 19836, 19837, 19838, 19839, 19840, 19841, 19842, 19843, 19844, 19845, 19846, 19847, 19848, 19849, 19850, 19851, 19852, 19853, 19854, 19855, 19856, 19857, 19858, 19859, 19860, 19861, 19862, 19863, 19864, 19865, 19866, 19867, 19868, 19869, 19870, 19871, 19872, 19873, 19874, 19875, 19876, 19877, 19878, 19879, 19880, 19881, 19882, 19883, 19884, 19885, 19886, 19887, 19888, 19889, 19890, 19891, 19892, 19893, 19894, 19895, 19896, 19897, 19898, 19899, 19900, 19901, 19902, 19903, 19904, 19905, 19906, 19907, 19908, 19909, 19910, 19911, 19912, 19913, 19914, 19915, 19916, 19917, 19918, 19919, 19920, 19921, 19922, 19923, 19924, 19925, 19926, 19927, 19928, 19929, 19930, 19931, 19932, 19933, 19934, 19935, 19936, 19937, 19938, 19939, 19940, 19941, 19942, 19943, 19944, 19945, 19946, 19947, 19948, 19949, 19950, 19951, 19952, 19953, 19954, 19955, 19956, 19957, 19958, 19959, 19960, 19961, 19962, 19963, 19964, 19965, 19966, 19967, 19968, 19969, 19970, 19971, 19972, 19973, 19974, 19975, 19976, 19977, 19978, 19979, 19980, 19981, 19982, 19983, 19984, 19985, 19986, 19987, 19988, 19989, 19990, 19991, 19992, 19993, 19994, 19995, 19996, 19997, 19998, 19999, 20000, 20001, 20002, 20003, 20004, 20005, 20006, 20007, 20008, 20009, 20010, 20011, 20012, 20013, 20014, 20015, 20016, 20017, 20018, 20019, 20020, 20021, 20022, 20023, 20024, 20025, 20026, 20027, 20028, 20029, 20030, 20031, 20032, 20033, 20034, 20035, 20036, 20037, 20038, 20039, 20040, 20041, 20042, 20043, 20044, 20045, 20046, 20047, 20048, 20049, 20050, 20051, 20052, 20053, 20054, 20055, 20056, 20057, 20058, 20059, 20060, 20061, 20062, 20063, 20064, 20065, 20066, 20067, 20068, 20069, 20070, 20071, 20072, 20073, 20074, 20075, 20076, 20077, 20078, 20079, 20080, 20081, 20082, 20083, 20084, 20085, 20086, 20087, 20088, 20089, 20090, 20091, 20092, 20093, 20094, 20095, 20096, 20097, 20098, 20099, 20100, 20101, 20102, 20103, 20104, 20105, 20106, 20107, 20108, 20109, 20110, 20111, 20112, 20113, 20114, 20115, 20116, 20117, 20118, 20119, 20120, 20121, 20122, 20123, 20124, 20125, 20126, 20127, 20128, 20129, 20130, 20131, 20132, 20133, 20134, 20135, 20136, 20137, 20138, 20139, 20140, 20141, 20142, 20143, 20144, 20145, 20146, 20147, 20148, 20149, 20150, 20151, 20152, 20153, 20154, 20155, 20156, 20157, 20158, 20159, 20160, 20161, 20162, 20163, 20164, 20165, 20166, 20167, 20168, 20169, 20170, 20171, 20172, 20173, 20174, 20175, 20176, 20177, 20178, 20179, 20180, 20181, 20182, 20183, 20184, 20185, 20186, 20187, 20188, 20189, 20190, 20191, 20192, 20193, 20194, 20195, 20196, 20197, 20198, 20199, 20200, 20201, 20202, 20203, 20204, 20205, 20206, 20207, 20208, 20209, 20210, 20211, 20212, 20213, 20214, 20215, 20216, 20217, 20218, 20219, 20220, 20221, 20222, 20223, 20224, 20225, 20226, 20227, 20228, 20229, 20230, 20231, 20232, 20233, 20234, 20235, 20236, 20237, 20238, 20239, 20240, 20241, 20242, 20243, 20244, 20245, 20246, 20247, 20248, 20249, 20250, 20251, 20252, 20253, 20254, 20255, 20256, 20257, 20258, 20259, 20260, 20261, 20262, 20263, 20264, 20265, 20266, 20267, 20268, 20269, 20270, 20271, 20272, 20273, 20274, 20275, 20276, 20277, 20278, 20279, 20280, 20281, 20282, 20283, 20284, 20285, 20286, 20287, 20288, 20289, 20290, 20291, 20292, 20293, 20294, 20295, 20296, 20297, 20298, 20299, 20300, 20301, 20302, 20303, 20304, 20305, 20306, 20307, 20308, 20309, 20310, 20311, 20312, 20313, 20314, 20315, 20316, 20317, 20318, 20319, 20320, 20321, 20322, 20323, 20324, 20325, 20326, 20327, 20328, 20329, 20330, 20331, 20332, 20333, 20334, 20335, 20336, 20337, 20338, 20339, 20340, 20341, 20342, 20343, 20344, 20345, 20346, 20347, 20348, 20349, 20350, 20351, 20352, 20353, 20354, 20355, 20356, 20357, 20358, 20359, 20360, 20361, 20362, 20363, 20364, 20365, 20366, 20367, 20368, 20369, 20370, 20371, 20372, 20373, 20374, 20375, 20376, 20377, 20378, 20379, 20380, 20381, 20382, 20383, 20384, 20385, 20386, 20387, 20388, 20389, 20390, 20391, 20392, 20393, 20394, 20395, 20396, 20397]\n"," This is the range of val:  [ 7700  7701  7702 ... 12097 12098 12099]\n","Starting training\n","shuffling\n","Epoch 1/350\n","2020-06-11 02:44:04.714397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2020-06-11 02:44:04.941760: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","500/500 [==============================] - 36s 71ms/step - loss: 5.1564 - val_loss: 6.3400\n","\n","Epoch 00001: loss improved from inf to 5.15568, saving model to final.h5\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n","  'TensorFlow optimizers do not '\n","Epoch 2/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.6784 - val_loss: 1.4738\n","\n","Epoch 00002: loss improved from 5.15568 to 4.67809, saving model to final.h5\n","Epoch 3/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.4254 - val_loss: 4.9842\n","\n","Epoch 00003: loss improved from 4.67809 to 4.42528, saving model to final.h5\n","Epoch 4/350\n","500/500 [==============================] - 31s 63ms/step - loss: 4.2134 - val_loss: 3.1610\n","\n","Epoch 00004: loss improved from 4.42528 to 4.21358, saving model to final.h5\n","Epoch 5/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.3322 - val_loss: 2.2317\n","\n","Epoch 00005: loss did not improve from 4.21358\n","Epoch 6/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.1742 - val_loss: 2.7509\n","\n","Epoch 00006: loss improved from 4.21358 to 4.17441, saving model to final.h5\n","Epoch 7/350\n","500/500 [==============================] - 31s 62ms/step - loss: 4.0170 - val_loss: 0.8302\n","\n","Epoch 00007: loss improved from 4.17441 to 4.01709, saving model to final.h5\n","Epoch 8/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.9535 - val_loss: 5.9638\n","\n","Epoch 00008: loss improved from 4.01709 to 3.95325, saving model to final.h5\n","Epoch 9/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.8243 - val_loss: 8.2398\n","\n","Epoch 00009: loss improved from 3.95325 to 3.82429, saving model to final.h5\n","Epoch 10/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.9108 - val_loss: 10.1188\n","\n","Epoch 00010: loss did not improve from 3.82429\n","Epoch 11/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.8213 - val_loss: 7.2292\n","\n","Epoch 00011: loss improved from 3.82429 to 3.82090, saving model to final.h5\n","Epoch 12/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.8478 - val_loss: 6.8534\n","\n","Epoch 00012: loss did not improve from 3.82090\n","Epoch 13/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.7140 - val_loss: 8.1418\n","\n","Epoch 00013: loss improved from 3.82090 to 3.71417, saving model to final.h5\n","Epoch 14/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.7434 - val_loss: 4.0966\n","\n","Epoch 00014: loss did not improve from 3.71417\n","Epoch 15/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.7130 - val_loss: 7.5234\n","\n","Epoch 00015: loss improved from 3.71417 to 3.71310, saving model to final.h5\n","Epoch 16/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.6759 - val_loss: 2.2275\n","\n","Epoch 00016: loss improved from 3.71310 to 3.67575, saving model to final.h5\n","Epoch 17/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5486 - val_loss: 2.2283\n","\n","Epoch 00017: loss improved from 3.67575 to 3.54861, saving model to final.h5\n","Epoch 18/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5697 - val_loss: 5.8345\n","\n","Epoch 00018: loss did not improve from 3.54861\n","Epoch 19/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5346 - val_loss: 4.7754\n","\n","Epoch 00019: loss improved from 3.54861 to 3.53458, saving model to final.h5\n","Epoch 20/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5554 - val_loss: 1.3313\n","\n","Epoch 00020: loss did not improve from 3.53458\n","Epoch 21/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.5272 - val_loss: 5.6134\n","\n","Epoch 00021: loss improved from 3.53458 to 3.52729, saving model to final.h5\n","Epoch 22/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5844 - val_loss: 4.0754\n","\n","Epoch 00022: loss did not improve from 3.52729\n","Epoch 23/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5278 - val_loss: 5.6298\n","\n","Epoch 00023: loss did not improve from 3.52729\n","Epoch 24/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.4401 - val_loss: 3.3317\n","\n","Epoch 00024: loss improved from 3.52729 to 3.44017, saving model to final.h5\n","Epoch 25/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5354 - val_loss: 2.0827\n","\n","Epoch 00025: loss did not improve from 3.44017\n","Epoch 26/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.5289 - val_loss: 5.2391\n","\n","Epoch 00026: loss did not improve from 3.44017\n","Epoch 27/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.4926 - val_loss: 3.9848\n","\n","Epoch 00027: loss did not improve from 3.44017\n","Epoch 28/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.4636 - val_loss: 2.0477\n","\n","Epoch 00028: loss did not improve from 3.44017\n","Epoch 29/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.4277 - val_loss: 0.0872\n","\n","Epoch 00029: loss improved from 3.44017 to 3.42763, saving model to final.h5\n","Epoch 30/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.4698 - val_loss: 3.7704\n","\n","Epoch 00030: loss did not improve from 3.42763\n","Epoch 31/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.3695 - val_loss: 4.8187\n","\n","Epoch 00031: loss improved from 3.42763 to 3.36952, saving model to final.h5\n","Epoch 32/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.4260 - val_loss: 7.0321\n","\n","Epoch 00032: loss did not improve from 3.36952\n","Epoch 33/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.3514 - val_loss: 14.8943\n","\n","Epoch 00033: loss improved from 3.36952 to 3.35143, saving model to final.h5\n","Epoch 34/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.4097 - val_loss: 3.0636\n","\n","Epoch 00034: loss did not improve from 3.35143\n","Epoch 35/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.3948 - val_loss: 0.1292\n","\n","Epoch 00035: loss did not improve from 3.35143\n","Epoch 36/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.3458 - val_loss: 2.7974\n","\n","Epoch 00036: loss improved from 3.35143 to 3.34585, saving model to final.h5\n","Epoch 37/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.2877 - val_loss: 1.7256\n","\n","Epoch 00037: loss improved from 3.34585 to 3.28755, saving model to final.h5\n","Epoch 38/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.3700 - val_loss: 2.9191\n","\n","Epoch 00038: loss did not improve from 3.28755\n","Epoch 39/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.3227 - val_loss: 7.1080\n","\n","Epoch 00039: loss did not improve from 3.28755\n","Epoch 40/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.2952 - val_loss: 3.6633\n","\n","Epoch 00040: loss did not improve from 3.28755\n","Epoch 41/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.2983 - val_loss: 2.1704\n","\n","Epoch 00041: loss did not improve from 3.28755\n","Epoch 42/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.2855 - val_loss: 1.8661\n","\n","Epoch 00042: loss improved from 3.28755 to 3.28556, saving model to final.h5\n","Epoch 43/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.3241 - val_loss: 17.3241\n","\n","Epoch 00043: loss did not improve from 3.28556\n","Epoch 44/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.2763 - val_loss: 6.2773\n","\n","Epoch 00044: loss improved from 3.28556 to 3.27622, saving model to final.h5\n","Epoch 45/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.2182 - val_loss: 5.5217\n","\n","Epoch 00045: loss improved from 3.27622 to 3.21818, saving model to final.h5\n","Epoch 46/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1862 - val_loss: 1.1988\n","\n","Epoch 00046: loss improved from 3.21818 to 3.18608, saving model to final.h5\n","Epoch 47/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.2391 - val_loss: 5.0388\n","\n","Epoch 00047: loss did not improve from 3.18608\n","Epoch 48/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.2819 - val_loss: 5.4166\n","\n","Epoch 00048: loss did not improve from 3.18608\n","Epoch 49/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1960 - val_loss: 1.8486\n","\n","Epoch 00049: loss did not improve from 3.18608\n","Epoch 50/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.1685 - val_loss: 4.1239\n","\n","Epoch 00050: loss improved from 3.18608 to 3.16838, saving model to final.h5\n","Epoch 51/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.2357 - val_loss: 3.3228\n","\n","Epoch 00051: loss did not improve from 3.16838\n","Epoch 52/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0985 - val_loss: 7.1720\n","\n","Epoch 00052: loss improved from 3.16838 to 3.09851, saving model to final.h5\n","Epoch 53/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.2288 - val_loss: 10.7483\n","\n","Epoch 00053: loss did not improve from 3.09851\n","Epoch 54/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1883 - val_loss: 2.1612\n","\n","Epoch 00054: loss did not improve from 3.09851\n","Epoch 55/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.2491 - val_loss: 14.4861\n","\n","Epoch 00055: loss did not improve from 3.09851\n","Epoch 56/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1585 - val_loss: 2.9396\n","\n","Epoch 00056: loss did not improve from 3.09851\n","Epoch 57/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.2081 - val_loss: 3.6891\n","\n","Epoch 00057: loss did not improve from 3.09851\n","Epoch 58/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1170 - val_loss: 5.6439\n","\n","Epoch 00058: loss did not improve from 3.09851\n","Epoch 59/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.2405 - val_loss: 3.0435\n","\n","Epoch 00059: loss did not improve from 3.09851\n","Epoch 60/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.1241 - val_loss: 2.4251\n","\n","Epoch 00060: loss did not improve from 3.09851\n","Epoch 61/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.2168 - val_loss: 2.5401\n","\n","Epoch 00061: loss did not improve from 3.09851\n","Epoch 62/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.1793 - val_loss: 1.0030\n","\n","Epoch 00062: loss did not improve from 3.09851\n","Epoch 63/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1036 - val_loss: 7.6237\n","\n","Epoch 00063: loss did not improve from 3.09851\n","Epoch 64/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1536 - val_loss: 4.7692\n","\n","Epoch 00064: loss did not improve from 3.09851\n","Epoch 65/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.1029 - val_loss: 2.1506\n","\n","Epoch 00065: loss did not improve from 3.09851\n","Epoch 66/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.1258 - val_loss: 0.1147\n","\n","Epoch 00066: loss did not improve from 3.09851\n","Epoch 67/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1538 - val_loss: 16.2011\n","\n","Epoch 00067: loss did not improve from 3.09851\n","Epoch 68/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1058 - val_loss: 2.7958\n","\n","Epoch 00068: loss did not improve from 3.09851\n","Epoch 69/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0995 - val_loss: 0.3680\n","\n","Epoch 00069: loss did not improve from 3.09851\n","Epoch 70/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.0927 - val_loss: 2.1822\n","\n","Epoch 00070: loss improved from 3.09851 to 3.09265, saving model to final.h5\n","Epoch 71/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0957 - val_loss: 0.7088\n","\n","Epoch 00071: loss did not improve from 3.09265\n","Epoch 72/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0295 - val_loss: 0.7441\n","\n","Epoch 00072: loss improved from 3.09265 to 3.02957, saving model to final.h5\n","Epoch 73/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1057 - val_loss: 1.6578\n","\n","Epoch 00073: loss did not improve from 3.02957\n","Epoch 74/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.1666 - val_loss: 2.1510\n","\n","Epoch 00074: loss did not improve from 3.02957\n","Epoch 75/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0819 - val_loss: 2.4355\n","\n","Epoch 00075: loss did not improve from 3.02957\n","Epoch 76/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0527 - val_loss: 3.4796\n","\n","Epoch 00076: loss did not improve from 3.02957\n","Epoch 77/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0477 - val_loss: 7.3895\n","\n","Epoch 00077: loss did not improve from 3.02957\n","Epoch 78/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0818 - val_loss: 0.7563\n","\n","Epoch 00078: loss did not improve from 3.02957\n","Epoch 79/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0290 - val_loss: 3.0739\n","\n","Epoch 00079: loss improved from 3.02957 to 3.02908, saving model to final.h5\n","Epoch 80/350\n","500/500 [==============================] - 32s 63ms/step - loss: 3.0937 - val_loss: 3.9065\n","\n","Epoch 00080: loss did not improve from 3.02908\n","Epoch 81/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0446 - val_loss: 5.1361\n","\n","Epoch 00081: loss did not improve from 3.02908\n","Epoch 82/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.0118 - val_loss: 0.0903\n","\n","Epoch 00082: loss improved from 3.02908 to 3.01180, saving model to final.h5\n","Epoch 83/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9863 - val_loss: 7.2281\n","\n","Epoch 00083: loss improved from 3.01180 to 2.98633, saving model to final.h5\n","Epoch 84/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.0949 - val_loss: 0.5190\n","\n","Epoch 00084: loss did not improve from 2.98633\n","Epoch 85/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.0306 - val_loss: 5.0731\n","\n","Epoch 00085: loss did not improve from 2.98633\n","Epoch 86/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.0810 - val_loss: 1.6308\n","\n","Epoch 00086: loss did not improve from 2.98633\n","Epoch 87/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9900 - val_loss: 1.9476\n","\n","Epoch 00087: loss did not improve from 2.98633\n","Epoch 88/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0385 - val_loss: 0.8553\n","\n","Epoch 00088: loss did not improve from 2.98633\n","Epoch 89/350\n","500/500 [==============================] - 31s 63ms/step - loss: 2.9944 - val_loss: 11.0044\n","\n","Epoch 00089: loss did not improve from 2.98633\n","Epoch 90/350\n","500/500 [==============================] - 31s 63ms/step - loss: 2.9784 - val_loss: 13.9203\n","\n","Epoch 00090: loss improved from 2.98633 to 2.97828, saving model to final.h5\n","Epoch 91/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9662 - val_loss: 0.1697\n","\n","Epoch 00091: loss improved from 2.97828 to 2.96626, saving model to final.h5\n","Epoch 92/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0105 - val_loss: 6.1288\n","\n","Epoch 00092: loss did not improve from 2.96626\n","Epoch 93/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0210 - val_loss: 0.9837\n","\n","Epoch 00093: loss did not improve from 2.96626\n","Epoch 94/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9682 - val_loss: 0.5208\n","\n","Epoch 00094: loss did not improve from 2.96626\n","Epoch 95/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9609 - val_loss: 0.8348\n","\n","Epoch 00095: loss improved from 2.96626 to 2.96079, saving model to final.h5\n","Epoch 96/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9781 - val_loss: 0.5522\n","\n","Epoch 00096: loss did not improve from 2.96079\n","Epoch 97/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9260 - val_loss: 0.7788\n","\n","Epoch 00097: loss improved from 2.96079 to 2.92574, saving model to final.h5\n","Epoch 98/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9087 - val_loss: 5.0452\n","\n","Epoch 00098: loss improved from 2.92574 to 2.90882, saving model to final.h5\n","Epoch 99/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9102 - val_loss: 2.0136\n","\n","Epoch 00099: loss did not improve from 2.90882\n","Epoch 100/350\n","500/500 [==============================] - 31s 63ms/step - loss: 3.0221 - val_loss: 7.6315\n","\n","Epoch 00100: loss did not improve from 2.90882\n","Epoch 101/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9633 - val_loss: 0.9808\n","\n","Epoch 00101: loss did not improve from 2.90882\n","Epoch 102/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9760 - val_loss: 1.6112\n","\n","Epoch 00102: loss did not improve from 2.90882\n","Epoch 103/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9788 - val_loss: 4.5197\n","\n","Epoch 00103: loss did not improve from 2.90882\n","Epoch 104/350\n","500/500 [==============================] - 31s 63ms/step - loss: 2.9400 - val_loss: 1.9314\n","\n","Epoch 00104: loss did not improve from 2.90882\n","Epoch 105/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9452 - val_loss: 0.8052\n","\n","Epoch 00105: loss did not improve from 2.90882\n","Epoch 106/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9935 - val_loss: 1.6806\n","\n","Epoch 00106: loss did not improve from 2.90882\n","Epoch 107/350\n","500/500 [==============================] - 31s 62ms/step - loss: 3.0084 - val_loss: 0.0969\n","\n","Epoch 00107: loss did not improve from 2.90882\n","Epoch 108/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9125 - val_loss: 2.7690\n","\n","Epoch 00108: loss did not improve from 2.90882\n","Epoch 109/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9189 - val_loss: 0.7571\n","\n","Epoch 00109: loss did not improve from 2.90882\n","Epoch 110/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9203 - val_loss: 0.0903\n","\n","Epoch 00110: loss did not improve from 2.90882\n","Epoch 111/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8660 - val_loss: 0.6258\n","\n","Epoch 00111: loss improved from 2.90882 to 2.86603, saving model to final.h5\n","Epoch 112/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9764 - val_loss: 6.4591\n","\n","Epoch 00112: loss did not improve from 2.86603\n","Epoch 113/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9121 - val_loss: 1.5203\n","\n","Epoch 00113: loss did not improve from 2.86603\n","Epoch 114/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8977 - val_loss: 1.9928\n","\n","Epoch 00114: loss did not improve from 2.86603\n","Epoch 115/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8284 - val_loss: 6.5724\n","\n","Epoch 00115: loss improved from 2.86603 to 2.82828, saving model to final.h5\n","Epoch 116/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9007 - val_loss: 1.6500\n","\n","Epoch 00116: loss did not improve from 2.82828\n","Epoch 117/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8880 - val_loss: 12.5748\n","\n","Epoch 00117: loss did not improve from 2.82828\n","Epoch 118/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9030 - val_loss: 0.2719\n","\n","Epoch 00118: loss did not improve from 2.82828\n","Epoch 119/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8174 - val_loss: 2.7144\n","\n","Epoch 00119: loss improved from 2.82828 to 2.81728, saving model to final.h5\n","Epoch 120/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8819 - val_loss: 1.2571\n","\n","Epoch 00120: loss did not improve from 2.81728\n","Epoch 121/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8946 - val_loss: 2.9625\n","\n","Epoch 00121: loss did not improve from 2.81728\n","Epoch 122/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8909 - val_loss: 4.1136\n","\n","Epoch 00122: loss did not improve from 2.81728\n","Epoch 123/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8593 - val_loss: 3.7170\n","\n","Epoch 00123: loss did not improve from 2.81728\n","Epoch 124/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9255 - val_loss: 1.5738\n","\n","Epoch 00124: loss did not improve from 2.81728\n","Epoch 125/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9042 - val_loss: 2.2069\n","\n","Epoch 00125: loss did not improve from 2.81728\n","Epoch 126/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8301 - val_loss: 0.0815\n","\n","Epoch 00126: loss did not improve from 2.81728\n","Epoch 127/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8700 - val_loss: 4.7299\n","\n","Epoch 00127: loss did not improve from 2.81728\n","Epoch 128/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8942 - val_loss: 0.7972\n","\n","Epoch 00128: loss did not improve from 2.81728\n","Epoch 129/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8801 - val_loss: 6.5958\n","\n","Epoch 00129: loss did not improve from 2.81728\n","Epoch 130/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8793 - val_loss: 0.0985\n","\n","Epoch 00130: loss did not improve from 2.81728\n","Epoch 131/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8818 - val_loss: 10.9576\n","\n","Epoch 00131: loss did not improve from 2.81728\n","Epoch 132/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8475 - val_loss: 13.6483\n","\n","Epoch 00132: loss did not improve from 2.81728\n","Epoch 133/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8914 - val_loss: 3.8913\n","\n","Epoch 00133: loss did not improve from 2.81728\n","Epoch 134/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9501 - val_loss: 4.8523\n","\n","Epoch 00134: loss did not improve from 2.81728\n","Epoch 135/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8447 - val_loss: 12.3232\n","\n","Epoch 00135: loss did not improve from 2.81728\n","Epoch 136/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8687 - val_loss: 11.8125\n","\n","Epoch 00136: loss did not improve from 2.81728\n","Epoch 137/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8671 - val_loss: 1.7372\n","\n","Epoch 00137: loss did not improve from 2.81728\n","Epoch 138/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8619 - val_loss: 0.9956\n","\n","Epoch 00138: loss did not improve from 2.81728\n","Epoch 139/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8128 - val_loss: 2.5536\n","\n","Epoch 00139: loss improved from 2.81728 to 2.81281, saving model to final.h5\n","Epoch 140/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8753 - val_loss: 1.4874\n","\n","Epoch 00140: loss did not improve from 2.81281\n","Epoch 141/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8412 - val_loss: 9.0262\n","\n","Epoch 00141: loss did not improve from 2.81281\n","Epoch 142/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.9093 - val_loss: 0.1844\n","\n","Epoch 00142: loss did not improve from 2.81281\n","Epoch 143/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8960 - val_loss: 0.5321\n","\n","Epoch 00143: loss did not improve from 2.81281\n","Epoch 144/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8518 - val_loss: 6.3551\n","\n","Epoch 00144: loss did not improve from 2.81281\n","Epoch 145/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7889 - val_loss: 0.2670\n","\n","Epoch 00145: loss improved from 2.81281 to 2.78895, saving model to final.h5\n","Epoch 146/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8905 - val_loss: 1.5092\n","\n","Epoch 00146: loss did not improve from 2.78895\n","Epoch 147/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8737 - val_loss: 0.5027\n","\n","Epoch 00147: loss did not improve from 2.78895\n","Epoch 148/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7785 - val_loss: 6.7547\n","\n","Epoch 00148: loss improved from 2.78895 to 2.77827, saving model to final.h5\n","Epoch 149/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8465 - val_loss: 7.2837\n","\n","Epoch 00149: loss did not improve from 2.77827\n","Epoch 150/350\n","500/500 [==============================] - 31s 63ms/step - loss: 2.8618 - val_loss: 1.4722\n","\n","Epoch 00150: loss did not improve from 2.77827\n","Epoch 151/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7812 - val_loss: 1.1118\n","\n","Epoch 00151: loss did not improve from 2.77827\n","Epoch 152/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8698 - val_loss: 2.2059\n","\n","Epoch 00152: loss did not improve from 2.77827\n","Epoch 153/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7983 - val_loss: 1.3934\n","\n","Epoch 00153: loss did not improve from 2.77827\n","Epoch 154/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8151 - val_loss: 1.2882\n","\n","Epoch 00154: loss did not improve from 2.77827\n","Epoch 155/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7818 - val_loss: 3.0696\n","\n","Epoch 00155: loss did not improve from 2.77827\n","Epoch 156/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8842 - val_loss: 2.1261\n","\n","Epoch 00156: loss did not improve from 2.77827\n","Epoch 157/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8569 - val_loss: 0.2018\n","\n","Epoch 00157: loss did not improve from 2.77827\n","Epoch 158/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7938 - val_loss: 1.4548\n","\n","Epoch 00158: loss did not improve from 2.77827\n","Epoch 159/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8745 - val_loss: 8.3497\n","\n","Epoch 00159: loss did not improve from 2.77827\n","Epoch 160/350\n","500/500 [==============================] - 31s 63ms/step - loss: 2.8581 - val_loss: 1.4779\n","\n","Epoch 00160: loss did not improve from 2.77827\n","Epoch 161/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7856 - val_loss: 5.0721\n","\n","Epoch 00161: loss did not improve from 2.77827\n","Epoch 162/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8342 - val_loss: 0.1032\n","\n","Epoch 00162: loss did not improve from 2.77827\n","Epoch 163/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7990 - val_loss: 1.9398\n","\n","Epoch 00163: loss did not improve from 2.77827\n","Epoch 164/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8101 - val_loss: 4.2876\n","\n","Epoch 00164: loss did not improve from 2.77827\n","Epoch 165/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8317 - val_loss: 0.2735\n","\n","Epoch 00165: loss did not improve from 2.77827\n","Epoch 166/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8709 - val_loss: 0.1937\n","\n","Epoch 00166: loss did not improve from 2.77827\n","Epoch 167/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8459 - val_loss: 5.2014\n","\n","Epoch 00167: loss did not improve from 2.77827\n","Epoch 168/350\n","500/500 [==============================] - 31s 63ms/step - loss: 2.7683 - val_loss: 12.1256\n","\n","Epoch 00168: loss improved from 2.77827 to 2.76814, saving model to final.h5\n","Epoch 169/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8374 - val_loss: 15.6520\n","\n","Epoch 00169: loss did not improve from 2.76814\n","Epoch 170/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8390 - val_loss: 6.4920\n","\n","Epoch 00170: loss did not improve from 2.76814\n","Epoch 171/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8637 - val_loss: 2.7057\n","\n","Epoch 00171: loss did not improve from 2.76814\n","Epoch 172/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7779 - val_loss: 1.8406\n","\n","Epoch 00172: loss did not improve from 2.76814\n","Epoch 173/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8163 - val_loss: 0.4152\n","\n","Epoch 00173: loss did not improve from 2.76814\n","Epoch 174/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8160 - val_loss: 0.4619\n","\n","Epoch 00174: loss did not improve from 2.76814\n","Epoch 175/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8077 - val_loss: 5.5243\n","\n","Epoch 00175: loss did not improve from 2.76814\n","Epoch 176/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7806 - val_loss: 1.1988\n","\n","Epoch 00176: loss did not improve from 2.76814\n","Epoch 177/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8689 - val_loss: 0.0799\n","\n","Epoch 00177: loss did not improve from 2.76814\n","Epoch 178/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8512 - val_loss: 10.4276\n","\n","Epoch 00178: loss did not improve from 2.76814\n","Epoch 179/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7883 - val_loss: 6.4373\n","\n","Epoch 00179: loss did not improve from 2.76814\n","Epoch 180/350\n","500/500 [==============================] - 31s 63ms/step - loss: 2.7526 - val_loss: 0.3798\n","\n","Epoch 00180: loss improved from 2.76814 to 2.75252, saving model to final.h5\n","Epoch 181/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7705 - val_loss: 2.0032\n","\n","Epoch 00181: loss did not improve from 2.75252\n","Epoch 182/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7177 - val_loss: 2.5130\n","\n","Epoch 00182: loss improved from 2.75252 to 2.71756, saving model to final.h5\n","Epoch 183/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8220 - val_loss: 0.1430\n","\n","Epoch 00183: loss did not improve from 2.71756\n","Epoch 184/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7780 - val_loss: 10.1504\n","\n","Epoch 00184: loss did not improve from 2.71756\n","Epoch 185/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7588 - val_loss: 0.0730\n","\n","Epoch 00185: loss did not improve from 2.71756\n","Epoch 186/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7828 - val_loss: 1.6240\n","\n","Epoch 00186: loss did not improve from 2.71756\n","Epoch 187/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7587 - val_loss: 0.3577\n","\n","Epoch 00187: loss did not improve from 2.71756\n","Epoch 188/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7775 - val_loss: 1.4602\n","\n","Epoch 00188: loss did not improve from 2.71756\n","Epoch 189/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7240 - val_loss: 4.5295\n","\n","Epoch 00189: loss did not improve from 2.71756\n","Epoch 190/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8121 - val_loss: 0.1278\n","\n","Epoch 00190: loss did not improve from 2.71756\n","Epoch 191/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7565 - val_loss: 0.0995\n","\n","Epoch 00191: loss did not improve from 2.71756\n","Epoch 192/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7535 - val_loss: 12.0584\n","\n","Epoch 00192: loss did not improve from 2.71756\n","Epoch 193/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7983 - val_loss: 3.3120\n","\n","Epoch 00193: loss did not improve from 2.71756\n","Epoch 194/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7641 - val_loss: 5.0501\n","\n","Epoch 00194: loss did not improve from 2.71756\n","Epoch 195/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7692 - val_loss: 0.2425\n","\n","Epoch 00195: loss did not improve from 2.71756\n","Epoch 196/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7618 - val_loss: 0.0755\n","\n","Epoch 00196: loss did not improve from 2.71756\n","Epoch 197/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7490 - val_loss: 12.1093\n","\n","Epoch 00197: loss did not improve from 2.71756\n","Epoch 198/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7234 - val_loss: 2.7121\n","\n","Epoch 00198: loss did not improve from 2.71756\n","Epoch 199/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.6914 - val_loss: 0.0886\n","\n","Epoch 00199: loss improved from 2.71756 to 2.69142, saving model to final.h5\n","Epoch 200/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7591 - val_loss: 0.2572\n","\n","Epoch 00200: loss did not improve from 2.69142\n","Epoch 201/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7863 - val_loss: 10.6126\n","\n","Epoch 00201: loss did not improve from 2.69142\n","Epoch 202/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.8364 - val_loss: 0.1319\n","\n","Epoch 00202: loss did not improve from 2.69142\n","Epoch 203/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7910 - val_loss: 0.0578\n","\n","Epoch 00203: loss did not improve from 2.69142\n","Epoch 204/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7988 - val_loss: 1.2157\n","\n","Epoch 00204: loss did not improve from 2.69142\n","Epoch 205/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7780 - val_loss: 2.0850\n","\n","Epoch 00205: loss did not improve from 2.69142\n","Epoch 206/350\n","500/500 [==============================] - 31s 61ms/step - loss: 2.7289 - val_loss: 12.0104\n","\n","Epoch 00206: loss did not improve from 2.69142\n","Epoch 207/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7789 - val_loss: 1.8186\n","\n","Epoch 00207: loss did not improve from 2.69142\n","Epoch 208/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7519 - val_loss: 2.7691\n","\n","Epoch 00208: loss did not improve from 2.69142\n","Epoch 209/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7834 - val_loss: 8.5901\n","\n","Epoch 00209: loss did not improve from 2.69142\n","Epoch 210/350\n","500/500 [==============================] - 31s 62ms/step - loss: 2.7090 - val_loss: 0.9527\n","\n","Epoch 00210: loss did not improve from 2.69142\n","Epoch 211/350\n","308/500 [=================>............] - ETA: 11s - loss: 2.7870"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yUegq4Oj0Ka-","colab_type":"code","colab":{}},"source":["!python speedchallenge.py --mode=test  --history {nhistory}  --model final.h5  train_Flipped_for_testing.mp4 train_Flipped_for_testing.txt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RaprcWYZMSKX","colab":{}},"source":["!python speedchallenge.py --mode=test  --history {nhistory}  --model final.h5  train.mp4 train.txt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WtFFDhamD79m","colab_type":"text"},"source":["## **Predicting**"]},{"cell_type":"code","metadata":{"id":"mBlyDdhkD-y_","colab_type":"code","colab":{}},"source":["!python speedchallenge.py --mode=predict  --history {nhistory}  --model final.h5  test.mp4 outTestPrediction.txt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RiOUagC7E5Fb","colab_type":"text"},"source":["## **Making A video with predictions**"]},{"cell_type":"code","metadata":{"id":"2FrRZkZXE8Bb","colab_type":"code","colab":{}},"source":["video=r\"test.mp4\"\n","text=r\"outTestPrediction.txt\"\n","f = open(text, \"r\")\n","predictions=f.readlines()\n","predictions=list(predictions)\n","frameList=[]\n","\n","import cv2\n","import itertools\n","\n","cap = cv2.VideoCapture(video) \n","ret, frame = cap.read() \n","frameList.append(frame)     \n","while(ret): \n","      \n","    # Capture frames in the video \n","    ret, frame = cap.read() \n","    # we are ignoring the first frame\n","    # describe the type of font \n","    # to be used. \n","    \n","    # Display the resulting frame \n","    frameList.append(frame) \n","  \n","# change the length\n","prediction_count=len(predictions)\n","last_frame_index=len(frameList)-1\n","\n","frameList=frameList[last_frame_index-prediction_count+1:]\n","\n","for frame,prediction in zip(frameList,predictions):\n","    font = cv2.FONT_HERSHEY_SIMPLEX \n","    speed=str(prediction)\n","    # Use putText() method for \n","    # inserting text on video \n","    cv2.putText(frame,  \n","                speed,  \n","                (50, 50),  \n","                font, 1,  \n","                (0, 255, 255),  \n","                2,  \n","                cv2.LINE_4) \n","  \n","\n","\n","\n","\n","# release the cap object \n","cap.release() \n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","out = cv2.VideoWriter('predictions.avi',fourcc, 20.0, (640,480))\n","for i in range(len(frameList)):\n","    out.write(frameList[i])\n","out.release() \n","\n","print(\"Prediciton video generated\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GlkqeKBHWRvY","colab":{}},"source":["!rm -r *optflow*"],"execution_count":0,"outputs":[]}]}